When HTC introduced the Windowsbased smartphone it recruited 1000 TMobile or ATT customers to write product reviews and Facebook and Twitter posts reaching more than 234000 consumers and significantly increasing the brand awareness 12
Consumers trust eWOM more than advertisements as they regard their peers as more reliable than companies 65 
Although eWOM is implemented by consumers companies can initiate eWOM campaigns for marketing communications 35 
To launch an effective eWOM campaign companies need to identify a small number of disseminators known as opinion leaders who exert personal influence upon other people 68 
Identification of opinion leaders relies on the twostep flow of communications theory as senders opinion leaders cultivate their knowledge from a variety of sources including mass media in the first step and then spread their opinions messages to the general public receivers via WOM in the second step 47
However a survey may capture selfconfidence rather than opinion leadership for two reasons 6168
Opinion leader WOM has long been used to promote products or to criticize competitors offerings 4447 its positive impact on new product introduction was first reported by Arndt 3 
One of the most important capabilities of the Internet is interactive communication at a larger scale for the first time in human history individuals can make their personal thoughts reactions and opinions easily accessible to the global community of Internet users and the interactive communication provides an online feedback mechanism to serve multiple functions including brand building and customer acquisition product development and quality control and supply chain quality assurance 26
Electronic commerce performs better than the traditional market in acquiring customers 77 
To be effective in viral marketing campaigns companies must identify opinion leaders properly and then let them communicate information to their followers 43
Opinion leaders are consumers who provide information to others that influences their consumption decisions 22 by obtaining key information through research and shaping their own opinions earlier than the general public
Opinion leaders in womens fashion for example acquire fashion knowledge from fashion magazines first and then spread it to followers via WOM 70
Rogers and Cartano 68 noted that senderbased surveys are dependent upon the accuracy with which respondents senders can assess and report their selfimages on opinion leadership
The network structure method has been widely used by marketers and network analysis researchers 4143 
Since the advent of the Internet in the 1990s WOM is no longer restricted to personal influence networks because the Internet allows one to reach strangers at a larger scale 26
Godes  Mayzlin 35 adopted the King and Summers scale to measure how many followers an opinion leader reaches
Buzz is generated around a product when a large number of followers receive eWOM 1131 Previous studies suggest that opinion leaders are progressive attentionseekers 70 and fulfill their selfenhancement motivation via buzz creation 33 
Trust is an important issue in electronic commerce and eWOM studies eg 2680 as it is one of the main reasons for followers to seek advice from opinion leaders
An indirect approach to measure trustworthiness is analyzing the structural lexical and semantic aspects of eWOM which are found to be associated with trustworthiness 10 
The amount of eWOM influences consumers in two ways eWOM increases exposure to a product and therefore increases consumer awareness of its existence 54 and a large amount of eWOM suggests a products popularity 1779 
Consumers communicate their satisfaction using online user ratings 1871 Positive ratings created by the general public can improve consumer attitude while negative ratings created by the general public can worsen consumer attitude 54 
They are also motivated to contribute their knowledge to other consumers 1939
 According to Childers 22 opinion leadership is product category specific The more a consumer purchases and consumes within the same product category the more likely the consumer is to acquire complex category knowledge 
Our dataset contains a sample of 350122 book music video and DVD titles which as experience goods have qualities difficult to ascertain before consumption making user reviews helpful for consumers 6063 
The overlap between different types of opinion leaders is consistent with extant literature 43
Previous research suggests that an opinion leader needs to have both knowledge and influence 4757  
Katz and Lazarsfeld 47 argue that personal influence does not flow from highly interested individuals to less interested individuals but rather between those with shared interests
Fashion industry companies use celebrities such as Madonna as opinion leaders 76 while the pharmaceutical industry uses physicians on editorial boardsscientific committees and with prestigious academic appointments 323849 
For example the eWOM company BzzAgent measures engagement as the number of likes comments and retweets a consumers post receives 9 
We recommend that companies implement online user review systems using Amazons patented design 8 and identify the top 1 of communicative buzzgenerating and trustworthy eWOM opinion leaders among users by measuring eWOM volume feedback received and helpful votes received 
This article presents a new opinion leader identification method rooted in the interpersonal communication theory that inspired sender and receiverbased methods 47 
By demonstrating how to identify opinion leaders from a large number of consumers this article shows that companies can collect useful business intelligence from increasingly large amounts of data available to them—a central theme emerging in big data analytics 16
Wireless Mesh Sensor Networks WMSNs comprise a technological field that has arisen as the natural evolution from the conventional Wireless Sensor Networks WSNs composed of only a few nodes to largescale networks in which sources and destinations are interconnected through different paths of intermediate nodes forming a mesh layout  1
To this end new mesh capabilities such as multihop mesh routing scalability robustness reliability selforganization or energy efficiency must be satisfied  2 
This fact makes IEEE 802155 standard and its ASES mechanism advantageous with respect to other WMSN approaches  2 such as Zigbee Pro  3 the International Engineering Task Force IETF with its solution IPv6 over Low power Wireless Personal Area Networks 6LoWPAN  4 WirelessHART  5 or ISA SP10011a  6
 Second physicalspace constraints eg obstacles such as walls and pillars  3839 impose strict restrictions in the application management such as the impossibility of using relaysink mobility to improve the network performance 
Despite these known problems related to largescale agile there is an industry trend towards adopting agile methodologies inthelarge  VersionOneInc 2016 Paasivaara et al 2013 2014 Dings yr and Moe 2014
According to the latest survey  VersionOne Inc 2016 62 of the almost 4000 respondents had more than a hundred people in their software organization and 43 of all the respondents worked in development organizations where more than half of the teams were agile
However this indicates that there seems to exist a large number of companies that have taken or are taking agile into use in largescale settings  VersionOne Inc2016
In two recent workshops on largescale agile development organized in XP2013 and XP2014 conferences adoption of agile methods was one of the highlighted themes needing more research  Dings yr and Moe20132014
Traditional methods focus on upfront planning and strict management of change but agile methods were designed to accept and efficiently manage change  Highsmith and Cockburn 2001 Cockburn and Highsmith 2001
Agile methods have been both criticized and advocated and research has shown that accommodating change may be a factor in both success and failure  Boehm 2002
It has been shown that agile methods have improved satisfaction of both customers and developers but on the other hand there is evidence that agile methods may not be a good fit for large undertakings  Dyb  and Dings yr 2009
For example one paper would highlight the viewpoint of the developers  Fry and Greene 2007 and another would consider the transformation from the user experience designers point of view  Federoff and Courage 2009
Surveys on challenges and success factors for agile projects in general have been conducted eg Chow and Cao 2008
 The traditional methods for estimating PDFs eg binning methods and kernel density techniques require specification of a bandwidth parameter that heavily influences the shape of the resulting PDF Silverman 1986 Wilks 2006 Srihera and Stute 2011 
While methods exist for estimating an optimal bandwidth these methods usually require some assumption about the shape of the underlying PDF Wand and Jones 1995 Srihera and Stute 2011 Bernacchia and Pigolotti 2011 
Kernel density estimation is a widely used method for estimating the probability distribution function PDF of a given dataset eg Silverman 1986 Wilks 2006 in which the PDF is approximated as a normalized sum of kernel functions K centered on each data point j
The convolution part of this algorithm requires íªNq calculations and the FFT portion requires íªMlogM Cooley and Tukey 1965 
We also note that we implemented the selective frequency filter In in a slightly different manner than Bernacchia and Pigolotti 2011
 We use a version of CAM4 that includes the Model for Prediction Across Scales atmospheric MPASA dynamical core which predicts the evolution of the atmosphere by evaluating conservation laws eg conservation of mass momentum etc on a centroidal Voronoi tessellation of the sphere Rauscher et al 2013 Skamarock et al 2012
As every software artifact also software models  B zivin2005 are subject to continuous evolution
Knowing the operations applied between two successive versions of a model is not only crucial for helping developers to efficiently understand the models evolution  Koegel et al 2010 but it is also a major prerequisite for model management tasks such as model coevolution  Herrmannsdoerfer et al 2009 Mens2008 and model versioning  Brosch et al 2010 Koegel et al 2010
The second category comprises composite operations  Suny et al2001 consisting of a set of cohesive atomic operations which are applied within one transaction to achieve one common goal
The most prominent class of such composite operations are refactorings introduced by Opdyke 1992
As reported in Herrmannsdoerfer et al2009 Mens 2008 Brosch et al 2010 and Koegel et al2010 the detection of applied refactorings is a crucial prerequisite for automating model management tasks
However composite operations are not limited to refactorings they may be used to implement any kind of inplace model transformation for a specific purpose such as model completion  Sen et al2010 refinement  Ruhroth and Wehrheim 2012 and evolution  Meyers and Vangheluwe 2011
One way to acquire the set of applied composite operations is to use operation recording  Herrmannsdoerfer and K gel 2010 Lippe and Oosterom 1992 that is the execution of operations is tracked within the modeling environment while they are performed
A set of manually applied atomic operations having together the intent of a composite operation which is indeed frequently happening in practice  MurphyHill et al2009 cannot be identified because no explicit command has been issued in the modeling environment
In the absence of an operation log the applied operations have to be detected a posteriori using statebased model comparison approaches using either generic model comparison algorithms  Brun and Pierantonio 2008 Kelter et al 2005 Lin et al 2007 Schmidt and Gloetzner 2008 or languagespecific comparison algorithms  Kolovos 2009 Xing and Stroulia 2005
In an endeavor to establish a commonly accepted conceptual framework for the rapidly growing number of domainspecific modeling environments the Object Management Group OMG released the specification for Model Driven Architecture MDA  Object Management Group 2005 standardizing the definition and usage of metametamodels resulting in a metamodeling stack as depicted in Fig1
Hartung et al2010 present an approach for generating so called semantically enriched evolution mappings between two versions of an ontology
For limiting the search space of combinable composite operations we may use the critical pair analysis comparable to how it has been done in Mens 2006
Modern trends in manufacturing are defined by mass customization small lot sizes high variability of product types and a changing product portfolio during the lifecycle of an automated production system aPS Lder et al 2005 Rzevski 2003 
Since the proportion of system functionality that is realized by software is increasing Thramboulidis 2010 concepts for supporting automation engineers in handling this complexity are strongly required 
Software as well as software engineering in this domain need to fulfill specific requirements eg regarding realtime and reliability VogelHeuser et al 2014a 2014b 2014c 
aPS are comprised of mechanical parts electrical and electronic parts automation hardware and software all closely interwoven and thus represent a special class of mechatronic systems Bonfè and Fantuzzi 2003 Rzevski 2003 and consist of mechatronic subsystems like sensors and actuators
 Lehman 1980 defined laws of software evolution and  among others  identified that systems are subject to dynamics causing continuing changes of software resulting into increasing complexity
Hence due to their evolution and the longliving character of aPs their life is characterized as a cycle Birkhofer et al 2010 
 Software maintainability is the ease with which a software system can be modified to correct faults improve performance or other attributes or adapt to change environments IEC 1990
Accordingly maintenance can either involve repair or modification actions which in turn can be adjustments to the environment referred to as adaptive maintenance or augmentation of a systems function Avizienis et al 2004 
In the context of maintainability obsolescence management ie managing mitigating and resolving the impact of subcomponent obsolescence is one important issue regarding longliving systems ISOIEC 25010 2011
 An approach to explicitly modeling changes of aPS physical structures and for analyzing their effects on the systems functions is proposed in Göring and Fay 2013 alongwith the physical causes of these changes
According to Wiendahl et al 2007 changeability is defined as characteristics to accomplish early and foresighted adjustments of the factorys structure and processes 
Drivers for evolution are manifold Westkmper 2003 and basically result in changes of requirements on the aPS Legat et al 2013
aPS are typically designedtoorder ie they are unique systems which are designed and implemented once a customer has awarded a contract to an aPS supplier Birkhofer et al 2010
The specification and implementation is carried out in the form of a project VDIVDE 3695 Part 2 2010
The development of these partial solutions follows a typical product development workflow cp upper part of Fig 1 as described eg in VDIVDE 2206 2004 and is decoupled from the projects on the timeline 
The categorization is based on and extends the one introduced in Ladiges et al 2013 VogelHeuser et al 2014c and 2014d
 Anticipated software changes in accordance to Buckley et al 2005 are software changes which can be foreseen during the initial development of the systems and as such can be accommodated in the design decision taken 
Unanticipated software changes according to Buckley et al 2005 are not foreseen during the development phase but they are frequently undertaken at short notice during commissioning and operation in order to eg clear defects in other disciplines such as an unexpected behavior of the mechanics 
The PPU performs a manufacturing discrete process and handles stamps and sorts different kinds of workpieces Fig 3 VogelHeuser et al 2014d
Regarding Buckley et als 2005 criteria temporal change also history of change is introduced with the characteristic sequential and parallel synchronous and parallel asynchronous changes
 Accordingly the development of requirements on a production plant is an integrated part of the production design Blanchard 2004 Buede 2000 
Similarly other common development process models like the spiral model Sage and Rouse 2009 for softwareintensive systems or the waterfall model include requirements elicitation and specification as a repetitive action during engineering 
 Quality requirements also called nonfunctional requirements or extrafunctional requirements are usually more general and include among others performance requirements expressed in Key Performance Indicators KPIs see eg ISO 224002 2012 flexibility requirements reliability and availability requirements safety and security requirements and maintainability requirements ISOIEC 25010 2011
Unfortunately the adaptation of the formal specification is often omitted especially when changes need to be implemented within a short time window and during operation VogelHeuser et al 2014a
The main requirements to be considered are reliability performance efficiency compatibility maintainability and portability Frank et al 2011
General sources of requirements are usually stakeholders documents and systems in operation Pohl and Rupp 2011 
 Haubeck et al 2013 for example derive four stages in the requirement hierarchy of a single system 
Similar hierarchies can be found in the other literatures like Blanchard 2004 and IEEEStandard 1220 1999 
Formalization may be a first refinement of the informal requirements VDIVDE 3694 2008 
Formalization usually results in a set of models and the variety of used models is huge and the choice of the right model type depends on the domain industry branch as well as the specific plant to be designed VDIVDE 3681 2005
These properties can be operationalized in a way that they are measurable by the available online measurements Ladiges et al 2013 
Feldmann et al 2014b presented a light weight notational approach cp 33 to model requirements for testing and evaluated it with experts from industry showing that the modeling approach provides the means to systematically comprehensively and efficiently specify mechatronic subsystems as sensors and actuators 
To increase the quality of requirements Rösch et al 2015 and Teufl and Hackenberg 2015 introduced description of behavior based on MSCs included in the MIRA Broy and Stølen 2001 approach and UML SD
An important part of the system design is the definition of components and interfaces which in software engineering is called the architectural configuration Barais et al 2008 Medvidovic and Taylor 2000 
The complete system design covers different aspects Goldschmidt et al 2012 
 In a multiplecase embedded system case study in 5 large software companies Martini et al 2014 different factors for architectural technical debt have been identified like pressure to deliver prioritization of features over products noncomplete refactoring or technology evolution
CONSENS Anacker et al 2011  CONceptual design Specification technique for ENgineering of complex Systems  is a method and specification language that targets the overall discipline independent system design 
The MechatronicUML Heinzemann et al 2013 is a modeling language and process for the engineering of mechatronic systems 
SysML4Mechatronics Kernschmidt and VogelHeuser 2013 is a language for interdisciplinary modeling which addresses mechanical electricalelectronic and software aspects in product automation and for aPS explicitly
 Shah et al 2010 present a multidiscipline modeling framework based on SysML 
A formal modeling framework for verifying aPS engineering models has been proposed in Hackenberg et al 2014 The formal models contain the necessary aspects for verifying the systems correctness after evolution changes
ne major language for the definition of consistency constraints is the Object Constraint Language OCL Object Management Group 2014 which allows for the specification of constraints based on firstorder predicate logic 
 A rule based approach to identify structural inconsistencies during evolution of aPS is presented in Strube et al 2011
Dynamic semantics is concerned with behavior of the model Here approaches can be used for example to compare whether two finite automata are consistent by identifying whether one simulates the other or both simulate each other bisimulation Clarke et al 1999 
More complex relations called refinement notions between automata exist which can be used to define and check various degrees of consistency Baier and Katoen 2008 Heinzemann and Henkler 2011 Jensen et al 2000 Weise and Lenzkes 1997
 In the area of model transformations Bidirectional Model Transformations like JTL Cicchetti et al 2010 and Triple Graph Grammars TGG Hermann et al 2014 Hildebrandt et al 2013 Schrr 1995 have been developed to transform between models 
Transformation languages that support incremental change propagation Giese and Wagner 2009 are particularly interesting as they enable to only propagate single changes from the source ie they change only those parts of the target model which are affected by the change in the source 
 Similarly Song et al 2013 propose a compositional approach for the probability reachability analysis of Discrete Time Markov Chains that decomposes the system into strongly connected components or even parts of them analyze them using GaussJordan Elimination Althoen and McLaughlin 1987 and afterwards use value iteration to compute the result for the complete model based on the individually analyzed parts
One specific type of architectural technical debt Martini et al 2014 is noncompliance between architectural guidelines and the system architecture 
Architectural guidelines patterns and styles have been presented in Buschmann et al 1996 
 specific architectural pattern for embedded systems is the Operator Controller Module OCM Burmester et al 2008 which defines concrete layers and interfaces between them for different parts of the embedded software  feedback controllers hard realtime communication and reconfiguration of the feedback controllers and a soft realtime layer
Herold et al 2013 present an approach to automatically check the conformance of the architecture against guidelines and architectural styles 
A complementary approach has been proposed in Herold and Mair 2014which uses a metaheuristic to efficiently search for violations of architectural compliance rules and to propose a sequence of repair actions to remove the identified violations automatically
Since the proportion of system functionality that is realized by software is increasing Thramboulidis 2010 concepts for supporting automation engineers in handling this complexity are required
Reusable artefacts are mostly finegrained and have to be modified on different positions so that errors are probable and important reuse potential is wasted Maga et al 2011 
But also in traditional software engineering the main technique to derive adapted or improved implementation versions of software functionality is clone and own Ray and Kim 2012 
Code clones have been known for more than two decades as serious flaw that may impede evolution For instance Juergens et al 2009 have shown that code clones are more prone to introduce errors 
Beyond code clones a broader notion of code smells has been introduced Fowler 1999 and extended by the notion of antipatterns Brown et al 1998 
 Particularly Abbes et al 2011 have shown that the presence of two antipatterns impedes the performance of developers
While other studies show that perception of code smells may differ between developers Yamashita et al 2012 it is generally admitted that code smells hinder evolution because they impede extending and maintaining the underlying system
Design patterns such as those proposed by Gamma et al 1994 are means in classical software engineering to support code modularity and evolution 
 Amaptzoglou et al 2011 reveal that using design patterns increases reusability and thus supports the evolution of software systems 
Fuchs et al 2014 conducted a detailed analysis of IEC 611313 code from machine manufacturing industry and introduced an analysis and visualization approach
 Feldmann et al 2012 and Fuchs et al 2012 analyzed and refactored the software structure on IEC code level in an industrial case study in a world market leading plant manufacturing company
For IEC 61499based applications IEC 2011 common solutions and guidelines were proposed for hierarchical automation solutions Zoitl and Prhofer 2013 failure management Serna et al 2010 and portable automation projects Dubinin and Vyatkin 2012
Even software design patterns for IEC 61499 programs eg Distributed Application Proxy and ModelViewController were defined Christensen 2000 and evaluated Strömman et al 2005 
 As such refactoring has been established and numerous studies have demonstrated its usefulness Fanta and Raijlich 1999 Kegel and Steinmann 2008 
The interdisciplinary dependencies as well as the complexity of aPS lead to the risk of unpredictable side effects of evolution in the resulting system Jaeger et al 2011 A detection of these side effects becomes necessary and should be carried out automatically to avoid much effort as explained in Braun et al 2012
Management of aPS evolution has to deal with diverging evolution cycles of the involved disciplines Li et al 2012 
Because of the high complexity of the automation software and the plant itself it is usually not obvious how the evolution in one part of the system affects other parts or the whole process Jaeger et al 2011 
Instead of analyzing the state space of a model within a certain time frame formal techniques aim at analyzing models exhaustively with respect to all reachable states Bérard et al 2001 Clarke et al 1999
Hametner et al 2010 and Hussain and Frey 2006 identify useful diagrams for modeling and deriving test cases from UML for the field of automation software development and especially for IEC 61499 implementations
In Hussain and Frey 2006 the extraction of test sequences from state charts using roundtrip path coverage is shown
Hametner et al 2011 show a first implementation of the recommended test case generation process using state chart diagrams especially for IEC 61499 applications 
Rösch et al 2014 realize this test case generation but focus especially on testing machines reaction to faults by using fault injection
 In Kumar et al 2011 UML test case generation approaches from state charts are combined with the aim of making them executable by mapping them to the Testing and Control Notation TTCN3 
In order to integrate the requirements and test specification a template for test cases has been developed by Feldmann et al 2014b 
Ulewicz et al 2014 present a first approach for selecting and reusing existing test cases based on changes within the control program for efficient testing of changes
Nevertheless in Fuchs et al 2014 and Prhofer et al 2012 the benefits of static code analysis for IEC 611313 software quality improvement are highlighted and an approach for improving compliance to programming conventions and guidelines is proposed
The compositional approach supports multiple refinement notions to guarantee different types of system behavior Heinzemann and Henkler 2011 
Snder et al 2013 propose an approach on verifying PLC programs by means of model checking taking predefined modifications of the software into account 
Gourcuff et al 2008 presented an approach for verifying cyclically executed PLC software 
Witsch and VogelHeuser 2011 proposed an approach to implement PLC software based on UML state charts so called PLC state charts and described its behavioral semantics and application for model checking
Mertke and Frey 2001 proposed an approach on formal verification based on Petri nets by using the SPIN model checker 
Machado et al 2006 investigated the impact of applying a model of the physical plant additionally to the formal specification of controller behavior 
An integrated approach to verify conformance of aPS designs is presented in Vyatkin et al 2009 
The inherent variability in such systems results in a huge number of possible variants which may be described by a product family eg a software product line SPL Clements and Northrop 2001 Pohl et al 2005 
Feldmann et al 2012 analyzed the approaches and challenges for modularity variant and version management in aPS
For variability management from an electrical engineering viewpoint tools like EPLAN Engineering Center7 as well as the Siemens Platform COMOS8 exist and for mechanical engineering Design Structure Matrices are most popular Kortler et al 2011 
Feldmann et al 2012 showed that module structures in different disciplines differ 
Variability modeling in the problem space defines the commonality and variability of a product family in order to specify the valid configuration space ie specifying the set of valid software variants Czarnecki and Eisenecker 2000 
In the context of featureoriented domain analysis Kang et al 1990 introduce feature models for capturing the commonality and variability of variantrich systems by means of features and their interrelations
Pohl et al 2005 proposed the orthogonal variability model also for capturing the commonality and variability of variantrich software systems in a graphical way by means of variation points
Another variability modeling approach is decision modeling introduced by the Synthesis project Synthesis 1993
Model transformations are for instance applied in the common variability language Haugen et al 2008 where the variability of a base model is described by rules how modeling elements of the base model have to be substituted in order to obtain a particular product model 
Delta modeling has so far been applied to represent variability of software architectures Haber et al 2011 Java programs Schaefer 2010 and a multiperspective modeling approach for manufacturing systems Kowal et al 2014
 Elsner et al 2010 consider evolving software product lines and focus on variability in time
Schubanz et al 2013developed their own specific modeling approach which is integrated in a prototypical tool chain and focuses on a high level of abstraction ie evolution of development goals and requirements 
The approach by Dhungana et al 2010 uses model fragments for capturing the solution space variability following the principle that smaller models are better to understand
While modeldriven engineering shows an increase in effectiveness and quality and is used in the embedded software industry Liebel et al 2014 by exploiting domain knowledge it also introduces general challenges and challenges specific to aPS
Typically the models that are developed during system design are in later phases converted to source code by code generation This process is called Forward Engineering Sendall and Kster 2004 
As VogelHeuser et al 2012 showed usability of MDE approaches is strongly related to students basic skills and appropriate fade out training approach
 Furthermore in Vepsalainen et al 2010 it was identified that the modeling of userdefined control logic is required in addition to the application of predefined control blocks 
 Obermeier et al 2014 introduce a domain specific UML for aPS modAT4rMS based on plcML supporting the quality of structural modeling for the creation of new models the reuse as well as the evolution building new variants
 Duschl et al 2014 attempt to reveal the reasons behind the errors especially in module creation in the above mentioned study conducting interviews after the experiments
As a basis to describe different aspects of aPS at different hierarchical levels adaptations of UML and SysML are proposed in Bassi et al 2011 Bonfè et al 2005 and Secchi et al 2007 
The integration of MDE ie plcML being a UML dialect into a PLC programming environment is realized by Witsch and VogelHeuser 2011 and available for state chart and class diagrams in CODESYS V39
Code generation from plcML to Siemens S710 platform has been introduced in Tikhonov et al 2014 For hybrid models combining closed loop control and interlocking Bayrak et al 2012 and Schneider et al 2014
Trace links can also be created using informationretrieval techniques ClelandHuang et al 2007 The authors give an overview on two techniques for automatic generation of trace link candidates from requirements and other system artifacts based on probabilistic network models Antoniol et al 2002 and vector space models ClelandHuang et al 2005 
Berkovich et al 2011 investigate tracing on interdisciplinary ProductService Systems PSS eg mechatronic products including services 
 Wolfenstetter et al 2015 analyzed and evaluated different traceability techniques from a requirements engineering perspective regarding ten criteria eg variability and configuration management version management simultaneous development of different views with the result provides sufficient support
In a Banach space setting the source inequality 30 has already been used in 3650 to derive convergence rates for Tikhonov regularization with convex functionals and in 34 for multiparameter regularization 
In the case that the operator F is linear and R is convex 2 and 3 are basically equivalent if is chosen according to Morozov discrepancy principle see 39 Chap3
While the theory of Tikhonov regularization has received much attention in the literature the same cannot be said about the residual method
In 2150 where convergence and stability of Tikhonov regularization have been investigated the stability results are of the following form for every sequence ykk N y and every sequence of minimizers xk argmin Fxyk 2 Rx there exists a subsequence of xkk N that converges to a minimizer of Fxy 2 Rx
In this paper we prove similar results for the residual method but with a different notation using a type of convergence of sets see for example 41 Section 29
If X satisfies the first axiom of countability then x Lim sup k k if and only if there exists a subsequence  kjj N of  kk N and a sequence of elements xj kj such that x j x see 41 Section 29IV
From Proposition 43 we now obtain that xkk N weakly converges to x and xk pp x pp Thus in fact the sequence xkk N strongly converges to x see 44 Corrollory 5219
This rcoercivity has already been applied in 2 for the minimization of Tikhonov functionals in Banach spaces
The spaces X  L p   for p 1 2 and some finite measure space    are examples of 2convex Banach spaces see 42 p81 Remarks following Theorem 1f1
In a finite dimensional setting with p  1 the minimization problem 40 has received a lot of attention during the last years under the name of compressed sensing see 781016 18202657
however for many LDSs the parameters are unknown and must be estimated in a process often called system identification17
Reducedrank LDS models can partially address this problem by reducing the number of latent states 7
An alternative is to use subspace identification methods such as N4SID and PCAID which give asymptotically unbiased closedform solutions 827
The Amari error 2 which is another permutationinvariant measure of similarity is also provided
Mohammad et al have discussed the impact of autocorrelation on functional connectivity which also provides some direction for extension 3
Diversifiersare people whose passion is to explore details They are in love with the heterogeneity of nature They are happy if they leave the universe a little more complicated than they found it1 chap 3 p 44
Dysons treatment of this is relatively short at just one 18page chapter 1 It is therefore important to moderate our contemporary biases if we are to understand what he intended
The field was spawned from the dreams of Artificial Intelligence though the reality has encompassed a far broader scope of study than originally envisioned at the Dartmouth Conference 2
Therefore my success as a man of science whatever this may have amounted to has been determined as far as I can judge by complex and diversified mental qualities and conditions 
The first academic city in the world was Athens and the first industrial city was Manchester so I like to use the names of Athens and Manchester as symbols of the two styles of scientific thinking1 p 37
Science belongs to both worlds but the style of academic science is different from the style of industrial science The science of the academic world tends to be dominated by unifiers while the science of the industrial world tends to be dominated by diversifiers1 p 36
The Manchester exemplar is particularly illuminating in this respect given a deeper look at its historical context Manchester situated in the North of England was the birthplace of the Industrial Revolution and the growth of its intellectual capital is well documented by Thackray 4
The driving force of the Manchester scientific renaissance were not technological and utilitarian they were cultural and aesthetic1 p 38
In later a communication he states that only roughly speaking unifiers are following the tradition of Descartes diversifiers are following the tradition of Bacon7
A controversial idea of the past century in philosophy has been the distinction between analytic and synthetic statements 8 
Kant 8 referred to analytic statements as clarifying or explicating our knowledge or in other words making explicit what was once implicit 
 A modern perspective on this divide is given by Jogalekar 9
In another communication 11 he recounts his discussions on Quantum ElectroDynamics with Richard P Feynman who was apparently obsessed with finding a unifying theory of the large gravity and small nuclear forces
a beautiful or elegant theory is more likely to be right than a theory that is inelegant12
The only instance even vaguely like this that I know of is the Boosting family of algorithms—the existence of which was predicted by studies in computational complexity theory 14 and discovered later by Schapire 15 
Though this article is focused on Pattern Recognition it is fair to note that similarly vague work appears in related communities 18 
Kuhn 20 presents a treatise on the nature and reasons behind revolutions in scientific understanding
Whilst we may never have a truly unified theory of inference there are a number of technical elements of our field which could benefit from a little unification in classic papers Breiman 22 and Langley 17 present ideas along this line 
Breiman 22 discusses two cultures of statistical modelling data modelling versus algorithmic modelling
Langley 17 wrote a striking editorial for an issue of Machine Learning Journal on the topic of unifying machine learning as it stood in the late 1980s
Essentialism is the view that entities in the world have inherent essential and immutable properties by which they can be described Pelillo and Scantamburlo 25 discuss how dissimilarity measures in PR sit in direct opposition to this philosophical view in that an entity is best described by its relation similarities to other entities
 It could be argued that such devices are at prototype stage already with neuromorphic computing 26
These include a fuzzy set based algorithm 3 that allows the detection of healthy brain substructures with a precision of 95 measured with the Dice similarity coefficient DSC 4
Another popular method of healthy brain segmentation based on active contours 6 results in an Fmeasure of 90
 Examples of such methods include template moderate classification proposed by Kaus et al 7 a method based on fluid vector flow by Lee et al 8 or a generative model based method proposed by Prastawa et al 9 
The diffusion of water molecules depends mostly on the temperature and tissue structure and is independent of patient gender and age 1419
DWI allows for the quantitative measurement of water molecules activity in clinical practice known as the apparent diffusion coefficient value ADC 1820
The DWI image translation into an ADC map was performed with the MITK framework 28 with the original DWIs additionally filtered with the nonlocal means filter 2930
 To efficiently filter the skull signal the T2 FLAIR sequence together with the FSL brain extraction tool FSLBET 31 were used
To neglect the problem of cerebrospinal fluid CSF distortion OTSU based CSF filtration routines 32 were applied giving a binary CSF mask
In the second step of MiMSeg algorithm kmeans clustering 36 was applied to find the groups of GMM components similar in mean value variance and weight
The clustering procedure was repeated for number of clusters ranging from 1 to 10 with intial conditions set according to 37 The optimal number of clusters was determined by Dunns cluster consistency criterion 38 
a neural network based technique named SelfOrganizingMap SOM 39 methods proposed by Murakami et al 40 and by Kang et al 41
The unconstrained version of classical GMM technique was used in a comparison study 42
According to the WHO the family of astrocytomas tumours a subgroup of gliomas originated from abnormal astrocytic cells is divided into the following four groups with respect to the increasing malignancy 44
However the absence of enhancement does not necessarily imply a histopathologic diagnosis of lowgrade tumor onethird of nonenhancing diffuse gliomas in adults are highgrade tumors 45
In addition ADC changes due to the presence of cystic necrotic andor hemorrhagic areas and the influence of artifacts caused by inhomogeneous structures such as the skull base bone and sinus air must be considered 48
To avoid the influence of susceptibility artifacts or ADC changes regions with infratentorial components or gross hemorrhage should be excluded from measurement or during data analysis by applying robust and efficient preprocessing techniques 32
Android is the dominant operating system for mobile devices it currently has the largest installed base  IDC2013 mainly because a it supports a huge variety of different devices such as watches tablets TV sets etc and b it provides endusers with a large variety of applications aka
We have performed a complete isolated evaluation of the policy evaluation delay and scalability issues in a previous publication by some of the coauthors  Neisse et al2015
According to Callaham 2014 users have an average of 95 apps installed in their Android phones and according to Au et al2012 there are around 750 API methods associated with the available permissions in Android version 431
This average number of apps and possible API methods would be equivalent to around 71 thousand policies which according to the PDP evaluation results published in Neisse et al 2015 would correspond to a response time of around 100 ms
Kirin proposed by Enck et al2009 is a security service running on the phone which analyses the requested permissions of an app and detects potential security flaws
Batyuk et al2011 introduced Androlyzer a server based solution that focuses mainly on informing users about apps potential security and privacy risks
Complementary AppFence proposed by Hornyack et al2011  also implemented as a modified OS on the basis of TaintDroid shadows and exfiltrates users private data according to their preferences
SEDalvik introduced by Bousquet et al 2013 proposes a MAC mechanism to regulate information flows between apps objects building on the advantages of Dalvik internal debugger
AppGuard introduced by Backes et al 2013 is an app instrumentation framework that runs directly in users device and allows usercentric security policies customisations
We also plan to launch a communitybased release of our tool where users can contribute with abstract policies and refinement models of privacy sensitive activities for example integrating the results of other Android security approaches eg Rasthofer et al2014 that cannot be easily reused at the moment
 The objective of PAP is more realistic than seeking an approach that performs the best for all problem instances since such an approach may not even exists because of the nofreelunch theorem 26
 In case that the number of candidate EAs is huge and direct enumeration becomes prohibitive Eq 4 can still be integrated with some existing search method say forward selection 12 to yield at least a suboptimal set of constituent algorithms within an acceptable time period
Despite the promising preliminary results both empirical and theoretical analysis revealed that the performance of PAP is sensitive to its constituent EAs 17
Although the accuracy of such an estimate is by no means guaranteed it has been commonly employed in many other scenarios eg finetuning the parameters of an EA 4 where the performance of an EA needs to be estimated
In case that the number of candidate EAs is huge and direct enumeration becomes prohibitive Eq 4 can still be integrated with some existing search method say forward selection 12 to yield at least a suboptimal set of constituent algorithms within an acceptable time period
In the context of ensemble learning the term different is referred to as diversity and a lot of studies have been carried out to investigate how diversity can be quantitatively defined and be utilized to construct good ensembles 22
Neural networks NN are mathematical representations modelled on the functionality of the human brain Bishop 1995 
For example in Yobas Crook and Ross 2000 the authors found that linear discriminant analysis LDA outperformed neural networks in the prediction of loan default whereas in Desai Crook and Overstreet 1996 neural networks were reported to actually perform significantly better than LDA
The logistic regression model then takes the form 1logit  log 1   Txwhere is the intercept parameter and T contains the variable coefficients  Hosmer  Stanley 2000
The least square support vector machine LSSVM proposed by Suykens Van Gestel De Brabanter De Moor and Vandewalle 2002 is a further adaptation of Vapnik original SVM formulation which leads to solving linear KKT Karush Kuhn Tucker systems rather than a more complex quadratic programing problem
The Nemenyi post hoc test states that the performances of two or more classifiers are significantly different if their average ranks differ by at least the critical difference CD given by 12CDq  KKK112DIn this formula the value q   K is based on the studentised range statistic  Nemenyi 1963
Note that even though the differences between the classifiers are small it is important to note that in a credit scoring context an increase in the discrimination ability of even a fraction of a percent may translate into significant future savings  Henley  Hand 1997
The analysis of texture images has played a fundamental role in computer vision and pattern recognition during the last decades and the importance of this area of study is illustrated by the number of applications appearing in diverse areas such as Engineering 12 Medicine 1 and Physics 6
Examples are the theory of textons 17 local patterns 14 local affine regions 11 invariants of scattering transforms 15 Fast Features Invariant to Rotation and Scale of Texture 16 and others 3
In simple terms it is a generalisation to the threedimensional Euclidean space of the local connected fractal dimension of binary images previously reported in 1018
The parameter rmax was set to 5 in all tests based on the observation in 5 that larger radii severely increase the computational cost and number of descriptors without a significant gain in the classification performance
In order to improve the diversity of the search in the PSO various kinds of models have been investigated 419
Moreover in 17 Tatsumi et al presented a sufficient condition for chaoticity of the system used in CPSOVQO and showed that the parameter values in the system can be selected by utilizing its bifurcation diagram
 Moreoveranother popular method based on the same ideawhich is called the selforganizing hierarchical PSO with timevarying acceleration coefficientsHPSOTVAC13was proposed
Besidesthere is another method of generating a chaotic sequence the gradient model with sinusoidal perturbations GP 16
Recently a combination of the PSO and GP methods CPSOVQOwas proposed 17
The chaoticity of system C1 is guaranteed by the property that a system derived by using the steepest descent method with a perturbation can be chaotic 16 and it is summarized
It is wellknown that there are an infinite number of orbits which are repelled by the snapback repeller but which are attracted to its neighborhood 12
Thenwe have the following theorem with respect to chaos in the sense of LiYorke9
In this subsection we show the results of numerical experiments in which the standard PSOPSO the four existing improved PSOsPSOIWACEPSOA1CPSOVQO17andHPSOTVAC13and the five proposed PSOsPSOTPCPSOSPCPSOSDPCIPSOSPCandIPSOSDPCwere applied to the following 50200and 400dimensional benchmark problems
For the standard PSOwe used wc1c2072914941494as suggested in5 
These values were also used for the standard updating system SPused in CPSOVQO PSOTPC PSOSPC and PSOSDPC For HPSOTVAC we set cucl 2505 based on the paper 13
For CPSOVQO we used wdαβmrmindF0903500453040400as shown in 17where the width of the rectangular feasible region of each problem in Table1 is scaled into the same constant dF
In the SADE the parameters F and CR were randomly selected from 0110 and 01 respectively and we used DErand1bin strategywhich was based in the paper 2
The users are malware analysts domain experts whose main tasks are to select different rules categorize them by their task and store them in the database as well as manual adaption andor tuning of found rules  Wagner et al2014
In a preliminary study we found that malware analysts preferred visualization concepts containing multiple views and arcdiagrams or word trees  Wagner et al2014
By externalizing and storing of the experts implicit knowledge  Chen et al 2009 it can be made systeminternally available as computerized knowledge to support further analysis or other analysts
Other than that Pretorius and Van Wijk2009 point out that it is important for visualization designers to ask themselves What does the user want to see and What do the data want to be as well as how these two points mutually enhance one another
Goodall et al 2004 conducted contextual interviews to gain a better understanding of the intrusion detection workflow and proposed a threephased model in which tasks could be decoupled by necessary knowhow to provide more flexibility for organizations in training new analysts
Conti 2007 dedicated a part of his book to visualization for malware detection but focused on the network level
On the one hand there are systems for Individual Malware Analysis which support the analysis of a single malware sample to gain new insights eg Donahue et al 2013 W chner et al2014
Additionally Gove et al 2014 introduced SEEM  which allows the behavioral comparison of a large set of malware samples in relation to the imported DLLs and callback domains
Some problemoriented projects eg Mistelbauer et al2012 and general frameworks eg Tominski2011 integrate explicit knowledge in visualization but not in a form similar to malware behavior patterns
Therefore we set up a design study project to find a visualization solution that followed a usercentered design process  Sharp et al2007
Furthermore the graphical summary is based on the Gestalt principle of similarity  Johnson2014 in order to support the analyst in quickly recognizing related call combinations
It lasted one hour on average and encompassed five analysis tasks the system usability scale questionnaire SUS  Brooke1996 and a semistructured interview
The SUS is a standardized technologyindependent questionnaire to evaluate the usability of a system  Brooke1996
These comments were also apparent concerning SUS question ten I needed to learn a lot of things before I could get going with this system  Brooke1996 with a score of 50
Categorization of KAMAS If we categorize KAMAS along the Malware Visualization Taxonomy  Wagner et al2015 we can see that KAMAS can be categorized on the one hand as a Malware Forensics tool which regards to the analysis of malicious software execution traces
On a general level the workflow for knowledge generation and extraction is mostly similar and always includes the system user as an integral part of the loop  Endert et al2014
It can be defined as the minimal total cost of a sequence of elementary edit operations to transform one sequence into the other for a se quence x of length m and a sequence y of length n  it can be com puted in time Omn  8 
In 21  the authors show that computing the edit distance can be used to classify handwritten digits where the contours of the digits are represented with an 8direction chaincode 11  a sequence over an eightletter alphabet representing the eight cardinal directions that the contour faces when following the outline of an image in a clockwise motion
Example applications where image retrieval is required include digital libraries and multimedia editing 28 
Few exact algorithms exist which are able to compute the cyclic edit distance between x and y  Maes designed an elegant divide andconquer algorithm which runs in time Omn log m  18 
Several heuristic approaches exist for approximating the cyclic edit distance One of the first ones is the Bunke and Buhler  BBA  algorithm 6 
The ex tended Bunke and Buhler method  EBBA  computes an estimation of the upper bound for the exact cyclic edit distance also in time Omn  22 
Informally the q gram similarity de fined as a distance in 29  is the number of q grams shared by the two sequences
The rotation of x that minimises a generalisation of the q gram distance between x and y is computed using the algorithm in 13
We begin with a few definitions following Crochemore et al 8  We think of a string x of length m as an array x 0  m 1  where every x  i  0 i  m  is a letter drawn from some fixed alphabet  of size     O1  We refer to any string x ∈  q as a qgram 
In many applications we only want to count the number of edit operations considering the cost of each to be 1 17  This distance is known as Levenshtein distance  a special case of edit distance where unit costs apply
The cyclic edit distance  denoted by CE  x y  is defined as CE x y   min i  min j E x i  y j   min i E x i  y  18 
We give some further definitions following Ukkonen 29  The qgram profile of a string x is the vector G q  x  where q  0 and G q  x  v  denotes the total number of occurrences of q gram v ∈  q in x 
Jokinen and Ukkonen 15 showed the following bound which is directly applicable to the Levenshtein distance Lemma 1 15  Let x and y be strings with Levenshtein distance k Then at least  x   1 k  1 q of the  x  q  1 qgrams of x occur in y
For a given integer parameter β 1 Grossi et al 13 defined a generalisation of the q gram distance by partitioning x and y in βblocks as evenly as possible and computing the q gram distance between each pair of blocks one from x and one from y  The ratio nale is to enforce locality in the resulting overall distance
The algorithm is based on constructing the suffix array 19 forstringxxyandas signing a rank to the prefix with length q of each suffix with length at least q  based on its order in the suffix array
Myers bitvector algorithm is used to compute the edit distance when using unit costs for insertion deletion and substitution 24 
Myers bitvector algorithm was implemented using the SeqAn library 9 
DNA datasets were simulated using INDELible 10  which produces sequences in a MultiFASTA file
Handwritten digits from the MNIST database 16 were also used and sorted into ten sets Each image was placed in one of ten datasets depending on the value of the drawn digit
Normalising the chaincode allows the image to be treated as a circular sequence of minimum magnitude This produces a sequence independent of the rotation of the image This was calculated by identifying the number of direction changes between two adjacent elements of the chaincode in an anticlockwise direction see 12  for details
Assuming a stationary camera which is a reasonable constraint for most applications using scene specific information can help to reduce the number of false alarms eg Hoiem et al 2006
To further improve the classification power and to further reduce the number of required training samples an adaptive classifier using an online learning algorithm can be applied Nair and Clark 2004 Javed et al 2005 Wu and Nevatia 2007a
The complexity object detection can be even further reduced by using classifier grids The main idea of classifier grids Grabner et al 2007 Roth et al 2009 is to exploit the prior knowledge that the camera is fixed
To overcome this problem at time t fixed updates Grabner et al 2007 can be applied for updating a classifier Cit1 Given a set of representative positive hand labeled
The second problem was addressed in Stalder et al 2009 and in Sternig et al 2010b Stalder et al 2009 introduced contextbased classifier grids to extract additional positive information from a specific scene
Multiple instance learning MIL was first introduced by Dietterich et al 1997
Most of these approaches are based on popular supervised learning algorithms such as SVM Andrews et al 2003 or boosting Viola et al 2005 that are adapted in order to incorporate the MIL constraints
Building on Roth et al 2009 the model describing the object class the positive model is precalculated by offline boosting for feature selection and only negative updates are performed
However in our case we apply the approximated median background model McFarlane and Schofield 1995
 We first give an illustrative comparison between the original grid approach eg Roth et al 2009 and the proposed method
We compare our approach to other stateoftheart person detectors namely the deformable part model Felzenszwalb et al 2008 FS and the Histograms of Oriented Gradients approach Dalal and Triggs 2005 DT
In addition we compared our method to the classifier grid CG approach Roth et al 2009
Finally we want to demonstrate that the longterm stability which was already shown for the original classifier grid approach in Roth et al 2009 also holds for the IMIL extension
In particular as in Roth et al 2009 we kept the positive representation fixed and generated a bag of negative samples from an estimated background model
In the European Union the population share of persons older than 60 was 17 percent in 1980 and increased to 22 percent in 20045 it is expected to reach 32 percent in 2030 Life expectancy of men women has risen from 68 76 years to 74 80 years during the same time period European Commission 2007
In addition many elderly people prefer to grow old in the privacy of their homes rather than in a nursing home On the other hand willingness for informal care by relatives is decreasing This is partly due to the fact that women and men are both working Tarricone  Tsouros 2008
Therefore organizations providing home care services are inclined to optimize their activities in order to meet the constantly increasing demand for home care Koeleman Bhulai  van Meersbergen 2012
The latter aspect concerns eg penalties for deviations from preferred visit times or from the set of preferred nurses Trautsamwieser et al 2011 consider seven different terms in the objective function and Hiermann et al 2015 consider as many as 13 see Table 1 column # OF terms
Besides the daily routing and scheduling problem authors have also addressed the long term problem Nickel Schröder and Steeg 2012 look at weekly schedules and link them to the operational planning problem
Weekly home care scheduling problems are also addressed in eg Borsani Matta Beschi and Sommaruga 2006 Gamst and Jensen 2012 Cappanera and Scutellà 2013 Maya Duque Castro Sörensen and Goos 2015 and Trautsamwieser and Hirsch 2014 while Nowak Hewitt and Nataraj 2013 investigate planning horizons of two to three months anticipating future requests
Successful implementations of home health care scheduling tools are described eg in Eveborn et al 2006 2009 or Begur et al 1997
An overview of home care routing and scheduling and related problems can be found in CastilloSalazar LandaSilva and Qu 2015 More information on home care worker scheduling is provided in the survey by Gutiérrez Gutiérrez and Vidal 2013 and on personnel scheduling in general by Van den Bergh Belin De Bruecker Demeulemeester and De Boeck 2013 and De Bruecker Van den Bergh Belin and Demeulemeester 2015
We then design a metaheuristic solution framework that is based on multidirectional local search Tricoire 2012 to solve instances of realistic size see Section 4
Using the notation of Vidal Crainic Gendreau and Prins 2015 the scheduling problem may be described as in 43
For a detailed description of these concepts and their underlying principles the reader is referred to Ehrgott and Gandibleux 2002 2004 and Ehrgott 2005
The algorithm is based on the multidirectional local search framework Tricoire 2012 and uses large neighborhood search LNS as a subheuristic
Multidirectional local search MDLS is a recently proposed metaheuristic framework for multiobjective optimization problems Tricoire 2012
Large neighborhood search LNS is a metaheuristic which was first introduced by Shaw 1998 
For details and an overview of recent developments on LNS the reader is referred to Pisinger and Ropke 2010
The structure of our MDLS algorithm is very similar to the structure described by Tricoire 2012 although in our case a singleobjective local search procedure may result in more than one new solution as is discussed in Section 42
Several standard removal and insertion operators from the LNS literature Pisinger  Ropke 2007 Ropke  Pisinger 2006 Shaw 1998 are adapted to the specific problem context of the BIHCRSP
The implementation of the operator is the same as in Shaw 1998 
The minimal overtime cost is used instead of the actual overtime cost as the latter would require resolving the scheduling problem when removing a job while the former may be calculated in constant time Vidal et al 2015
As in Shaw 1998 and Ropke and Pisinger 2006 a parameter P   1 is used in all worst removal operators to introduce some randomness in the selection of jobs thereby avoiding the same jobs to be removed over and over again
For a detailed discussion of these operators the reader is referred to Ropke and Pisinger 2006 and Pisinger and Ropke 2007
To diversify the search a noise term may be added to the objective functions of the insertion heuristics Pisinger  Ropke 2007 Ropke  Pisinger 2006
However the parameter values that have been applied are based on both reallife data of two Viennese companies and reallifebased benchmark data for a related problem Hiermann et al 2015
The first three types are based on the travel time matrices for car and public transportation provided by Hiermann et al 2015 and are generated using OpenStreetMap
Several quality indicators have been proposed in the literature to evaluate approximations of the Pareto frontier generated by heuristic solution procedures Knowles Thiele  Zitzler 2006 Zitzler Thiele Laumanns Fonseca  Grunert da Fonseca 2003
The hypervolume indicator IHA introduced by Zitzler and Thiele 1999 measures the portion of the objective space that is weakly dominated by an approximation set A
Technical debt TD is a metaphor used to describe a situation in software development where a shortcut or workaround is used in a technical decision Kruchten et al 2012b
TD has also similarities to three aspects of financial debt repayment interest and in some cases high cost Allman 2012
In software development a shortcut or workaround can give the company a benefit in the short term with quicker release to the customer and an advantage in timetomarket over the competition Kruchten et al 2012a YliHuumo et al 2015a 
However if these shortcuts and workarounds are not repaid TD can accumulate and hurt the overall quality of the software and the productivity of the development team in the long term Zazworka et al 2011b
However according to a recent mapping study the problem is the lack of empirical evidence about TDM in a reallife software development environment Li et al 2015a
For data collection and analysis we used the eight TDM activities identified by Li et al 2015a in semistructured interviews to gather empirical data about TDM in the selected software development teams
We used the exploratory case study method Robson 2002 to answer the following main research question
Technical debt management can be separated into the following activities identification measurement prioritization prevention monitoring repayment representationdocumentation and communication Li et al 2015a 
There are a number of possible methods models practices or tools for every TDM activity Li et al 2015a They have been developed and suggested in the literature but they lack empirical evidence of their usability and functionality ibid 
Some software development teams may use more time on TDM while some development teams may not pay much attention to it Power 2013 
The TD metaphor was first associated with compromises on the code level of software Cunningham 1992 
In addition terms like code smells Fowler et al 1999 have described situations where poor technical choices in software development have caused problems in code quality and architectural soundness
 However the TD metaphor has been rapidly expanded after the initial concept on the code level and it has been associated with other stages of the software development lifecycle as well Tom et al 2013 Alves et al 2014
The current literature identifies such terms as requirements Brown et al 2010 design Zazworka et al 2011b Zazworka et al 2011a architectural Nord et al 2012 test Brown et al 2010 process Lim et al 2012 documentation Kruchten et al 2012a and people debt Kruchten et al 2012b to demonstrate the same effect of shortcuts or workarounds happening in the other stages of the software development lifecycle
Shortcuts and workarounds in software development usually happen for intentional reasons such as for business deadlines and development complexity YliHuumo et al 2015a
Therefore strict deadlines may sometimes force the development team to create solutions with secondtier quality to meet the requirements within the deadlines set by the business stakeholders YliHuumo et al 2014
TD can also occur unintentionally McConnell 2007 The reason for unintentional TD can be lack of competence a need to upgrade existing technologies or a customer or market induced need for change
When taking TD companies are able to speed up the release cycles to the customer which can increase customer satisfaction and provide advantage in the market Another benefit for companies is customer feedback YliHuumo et al 2015b 
According to some scholars TD should be associated only with intentional decisions happening in the code base and messy code should not be counted as TD while some think that old technologies in legacy software should also be counted as TD Norton 2009 Fowler 2009
Even though concepts such as social debt Tamburri et al 2013 and people debt Alves et al 2014 describe similar phenomena of having shortcuts and nonoptimal solutions in software development and organization we believe that they should be categorized as sources for TD rather than as actual TD
 Li et al 2015b have also developed similar TD list management for architectural technical debt ATD
 Implementing coding standards to the development process can prevent TD when the developers have a cohesive way to produce a similar style code which makes it readable and modifiable Green and Ledgard 2011
Code reviews can be used to check other developers solutions before the release to catch possible TD issues in the design Mantyla and Lassenius 2009 
 A similar approach has also been used by other researchers to identify and document TD issues in order to make TD easier to manage Zazworka et al 2013
Klinger et al 2011 interviewed four experienced software architects to understand how decisionmaking regarding TD was conducted in an enterprise environment 
In addition an interpretive case study aims at understanding phenomena through the participants interpretation of their context Runeson and Höst 2008
We decided to use semistructured interviews Charmaz 2014 for data collection which makes this research a flexible study Runeson and Höst 2008 
The analysis of causes and effects of TD is available as a separate publication by YliHuumo et al 2014 
In exploratory case studies the technique for the analysis of qualitative data is hypothesis generation Seaman 1999 
The data coding and analysis were completed in various steps guided by the work of Robson 2002 
Similar practices identified in a study by Codabux and Williams 2013 were reengineering and repackaging
 A set of other practices for TD prevention have been identified in other studies Codabux et al 2014 Krishna and Basu 2012 
Developers may also value documentation differently and they document only issues that they personally think are important Lethbridge et al 2003 
The literature has suggested approaches for TD prioritization Eisenberg 2012 Seaman et al 2012 Theodoropoulos et al 2011 Zazworka et al 2011a 
 Prioritization can also be based on customer needs but this can leave the most important TD from the technical perspective out of sight Codabux and Williams 2013 These prioritization issues exist also in requirements prioritization Lehtola and Kauppinen 2006
When a development team considers for example the criteria in SonarQube 2015 as TD in can guide TD management and other TD activities
The goal of TDM is to provide practices and tools to manage and reduce TD during software development Li et al 2015a 
External validity is concerned with ‘to what extent it is possible to generalize the findings and to what extent the findings are of interest to other people outside the investigated case Runeson and Höst 2008 p 154 
On the basis of our findings we believe that TDM in software development has similarities to the characteristics of the capability maturity model CMM Paulk et al 1993 
This kind of maturity as a concept has been applied to other processes and domains as well De Bruin et al 2005
 Chen 7 used fuzzy numbers to determine the fuzzy reliability of the two systems whereas Singer 12 used LRtype fuzzy numbers to consider the fuzzy reliability problem
Therefore the triangular fuzzy numbers used by Chen 7 are R AR B0008880020031121R CR D0755520800844481R E0944341001055661R FR G0047220050052781R H0009440010105601 Using Definition 8 we defuzzified R X1 and obtained the estimated reliability of the system in the fuzzy sense as dR X10 0137755
C Comparison of this study with that of Cheng and Mon 6Cheng and Mon 6 considered the following
 No model is available which is valid for a wider range of flow situations 78 
 In addition the vortex number distributions should be used to derive expressions relating to the fractional rate of surface renewal and mass transfer coefficients across gasliquid and solidliquid interfaces 9 
These criteria includes the isosurfaces of vorticity stream lines Helicity Qcriterion complex eigenvalues of the velocity gradient tensor  2 swirl strength and pressure minimum 21 
The applications are of a wide range and significance including gesture recognition and human computer interaction  Bilalet al 2012 Radlak and Smolka 2012 Nalepa et al 2014 objectionable content filtering  Lee et al2007 image retrieval  Kruppa et al 2002 image coding using regions of interest  Chen et al 2003 and many more
An approach introduced by Hsu et al 2002 takes advantage of common skin color properties in nonlinearly transformed YCbCr color space using an elliptical skin color model
Skin color can be modeled using a number of techniques including the Gaussian mixture model  Greenspan et al 2001 and the Bayesian classifier  Jones and Rehg 2002
An approach for adapting the segmentation threshold in the probability map based on the assumption that a skin region is coherent and should have homogenous textural features was introduced by Phung et al 2003
Analysis of facial regions for effective adaptation of the skin model to local conditions was investigated by Fritsch et al2002 Stern and Efros 2002 Kawulok 2008 Kawulok et al 2013 and Yogarajah et al 2012
Simple textural features were used to boost the performance of a number of skin detection techniques and classifiers including the ANN  Taqa and Jalab2010 nonparametric density estimation of skin and nonskin classes  Zafarifar et al 2010  Gaussian mixture models  Ng and Pun 2011 and many more  Forsyth and Fleck1999 Conci et al 2008 Fotouhi et al 2009 Abin et al2009
A number of skin segmentation techniques emerged based on this observation Kruppa et al 2002 assumed that the skin blobs are of an elliptical shape a threshold hysteresis was applied by Argyros and Lourakis 2004 and recently by Baltzakis et al 2012
In the case of the DSA  del Solar and Verschae 2004 the ROC curves were obtained by applying different values of the diffusion threshold   explained earlier in Section 2
In order to provide a thorough comparison we also investigated how the waveletbased method behaves when the textural features are extracted from the skin probability maps rather than from the luminance as proposed in the original work of Jiang et al 2007
The analysis of skin probability map domain for skin segmentation using a controlled diffusion was proposed by del Solar and Verschae 2004
Although the colorbased skin models can be efficiently adapted to a given image it was proved by Zhu et al    2004  that it is hardly possible to separate skin from nonskin pixels using such approaches
Conditional random fields were used by Chenaoua and Bouridane 2006 to exploit spatial properties of skin regions
An approach based on the cellular automata for determining skin regions was proposed by Abin et al 2009
Although empirical experiments have shown that it is practically usable on a wide range of programs  Fraser and Arcuri 2012 Genetic Algorithms are a global search technique which tend to induce macrochanges on the test suite
For example to generate tests for specific branches to achieve branch coverage of a program a common fitness function  McMinn 2004 integrates the approach level number of unsatisfied control dependencies and the branch distance estimation of how close a branching condition is to being evaluated as desired
However there remain several different parameters  ElMihoub et al 2006 How often to apply the individual learning 
As the problem is too complex to perform a theoretical runtime analysis eg such as that presented by Arcuri 2009 we therefore aim to empirically answer the following research questions
This is different from the result of our initial experiment  Fraser et al2013 where the best configuration used seeding small population size five individuals low rate of local search every 75 generations and a small budget of five fitness evaluations for local search
A particular aspect of this reallife unbiased sample of classes is that the problems it represents are quite different to those considered as difficult search problems  Fraser and Arcuri2012 for example a large share of the classes have environmental dependencies that make high coverage with EvoSuite impossible
Methods by which such OSINT data may be used to increase effectiveness in this manner include but are not limited to selection of vulnerable personalities inclusion of ploys personally attractive to the target and impersonation of a person in authority  Huber et al2009
The nearest approximation we are aware of is Scheelen et al2012 who investigated a single company by connecting with followers on LinkedIn where the social media structure is based around employment
Irani et al 2009 suggest that a recordmatching approach to the problem can be fruitful with identifiers like last name birth year and country unlikely to change across records
Working with a wider range of features Malhotra et al 2012 design an ensemble classifier with subclassifiers relying on individual features such as profile pictures and usernames
Goga et al2013 exploit innocuous social media profile information such as timestamps geographical location and writing styles to match user profiles demonstrating that even where usernames and other traditional identifiers are disguised users can still be identified based on their usage of the media
The general identity resolution approach of pairwise comparison is supported even more broadly by similar approaches in other domains of security analytics  Zhang et al 2016
Each interviewee held internationally recognised certification in the domain area reaching a minimum level of CREST CRT  allowing level of expertise to be compared and verified to international standards  Knowleset al2016
Furthermore Kotson and Schulz 2015 and Dewan et al 2014 propose that organisations could use natural language processing techniques to maintain awareness of their online footprint to be compared with received phishing emails allowing identification of collection of OSINT by an attacker and potential early warning of an Advanced Persistent Threat APT
neither general MValgebras 19 nor even the special case of Łukasiewicz MValgebra are considered in this paper
According to the most commonly accepted definition Big Data is characterized by the 4 Vs  3 volume velocity variety and veracity
From the business perspective the goal of these technologies is to process fast input streams obtain realtime insights and enable prompt reaction to them  4
The resurgence of interest in CEP and SP systems has been accompanied by the use of cloud environments as their runtime platform Clouds are leveraged to provide the low latency and scalability needed by modern applications  57
First cloud environments are subject to variations that make it difficult to reproduce the environment and conditions of an experiment  9
Simulators have been used in many different fields to overcome the difficulty of executing repeatable and reproducible experiments Early research into distributed systems  10 and grid computing  11 used simulators as well as the more recent field of cloud computing  1214
CEPSim extends CloudSim   12 using a query model based on Directed Acyclic Graphs DAGs and introduces a simulation algorithm based on a novel abstraction called event sets
This article significantly extends the authors previous work  15 by improving the discussion about CEPSims goals and assumptions by introducing the event set concept by presenting detailed descriptions of all simulation algorithms and a thorough evaluation of CEPSim
The basis of Complex Event Processing CEP was established by the work of Luckham on Rapide   10 a distributed system simulator
In CEP systems users create queries or rules that specify how to process input event streams and derive complex events These queries have usually been defined by means of proprietary languages such as Aurora Stream Query Algebra SQuAl  17 and CQL  21
Despite standardization efforts  22 a variety of languages are still in use today Cugola and Margara  19 classify existing languages into three groups
Patternbased languages are used to define patterns of events using logical operators causality relationships and time constraints The Rapide   10 language is an example of this category
 Many authors have recognized the symbiotic relationship between these areas  2325 as cloud computing environments can be used to store and process Big Data and also to enable new models for data services 
For instance TimeStream   5 StreamCloud   6 and StreamHub   7 are CEP systems that use cloud infrastructures as their runtime environments
StreamMapReduce   29 and M3   30 are examples of MapReduceinspired systems intended for stream processing
Simulators are a popular tool that has been used in Grid Computing research  1133 for many years
Because of its limitations CloudSim has originated many extensions in the literature  93435
Guérout et al  34 on the other hand focused on implementing the DVFS model on CloudSim Finally Grozev and Buyya  35 presented a model for threetier Web applications and incorporated it into CloudSim
Finally the iCanCloud simulator  14 is similar to CloudSim but it can also parallelize simulations and has a GUI to interact with the simulator Its application model however is based on lowlevel primitives and needs to be significantly customized to represent CEP applications
For instance most CEP systems based on imperative languages also use DAGs to represent user queries This is the case with Aurora   17 StreamCloud   6 Storm   31 S4   32 FUGU   37 and many others
Systems using declarative languages on the other hand create execution plans from queries that can often be mapped into DAGs  518 Even for patternbased query languages previous studies  38 have shown that is possible to transform them into DAGs
A stateless operator or simply an operator can process incoming events in isolation with no dependency on any state computed from previous events For example an Aurora filter is an operator that routes events to alternative outputs based on attribute values  17
A scheduling strategy can fundamentally determine the performance of a CEP system by optimizing for different aspects of the system such as overall QoS  17 or memory consumption  40 
The queries used in the experiments in this section have been extracted from Powersmiths WOW system  41 a sustainability management platform that draws on live measurements of buildings to support energy management
This validation approach is similar to the ones adopted by other simulators such as NetworkCloudSim  9 iCanCloud  14 and Grozev and Buyya  35
This experiment analysed CEPSims behaviour when simulating multiple queries running concurrently To do so first a Storm cluster was created at the Amazon EC2 service  45
DSPLs produce software capable of adapting to changes by means of binding the variation points at runtime  1 This means that we have to model the elements that could be adapted dynamically as dynamic variation points and generate at runtime the different variants of the DSPL
Variability is modelled at different abstraction levels mostly using feature models FM  3 at the requirements level and UML profiles or Architecture Description Languages ADLs  46 at the architectural level
Both services are designed to be integrated in a middleware for adaptive applications development  21 although in this paper we focus on presenting the details of how the DRS accomplishes the runtime reconfiguration of mobile applications 
Therefore in DSPLs the system architecture supports all possible adaptations defined by the set of dynamic variation points  1Then as part of a DSPL definition the engineer must define
In this context most DSPL approaches share some common properties with the Autonomic Computing paradigm  2 AC such as the monitoring of the environment and the generation of successive configurations
The main ones are  24 1 how they model the dynamic variation points 2 whether the successive configurations generated by the approach are optimal regarding some criteria or not 3 if the reconfiguration plan is generated at design or runtime 4 the decision making process used to trigger a reconfiguration and 5 if they can be used to develop applications for resource constraint devices or not
 So at runtime we generate the successive software architecture configurations by binding the architectural variation points specified in a variability language  25 
 Note that an exact algorithm cannot be used because the problem to be solved has been proven to be NPhard nondeterministic polynomialtime hard  26
Generating the reconfiguration plan at runtime Most DSPL approaches generate at design time the configurations that will be deployed at runtime  132729 
Concretely as shown in  23 exact techniques can only be applied to small cases at the cost of a very high execution time Nevertheless artificial intelligence algorithms can find nearlyoptimal solutions in an efficient and scalable way 
Synthesis also known as Churchs solvability problem 5 is the problem of defining a circuit that continually reacts on an infinite input stream by producing one output letter after receiving one input letter
We assume that the reader is familiar with temporal logics 116 and the distributed synthesis question 121097 
e follow 7 by restricting the variables such that each variable can only occur on the outgoing edges of one process but may be read by many processes and by allowing for processes that have a fixed finite implementation
The extension to decidable architectures is a straightforward adaptation from the extension described by Finkbeiner and Schewe 7 
In this section we establish a close relation between our notion of strong regularity and the one usually studied in max min algebra 2 41214
As K is closed by the usual Minkowski theorem 25 Corollary 1851 it can be represented as the set of positive linear combinations of its extremal rays recall that w K is called extremal if uvw and uv K imply that u and v are proportional with w which generate the whole LinC y
For a monograph in conventional convexity see eg Rockafellar 25
The utility function introduced there relies on the notion of possibilistic mixture where the possibilistic mixture which under some natural conditions is also a possibilistic measure 10 of the possibilistic measures 1 2 with possibilities  max   1 is defined as max min   1min   2 that is as a point on the max min segment  1 2
For example in 8 Dubois and Prade developed an axiomatic approach to quantitative utility theory
The ability to integrate multisensory information is a fundamental feature of the brain that provides a robust perceptual experience for an efficient interaction with the environment  Ernst  Bulthoff 2004 Stein  Meredith 1993 Stein Stanford  Rowland2009
Similarly computational models for multimodal integration are a paramount ingredient of autonomous robots to forming robust and meaningful representations of perceived events see Ursino Cuppini  Magosso 2014 for a recent review
Multimodal representations have been shown to improve robustness in the context of action recognition and actiondriven perception humanrobot interaction and sensorydriven motor behavior  Bauer Magg  Wermter 2015 Kachouie Sedighadeli Khosla  Chu 2014 Noda Arie Suga  Ogata 2014
Since realworld events unfold at multiple spatial and temporal scales artificial neurocognitive architectures should account for the efficient processing and integration of spatiotemporal stimuli with different levels of complexity  Fonlupt 2003 Hasson Yang Vallines Heeger  Rubin 2008 Lerner Honey Silbert  Hasson 2011 Taylor Hobbs Burroni  Siegelmann 2015
The human cortex comprises a hierarchy of spatiotemporal receptive fields for features with increasing complexity of representation  Hasson et al 2008 Lerner et al 2011 Taylor et al 2015 ie higherlevel areas process information accumulated over larger spatiotemporal receptive windows
During their development children have a wide range of perceptual social and linguistic cues at their disposal that they can use to attach a novel label to a novel referent  HirschPasek Golinkoff  Hollich 2000 chapter 6
Recent experiments have shown that human infants are able to learn action word mappings using crosssituational statistics thus in the presence of sometimes unavailable groundtruth action words  Smith  Yu 2008
This hypothesis is supported by neurophysiological studies evidencing strong links between the cortical areas governing visual and language processing and suggesting high levels of functional interaction of these areas for the formation of multimodal representations of audiovisual stimuli  Belin Zatorre Lafaille Ahad  Pike 2000 Foxe et al 2000 Belin Zatorre  Ahad 2002 Pulverm ller 2005 Raij Uutela  Hari 2000
From a neurobiological perspective neurons selective to actions in terms of timevarying patterns of body pose and motion features have been found in a wide number of brain structures such as the superior temporal sulcus STS the parietal the premotor and the motor cortex  Giese  Rizzolatti 2015
In particular it has been argued that the STS in the mammalian brain may be the basis of an actionencoding network with neurons driven by the perception of dynamic human bodies  Vangeneugden Pollick  Vogels 2009 and that for this purpose it receives converging inputs from earlier visual areas from both the ventral and dorsal pathways  Beauchamp 2005 Garcia  Grossman 2008 Thirkettle Benton  ScottSamuel 2009
Furthermore neuroimaging studies have shown that the posterior STS shows a greater response for audiovisual stimuli than to unimodal visual or auditory stimuli  BeauchampLee Argall  Martin 2004 Calvert 2001 Senkowski SaintAmour Hfle  Foxe 2011 Wright Pelphrey Allison Mckeown  Mccarthy 2003
Thus the STS area is thought to be an associative learning device for linking different unimodal representations and accounting for the mapping of naturally occurring highly correlated features such as body pose and motion the characteristic sound of an action  Barraclough Xiao Baker Oram  Perrett 2005 Beauchamp et al 2004 and linguistic stimuli  Belin et al 2002 Stevenson  James 2009 Wright et al 2003
These findings together suggest that multimodal representations of actions in the brain play an important role for a robust perception of complex action patterns with the STS representing a multisensory area in the brain network for social cognition  Adolphs 2003 Allison Puce  McCarthy 2000 Beauchamp 2005 Beauchamp Yasar Frye  Ro 2008
Each network layer comprises a selforganizing neural network that employs neurobiologicallymotivated Hebbianlike plasticity and habituation for stable incremental learning  Marsland Shapiro  Nehmzow 2002
Network layers 1 and 2 comprise a twostream hierarchy for the processing and subsequent integration of body pose and motion features resembling the ventral and the dorsal pathway respectively for the processing of complex motion patterns  Giese  Poggio 2003
The integration of pose and motion cues is carried out in network layer 3 or G STS to provide movement dynamics in the joint feature space  Parisi et al2015
Hierarchical learning from contiguous Growing When Required GWR networks  Marsland et al 2002 shapes a functional hierarchy that processes spatiotemporal visual patterns with an increasing level of complexity by using neural activation trajectories from lowerlevel layers for training higherlevel layers
The modeling of neurobiologically observed principles underlying audiovisual integration in the STS for speech and nonspeech stimuli such as superadditivity  Calvert Campbell  Brammer 2000 spatial and temporal congruence  Bushara Grafman  Hallett 2001 MacalusoGeorge Dolan Spence  Driver 2004 and inverse effectiveness  Stevenson  James 2009 was out of the scope of this paper and will be subject of future research
For instance several neurophysiological studies have evidenced strong interaction between the visual and motor representations more specifically including the STS parietal cortex and premotor cortex see Giese  Rizzolatti 2015 for a recent survey with higher activation of neurons in the motor system for biomechanicallyplausible perceived motion sequences  Miller  Saygin 2013
Motivated by the process of inputdriven selforganization exhibited by topographic maps in the cortex  Miikkulainen Bednar Choe  Sirosh 2005 Nelson 2000 Willshaw  von der Malsburg 1976  we proposed a learning model encompassing a hierarchy of Growing When Required GWR networks  Marsland et al 2002
This kind of hierarchical aggregation is a fundamental organizational principle of cortical networks for dealing with perceptual and cognitive processes that unfold over time  Fonlupt 2003
It has been shown that lexical features can be learned using recursive selforganizing architectures  Strickert  Hammer 2005 obtaining action word representations from a phonemic representation of recognized audio
Such a processing scheme would be in line with neurophysiological evidence supporting the hierarchical processing of aural features in the auditory cortex with increasing temporal receptive windows  Lerner et al 2011 
Mental imagery concerns cognitive processes for the creation and manipulation of mental images and for decision making tasks on visual object matching Kosslyn1996 Lamm Windischberger Moser Bauer2007
Mental rotation has been widely investigated not only incognitive psychology but also in cognitive neuroscience andcomputational modelling Kosslyn 1996 Zacks 2008  
According to this view offlinecognition such as mental rotation and imaging is body based‘‘even when decoupled from the environment the activity of themind is grounded in mechanisms that evolved for interactionwith the environment—that is mechanisms of sensory processing and motor control Wilson 2002 
Brainimaging evidence on the brain areasmost involved in mental rotation supports the idea that mentalrotation indeed depends on a strong integration of sensorimotor processes and covert mental simulation of motor movements see Zacks 2008 for a review 
Indeed endowing robots with mental rotation capabilities could increase their planning abilities in relation to the manipulation of objects in particular as planning in robots has mainly focused on navigation 
Based on this evidence these areas are thought to play a key role in implementing the proprioceptive and visual information integration and transformation supporting the core processes of the dynamic mental rotation processesZacks2008
Given its highlevel with in the motor hierarchythis are a might orchestrate mental rotation at a highlevel as suggested by its role in motor imaging GraftonArbibFadigaRizzolatti1996
Population codes Pouget Dayan Zemel 2003 are based on the idea that information on stimuli and actions is encoded in the brain on the basis of the activation of populations of units organised in neural maps having a broad response field
To implement the decision making process involved in themental rotation task the model uses amutual inhibition model Bogacz et al 2006 Usher  McClelland 2001
This model together with otheranalogous models eg Bogacz et al 2006 is very important as it allows there production of there action times of ten recorded in psychological experiments Caligioreetal 20102008 Erlhagen  Schöner 2002
For example the interpolation methods used in some commercial mocap systems such as EVaRT and Vicon are only suitable to deal with the shorttime 05s data missing problem 1 
Aristidou et al 1 used Kalman Filters to estimate missing markers in realtime without the support of any human model However the filtered human motion exhibits visible short latency 
As shown in the last subimage of Fig11c one head marker predicted by Dynammo 30 drifts a little away due to the reason that this marker is missing for a long period of time at the end of the motion data
To demystify this inconsistency we find that SVT 30 convergences too early in handling a largescale imperfect human motion matrix in these two cases where the total frame numbers are 4840 and 4905
As shown in 14 optimal preview control for linear systems has been well studied The problem of preview control has been extended via further research
In 59 H∞ and H2 criteria were introduced into preview control and robust optimal preview control for linear and nonlinear systems has been discussed
The authors in 1012 investigated optimal preview control for systems with multiple sampling rates by using the discretetime lifting technique
In 1315 optimal preview control for linear timevariant systems was considered
In recent years based on descriptor system theory researchers have considered some descriptor causal systems and designed an optimal controller with preview compensator 1618
The authors in 31 designed a piecewise linear timeinvariant switching control law which leads to a guarantee of Lyapunov stability with an exponential rate of convergence for the state
Based on the method a kind of switching controller for discrete time system was proposed 32
In 33 a localizationbased method was introduced which is manifested as the rapid convergence of the switching controller Ref 34 promoted the results and discussed the problem of optimal localization 
 By defining monitoring function and switching principle similarly to 31 the problem is formally converted into a normal optimal control problem that includes preview information 
Nuclear power is often regarded to be amongst the safest forms of electricity generation taking into account the complete worldwide electricity production chains with some arguing that this result holds even after the possibility of large nuclear accidents is included in the analysis Kearns Thomas Taylor  Boyle 2012 
A number of methodologies and software packages often referred to as Decision Support Systems have been developed to aid this Bartzis et al 2000 French 1996 Geldermann et al 2009 Hmlinen Lindstedt  Sinkko 2000 Hoe  Mller 2003 Landman PslerSauer  Raskob 2014 OECDNEA 2000 Papamichail  French 2005 Wex Schryen Feuerriegel  Neumann 2014 
The literature on dynamic decision making and economic optimisation in the response and recovery phases is however considerably less mature Altay  Green III 2006
Immediate response to a nuclear accident involves procedures for evacuation sheltering iodine tablets distribution whereas recovery measures include longterm relocation and remediation as well as potential repopulation of the affected areas Gering Gerich Wirth  Kirchner 2013 Higgins et al 2008 Munro 2013 
The key difference between the response and recovery stages is therefore the timescale on which the relevant measures are implemented while emergency response can take place on the timescale of minutes hours and days in the immediate aftermath of a nuclear disaster recovery measures often span over weeks months and years DECC 2013
Shortterm response therefore is expected to have typical features of emergency planning when precautionary actions may not necessarily be justified in economic terms Dana 2002 Klinke  Renn 2001
Longterm postaccident response and recovery planning on the other hand is characterised by significantly lower levels of radiological uncertainty and requires multiple economic as well as noneconomic factors to be taken into account French 1996 Geldermann et al 2009 Hmlinen et al 2000
It is possible that governments are going to prioritise the economic factors by seeking to minimise the total cost of preventative and recovery measures Munro 2011 2012
Of the vast literature on Chernobyl the two key studies on the longterm measures are Lochard  Schneider 1992 contamination and population distributions resettlement and health costs and Jacob et al 2009 remediation costs in rural areas see also IAEA 2006 Karaoglou 1996
The existing studies on Fukushima have focused on providing contamination maps and summarizing earlystage radiological impacts on the environment IRSN 2011 2012 analysing health effects for the affected populations González et al 2013 Harada et al 2014 WHO 2013 assessing economics of decontamination Munro 2013 and peoples intention to return to their evacuated family homes Munro  Managi 2014
The detailed input data for these packages can be obtained from economic and population databases such as GIS for many locations throughout the world De Silva  Eglese 2000 including those near the existing nuclear installations
The DSS that is most relevant to the present study is COCO2 Higgins et al 2008
It is possible that emergency evacuations and longterm relocations inside these relatively unaffected areas might have caused psychological and economic harm comparable to or even exceeding the potential radiological harm averted by these actions Ahn et al 2015 as arguably was also the case in for a number of evacuations in the aftermath of Chernobyl IAEA 2006 Karaoglou 1996 Walinder 1995
A dartboardlike structure Gering et al 2013 might be a good starting point although a more detailed mosaiclike pattern tailored around the urban areas within the circular zones could provide a greater level of control 
Ultimately the feasibility of having different treatments within specified boundaries will depend on how densely populated the entire prototype exclusion zone is which is part of a wider issue of siting for nuclear power installations Grimston et al 2014
To address the need for the spatial and temporal flexibilities we develop a decisionmaking model for a single economic location say a town a village or an area of agricultural land based on Bellmans Optimal Control Theory Bellman 1956 which is at the basis of the Operations Research OR methodology 
In reality any longterm decision making takes into account a number of noneconomic factors such as acceptance of a decision by various groups including the public decisionmakers stakeholders and experts Geldermann et al 2009 alongside the standard radiological and economic evaluation methods
Even though some shortterm temporal and spatial flexibilities in sheltering times iodine prophylaxis and evacuation are also possible Dillon 2014 Gering et al 2013 the present study only focuses on the largely deterministic mid and longterm problem setting
Much of the midterm and longterm radiation exposure both in urban and in rural areas comes from ground shine defined as external dose direct from radioactive materials deposited on the ground WHO 2013 which is a result of the initial radioactive deposition Harada et al 2014 Lochard  Schneider 1992
We restrict our analysis to ground shine and ingestion of contaminated food produce originated in rural areas the two types of exposure most relevant on the longer timescales Jacob et al 2009
The deposition period could last several hours days or weeks Ahn et al 2015 Katata et al 2015 and the radioactive material will usually be carried in a plume of smoke or ash depending on the type of accident that has occurred 
Chernobyl the biggest nuclear disaster in history provided extensive information on the economics of a severe nuclear accident Jacob et al 2009 Lochard  Schneider 1992 
The economics of nuclear decontamination and assessment of policy options for the management of land around Fukushima is described in Munro 2013 see also Munro 2011 2012 and the relevant WNN reports
According to Walinder 1995 it is impossible to predict by means of a mathematical expression the specific outcome of a low radiation dose
The concept of VSLY was developed in public policy making to put monetary value on the reduction of risk of death for an average ‘statistical individual Higgins et al 2008
In addition a number of studies since Chernobyl have shown that the health effects due to stress may be commensurate with the health effects associated directly with the radiological exposure IAEA 2006 Karaoglou 1996
It is assumed that the collaborating organisations follow OWLS ontology for services as OWLS is the most widely used standard specification for adding semantics to web services 10 
The existing execution mechanisms from literature enact automatically generated workflows for single organisations only however they can handle adhoc processes that are outside the boundaries of the organisation in the workflows 1
A Business process can be defined as a set of one or more linked procedures or activities which collectively realise a business objective or policy goal normally within the context of an organisational structure defining functional roles and relationships 32
1 is the automation of a business process in whole or part during which documents information or tasks are passed from one participant to another for action according to a set of procedural rules 32
A workflow has two main stages 32 Buildtime stage this refers to the stage where workflow descriptions of the business process are defined or changed
For any two organisations to proceed in business they need to have compatible workflows and compatible means that there should be an agreed sequence of activities exchanging collaborative messages and information 6
The set of all interface activities in a workflow is called interface process 6
When two of more organisations do business together the need for workflow collaboration across multiple organisations arises 5
Considerable amount of effort is needed to ensure that workflows are compatible 723 and proceed into the execution stage
Recent research on workflow collaboration focuses on reconciling existing incompatible workflows 133
In an alternative topdown approach organisations meet discuss and design collaborative processes and then implement them 5
Another paradigm in the literature is automatic workflow generation which is based on AI planning where a workflow is considered as a plan 210
If every activity in a workflow is treated as a web service a workflow represents a plan of web services to achieve the desired goal state from a given initial state 22
Furthermore it is a highly efficient planning system and has a Web Ontology Language for Services OWLS type mechanism for representing atomic tasks and decomposing composite tasks into atomic tasks 27
The similar mechanism of OWLS and SHOP2 to hierarchically decompose complex tasks into sub tasks makes it straightforward to map OWLS definitions directly into SHOP2 domain 2733 and create workflows based on the translated domain
In the context of semantic web services PDDL is neither too restrictive nor too expressive and is considered as a viable compromise between expressivity and efficiency 20
It is a workflow modelling language 16 and lacks the semantic precision required for automatic business process generation and execution 17 and so we cannot use it as a notation for the proposed framework
An aggregation significantly can reduce computational costs for obtaining inference results in DPGMs but requires a careful consideration of indirect influences as Motzek and Mller2015a has shown
Then in fact the presented DMIA model represents a socalled activator dynamic Bayesian network ADBN Motzek and Mller 2015a2015b and one obtains welldefined semantics
One has to differentiate between induced observations and true observations a differentiation related to Pearls  Pearl2002 introduction on the docalculus and is best explained at an example Considering an infectiousdisease monitoring system one has to differentiate between a person being healthy because he has been healed and between a person being tested to be healthy
Boutilier et al1996 of Xt on all its possible dependencies Z  given set Xt is decoupled from all other potential sources of nonimpact and the observation xt is completely accredited to SEt
1 1Stephen Elop the former Executive Vice President of Microsofts Devices and Services and at the time of the comment the CEO of Nokia Corporation speech at D9 June 1 2011 Hence the sheer number of applications in the marketplace has become increasingly important in marketing new mobile devices see eg Chen 2010Reuters 2012 Lee 2015 Smith 2015
The logic behind establishing the ecosystems is grounded on the theory of network externalities  Katz and Shapiro1985
Due to network externalities a large number of application developers within the ecosystem is expected to lead to a large number of applications that in turn will attract customers and drive device sales leading to a virtuous circle  Holzer and Ondrus 2011
In this study the concept of mobile application ecosystem refers to an interconnected system comprising an ecosystem orchestrator mobile application developers and mobile device owners all of whom are connected through a marketplace platform  Hyrynsalmi Sepp nen and Suominen2014
Hence a mobile application ecosystem is a derivate of the more general concept of a software ecosystem  Jansen Finkelstein and Brinkkemper 2009 Bosch 2009 Manikas and Hanssen 2013 
The increased complexity calls for a better understanding of the boundaries and structures of the ecosystems eg Jansen et al 2009 Gueguen and Isckia 2011 Hanssen 2012
Prior research has investigated the success factors of the iPhone  Laugesen and Yuan 2010 West and Mace2010 the distribution and capture of value in the mobile phone supply chains  Dedrick Kraemer andLinden 2011 developers perspectives on the mobile application markets  Lee Lee Shim andChoi 2010 Holzer and Ondrus 2011 Schultz Zarnekow Wulf and Nguyen 2011  the dynamics of the application marketplaces  J rvi and Kortelainen 2011 Hyrynsalmi Suominen and Sepp nen 2013 Jansen and Bloemendal 2013 standard wars and platform battles  Heinrich 2014 Gallagher 2012 van de Kaa and de Vries 2015 van de Kaa van den Ende de Vries and van Heck 2011  and cooperation within ecosystems  Gueguen and Isckia 2011
Network effects can accrue from direct externalities whereby utility increases as the number of users consuming increases and indirect externalities whereby the demand for a product depends on the existence of another product  Katz and Shapiro 1985
Sellers engage in multihoming to gain access to larger potential markets  Rochet and Tirole 2006 to offer their products to the same customers across different platforms and to reduce dependency on a single market and orchestrator  Idu van de Zande and Jansen 2011
Prior research has focused on software vendors multihoming in console games marketplaces  Landsman and Stremersch2011 Software as a Service SaaS marketplaces  Burkard Draisbach Widjaja and Buxmann 2011 Burkard Widjaja and Buxmann 2012 and also within Apples ecosystem  Idu et al2011
In their study on the gaming console market Landsman and Stremersch 2011 found that the multihoming of games has a negative effect on sales at the marketplace level although the negative effect decreases when a platform matures or gains market share
Idu et al 2011 investigated the iPhone iPad and Mac software marketplaces and found that out of the top 1800 applications 172 were multihomed in two marketplaces and 21 in all three marketplaces
Furthermore as pointed out by Hyrynsalmi et al 2012  only a small share of all applications published in the marketplace are actually downloaded and even fewer are actually used by customers
For instance in the fields of Natural Language Processing NLP and IR ontologybased semantic similarity measures have been used in Word Sense Disambiguation WSD methods 92  text similarity measures 86 spelling error detection 20 sentence similarity models 446691 paraphrase detection 36 unified sense disambiguation methods for different types of structured sources of knowledge 73 document clustering 31 ontology alignment 30 document 74 and query anonymization 11 clustering of nominal information 910 chemical entity identification 40 interoperability among agentbased systems 34 and ontologybased Information Retrieval IR models 5562 to solve the lack of an intrinsic semantic distance in vector ontologybased IR models 23 
For this reason 5711 ontologybased semantic similarity measures exclusively based on isa relationships are currently the best and most reliable strategy to estimate the degree of similarity between words and concepts 58 whilst the corpusbased similarity measures are the best strategy for estimating their degree of relatedness 8
Thus the proposal for concept similarity models to estimate the degree of similarity between word and concept pairs has been a very active line of research in the fields of cognitive sciences 106124 artificial intelligence and Information Retrieval IR 107
In the field of bioengineering ontologybased similarity measures have been proposed for synonym recognition 24 and biomedical text mining 1498112
Mendling et al 80 study the current practices in the activity labeling of business processes whilst Dijkman et al 32 propose a similarity metric between business process models based on an adhoc semantic similarity metric between words in the node labels and attributes as well as the structural similarity encoded by the concept map topology
HESML V1R2 60 is distributed as a Java class library  HESMLV1R2jar plus a test driver application  HESMLclientjar which have been developed using NetBeans 802 for Windows although it has been also compiled and evaluated on Linuxbased platforms using the corresponding NetBeans versions
Likewise Leopold et al 68 propose an automatic refactoring method of activity labels in business process modeling based on the automatic recognition of labeling styles and Leopold et al 67 propose the inference of suitable names for business process models automatically
In addition to the aforementioned IC models 46 Seddiqui and Aono 120 and Pirr and Euzenat 104 propose two further intrinsic IC models not implemented by HESML which are based on the integration of all types of taxonomical relationships and thus especially designed for semantic relatedness measures
In addition we plan to provide ongoing support for further ontologies such as Wikidata 126 and the Gene Ontology GO 5 among others as well as further similarity and relatedness measures
Poset HERep is based on our adaptation of the wellknown halfedge representation in the field of computational geometry 19 also known as a doubleconnected edge list 1722 in order to efficiently represent and interrogate large taxonomies
The functionality and software architecture of HESML allow the efficient and practical evaluation of large word similarity benchmarks such as SimLex 50 and ontologybased similarity measures based on the length of the shortest path whose implementation in other software libraries requires a high computational cost that prevents their evaluation in large experimental surveys 58 and datasets
All the experiments compute the Pearson and Spearman correlation metrics for a set of ontologybased similarity measures on each word similarity benchmark shown in table 22 as detailed in 56
These scripts take the raw output files generated by the experiments in table 11 and produce the final assembled tables as shown in 56 58 as well asfigures 2 and 3 showing the interval significance analysis in 56
The ReproZip program was used for recording and packaging the running of the HESMLclient program with all the reproducible experiments shown in table 11 in the HESMLv1r1 reproducible expsrpz file available at 64
Because of the lack of space WNSimRep v1 is detailed in a complementary paper which together with the dataset files is publicly available at 63
All the corpusbased IC models are derived from the family of *add1dat WordNetbased frequency files included in the 95 dataset which is a dataset of corpusbased files created for a series of papers on similarity measures in WordNet such as 93 and 96
The goals of the experiments described in this section are as follows 1 the experimental evaluation of the PosetHERep representation model and HESML as well as their comparison with the stateoftheart semantic measures libraries called SML 48 and WNetSS 15 2 a study of the impact of the size of the taxonomy on the performance and scalability of the stateoftheart semantic measures libraries and finally 3 the confirmation or refutation of our main hypothesis and research questions Q1 and Q2 introduced in section 11
On the other hand in order to evaluate and compare the performance of WNetSS with HESML and SML we compare the runningtime of the three libraries in the evaluation of the JiangConrath similarity measure 52 with the Seco et al IC model 119 in the SimLex665 dataset 50
We have introduced a new and linearly scalable representation model for large taxonomies called PosetHERep and the HESML V1R2 60 semantic measures library based on the former
ISOIECIEEE 2007 defines an architecture as composed of a the fundamental organization of a system embodied in its components b their relationships to each other and to the environment and c the principles guiding its design and evolution 
On that basis Panunzio and Vardanega 2013 regards the concept of software reference architecture as proceeding from
In his 1972 ACM Turing lecture Dijkstra 1972 EW Dijkstra advocated a constructive approach to program correctness where program construction should follow  instead of precede  the construction of a solid proof of correctness
In accord with Sifakis 2005 we maintain that composability is achieved when the designated properties of individual components captured in terms of needs and obligations are preserved on component composition deployment on target and execution
The WCET is a local property of the program that is the service attached to the interface in question composability in the time dimension Puschner et al 2009 is achieved if the interfering effect caused by the presence of other components in the system does not prevent a safe and tight WCET bound to be determined for every single interface service
They are used in approaches like the one presented herein which present an endogenous treatment of nonfunctional properties ie outside of the component Crnkovic et al 2011 
Chaudron and Crnkovic 2008 defines a software component as a software building block that conforms to a component model
The requirement of independent deployment of components entailed by the definition by Szyperski 2002 is currently not a core requirement for us and neither is in many other componentoriented approaches 
The semantics of that declarative language emanates from the chosen computational model the Ravenscar Computational Model RCM Burns et al 2014 in our case 
In terms of the rich classification proposed in Crnkovic et al 2011 our component model i addresses the modeling and implementation phases of the development life cycle ii is independent from the programming language iii provides constructs for interface specification iv allows expressing a limited set of interaction patterns v supports specification composition and analysis of nonfunctional properties and vi is specialpurpose as intended to highintegrity embedded realtime systems
Gößler and Sifakis 2002 focuses on integration of components with heterogeneous interactions and execution paradigms 
The framework aims at correctbyconstruction design by achieving component composability and compositionality That work later evolved in the conception of the BIP framework Behaviour Interaction Priority Basu et al 2006
SaveCCM Hansson et al 2004 targets heavy vehicular systems That component model supports both timetriggered and eventtriggered activation events and its components are hierarchical
The ROBOCOP component model Muskens et al 2005 targets the consumer electronic domain 
Giotto Henzinger et al 2001 is a progenitor in a family of timetriggered languages and tools which specialize for control processing where deterministic time of execution is inbred to the domain culture Henzinger et al 2003 
The Ptolemy project Lee 2001 studies modeling simulation and design of concurrent realtime embedded systems realized as an assembly of concurrent components
The ISO standard 42010 ISOIECIEEE 2007 stipulates that architectural description of the system is organized into one or more constituents called views and a view is a partial representation of a system from a particular viewpoint which is the expression of some stakeholders concerns
The value for the WCET can later be refined with bounds for a given target platform obtained by timing analysis Wilhelm et al 2008
we defined the whole set of allowable containers and connectors in a library of code archetypes Panunzio and Vardanega 2012 which vastly simplifies automatic code generation
The possibility that synchronization increases with frequency is commensurate with invitro cell recordings  Rocha Doiron SheaBrown JosicReyes2007 and computer simulation of both integrate and fire and Hodgkin Huxley type models  Chawla Lumer Friston 1999
The first and second order derivatives of the MFCCs with respect to time are sometimes used as additional features  Gutig  Sompolinsky2009 but did not improve recognition performance on our database
Whilst the work in this paper provides a useful starting point it does not make use of the network view of brain function Price Thierry and Griffiths 2005 for example propose that the human brain has not developed macroanatomical structures dedicated to speech processing but rather that speechspecific processing emerges at the level of functional connectivity among distributed regions
As the magnitude spectrum of speech is known to be stationary over a period of approximately twin20 100ms  Rabiner  Juang1993 we broke up each speech time series into frames of length twin50ms
The importance of a hierarchy of temporal scales is emphasized in recent work by Ghitza 2011 who provides evidence that current models of speech perception which are driven by acoustic features alone provide an incomplete description of speech recognition phenomena
The notion that regions higher up in the auditory cortical hierarchy process information at longer time scales has recently been made use of in a model of auditory sequence recognition based on stable heteroclinic channels  Kiebel Kriegstein Daunizeau  Friston 2009
The Liquid State Machine LSM  Maass Natschlager  Markram 2002 for example uses OT features and the temporal embedding idea proposed in the HB model but then applies standard methods for recognizing the resulting static patterns
This results in good pattern discrimination abilities  Verstraetenet al2005 though not as accurate as a recent approach based on OT features  Gutig  Sompolinsky2009
As the way in which one cell or circuit couples with another can be summarized using phase interaction functions  Penny et al2009 we envisage that it should be possible to identify families of neurons or neural circuits that have the appropriate synchronization properties
We thought it might be possible that the OT system performed better at low signal levels because it had fewer parameters than the MFCC system and so might generalize better  Bishop1995
We also compare augmented and minimal models using the model evidence as computed using the Posterior Harmonic Mean PHM  Gelman et al1995
The resulting Bayes factors were 098 and 099 indicating that neither of the augmented models are significantly better than the minimal model this would require a Bayes factor of at least three  Gelman et al1995
Human speech is characterized by a fourfold variation in the speed at which words are spoken  Miller Grosjean  Lomanto1984 and any speech recognition system whether artificial or natural will have to deal with this range of timewarp
Each nonword matched one of the words action verbs in duration intensity and power spectrum but was rendered unintelligible by removing components of the modulation power spectrum using the Modulation Transfer Function MTF algorithm described in Elliott and Theunissen 2009
The corresponding spectrograms Gywift and Gynift were then computed using a windowed multitaper method with window size N256 samples 0128 s a window offset of 32 samples 0016 s and timebandwidth parameter set to NW3  Mitra  Pesaran 1999 
More physiologically realistic filters can be implemented by linear spacing the filter bands on a melfrequency scale  Ghitza 1986 and this was implemented for the pattern recognition results described in Section 31
Such frequency tuned onset and offset detectors have been observed in the inferior colliculus of the auditory midbrain  Casseday et al2002
Optical imaging reveals larger time windows of temporal integration as one moves from primary to secondary auditory areas  Harrison Harel PanesarMori Mount 2000
 To motivate this enterprise and to understand the importance of high thread counts on highlythreaded manycore machines let us consider a simple application that performs Bloom filter set membership tests on an input stream of biosequence data on GPUs  3
 Using a different approach Hong et al  17 propose another analytical model to capture the cost of memory operations by counting the number of parallel memory requests in terms of memorywarp parallelism MWP and computationwarp parallelism CWP
Accessing the offchip global memory usually takes 20 to 40 times more clock cycles than accessing the onchip shared memoryL1 cache  51 
 Based on the description from Alverson et al  52 about the nature of the computations this processor was designed to run it is a purposebuilt appliance for realtime graph analytics featuring graphoptimized hardware that provides up to 512 terabytes of global shared memory massivelymultithreaded graph processors named Threadstorm supporting 128 threadsprocessor and highly scalable IO 
Dijkstra algorithm is a greedy algorithm for calculating single source shortest paths The pseudocode for Dijkstra algorithm is given in Algorithm 55 
Such overapproximations have been used among other things in the analysis of cryptographic protocols 8 for termination analysis 913 and for establishing nonconfluence of term rewrite systems 19
This research was born of involvement in the development of three tools for term rewriting CeTA 18 a certifier for termination and confluence proofs generated by provers CSI 19 an automated confluence prover and Image 1 11 an automated termination prover
The distinguishing feature of a certifier is that it is highly trustworthy in the case of CeTA this is achieved by proving its code correct in the proof assistant Isabelle 17
Both CSI and Image 1 use quasideterministic automata 13 to produce overapproximations of reachable terms and CeTA could not certify the resulting proofs
Note that in both cases we did not define new recursive functions but just proved equalities which are treated as function definitions by Isabelles code generator 10
Consider the famous Lorenz system 60 1x  yxy  xyxzz  zxywhere   are positive parameters
Consider the Chen system 12 2x ayxy caxcyxzz bzxy and the Lu system 61 3x ayxy cyxzz bzxywhere abc are real parameters
Leonov suggested to consider the following substitutions 35 4x hxy hyz hzt h1twith ha
Remark that the transformation 4 with ha does not change the direction of time for the positive chaotic parameters considered in the works 1261
Note that here in contrast to the previous transformation 1 the transformation 4 with hc change the direction of time for c0 considered in the works 1261 2 for c0 the Chen and the Lu systems do not become linear and their dynamics may be of interest
Various rigorous approaches to the justification of its existence are based for example on the investigation of instability hyperbolicity of trajectories with the help of computing Lyapunov exponents or the computation of attractor dimensions see eg 2719 and many others
On the other hand we would like to recall the classical 16th Hilbert problem second part 23 on the number and mutual disposition of limit cycles in twodimensional polynomial systems where one of the tasks is to find the simplest system from a certain class with the maximum possible number of limit cycles
Many chaotic polynomial systems have been discovered eg such particular cases of threedimensional quadratic systems as the Lorenz the Rossler the Sprott the Chen the Lu and other systems and studied over the years but it is not known whether the algebraically simplest chaotic flow has been identified 7372
While Lyapunov dimension of the Lorenz attractor can be obtained analytically 47 for the Chen and Lu attractors it is still an open problem
The Chen system is a special case of the Lorenz system and The Lu system is a particular case of the Lorenz system of 23 may lead to the conclusion that the study of the Chen and the Lu systems is useless the Chen and the Lu systems stimulate the development of new methods for the analysis of chaotic systems
The estimate from above of the Lyapunov dimension by Lyapunov functions 36 and its comparison with the local Lyapunov dimension in zero stationary point permit one to obtain the exact formula of dimension for a generalized Lorenz system 9 with a certain parameter d
 In the mechanical and mathematical literature concerning the models for piezoviscous hydrodynamic thin film lubrication problems different classical devices have been considered such as journalbearings rollingbearings or rollingballbearings see 1 for example 
In these more complex settings this way of including piezoviscous regimes has given rise to several mathematical analysis results that state the existence and the uniqueness of solution as well as to the design of suitable numerical methods to approximate the corresponding solutions for which there are no analytical expressions see for example 48
For the spatial discretization we consider a classical piecewise linear Lagrange finite element space so that the discretized problem consists of finding p hn1 Kh such that 38  2 2Gtp hnp hn1  hp hn1 dt 6  2 2h  hp hn1dt h Khwhere Vh denotes the finite element space see 22 for example Vh h C0  hE P1 E h
In order to apply the duality method in 20 we introduce the indicatrix function of the convex Kh denoted by IKh so that the variational inequality of first kind 38 can be transformed in the equivalent variational inequality of second kind that consists of finding p n1 V0h such that  2 2Gtp np n1  p n1 dtIKh IKhp n1 6  2 2h  p n1dt V0h
In order to apply the results in 20 for a given parameter 0 we introduce the new variable 42 n1 IKhp n1 p n1In terms of this new variable n1 the equivalent formulation to 40 41 can be written in the form
The idea is to make a comparative analysis of the different models formulations and numerical schemes presented in the previous sections mainly taking as a reference the data set of the numerical examples in 12
It seems that the results obtained with the numerical schemes here implemented for the Rajagopal and Szeri model are very similar to the solution presented in 12
Predicting the effort required to create software has been based on numerous software size models such as the Constructive Cost Model Anandhi and Chezian 2014 Clark 1996 Manalif et al 2014 and all its alternatives Attarzadeh and Ow 2011 Kazemifard et al 2011 Tadayon 2004 Yang et al 2006 as well as on function points Borandag et al 2016 and analogy based models Idri et al 2015 
Analogy based size estimation is commonly used for prediction in all the methods mentioned above Idri et al 2015 Shepperd and MacDonell 2012 
The UCP method is based on use case models which are commonly used as functional descriptions of proposed systems or software The method involves assigning weights to groups of actors and use cases Karners original UCP method Karner 1993 identifies three groups simple average and complex
A number of use case scenario steps are typically involved in the initial estimation process There have also been several modifications of the original UCP principles including use case size points Braz and Vergilio 2006 extended UCP Wang et al 2009 modified UCP Diev 2006 adapted UCP Mohagheghi et al 2005 and transaction or path analysis Robiolo et al 2009
This approach is based on analysing a scenario not step by step but using steps merged logically into socalled transactions in which each transaction should include more than one step Robiolo et al 2009 improved transactions by calculating paths by which the complexity of each transaction is based on the number of binary or multiple conditions used in the scenarios
Their approach is based on Robiolo and Orosco 2008 where number of transactions is equal to the number of stimuli 
Ochodek et al 2011a discusses a reliability of transaction identification process and Jurkiewicz et al 2015 discusses event identification in use cases which should be useful for path identification
Diev 2006 noted that when the actors and use cases are precisely defined unadjusted UCP the sum of the UAW and the UUCW can be multiplied by the technical factors
According to Nageswaran 2001 added effort must be taken to consider support activities such as configuration management or testing
Yet another modification to the UCP is called adapted UCP Mohagheghi et al 2005 In this method the UCP method is adapted to provide incremental development estimations for largescale projects 
Ochodek et al 2011b also proposed omitting UAW and the decomposition of use cases into smaller ones which are then classified into the typical three use case categories
Analogy based estimation methods are discussed in Azzeh et al 2015b which evaluated 40 variants of the single adjustment method using four performance measures and eight test datasets
Amasaki and Lokan 2015 addressed the problem of selecting projects using a linear regression model by testing the window principle The window principle involves first selecting a subset of the data
Instead the study by Urbanek et al 2015b is based on artificial intelligence and is an application of the approach proposed by Senkerik et al 2014 but with theoretical aspects of Oplatkova et al 2013 Urbanek et al 2015b used a symbolic regression tool analytic programming together with differential evolution
Several works have attempted to apply various prediction models to UCP Nassif et al 2013 presented a linear regression model with a logarithmic transformation that they created to estimate software effort from use case diagrams
 In Nassif et al 2011 a multiple linear regression model was developed to predict the values of the productivity factor 
Actors play roles in the UAW variables Azzeh et al 2015a Silhavy et al 2015a 
The least squares method is the most common method used to fit a regression line The case when a linear regression has only one independent variable is called simple linear regression Bardsiri et al 2014 Jorgensen 2004 Montgomery et al 2012 Shepperd and MacDonell 2012 whereas multiple linear regression Bardsiri et al 2014 Jorgensen 2004 Montgomery et al 2012 Shepperd and MacDonell 2012 involves more than one independent variable
Another type of linear regression is polynomial regression Bardsiri et al 2014 Jorgensen 2004 Shepperd and MacDonell 2012 in which the relationship between the dependent variable and the independent variables is modelled as an mth degree polynomial
In the case of multiple independent variables it is appropriate to use stepwise regression Bardsiri et al 2014 Jorgensen 2004 Shepperd and MacDonell 2012 
The experiment described above was evaluated using two datasets Dataset 1 was obtained from Silhavy et al 2015a in which the dataset was based on Ochodek et al 2011b and Subriadi and Ningrum 2014 
We can conclude that the UAW also has an effect on size estimation which is different than the findings published previously in Ochodek et al 2011b 
Human motion estimation is one of the most important areas of computer vision study 111 It refers to the automated prediction and estimation of human motion posture based on rigid body motion joint angles and body segment location
Previous studies have shown that the performance of motion estimation is highly dependent on the quality of motion data as well as the algorithm that is developed for modeling and estimation of the model 148911 
The approaches to motion estimation are biomechanical based 57 silhouette based 2361112 and image based 14810 A biomechanicalbased approach involves tissue analysis and bone and joint location which requires expensive devices and equipment
The approaches to motion estimation are biomechanical based 57 silhouette based 2361112 and image based 14810 
The methods used for estimation include crossentropy regularization 1 Artificial Neural Network ANN 8 hierarchical fitting 9 and human pose recovery 10
 The model is applied to short temporal daily activities include walking running and jumping obtained from publicly available video 2527and experimental captures of Yoga motion activity
Typically motion postures in the whole time duration are framed in image snapshots similar to those presented in Wang and Baciu 1 Tong et al 4 Zhang et al 8 Shen et al 9 and Hofmann and Gavrila 10 
Instead of using the pointcluster technique and the Kalman filter approach as in the case of Wolf and Senesh 7 we use a polynomial fitting approach 
In addition classification performance is compared between the actual and estimated model data aided by the Waikato Environment for Knowledge Analysis WEKA software 30 
Human motion is often represented by the original motion frame or by representing the original motion frames with a parametric or probabilistic model 31
 By creating parameterized motions human action can be altered based on momentary moods that describe emotions such as happiness and sadness 32 
On the other hand motion estimation is the process of estimating the configuration of the underlying kinematic or skeletal articulation structure of a person 34 
 For instance Veeraraghavan et al 36 created a shapebased recognition system for gait motion while Zhang et al 37 proposed a visual gait generative model VGGM and a kinematic gait generative model KGGM to represent part or whole gait modelling
Shen et al 12 used a Gaussian process to study the lowdimensional manifold of visual input data to reconstruct the corrupted silhouette for motion estimation 
 Luo et al 11 proved that multiview video is efficient in solving highdimensional space problems and estimating a 3D surface in a temporal sequence 
Biomechanicalbased approaches involve the analysis of soft tissue bone and joint locations in the human body For this purpose Xiao et al 5 performed an optical motion data capture via a markerbased approach 
 Wolf and Senesh 7 on the other hand proposed a numerical model with no consideration for mechanical properties 
Of all of the approaches the imagebased approach is the most common method for motion estimation an example is the monocular image sequence a modelbased approach whereby the object shape is employed in motion estimation and an independent method is used for a priori shaped models 1 
An example of object shape employment in model estimation was presented by Tong et al 4 who estimated the joint and global location parameters of a human pose based on a monocular image using the deterministic nonlinear constraint optimization method 
 In addition an optical flow method to estimate the motion of gestures was proposed by Zhang et al 8 
Another simple and straightforward approach by Daubney et al 41 was the sparse set of features for pose estimation in lowlevel motions 
Whereas the studies on lowlevel motion focus on a single type of motion activity Bruderlin and Williams 51 proposed a timewarping method as a nonlinear method to combine different movements and control the speed of motion
Nanni et al 17 and Saripalle et al 18 proposed methods using support vector machine for three orthogonal planes of motion data and posturography data respectively 
To achieve a perfect fit on every segment model the BB segment has a 2nd order polynomial fitting while the UB and LB have a 6th order of polynomial fitting in our motion model
the data elimination cum regression imputation approach as reported in Chan et al 62 is applied to eliminate and impute the missing data
Despite assertions some thirtyfive years ago that thefuture of operational research is past Ackoff 1979 the techniques and methodologies are still taught in universities across theglobe and regularly used in business decisionmaking in both thepublic and private sectors 
Many organisations have changed the name oftheir departments to include analytics such as IBMsBusiness Analytics and Mathematical SciencesSutor 2013 and Proctor and GamblesGlobal AnalyticsEricson 2006 teams
This conclusion confirms that periodization is not only the product of theory but it is also a producer of theory Green 1995 
Whilst examples of data mining and machine learning algorithms applied within distributed systems are numerous eg Zaki  Ho 2000 no academic literature on the application of ORMS methods within these new architectures was found
The bioinformatics analysis reveals that many human diseases such as cancer cardiovascular disease amyloidoses neurodegenerative diseases and diabetes are correlated with proteins diagnosed to be disordered 12
The nuclear PolyABinding Protein 1 PABPN1 induces the formation of muscle Intranuclear Inclusions INIs that are the pathological hallmarks of OPMD There is currently no cure for OPMD 24
In image analysis the HSV Hue Saturation Value transformation is useful for developing image processing algorithms based on descriptions of colors that are natural to humans 16
There are existing nonlinear functions of features known to be effective which can be interpreted as mathematical expression models 1721 
 The Expectation Maximization EM is a widely used approach to learning in the presence of unobserved variables such as in the applications of fitting highdimensional Gaussian mixture models 24 and reducing the difference in feature distributions 25
Mathematical morphology provides an approach to extracting image components such as size shape boundary and connectivity and to eliminating irrelevancies for detecting various blood cells 1112 and cancer cells 13
 Yet the graphical model method with a prior knowledge of object shapes is able to provide a probabilistic model to represent the relationship among the image pixels region labels and underlying object contour 7
 Since each iteration of the algorithm consists of an Expectation step Estep followed by a Maximization step Mstep we call it the EM algorithm 2627
The advantage of the crossvalidation is that each trainingtest subset is independent of the others 3334
According to a recent survey of 600 software developers managers and executives in the United States and the United Kingdom only 3 of the respondents said they had no plans to adopt CD  Perforce Software Inc 2015
However implementing CD can be quite challenging  Chen 2015 Leppanen et al 2015 Claps et al 2015 
Although CD as a goal a target state is no longer a new idea and has been well documented  Humble and Farley 2010 the adoption journey for CD is not yet a smooth path
Many times these releases would be followed by P1 priority 1 incidents  Rob 2007 meaning that release activity was always full of uncertainty failures and stress
According to this distinction Continuous Delivery is compatible with a wide range of scenarios but Continuous Deployment is suitable only under special conditions  ODell and Skelton 2016
For example in our organization we needed to create a change ticket place the change request on the agenda of the next Change Advisory Board CAB meeting  Rob 2007 present the change at the meeting receive CAB approval confirm the deployment window and so forth
However little if any work has been reported that specifically seeks to understand and address this important type of software  Rodr guez et al2016
Recent years have witnessed tremendous growth in video traffic on the Internet as a result of higher broadband data rates proliferation in smart handheld devices 2495and affordable unlimited data plans offered by Internet Service Providers 51
An estimated onethird of all online activities on the Internet is spent watching video according to the recent report 100 
 Netflix alone is reportedly streaming over 1 billion h of video each month which is equivalent to almost 7200000 Terabytes of video traffic 37 and this figure is rising constantly 
The skyrocketing demand for serving video traffic have questioned the effectiveness of the traditional solution of employing special purpose Content Delivery Networks CDNs to serve such content Invented at the turn of the century 96 CDNs now constitute the backbone for serving content 2580 
Although replacing selfish selforganising swarming with centrally managed P2P content exchange can improve completion rates considerably 81 for CDNgrade reliability peers would still need to stick around to allow other users in the swarm to complete or to maintain a distributed copy of the entire content item across the set of active users in the swarm
In short PACDNs work as follows Whenever possible ie whenever there is sufficient capacity to deliver content in the swarm peers distribute chunks of content amongst each other typically using centrally managed swarming techniques 8193124
Unlike the previous study 66 we analyze a significantly wider range of obstacle factors including not only the traditionally discussed technical challenges such as reliability and QoS but also various other factors including heterogeneity and scale and inhomogeneous distribution of resources among users
Our work with its focus on deployable peerassisted content delivery is complementary to several survey articles which have focused on traditional P2Pbased content delivery 7 or on specific P2P issues such as P2P overlay construction 68 or chunk scheduling 62 
The closest work is that of Lu et al 66 which surveys the design space of PACDN architectures and highlights a fundamental choice between a tightly coupled or loosely coupled model of cooperation between the serverassisted and peerassisted components of the PACDN
The requestrouting system in CDNs typically includes two basic modules for routing user requests to the most suitable edge server i requestrouting algorithm and ii requestrouting mechanism 80 
A special purpose DNS server is programmed to redirect users requests to the IP address of an appropriate edge server by considering some important parameters such as load on edge server and its distance from the client network proximity and user perceived latency 19
Content Delivery Network is a complex content distribution system and several issues and decisions are involved in managing and administrating the entire CDN infrastructure including where to place edge servers 18 what content to replicate 324897 and on which cluster of servers to copy each piece of content 8 
To provide a cheap payasyougo service to a broad variety of customers some CDNs have adopted cloud technologies which became known in the literature as cloud CDNs 4 
Last but not least multiple telecom operators ATT BT Orange Telefonica KPN and Verizon among others to gain a better control over the data services served to their users have deployed their private telcoCDNs 1630
Overall Cisco has estimated that content delivery network traffic will carry nearly twothirds of all Internet video traffic by 2020 24
when users agree to share their resources in return of access to the system  the content swarms are said to be selfscalable 28 as an increase in demand for a content item yields an equal increase in the number of the content suppliers 7383 
Some peertopeer applications may rely on dedicated nodes to control coordinate and manage content swarm This structure is referred to as partially centralized P2P system  88
Similarly PPLive  the largest P2P live streaming service  rely on dedicated Trackernodes to store the information about streaming channels available video chunks and peers 40
For instance the authors of 7 present a critical analysis on different design features and infrastructural properties of P2P systems and their influence on nonfunctional aspects such as scalability resource management security fairness and selforganization
A comprehensive survey of various techniques proposed for structured and unstructured P2P networks has been presented in 68 
Not surprisingly the majority of research efforts in peerassisted content delivery literature have focused on developing strategies for improving quality of service in terms of reducing startup delay and playback delay 394153586598112
Indeed it has been reported in  31 that the presence of middle boxes is a challenging issue
 In 11 the authors have analyzed the user trace collected from the Conviva media platform and reported a very low completion ratio among users when they abandon sessions after watching first few chunks
Similarly a measurement study of PowerInfo 119  a videoondemand system deployed by the worlds largest mobile phone operator China Mobile  has reported a 70 abandon rate among users as measured by the fraction of sessions which were abandoned after first 20 mins
The results suggested that mobile users abandon sessions with a higher rate ie only around 30 of mobile sessions last for longer than a half of a contents duration in comparison to around 50 for the fixedline sessions 51 
This phenomenon can be explained by the typical diurnal patterns in user accesses when most of the users come online in the evening peak hours 11 but also by the content availability policies specific for some video ondemand websites
 for example in catchup TV systems such as BBC iPlayer 51 the content items are typically released for a limited amount of time eg 7 to 30 days and feature a burst of accesses in the first few hours after the release 50 
The length of the popular video content matters too It has been reported in 42 that for small size MSN videos users generally opt to view the entire or most of the video clip and only 20 of users watch 60 of video content with the length greater than 30 min 
A user session might be interrupted due to a network failure overloaded CPU or a software crash 55 
The authors in  38 devised a mathematical model to evaluate the impact of peerchurn on a PACDN when the users of settopboxes are not willing to share their resources 
Similarly to deal with peer churn the authors in  72 proposed Home Boxassisted approach which relies on exploiting settopboxes as proxies between users and CDNs
To improve the quality of service without putting too much of a burden on the peers the authors of  42 also suggest two different peer selection policies Water Levelling WL and Greedy Policy GP 
A biased selection of peers without considering underlying physical topology might lead to severe performance degradation in terms of access delays and bandwidth wastage if for instance peers located within the same building use two different ISPs and so although physically placed close to each other are very distant in network terms 5 
The authors of  111 proposed to limit the P2P traffic within subnetworks or behind common gateways 
To achieve a lower startup delay the authors in 112 proposed a three phase streaming hybrid CDNP2P architecture that allows peers to download initial chunks of a content item from the geographically closed CDN nodes and remaining chunks from a P2P swarm 
The authors of  3965 proposed a strategy for improving startup delay via an effective buffer management on a peers side
Particularly Ha et al in 39suggested that for minimizing startup latency the buffers part at the start of the playback must be filled in with a high priority 
Similarly Lu et al in  65 suggested organizing the playback buffer into three different regions where a startup region and a common region are equivalent to the ones proposed in  39 
The authors of  5898 exploit the social ties between users and the locality of interests to assist peerassisted sharing of content in Facebook
An SP exploits a distributed hash tables algorithm DHT called a content addressable network CAN 84 The algorithm is based on the binning technique proposed in  85 and operates as follows
To address this concern the authors of  115proposed a peers authorization mechanism and a network coding scheme in which each packet is encoded and decoded at the node level using efficient linear codes thus allowing for copyrights protection
The authors of  49 proposed a control schema over copyrights at a Tracker server in which only legally authorized content items are distributed to the peers
A limited contribution policy presented in  112 obliges every user to contribute some fraction of its upload bandwidth resources to a limited number of sessions for a limited period of time or both 
In  34 the authors present an economic model for PACDNs in which user participation in peerassistance is incentivized via free high quality video offers 
The authors of  76 have proposed a peerassisted model with economic incentives for all participating parties including both peers and ISPs
Cho and Yi  23 presented a cooperative game theory approach to validate a profit sharing mechanism with multiple content providers and peers 
Historically peertopeer systems have been ISPoblivious and could generate significant amounts of the crossISP traffic  a fact which reportedly polarized ISPs attitude towards peertopeer systems  88
Xunlei is the 10th largest Internet company in China and Kankan is its peerassisted ondemand streaming service with 314 million unique daily users as of the end of 2012 122 
In 2007 ChinaCache deployed about 500 cache servers in 8 districts of China out of which 50 are the core service nodes responsible for live streaming and over 400 edge caches deployed in a close proximity to users 117
NetSession 124 is a global peerassisted content distribution network originally developed by Red Swoosh a P2P content delivery company founded by Travis Kalanick and Michael Todd in 2001 and acquired by Akamai Technology in 2007 86 
Spotify is a popular ondemand music streaming service which according to 35exploits peerassistance to serve its 10 millionlarge user base around the world 
NetSession and LiveSky 6163 use standard DNS request routing techniques and redirect users based on their location and the current load on the edge servers if the nearest edge server is overloaded the request is redirected to the next nearest and less loaded one
Yet it is reported in 124 that only around 30 of NetSession users agree to participate in peerassistance
However  59 has raised an important concern with respect to ISPfriendly peerassisted design suggesting that localizing ISP traffic may negatively impact the quality of service 
Now MNOs have started to deploy their own CDN infrastructure for better control and management on the resources and bulk of video traffic 12118 
iii For input that is not in general position even more complex interactions such as vertexevents or multisplitevents are possible 23 
Several algorithms are known for constructing unweighted straight skeletons such as those by Aichholzer et al 1 Eppstein and Erickson 2 Cheng and Vigneron 6 Huber and Held 3 or Vigneron and Yan 7
These may be Diggle et al 2013 Molenberghs and Verbeke 2005 grouped into three broad classes i marginal models that seek to relate the marginal distribution of the response variable at each time point to explanatory variables ii subjectspecific or random effect models which account for heterogeneity between individuals by adopting regressiontype models with random subject effects and iii conditional or transition models that focus on the conditional distribution of the response at each timepoint given prior responses and possibly explanatory variables
The  prior responses and other covariates are treated on an equal footing as explanatory variables in a convenient parametric model for example a generalized linear model Diggle et al 2013 Chapter 10
Other ways to construct parsimonious higherorder Markov chain models have been proposed Raftery and Tavare 1994
There is no requirement that the determinants of  be immediately prior to  in the ordering An example concerning sideeffect profiles in a clinical trial of neuroleptica is given in Edwards 2000 Section 713
Some recent work has extended Bayesian network methodology to support contextspecific modelling Boutilier et al 1996 Myers and Troyanskaya 2007
In this paper we study a class of models due to Ron et al 1998 called acyclic probabilistic finite automata Note that we use the same acronym APFA for both singular and plural forms automaton and automata
So an APFA can be regarded as a timeheterogeneous contextspecific graphical model for discrete longitudinal data See Edwards and Ankinakatte in press Section 10 for a more precise comparison of APFA with Bayesian and Markov networks
The structure of the paper is as follows Section  2 introduces APFA from a statistical perspective Section  3 describes the model selection algorithm of Ron et al 1998 which in a modified form forms the core of the haplotype clustering algorithm implemented in the Beagle program Browning and Browning 2007ab that is widely used for phasing and imputation of DNA chip data
This section gives a brief introduction to APFA from a statistical perspective see Edwards and Ankinakatte in press for a more detailed exposition We first describe sample trees that are closely related to APFA
Sample trees are also known as prefix tree acceptors in machine learning Carrasco and Oncina 1994 and event trees in Bayesian decision theory Smith and Anderson 2008
 Probabilistic finite automata PFA are automata in which strings are generated in a probabilistic manner Vidal et al 2005ab and APFA are the subclass of PFA that generate symbol strings of constant length and so can be regarded as probability models for ordered sequences of discrete random variables
Thus an APFA expresses a set of contextspecific conditional independence constraints on the distribution of  and in this respect it resembles the dependence graph of a traditional graphical model Lauritzen 1996 Edwards 2000
The state merging operation and corresponding LRTs are studied in detail in Edwards and Ankinakatte in press It is shown that the tests are closely related to standard LRTs of independence  in twoway contingency tables
The algorithm proposed by Ron et al 1998 to select an APFA given a data sample proceeds as follows The sample APFA is constructed and then simplified in a series of state merging operations 
As mentioned above the idea is to merge two nodes  and  at stage  whenever the distributions of the future  given that the process has passed through  or  are similar To assess this Ron et al 1998 proposed a dissimilarity score between nodes  and  written  and a fixed threshold  
For various values of  we take  independent samples from a given APFA  using the data generating process described in Section  2 The simulated data sets of varying sizes are then used to build the APFA  using the model selection methods under comparison 
This process is performed three times once for each of the three data sets described in Section  5 
Suppose we are given an APFA  with known edge probabilities  and a commensurate data set  of the form  for 
 As a measure of how well the model  fits the data set Thollard et al 2000 and others suggest using a quantity called the per symbol log likelihood psLL for this purpose 
where  is the number of observations in  whose roottosink path in  passes through  and  are the known edge probabilities Note that since each observation in passes through  edges in  psLL is the average value of  obtained when  generates 
The mildew data stem from a cross between two isolates of the barley powdery mildew fungus Christiansen and Giese 1990 For each of  offspring  binary markers each corresponding to a single locus were recorded
The data were obtained from the experimenters are analysed in Edwards 1992 and are available from the Comprehensive R Archive Network CRAN being supplied along with the package gRapfa
The Duroc SNP data come from a study in which 4239 pigs of the Duroc breed were genotyped using the Illumina Porcine SNP60 BeadChip In all approximately 60 000 single nucleotide polymorphisms SNPs were recorded for each pig The data and its preprocessing are further described in Edwards 2013 
The data analysed here consist of  observations of  SNPs the first 100 SNPs on chromosome 1 From a statistical point of view SNPs are trichotomous variables two homozygotes and a heterozygote
To illustrate application of the methodology to data outside of genetics we consider the analysis of a social science data set The Biofam data set is derived from data obtained in a retrospective biographical survey carried out by the Swiss Household Panel in 2002
 The data are freely available to the scientific community and can be downloaded from CRAN as part of the package TraMineR Gabadinho et al 2011 
Note also that all six covariate nodes at stage one in the sample tree are merged into one node at stage one in Fig 4b implying that sex and religion do not affect the future life courses 
 Beagle version 332 was used to perform the model selection algorithm of Browning and Browning 2007ab 
A further advantage of the likelihoodbased approach is that is easily extended for example selection algorithms may consider steps in which more than two nodes are merged 
In the last years there has been a great deal of research on applying machine learning techniques and tools to processing of the functional Magnetic Resonance Imaging fMRI outputs 152227
The research has been mostly organized around the following three key areas 19 1 application of classification methods to fMRI data eg 18112326 including combinations of classifiers also known as ensemble models eg 172829 2 dimensionality reduction techniques eg 2310212330 and 3 spatiotemporal filtering eg 18 
The alignment issue is however often ignored and all snapshots taken while a certain stimulus is active are routinely averaged 27 in some cases trimming 12 initial snapshots or labelled with this particular stimulus 17
 Following 17 we have first selected a subset of voxels by crosstraining6 10 Support Vector Machines SVMs with linear kernels and then extracting top 200 contribution weights of voxels in terms of their absolute value from each model
 In our experiments a Support Vector Classifier svc from the PRTools Pattern Recognition Toolbox version 421 for MATLAB 12 has been used
 Thus any unordered pair x y with xBSlicey§yBSlicex creates an edge x y in an undirected graph in which a complete subgraph is equivalent to a backwardslice MDS and a backwardslice cluster is equivalent to a maximal clique
 Therefore the clustering problem is the NPHard maximal cliques problem Bomze et al 1999 making Definition 22 prohibitively expensive to implement
Recall that the definition of a coherent dependence cluster is based on an underlying dependson relation which is approximated using program slicing Pointer analysis plays a key role in the precision of slicing and the interplay between pointer analysis and downstream dependence analysis precision is complex Shapiro and Horwitz 1997
In testing dependence analysis has been shown to be effective at reducing the computational effort required to automate the testdata generation process Ali et al 2010 
They are used by management researchers and practitioners as well as other social scientists in the context of interventions to stimulate deliberative dialogue and the development of change proposals Beierle and Cayford 2002 Rowe and Frewer 2004
A substantial number of these have been developed by operational researchers over the past 50 years although the term ‘problem structuring itself was only introduced into the operational research OR lexicon a couple of decades ago Rosenhead 1989 2006 Rosenhead and Mingers 2001 2004
A distinguishing feature of PSMs compared with many other participative methods developed by social scientists is the use of models as ‘transitional objects to structure stakeholder engagement Eden and Sims 1979 Eden and Ackermann 2006 and provide a focus for dialogue Franco 2006 
 These evaluations are often based on explicit criteria reflecting the researchers experience a given theory a literature review andor stakeholder expectations generated through a consultative exercise Beierle and Konisky 2000 Rowe and Frewer 2004 
In some cases formal evaluation instruments have been developed and applied eg Duram and Brown 1999 Rowe et al 2004 Berry et al 2006 Rouwette 2011
What is clear from the literature however is that only a very small minority of studies eg Valacich and Schwenk 1995a Halvorsen 2001 Rouwette et al 2011 seek to compare between methods or across case studies undertaken by different researchers
A particularly significant study was undertaken by Beierle and Cayford 2002 who quantitatively compared broad classes of methods using a standard set of variables applied to 239 case studies of public participation 
Rowe and Frewer 2004 reflecting on social science approaches to evaluating participative methods classify them into three types
White 2006 argues that very similar distinctions have been made in the OR and group decision support literatures and preferences for universality to a greater or lesser extent or specificity reflect the positivist and interpretivist paradigms respectively
Our epistemological argument is that knowledge or understanding is always linked to the purposes and values of those producing or using it and is dependent on the boundary judgements that they make Churchman 1970 Ulrich 1994 Alrøe 2000 Midgley 2000
Hence the utility of an emergent approach for the evaluation of methods which remains open to new understandings as inquiry deepens eg Kelly and Van Vlaenderen 1995 Jenkins and Bennett 1999 Gopal and Prasad 2000 Allsop and Taket 2003
Relevant aspects of context identified by Jackson and Keys 1984 are the complexity of the issue being addressed using a systemic method and the relationships between the participants 
Ong 2000 discusses the facilitative effects of strong social capital and Alberts 2007 documents the negative effects of participant inexperience and ignorance of technical issues
Therefore exploring diverse perspectives eg as advocated by Checkland 1981 may lead to the identification of alternative possible ways of bounding a contextual analysis Ulrich 1994
McCartt and Rohrbaugh 1995 argue that a key aspect of managerial attitude is openness to change and participative methods are often ineffective without it
Kelly and Van Vlaenderen 1995 and Brocklesby 2009 concentrate on stakeholder interactions looking at how patterns of mistrust and miscommunication can become established and affect the use of participative methods
Champion and Wilson 2010 provide a particularly useful set of contextual variables to be considered based on a literature review and feedback from practitioners organisational structure influence of the external environment length of history of the problem in focus politics and personalities perceived implementation difficulty and the level of experience of stakeholders
Underpinning different boundary judgements may be quite different perspectives on the nature of the context Churchman 1970 
Purposes are closely linked with values and motivations McAllister 1999 and they are important to an evaluation because particular methods are likely to appear more or less useful depending on the purposes being pursued 
Different methods are generally good for different things Flood and Jackson 1991 and it is the perceived ‘fit between purpose and method that is important to evaluate a disjunction may be responsible for an attribution of failure
It is important to consider possible hidden agendas as well as explicitly articulated purposes These may significantly affect the trajectory of an intervention for instance through sabotage and thereby the evaluation of the method used Ho 1997
The process of application of a method is important as well not just the method as formally constructed Keys 1994 
Compare for example two significantly different accounts of soft systems methodology SSM Checkland and Scholes 1990 discuss how the methods from SSM should be utilised in a flexible and iterative manner while Li and Zheng 1995 insert some of the same methods into a ‘general systems methodology 
 Mingers 1997 describes these as the intellectual resources that the researcher brings into an intervention and it is important to be able to distinguish whether problems encountered in the use of a method derive from the limitations of the method itself or from the inadequate resources of the researcher 
Our questionnaire was first developed in the context of a research programme aiming to generate and evaluate new systemic problem structuring methods for use in promoting sustainable resource use Winstanley et al 2005 Hepi et al 2008
 Because of the latter the questions had to be reasonably generic Other authors suggest a number of different ways of producing generic evaluation criteria and these have been summarised by Beierle and Konisky 2000 and Rowe and Frewer 2004
facilitating consultation with land owners and community interest groups as part of a feasibility study for the construction of a new water storage dam Winstanley et al 2005
working with an Australian NGO and its stakeholders in exploring policy options to address the public injecting of illicit drugs Midgley et al 2005
facilitating workshops with the police and other stakeholders in the New Zealand criminal justice system to look at ethical issues associated with anticipated future developments of forensic DNA technologies Baker et al 2006
reviewing the process used by the New Zealand Ministry of Research Science and Technology to develop ‘roadmaps for longterm investments in environment energy biotechnology and nanotechnology research Baker and Midgley 2007
developing a new collaborative evaluation approach in partnership with regional council staff responsible for facilitating community engagement in sustainability initiatives Hepi et al 2008
At its most flexible a pluralist practice may involve the integration of several previously distinct methods into a new whole perhaps also incorporating the design of novel elements Midgley 2000
The field of numerical algebraic geometry 324 includes a wide array of algorithms for finding and manipulating the solution sets Vf of polynomial systems including both isolated solutions points and positivedimensional solution sets curves surfaces etc
All isolated solutions have associated to them a positive integer the multiplicity of the solution which is greater than 1 for singular solutions 3
Section 5 we describe the connection of this perturbation approach to the deflation approach of 9 the method of regenerative cascade 10 and a very early technique in the field known as the cheaters homotopy 17 in which the authors made use of a perturbation of fz for somewhat different reasons 
It is observed in 1724 that a perturbation can cause positivedimensional irreducible components to break into a possibly very large number of isolated solutions
 The major software packages for carrying out such computations include Bertini 2 PHCpack 25 and HOM4PS20 13
One very special type of homotopy is the parameter homotopy 1719
This and other optimizations of regeneration are described in 9
Much of the theory underlying the ideas of this article was known by Morgan and Sommese in the 1980s 18 and has since been repeated in various forms for example in 248
Let rkf denote the rank of the polynomial system fz ie the dimension of the closure of the image of fz fCN¾CN The rank of fz is an upper bound on the codimension of the irreducible components of Vf 24
It should be noted that this theorem is in fact a corollary to the main result in 17
 Regeneration can compute all of these nonsingular solutions 9
First we may trivially compute the multiplicity ¼zi of each isolated solution zi of fz0 as defined in 24
This is based on the fact proved as Theorem A1413 in 24 that each isolated solution zi will be the endpoint of ¼zi paths beginning at points in Vfp
In this section we consider several examples where perturbed regeneration provides some benefit All runs made use of Bertini 14 2 All reported timings except those of the last example come from runs on a 32GHz core of a Dell Precision Workstation with 12GB of memory
Next we consider the system cpdm5 from the repository of systems 25 but originally considered in 6
In the article 20 Morrison and Swinarski study a polynomial system with 13 equations having 51 isolated solutions
A more specialized sort of homotopy the 2homogeneous homotopy 24 performs even better in this case
Computing the numerical irreducible decomposition 324 the solution set consists of 10 irreducible components of various dimensions
As a final example we consider the ninepoint fourbar design problem exactly as formulated in Chapter 5 of 3
The regenerative cascade of 10 provides an equationbyequation approach to computing the numerical irreducible decomposition of the solution set of a polynomial system
Parameter homotopies are the right tool for this job as described briefly in Section 21 This idea has been implemented in Bertini 2 and Paramotopy 4 
The trick to such methods is choosing an intermediate system fvp which satisfies some necessary properties including that the solutions are smooth 
The numerical irreducible decomposition is the data type used in numerical algebraic geometry to store positivedimensional solution sets
 If desired monodromy and the trace test 323 could be used to find dZ points on each component Z
However the introduced class of multivariate FayHerriot models does not contain the Rao and Yu 1994 model or the GonzÃ¡lezManteiga et al 2008b model as special cases
As Chalmers 1995 has noted The really hard problem of consciousness is the problem of  experience When we think and perceive there is a whir of informationprocessing but there is also a subjective aspect
As Nagel 1974 has put it there is  something it is like  to be a conscious organism This subjective aspect is experience
Indeed early mathematical results about the brains functional units of shortterm memory STM and longterm memory LTM proved that the functional units of both STM and LTM are distributed patterns across networks of featureselective cells Grossberg 1968a 1968b 1973 
ART predicts that all brain representations that solve the stabilityplasticity dilemma use variations of CLEARS mechanisms Grossberg 1978a 1980 2007 2013a
Since ART was introduced in Grossberg 1976a 1976b it has undergone continual development to explain and predict increasingly large behavioral and neurobiological databases ranging from normal and abnormal aspects of human and animal perception and cognition to the spiking and oscillatory dynamics of hierarchicallyorganized laminar thalamocortical and corticocortical networks in multiple modalities 
The first paradigm is called Complementary Computing Grossberg 2000a 
Likewise because excitatory matching is needed to generate resonances that support conscious internal representations spatial and motor processes procedural memories Cohen  Squire 1980 Mishkin 1982 Scoville  Milner 1957 Squire  Cohen 1984 that use inhibitory matching cannot generate conscious internal representations
Perhaps the most basic fact about 3D vision and figureground perception is that its functional units are 3D boundaries and surfaces where these words need to be properly understood 
Neon color spreading was reported in Varin 1971 who studied a chromatic spreading effect that was induced when viewing an image similar to the one 
In summary end gaps and end cuts are formed as a result of two successive stages of spatial and orientational competition between contrastsensitive and orientationally tuned boundary cells Grossberg 1984a Grossberg  Mingolla 1985
These orientationallytuned simple cells  Hubel  Wiesel 1968 can respond to an oriented distribution of contrasts in response to scenic lines edges textures and shading not just edges alone Fig 4a
They cannot discriminate between darklight and lightdark contrasts or redgreen and greenred contrasts or blueyellow and yellowblue contrasts because they pool together inputs from simple cells that are sensitive to all of these differences Thorell De Valois  Albrecht 1984 to form the best possible boundaries
These boundary completion cells are often called bipole cells Cohen  Grossberg 1984 Grossberg 1984a Grossberg  Mingolla 1985 because they complete boundaries inwardly in an oriented manner between pairs bipoles! of boundary inducers 
More recently Brincat and Miller 2015 have reported neurophysiological data that support the distinction between category learning within the attentional system that includes prefrontal cortex and the orienting system that includes the hippocampus 
Among othersBanquet and Grossberg 1987 provide ERP markers during memory search Brincat and Miller 2015 provide oscillatory neurophysiological markers of the interplay between prefrontal cortex and hippocampus during learning and mismatch Otto and Eichenbaum 1992 provide neurophysiological data during hippocampal mismatch processing and Spitzer Desimone and Moran 1988 provide neurophysiological data from cortical area V4 during the learning of easy vs difficult discriminations a process that is regulated within ART by a vigilance parameter
Spitzer et al 1988 report in the difficult condition the animals adopted a stricter internal criterion for discriminating matching from nonmatching stimuli
More difficult discriminations at least under proper circumstances should lead to higher vigilance more mismatch events and thus more of the hippocampal novelty responses found by Brincat and Miller 2015 and Otto and Eichenbaum 1992 
Grossberg and Versace 2008 Palma Grossberg and Versace 2012 and Palma Versace Grossbergand 2012 have furthermore proposed how mismatchactivated acetylcholine release may regulate vigilance in laminar neocortical circuits that are described by spiking neurons during category learning
A related set of experiments concerns measuring more carefully what happens during both attentive recognition vs mismatch reset in response to sequences of familiar vs unfamiliar cues Here the following surprising discovery in Grossberg and Versace 2008 may provide a useful marker
In addition the Lundqvist et al 2016 article describes modeling ideas in which there is no temporal order represented in working memory although temporal order information is essential for proper functioning of a working memory
 The term attentional shroud for such a formfitting distribution of spatial attention was introduced by Tyler and Kontsevich 1995
Kelly and Grossberg 2000 explain stratification percepts including simulations of their conscious 3D surface percept properties 
Grossberg and Yazdanbakhsh 2005provide model explanations and simulations of these transparency percepts among others including simulations of the consciously seen surface percepts much as the model has simulated 3D surface percepts in response to many other stimuli including stereogram images 
Crick and Koch 1995 also proposed that visual awareness may be related to planning of voluntary movements but without any analysis of how 3D vision occurs 
The classical article of Driver and Mattingley 1998 reviews visual neglect properties in individuals who have experienced IPL lesions particularly in the right hemisphere
A neglect patient who appeared to be blind in the left visual field when fixating straight ahead or to her left could detect events in her left visual field when she fixated to the right Kooistra  Heilman 1989
The implicit knowledge of parietal patients includes object attributes of neglected stimuli such as their color shape identity and meaning Mattingley Bradshaw  Bradshaw 1995 McGlincheyBerroth Milberg Verfaellie Alexander  Kilduff 1993
Lesions of the right IPL that cause visual neglect also impair the ability to maintain visual attention over sustained temporal intervals Rueckert  Grafman 1998 whether for visual or auditory attention Robertson et al 1997
ARTSCANs explanation of visual crowding Foley et al 2012 and with it an explanation of the Koch and Tsuchiya 2007 discussion of how subjects can attend to a location for many seconds and yet fail to see one or more attributes of an object at that location 
Foley et al 2012 have supported this qualitative explanation of visual crowding by using the distributed ARTSCAN or dARTSCAN model to simulate how objects that have their own shrouds when viewed by the fovea may be enveloped by a single shroud when they are moved to the retinal periphery 
That is the main point of the article by Grossberg Mingolla and Ross 1994 whose title A neural theory of attentive visual search Interactions of boundary surface spatial and object representations emphasized the role of these four kinds of processes
Further experimental and modeling studies of crowding and visual search and their interactions from this perspective are much to be desired and as illustrated by the modeling simulations of Fazl et al 2009 and Foley et al 2012 need to include surfaceshroud resonances as one part of a unifying theory
 In one striking classical example alternating displays of an original and a modified scene are separated in time by brief blank fields Rensink et al 1997
This property is called contrast normalization Grossberg 1973 1980 Heeger 1992 
Foley et al 2012 also model why the remainder of a scene does not go totally dark when such a surfaceshroud resonance of focused spatial attention forms
Mitroff and Scholl 2005 showed in this case that object representations can be formed and updated without awareness by making changes in displays when they were out of awareness 
Chiu and Yantis 2009 used rapid eventrelated MRI in humans to provide strong evidence for the ARTSCAN prediction of how a surfaceshroud resonance in the WhereHow stream protects an emerging viewinvariant category from being prematurely reset in the What stream
In particular Cao et al 2011 developed the positional ARTSCAN or pARTSCAN extension of the ARTSCAN model to explain how these additional object category invariances can be learned 
They have used this extended model to simulate neurophysiological data of Li and DiCarlo 2008 see also Li  DiCarlo 2010 which show that features from different objects can be merged through learning within a single invariant IT category when monkeys are presented with an object that is swapped with another object during an eye movement to foveate the original object
These target positions hereby control shifts of spatial attention across an attended object and have properties that Cavanagh Hunt Afraz and Rolfs 2010 have called attention pointers
This is proposed to occur in cortical area V3A Fig 21 As noted by Caplovitz and Tse 2007 p 1179 neurons within V3Aprocess continuously moving contour curvature as a trackable featurenot to solve the ‘ventral problem of determining object shape but in order to solve the ‘dorsal problem of what is going where
ART proposes that this happens because both resonances interact with shared visual cortical areas such as V4 and can thus synchronize with each other often with gamma oscillations Fries 2009 Grossberg  Versace 2008
 An outflow representation of the current handarm position called the present position vector or P is subtracted from the target position to compute a difference vector or D Georgopoulos Kalaska Caminiti  Massey 1982 Georgopoulos Schwartz  Kettner 1986 that codes the direction and distance that the handarm needs to move to reach the target 
The Vector Integration to Endpoint or VITE model Fig 27 left panel Bullock  Grossberg 1988 modeled these processes to clarify how the Three Ss of reaching are carried out the flexible choice of motor Synergies and their Synchronous performance at variable Speeds
A refinement that sheds the most light on auditoryvisual homologs of reaching and speaking circuits is called the DIRECT model Bullock Grossberg  Guenther 1993 which also learns through a circular reaction
During the development of the DIRECT model by Bullock et al 1993 an evolutionary rationale was noted for why both the handarm and speech articulator systems may use similar indeed homologous neural circuits namely eating preceded speech during human evolution MacNeilage 1998 and skillful eating requires movements that coordinate handarm and mouththroat articulators including motorequivalent solutions for reaching and chewing 
The auditory continuity illusion Bregman 1990 illustrates ART properties during auditory streaming
Grossberg 1978a 1978b introduced a neural model of working memory upon which the more recent models listed above consistently built
A more recent name for this class of models is competitive queuing Houghton 1990 
When an ItemandOrder working memory can store repeated items in a sequence it is called an ItemOrderRank working memory Bradski Carpenter  Grossberg 1994 Grossberg  Pearson 2008 Silver et al 2011
Phonemic restoration Warren  Sherman 1974 illustrates the operation of ART mechanisms during speech perception in much the same way as the auditory continuity illusion represents them during auditory streaming
Jones Farrand Stuart and Morris 1995 reported similar performance characteristics to those of verbal working memory for a spatial serial recall task in which visual locations were remembered 
Agam Bullock and Sekuler 2005 reported psychophysical evidence of ItemandOrder working memory properties in humans as they performed sequential copying movements and Averbeck et al 2002 Averbeck Crowe Chafee and Georgopoulos 2003a 2003b reported neurophysiological evidence for such a working memory in monkeys during performance of sequential copying movements
The motortoauditory selection process mechanistically explicates the motor theory of speech perception Galantucci Fowley  Turvey 2006 Liberman  Mattingly 1985
The Neural Normalization Network or NormNet model Ames  Grossberg 2008 proposes that speaker normalization specializes the same kinds of neural mechanisms that are used to form auditory streams
This proposal clarifies how speaker normalization can transform auditory signals right after they are separated into separate streams for purposes of speech and language classification and meaning extraction yet how the frequency content of the streams can be preserved for purposes of speaker identification in a separate processing stream as illustrated in the ARTSPEECH architecture Fig 33 right panel of Ames and Grossberg 2008 
Boardman et al 1999 developed the PHONET model to quantify how T and S working memories can use asymmetric TtoS gain control to create rateinvariant representations of individual speech syllables or words 
A classical example of this phenomenon was reported by Repp 1980 Repp constructed VCCV syllables from the syllables ib ga and ba to form ibga and ibba
The comparison between resonant fusion and resonant reset that plays an important role in explaining the Repp 1980 data on category boundary shifts also helps to explain data about the way in which masking stimuli can influence error rates and reaction times during lexical decision tasks
Grossberg and Stone 1986b explained the paradoxical pattern of experimental results in terms of how the ART Matching Rule works in different priming situations including the inability of topdown expectations to act before the mask interferes with the persistence of word and nonword target representations in working memory
The TELOS model Brown et al 2004 predicted how agreement between prefrontal and parietal representations of a target position causes a parietalprefrontal resonance that selects this target position and opens the correct basal ganglia gate while also enabling basal gangliamediated release in a different part of the brain of a contextuallyappropriate movement command to that position
Subsequent neurophysiological data of Buschman and Miller 2007supported this prediction by describing such a parietalprefrontal resonance during movement control and Pasupathy and Miller 2004 additionally described different time courses of activation in the prefrontal cortex and basal ganglia that are consistent with how basal gangliamediated gating of prefrontal cortex occurs in TELOS
The lisTELOS model Silver et al 2011 extended the TELOS model to explain and predict how sequences or lists of eye movements can be carried out while continuing to simulate everything that TELOS could 
On the other hand it does propose how parallel neural mechanisms for rateinvariant and speakernormalized representations of speech and for pitchdependent and rhythmdependent speech intonation Ladefoged  Disner 2012 may interact to achieve online sequencing of syllables into fast smooth and rhythmically organized larger utterances Ackerman 2008 p 265 including how these several kinds of information are learned stored and combined during fluent performance and conscious awareness thereof 
Learning also goes on throughout life of a parallel circular reaction that links learned spectralpitchandtimbre categories for the recognition of heard sounds Table 2 which are not speakernormalized to the motor synergies that control the pitches generated by the vocal folds Sundberg 1977
Neural models of cognitiveemotional resonances began with the articles of Grossberg 1971 1972a 1972b and Grossberg 1975 at a time when there was a major split between studies of cognition as exemplified by the work of Chomsky 1957 in linguistics and of emotion as exemplified by the work of Skinner 1938 on instrumental conditioning
For simplicity consider only the simplest kind of reinforcement learning called Pavlovian or classical conditioning Kamin 1968 1969 Pavlov 1927 during which a conditioned stimulus or CS that initially may have no emotional significance is paired with an unconditioned stimulus or US that can from the start generate a strong emotional response 
Damasio 1999 has derived from clinical data what can be viewed as a heuristic version of the CogEM model and has used it to describe what can be interpreted as cognitiveemotional resonances that support the feeling of what happens 
Damasio 1999 p 171 notes Attention is driven to focus on an object and the result is saliency of the images of that object in mind leading to what Damasio calls core consciousness Damasio 1999 also went on to write I do not know how the fusing blending and smoothing are achieved p 180 
These include the dual competition model of Pessoa 2009 p 160 The proposed framework is referred to as the ‘dual competition model to reflect the suggestion that affective significance influences competition at both the perceptual and executive levels—and because the impact is caused by both emotion and motivation
The ARTSCENE model Grossberg  Huang 2009 and ARTSCENE Search model Huang  Grossberg 2010 illustrate how humans accomplish these goals
Tamietto and de Gelder 2010 have reviewed several different kinds of experimental evidence that led them to a similar viewpoint but without mechanisms of adaptive resonance to derive mechanistic conclusions 
Clark and Squire 1998 p 79 postulated that normal humans acquire trace conditioning because they have intact declarative or episodic memory and therefore can demonstrate conscious knowledge of a temporal relationship between CS and US trace conditioning requires the acquisition and retention of conscious knowledge and would require the hippocampus and related structures to work conjointly with the neocortex
In addition to explaining data about normal delay and trace conditioning the nSTART model explains and simulates many subtle data about how learning and memory consolidation are influenced by different brain lesions Franklin  Grossberg 2016
 Crick and Koch 1990described two forms of consciousness a very fast form linked to iconic memory and a slower one wherein an attentional mechanism transiently binds together all those neurons whose activity relates to the relevant features of a single visual object 
For example the neural global workspace of Dehaene 2014 which builds upon the global workspace of Baars 2005 claims that consciousness is global information broadcasting within the cortex to achieve massive sharing of pertinent information throughout the brain p 13
Continuing in the spirit of Edelman and Tononi 2000 Tononi 2004 defined a scalar function Φ the quantity of consciousness available to a system as the value of a complex of elements
Unlike traditional teambased work however members of the crowd are distributed and in many cases without those obligations as found in companies longterm contracts or roles 1 The crowd presents a pool of experts who are connected amongst themselves forming a social network
Most approaches model the problem as finding the best match of experts to required skills taking into account multiple dimensions from technical skills cognitive properties and personal motivation 24 
Sozio and Gionis describe the community formation problem 6 Given a set of fixed members the approach expands the team up to a maximum upper size boundary such that the communication cost within the community remains small
Anagnostopoulos et al 7address fair task distribution within a team 
Yang et al 8 apply integer programming to determine the best set of group members available at a certain point in time Their temporal scheduling technique considers the social distance between group members to avoid lacking too many direct links 
Craig et al 9 propose an algorithm for reasonably optimal distribution of students into groups according to group and student attributes
Xie et al 10 aggregate a set of recommender results to optimally compose a package of items given some relation between the individual items and an overall package property eg holiday package
To the best of our knowledge Theodoros et al 13 discuss the only team composition approach that specifically focuses on the expert network for determining the most suitable team Our approach differs in three significant aspects 
First we model a tradeoff between skill coverage and team connectivity whereas 13 treats every expert above a certain skill threshold as equally suitable and ignores every expert below that threshold
Also Singh 14 shows that a densely connected team is vital for successful open source developer cooperation Most importantly we apply recommendations instead of direct interaction links when the underlying network becomes too sparsely connected
Analysis of various network topologies 1516 has demonstrated the impact of the network structure on efficient team composition
Investigations into the structure of various realworld networks provides vital understanding of the underlying network characteristics relevant to the team composition problem 1819
Complementary approaches regarding extraction of expert networks and their skill profile include mining of email data sets 2223 or open source software repositories 24 
Additional sources include scientific publications and financial data 25 social network page visits 26 telecommunication data 27 and online forum posts 28
Related research efforts based on nonfunctional aspects ie nonskill related aspects can also be found in the domain of service composition 29 Here services with the required capabilities need to be combined to provide a desirable overall functionality 
For instance work by 4041 discusses link prediction based on similarity focusing on structural graph properties such as number of neighbors and number of inout links
In social trust networks 42 recommendations reflect transitive relations among members In that case unconnected nodes in a trust network are connected through an intermediate node that mediates second hand knowledge among its neighbors 
In the future direct trust between humans will play an ever more important role as privacy remains a largely unsolved challenge 43 
Hence we believe that establishing explicit trust in social networks eg 4445 respectively becoming aware of distrust will become a significant factor in team formation
Since interactions and collaborations on the Web are observable systems can analyze tasks performed in the past in order to determine network structures and member profiles automatically 4642
Simulated Annealing 47 SA and Genetic Algorithms 48 GA are two common heuristics suitable for the underlying problem type
Previous work suggests dynamic adaptation for crossover and mutation probabilities 49 whereas 50 applies clustering techniques to determine suitable values 
The correlation of population size and cross over is investigated in 51 
In the case of simulated annealing work on optimizing parameters is similarly problem specific 52 addressing a graph partitioning problem 53 focusing on the longest common subsequence problem and 54 dealing with distributing workload across multiple processors
 Parameterless multiobjective algorithms such as NSGAII 55 provide multiple paretooptimal solutions to the team formation problem without setting α to any particular value 
Finding a single shortest path is in OECand  CandlogCand 56 
The bullwhip effect also has a close link with the philosophy of lean production Ohno 1988 Mura—the waste of unevenness—is the failure to smooth demand and is recognised as the root cause of both Muda the seven lean wastes and Muri the waste of overburden Indeed Ohno 1988 discusses the benefits of bullwhip avoidance
Geary Disney and Towill 2006 classified five routes to increase our knowledge of bullwhip effect and 10 principles to reduce it
 Miragliotta 2006 reviewed bullwhip research in three categories empirical assessment causes and remedies and then proposed a new taxonomy to model this problem 
 Giard and Sali 2013 categorised 53 bullwhip papers within 13 coordinates including modelling approaches demand models measures and causes 
Other reviews are more conceptually oriented attempting to offer a new perspective on bullwhip Towill Zhou  Disney 2007
Some reviews are not solely confined to the bullwhip effect but also cover other supply chain modelling issues Beamon 1998 Min  Zhou 2002 Sarimveis Patrinos Tarantilis  Kiranoudis 2008 
Interestingly a similar phenomenon between PG and its wholesalers has been documented during 1910s Schisgall 1981 
Forrester 1961 first formalised the variance amplification effect using the ‘industrial dynamics approach He later established a simulation experiment mimicking the decision making behaviour in supply chains—the famous ‘Beer Game
 Sterman 1989 published 20 years of data from the game attributing the amplification to the tendency that players overlook the inventoryonorder the orders placed but not yet received a cause of amplification known as ‘irrational behaviour
The production smoothing hypothesis Holt Modigliani Muth  Simon 1960 assumes that production fluctuations increase the operational cost to the manufacturer by inducing excess machine setup idle time and workforce hiringfiring 
There is also a tradeoff between inventory cost and production cost due to the stabilizing effect of inventory Baganha  Cohen 1998 Disney Towill  van de Velde 2004 
Chen and Samroengraja 2004 showed that when the cost function is concave the replenishment policy that minimises order fluctuations is not necessarily the one that minimises total cost
Under nonstationary demand it is necessary to perform difference operations on the time series That is to measure bullwhip by the variance of order changes instead of the variance of orders itself West 1986 
Alternatively one may compare the difference between order variances and demand variances which has been proved to be finite Gaalman  Disney 2012 If the inventory system is to be modelled linearly then the variance ratio is convenient because it coincides with an engineering concept called the noise bandwidth a concept with an established theoretical basis Åström 1970
In 1960 Holt et al 1960 proposed the production smoothing model assuming that rational decisions regarding production quantities would lower costs by levelling production with inventory being used as a buffer Efforts have been made to optimise this model under various assumptions Gaalman 1978 Schneeweiss 1974 Zangwill 1966 
 Quite contrarily many empirical studies have found amplification between retail sales and production orders as well as positive correlation between demand and inventory Blanchard 1983 Blinder 1986 Blinder  Maccini 1991 West 1986 
 Ghali 2003 showed that production smoothing can be found only in a small number of industries where seasonality is stable and inventory holding cost is low
 In 75 industries Cachon et al 2007 observed that 61 exhibited bullwhip when seasonality was removed but only 39 when not Similar findings have been reported by Bray and Mendelson 2012 on the basis of firmlevel rather than industrylevel data 
Baganha and Cohen 1998 observed that bullwhip effect appears in the wholesalers echelon and argued that the wholesalers inventory acts as a stabiliser in the chain
 Mollick 2004 described evidence of production smoothing in the Japanese automotive industry where the production smoothing is more common due to the prevalence of Heijunka levelling and JustInTime manufacturing strategies 
 Cantor and Katok 2012 introduced a cost for production and order changes and found that production is smoothed when demand is seasonal and that the smoothing behaviour is more eminent when the production change cost is high
The simplest demand model is an independently and identically distributed iid Gaussian white noise process Deziel  Eilon 1967 This model has some mathematical advantages but may be an oversimplification as it overlooks temporal correlation in the demand signal
More complex ARIMA models for demand have also been studied AR2 ARp Luong  Pien 2007 ARMA11 Alwan Liu  Yao 2003 ARMA22 Gaalman  Disney 2009 and ARMApq Gaalman 2006
A wide range of forecasting methods have been investigated in the bullwhip literature Chen et al 2000a and Duc et al 2008a studied the moving average MA forecasting method while Chen Ryan and SimchiLevi 2000b and Dejonckheere Disney Lambrecht and Towill 2003 investigated the simple exponential smoothing SES method 
The impact of more sophisticated forecasting methods such as Holts Browns and Damped Trend forecasting was discussed by Wright and Yuan 2008 and Li Disney and Gaalman 2014 
Another interesting topic is the relationship between forecast accuracy and total cost Zhang 2004a suggested that MMSE forecasting minimises inventoryrelated cost This was supported by Hussain et al 2012 in a simulation study 
However according to some empirical Flores Olson  Pearce 1993 and analytical research Hosoda  Disney 2009 the most accurate forecasting does not always result in an optimal supply chain when local bullwhip or global inventory costs are taken into account Disney Lambrecht Towill  Van de Velde 2008 Gaalman 2006 Gaalman  Disney 2006 Gaalman  Disney 2009
Forrester 1961 highlighted that the delays in information and material flow aka the leadtimes is a driving factor of demand amplification
 Lee et al 1997 and Chen et al 2000a argued that bullwhip increases in leadtime as did Steckel Gupta and Banerji 2004 and Agrawal Sengupta and Shanker 2009
Modelling leadtime as a random variable mimics the volatility of reallife logistics Chatfield Kim and Harrison 2004 Kim Chatfield Harrison and Hayya 2006 and Duc Luong and Kim 2008b showed that order variability increases with leadtime variability a result that is also supported by the behavioural experiment conducted by Ancarani Di Mauro and DUrso 2013
Statedependent leadtimes have been examined by So and Zheng 2003 and Boute Disney Lambrecht and Van Houdt 2007 and both studies found that bullwhip is underestimated if the endogeneity of leadtime is neglected
The automatic pipeline inventory and orderbased production control system APIOBPCS proposed by John Naim and Towill 1994 is mathematically equivalent to Stermans 1989‘anchoring and adjustment heuristic
Deziel and Eilon 1967 proposed the first linear proportional production control policy where the same feedback parameter is assigned to both the inventory and pipeline levels
General guidance on tuning the feedback parameters is given by Balakrishnan Geunes and Pangburn 2004 Papanagnou and Halikias 2008 Graves Kletter and Hetzel 1998 and Boute and Van Miegham 2015 describe other proportional ordering policies
This problem has been investigated under sS Caplin 1985 Kelle  Milne 1999 QT Cachon 1999 Lee et al 1997 and base stock Sucky 2009 policies
Lee and Whang 2000 summarised the common schemes for sharing information on inventory levels sales data sales forecast order status and productiondelivery schedules
From experimental and analytical evidence some authors have found that information sharing alone cannot eliminate the bullwhip effect Chen et al 2000a Croson  Donohue 2006 Ouyang 2007 Sodhi  Tang 2011 
It has been discovered that if the order quantity is constrained to nonnegativity as opposed to the ‘costless return assumption Lee et al 1997 then highly complex and sophisticated dynamical behaviours can be found in supply chains Mosekilde  Larsen 1988 
Moreover this dynamical complexity is also amplified along the chain in an effect known as chaos amplification Hwarng  Xie 2008 
Ouyang and Li 2010 proposed a general supply network model that allows for transhipment information sharing and collaboration they identified conditions for bullwhip
Akkermans and Vos 2003 measured workload in a major US telecom company 
Among these are Zhang and Burke 2011 who showed that introducing price fluctuations can either exacerbate or mitigate the bullwhip effect based on the auto and mutualcorrelation between price and demand 
Recently Sodhi Sodhi and Tang 2014 incorporated a discretely distributed stochastic price into the economic order quantity model 
Molodtsov 1 proposed an uncertaintysoft set theory that is completely new approach for modeling vagueness
Soft set has been extensively and successfully applied in decision making 219 data analysis 2022 forecasting 23 simulation 24 evaluation of sound quality 25 rule mining 26 and so on
Combining soft sets with others mathematical theories such as fuzzy sets 152731 rough set 283133 vague sets 34 intervalvalued fuzzy sets 101235 intervalvalued intuitionistic fuzzy soft set 36 intuitionistic fuzzy soft set 353739 and so on has come forth rapidly to meet various demands in practical situations
Maji et al considered the initial level reduction soft set with the help of rough set approach 2
However Chen et al pointed out that the errors of soft set reduction and presented a new notion of parameterization reduction in soft set 40
Kong et al analyzed two cases suboptimal choice and added parameter set and introduced the definition of normal parameter reduction and its algorithm in soft set 42
Gong et al proposed two parameters reduction algorithms based on bijective fuzzy soft set system 14
Kong et al 42 introduced the definition of normal parameter reduction in soft set and its algorithm 
Managers spend up to one fifth of their working time with conflict resolution and negotiation 1563 
They increasingly negotiate via electronic media such as email emeeting and enegotiation systems 73 
Electronic negotiations are not mere translations of traditional negotiations onto electronic media but rather they provide additional value by supporting the decision making andor communication process 6274 
 We know from empirical evidence that the way information is presented strongly influences human perceptions preferences and decision making eg 576 
Except for the suggested utilization of the negotiation dance graph 56 to date only a history graph has been proposed and implemented 2763
Using the NSS Negoisst 6263 subjects were divided into three treatment groups using the three different representation aids on the negotiation process a table a negotiation history graph or a negotiation dance graph
The paradigm of cognitive fit suggests that effective and efficient problem solving is obtained when all tools or aids used in the problem solving process correspond to the requirements of the task 7880 
 Numerical systems require wellstructured inputs in a predefined format 19 show impacts of variables on results 7 and provide assessment scores 36 
Graphs have visuospatial properties meaning they stress information on data relationship rather than on linguistic intelligence 45 
They also allow for the grouping of information 35 and the establishment of associations among the values of each information package or variable across time periods without addressing the elements separately or analytically eg 47879
 Many perceptual inferences including perceiving and drawing inferences are automatically supported at low cognitive costs 834 
 Empirical research has reported that subjects provided with graphical formats are more effective in trend pattern and time sequence data detection eg 126877 and in task execution in terms of processing time eg 313244
Concerning the level of complexity tables outperform graphs regarding time and decision accuracy in simple decision making settings 4558 
The most common and straightforward way to provide users of NSS with information about multiissue offers is to present the utility values 27 
One way to visualize the negotiation process graphically is the history graph see Fig 2 which has already been implemented in NSS 636482 
To answer the research questions an electronic negotiation support system is required that supports business negotiations rich communication support and various forms of decision support Negoisst is a webbased NSS offering sophisticated support and formal document management 6263 
In the negotiation theory three further indicators are often used to measure the quality of negotiation outcomes joint outcome as an indicator for efficiency contract balance as an indicator for fairness and negotiator satisfaction with agreement as a holistic assessment 163357 
Decision makers aware of this fact should consequently ask their opponent for fair treatment and stress the importance of fairness more often 47 
The visualization of changes in utilities due to modifications in single issues in the negotiation dance graph helps negotiators to identify Pareto movements and efficient alternatives 56 
We assume that the visibility of differences in utilities during the negotiation process makes it more difficult to demand the bigger share of the cake 60 
Consequently we expect negotiators who reach higher joint outcomes and more balanced agreements will be more content eg 173777
 First several studies show that the level of conflict in simulation cases influences results significantly 1153 
 A particular focus should be placed on the stages in which information is acquired and in which the information is evaluated The issue of time duration of the experiment must be taken into account 51 
The effects of additional support provided by graphical aids are often seen as a tradeoff between the benefits of minimizing errors and the cognitive effort or time needed in a particular task environment 22 
In the present study there was an imposed time deadline for all users thus the variable time was kept constant and all impacts could be considered only with regard to proxies for the quality of decisions
 Raiffa 56 argues that a negotiation resembles a dance of negotiation partners 
 Euler diagrams support the interpretation of grouping information since elements in a common set are located in the same region 31
This was carried out by analyzing the layout and structure of socially constructed texts of organizational communication Yates  Orlikowski 1992 amongst people in a particular workplace or in a community of practice CoP as described by Wenger 1999 where genre in a textual sense is sometimes defined as a group of texts or documents that share a communicative purpose as determined by the discourse community which produces andor reads them Swales 1990 
Collins Mulholland and Watt 2001explained that what the community sees as important will be reflected in the implicit structures found in the objects they create and share and as Watt 2009 chap 8 
Layout in organizational communities causes people to focus perceptually on key parts of the text Schmid  Baccino 2002 and our empirical research has previously demonstrated that people use layout and other related cues to focus on key parts of the text Clark 2008 Clark Ruthven  Holt 2008 Clark Ruthven  Holt 2010 Clark Ruthven Holt  Song 2012 
The reader is able to perceive the meaning through interaction with the cues which exist on the outside and inside of the frame Frow 2006 chap 5  a term that Frow uses synonymously with genre
To examine genre and ways of perceiving we used specific eye movement behavior metrics or ‘ocular metrics Rayner 1998 which have been in fairly common use in contemporary eye tracking experiments ie Scanpath Duration and Scanpath Length cf Goldberg and Kotval 1999 p 638 
Aristotle 1954 considered that whatever was perceivable by the individual was reality 
Outside objects imposed upon the senses and due to the power of reason the mind was able to extricate the form which determined the nature of the perceived object Breure 2001 
 We contend that the specific contexts of researchers guide the way they delineate genre as Kwaśnik  Crowston 2005argue the researcher chooses the definition applicable to the current context of the study
Readers viewing texts are always involved or relate to the complete arrays of textual meaning This is quite closely related to Semiotic ‘intertextuality a term that is said to have been coined by the poststructuralist semiotician Kristeva 1980 
In other words an author or artist refers to an earlier work and subsequently converts a previous creation with it then being referred to in the new text 
Hirsch Jr 1967 p 76 explains that genre is an interpretative process called into being by the fact that all understanding of verbal meaning is necessarily genrebound
Hirschs explanation could be appropriately linked to the work pertaining to perceptual hypotheses by Gregory 1980 or indeed as we like to refer to it ‘perpetual perceptual hypotheses where we are continuously trying to ascertain what an object or text is 
 Lorch 1989 identifies textual signals such as headings previews summaries titles numeric signals and so on All texts are accompanied by these types of cues or signals which ‘present the texts to the reader or ensure the presence of the texts in the world
For the purpose of this study genre was defined by its purpose sometimes known as substance but mainly by form see Fig 2 for categories of form as described in Dewdney VanEssDykema  MacMillan 2001 and Yates  Orlikowski 2002 p 15 
The constructivists assert that the final goal in the perception process is recognition which would require intense cognitive processing for example Gregory 1980 and his theory of perceptual hypothesis 
Many consider skimming and scanning to be techniques related to searching as opposed to strategies for reading for example Just  Carpenter 1987 In fact they are both correct but reading and searching are two different contexts 
The mental spotlight is quite a helpful analogy to describe a task such as searching for a keyword etc Masson 1983 describes skimming for most of us rapid reading involves some form of skimming in which we try to focus on information relevant to our goal and skip over irrelevant information 
 Holmqvist et al 2011suggests that a sequence of long saccades is likely to reflect skimming over the text 
As Rayner 2009 pp 1484 points out equivalents between visual search and scene perception are greater than with reading in that visual saliency plays a greater role in directing fixations
In each case different ocular behavior would be expected Rayner 2009 pp 1484 Many methodologies and algorithms have been devised for the detection of reading firstly for a baseline then secondly comparing those results to other data to detect skimming scanning or both for example Campbell  Maglio 2001 Buscher Dengel van Elst  Mittag 2008 and Buscher Dengel  van Elst 2008 
 In Watt 2009 p 171 chap 8 he opines genres behave as affordances and in essence can be filtered and categorized by form
Gibsons affordances are intended to describe how meaning and perception are interrelated he argues in Gibson 1986 p 127 that instead of perceiving objects for example texts and then adding meaning later there are visual combinations of invariant and distinctive characteristics of objects which provide cues on how to act and behave in relation to these objects in this case textual emails
Alternatively Toms  Campbell 1999b in their study leaned towards the Constructivist perception for recognition process since they aimed to contrast the content function and form in order to discover whether readers can perceive and process form on its own or need semantic content to identify it 
Although the research carried out by Toms  Campbell 1999a 1999b Toms Campbell  Blades 1999 Toms 2001 and then Watt 2009 chap 8 seems to indicate a leaning towards one process or another Watt Ecological and Toms Constructivist the latter does explore Ecological in her thesis Toms 1997 it may emerge that they are both correct or indeed wrong but for different information searching tasks and in different contexts
We conducted an analysis of the eyetracking data studying such basic metrics based on fixations saccades and number of genres identified correctly along with length of time to identify cf Clark et al 2010
 Goldberg  Kotval 1999conducted computer interface evaluations with twelve participants testing the interfaces whilst analyzing their scanpath behavior 
Lorigo et al 2006 in an extension of the work in Pan et al 2004 used scanpath fixations patternfinding to compare the differences in gender and task type during a web search
Joachims Granka Pan Hembrooke  Gay 2005 used scanpath measurements to examine the reliability of implicit feedback generated from click through data in Web searches
 Brandt  Stark 1997 showed their participants visual imagery of irregularlychequered diagrams
The experimental setup of the evaluation was based on commonly used standards cf Joachims et al 2007 and Kelly 2009 
The experimental procedures such as questionnaire design were based on methods and protocol used by previous interactive experiments Dupont et al 2010 Harper  Kelly 2006 Huang et al 2006 Kelly et al 2007 Kelly Harper  Landau 2008 White Ruthven  Jose 2002 White et al 2006 
The experimental eyetracking data was input into the SPSS software along with the data used in Clark et al 2010 and then statistically evaluated
The use of skimming and scanning techniques was detected by referring to the 20 possible permutations found in Campbell  Maglio 2001 p 3and Buscher Dengel van Elst and Mittag 2008 scoring was based on the short medium or long movements which were given a particular score whenever they occurred on the X or Y axes gaze point
 Just like Watt 2009 chap 8  in his timed response design  we balanced for length and still found a very strong effect an interaction  between layout representations which indicated that genre speed was a factor independent of length as in Clark Ruthven  Holt 2009b and Clark et al 2010
The scanpath duration measure is used to see how much time participants spend on processing information and complexity Goldberg  Kotval 1999 p 638 a longer scanpath duration indicates participants are spending more time processing information and hence classifying information is far more ‘intensive 
We intend to continue our research by looking at other genres on other web communities of practice notably Wikipedia to expand on previous work in Clark et al 2009a and Clark et al 2012 and using in addition web data collected from two university intranets 
When I was looking for a PhD dissertation topic I accidentally came across a paper by W Larimore on Statistical Inference on Random Fields in the Proceedings of the IEEE 6
A deterministic alternative known as the iterated condition mode was presented in the paper by Besag 3 on statistical analysis of dirty pictures that appeared in the Journal of Royal Statistical Society
The vision of Cloud Computing is to provide computing power as a utility like gas electricity or water 1
This work can be integrated into the Foundations of Selfgoverning ICT Infrastructure FoSII project 2 but is on its own completely selfsufficient
Besides the already implemented LoM2HiS framework 3 that takes care of monitoring the state of the Cloud infrastructure and its applications the knowledge management KM system presented in this article can be viewed as another building block of the FoSII infrastructure
4 proposes an approach to manage Cloud infrastructures by means of Autonomic Computing which in a control loop monitors M Cloud parameters analyzes A them plans P actions and executes E them the full cycle is known as MAPE 5
According to 6 a MAPEK loop stores knowledge K required for decisionmaking in a knowledge base KB that is accessed by the individual phases 
On the other hand we gathered real world data from monitoring scientific workflow applications in the field of bioinformatics 9
These workflows need a huge yet unpredictable and varying amount of resources and are thusdue to the needed flexibility and scalabilitya perfect match for a Cloud computing application 10
 1718 focus on VM migration and 19 on turning on and off physical machines whereas our paper focuses on VM reconfiguration
Stillwell et al 22 in a similar setting define the resource allocation problem for static workloads present the optimal solution for small instances and evaluate heuristics by simulations
Nathani et al 23 eg also deal with VM placement on PMs using scheduling techniques 
24 react to changing workload demands by starting new VM instances taking into account VM startup time they use prediction models to have VMs available already before the peak occurs
Other works such as 25 have already considered the last escalation level see Section 4 ie outsourcing of applications to other Clouds 
Paschke and Bichler 26 look into a rule based approach in combination with the logical formalism ContractLog
Bahati and Bauer 28 also use policies ie rules to achieve autonomic management 
Fourthly compared to other SLA management projects like SLA@SOI 32 the FoSII project in general is more specific on Cloud Computing aspects like deployment monitoring of resources and their translation into high level SLAs instead of just working on highlevel SLAs in general serviceoriented architectures
The idea of bandwidth sharing is a common idea in network systems as described in 36 
 The problem stemming from escalation level 3 alone can be formulated into a binary integer problem BIP which is known to be NPcomplete 37
The proof is out of scope for this paper but a similar approach can be seen in 12 
 Finally the last escalation level 5 tries to outsource the application to another Cloud provider as explained eg in the Reservoir project 38 
Case Based ReasoningCase Based Reasoning is the process of solving problems based on past experience 39
Following the principle of semantic similarity from 40 for the summation part this leads to the following equation 3
The rules have been implemented using the Java rule engine Drools 41 
 Then an up or downtrend is randomly drawn as well as a duration of this trend between a predefined number of iterations for our evaluation this interval of iterations equals 26
Applying and evaluating a bioinformatics workflow to the rulebased approachAs detailed in 4344 bioinformatics workflows have gained a great need for largescale data analysis
Thus Cloud computing infrastructures offer a promising way to host these sorts of applications 10
The monitoring data presented in this Section was gathered with the help of the Cloud monitoring framework Lom2His 3
Using Lom2His we measured utilized resources of TopHat 45 a typical bioinformatics workflow application analyzing RNASeq data 46 for a duration of about three hours 9
The aligner presented here TopHat 45 consists of many subtasks some of them have to be executed sequentially whereas others can run in parallel Fig 10
The first subtask aligns input reads to the given genome using the Bowtie program 47
Normally when setting up such a testbed as described in 9 an initial guess of possible resource consumption is done based on early monitoring data 
For example in the studies on the automated segmentation from magnetic resonance images 19 21 the number of training examples is very huge up to millions the classes are strongly imbalanced and generating accurate statistical solution is not trivial
In addition data imbalance in huge data sets is also reported in other applicative domains such as marketing data 22 oil spill detection or land cover changes from remote sensing images 1627 text classification 18and scene classification 35
Many classification algorithms present great limitations on large data sets and show a performance degradation due to class imbalance 14
Moreover SVM classification performance can be hindered by class imbalance 130
In fact it is prone to generate classifier that has a strong estimation bias toward the majority class since the number of majority class patterns exceeds that of the minority class the class boundary becomes vulnerable to be distorted 15
Nevertheless these limitations are common to many other classification schemes such as MultiLayer Perceptron MLP 7 and Logistic Regression LR 23
Several methods to select examples in a classification problem are presented in literature using two different approaches the exampleselection method can be embedded within the learning algorithm or the examples can be filtered before passing them to the classification scheme 226
It is worth noting that the first type of selection methods generally work by preserving the original ratio between classes 611 if there is a great skew in the data it continues to be
A first approach consists of modifying SVM algorithm in order to make faster the training on large data sets for example Sequential Minimal Optimization SMO breaks the large QP problem into a series of smallest possible QP problems 25 allowing SMO to handle large training sets 25
This can be considered as a good performance in terms of simple accuracy but this is of no use since the classifier does not catch any important information on the patterns of the minority class 12
Intuitively precision is a measure of exactness ie of the examples predicted as positive how many are actually labelled correctly whereas recall is a measure of completeness ie how many examples of the positive class were labelled correctly 12
The precision of a test is very useful to clinicians since it answers the question How likely is it that this patient has the disease given that the test result is positive 17
These metrics are simple and useful summary measures of overlapping between actual and predicted labels which are interestingly applied to studies of reproducibility and accuracy in medical image segmentation 36
 Fewer values requiring interpolation also means it becomes feasible to inspect the smoothness of these values with respect to parameter variation and to apply adaptive interpolation schemes 37
Today modern services require combinations of NFs known as service chains to satisfy their QoS requirements  Quinn and Nadeau 2015
For instance Amazon offers services that allow tenants to build their own virtual infrastructure by combining functions such as filtering routing slicing and load balancing  Amazon2016
In such an environment even state of the art frameworks such as ClickOS  Martins et al 2014 and NetVM  Hwang et al 2014 cannot achieve highperformance as there is a substantial throughput degradation when interconnecting multiple NFs
Recent efforts such as E2  Palkar et al2015 and OpenNetVM  Zhang et al2016 overcome this problem by eliminating hypervisor and paravirtualization overheads via lightweight NFs eg placed in containers interconnected with fast custom software switches
