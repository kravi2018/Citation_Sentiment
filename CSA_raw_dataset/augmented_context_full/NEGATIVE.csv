"Zhou and Nakagawa (1994) have shown, in the experiments of word prediction from the previous word sequence, that the HMM is more powerful than the bigram model and is nearly equivalent to the trigram model, though the number of parameters of the HMM is less than that in the N- ram model. In general, models with fewer parameters are more Robust. "
"However, since Japanese sentences tend to be relatively long and the recent Japanese dictionary for research is large, under-flow is sometimes a problem. For example, the EDIt Japanese corpus (EDR, 1994) includes sentences that consist of more than fifty words at a frequency of one percent. In fact, we experienced the underflow problem in preliminary experiments with the EDR corpus. "
"Merialdo (1994) and Elworthy (1994) have insisted, based on their experimental results, that the maximum likelihood training using an untagged corpus does not necessarily improve tagging accuracy. However, their likelihood was the probability with all paths weighted equivalently. Since more than half of the symbols in the observations may be noise, models estimated in this way are not reliable."
"Takeuchi and Matsumoto (1995) proposed the bigram estimation method from an untagged Japanese corpus. Their algorithm divides a morpheme network into possible sequences that are then used for the normal Baum-Welch algorithm. This algorithm cannot take advantage of the scaling procedure, because it requires the synchronous calculation of all possible sequences in the morpheme network. "
"Nagata (1996) recently proposed a generalized forward-backward algorithm that is a character synchronous method for unsegmented languages. He applied this algorithm to bigram model training from untagged Japanese text for new word extraction. However, he did not apply this algorithm to the estimation of HMM parameters.  "
"The simplest part-of-speech taggers are bigram or trigram models (Church, 1989; Charniak et al., 1993). They require a relatively large tagged training text. Transformation-based tagging as introduced by Brill (1993) also requires a hand- tagged text for training. . "
"In a previous paper (Schfitze, 1993), we trained a neural network to disambiguate part-of-speech using context; however, no information about the word that is to be categorized was used. This scheme fails for cases like The soldiers rarely come home. vs. The soldiers will come home.   "
"Compared to learning rule-based approaches such as the one by Brill (1992), a k-nn approach provides a uniform approach for all disambiguation tasks, more flexibility in the engineering of case representations, and a more elegant approach to handling of unknown words (see e.g. Cardie 1994). "
"A remedy is to aggressively limit the feature space, e.g. to syntactic labels or a small fraction of the bi-lingual features available, as in (Chiang et al., 2008; Chiang et al., 2009), but that reduces the benefit of lexical features. "
"The other approach is to estimate a single score or likelihood of a translation with rich features, for example, with the maximum entropy (Max- Ent) method as in (Carpuat and Wu, 2007; Itty- cheriah and Roukos, 2007; He et al., 2008). This method avoids the over-fitting problem, at the expense of losing the benefit of discriminative training of rich features directly for MT. However, the feature space problem still exists in these published models. "
"The MaxEnt model in (Ittycheriah and Roukos 2007) was optimized globally, so that it could better employ the distribution of the training data. However, one has to filter the training data according to the test data to get competitive performance with this model 1 . In addition, the filtering method causes some practical issues. First, such methods are not suitable for real MT tasks, especially for applications with streamed input, since the model has to be retrained with each new input sentence or document and training is slow.  "
"Similar ideas were explored in (He et al., 2008). However their length features only provided insignificant improvement of 0.1 BLEU point. A crucial difference of our approach is how the length preference is modeled. We approximate the length distribution of non-terminals with a smoothed Gaussian, which is more robust and gives rise to much larger improvement consistently. "
"We refer to He and Gildea (2006) who tested active learning and co-training methods, but found little or no gain from semi-supervised learning, and to Swier and Stevenson (2004), who achieved good results using semi-supervised methods, but tested their methods on a small number of VerbNet roles, which have not been used by other SRL Systems."
"Discriminative models have been found to outperform generative models for many different tasks including SRL (Lim et al., 2004). For this reason we also employ discriminative models here. "
"Compared to previous approaches (Raghavan and Allan, 2007), our method can be used for both classification and structured tasks, and the feature query selection methods we propose perform better. "
"Raghavan and Allan (2007) also propose several methods for learning with labeled features, but in a previous comparison GE gave better results (Druck et al., 2008). Additionally, the generalization of these methods to structured output spaces is not straightforward. "
"Note that Liang et al. (2009) only use this method in synthetic experiments, and instead use a method similar to total uncertainty for experiments in part-of-speech tagging. Unlike the experiments presented in this paper, Liang et al. (2009) conduct only simulated active learning experiments and do not consider skipping queries. "
"Sindhwani (Sindhwani et al., 2009) simultaneously developed an active learning method that queries for both instance and feature labels that are then used in a graph-based learning algorithm. They find that querying certain features outperforms querying uncertain features, but this is likely because their query selection method is similar to the expectation uncertainty method described above, and consequently non-discriminative features may be queried often. It is also not clear how this graph-based training method would generalize to structured output spaces. "
"The 74.6% final accuracy on apartments is higher than any result obtained by Haghighi and Klein (2006) (the highest is 74.1%), higher than the supervised HMM results reported by Grenager et al. (2005) (74.4%), and matches the results of Mann and Mc- Callum (2008) with GE with more accurate sampled label distributions and 10 labeled examples"
"The conclusions are broadly in agreement with those of Merialdo (1994), but give greater detail about the contributions of different parts of the Model."
"The task of identifying sentence boundaries in text has not received as much attention as it deserves. Many freely available natural language processing tools require their input to be divided into sentences, but make no mention of how to accomplish this (e.g. (Brill, 1994; Collins, 1996)). "
"Others perform the division implicitly without discussing performance (e.g. (Cutting et al., 1992)). "
"There is also a less detailed description of Pahner and Hearst's system, SATZ, in (Pahuer and Hearst, 1994). "
"Liberman and Church suggest in (Liberlnan and Church, 1992) that. a system could be quickly built to divide newswire text into sentences with a nearly negligible error rate. but, do not actually build such a system. "
"The work on discourse parsing that is most similar to ours is that of Baldridge and Lascarides (2005). They used a probabilistic head- driven parsing method (described in (Collins, 2003)) to construct rhetorical structure trees for a spoken dialog corpus. However, their parser was not incremental; it used global features such as the number of turn changes. Also, it focused strictly in interpretation of input utterances; it could not predict actions by either dialog partner."
"In this area, the work most closely related to ours is that of Barrett and Weld (Barrett and Weld, 1994), who build an incremental bottom-up parser to parse plans. Their parser, however, was not probabilistic or targeted at dialog processing. "
"We call this model a chunk-based dialog model (Bangalore et al., 2006). The chunkbased model has limitations. For example, dominance relations among subtasks are important for dialog processes such as anaphora resolution (Grosz and Sidner, 1986). Also, the chunkbased model is representationally inadequate for center-embedded nestings of subtasks, which do occur in our domain, although less frequently than the more prevalent “tail-recursive” structures."
"We presented first-order expectation semirings and inside-outside computation in more detail than (Eisner, 2002), and developed extensions to higher-order expectation semirings. This enables efficient computation of many interesting quantities over the exponentially many derivations encoded in a hypergraph "
We speculate that this contrasts with the disappointing findings of Kehler et al. (2004) since SRL provides a more fine grained level of information when compared to predicate argument statistics. 
"Most empirical work in translation analyzes mod els and algorithms using BLEU (Papineni et al., 2002) and related metrics. Though such metrics are useful as sanity checks in iterative system development, they are less useful as analytial tools."
"In an interesting analysis of phrase-based and hierarchical translation, Zollmann et al. (2008) forced a phrase-based system to produce the translations generated by a hierarchical system. Unfortunately, their analysis is incomplete; they do not perform the analysis in both directions."
"The Systran system provides a dictionary coding tool (Senellart et al., 2003). This tool allows the manual task of coding entries to be partially automated with the use of monolingual dictionaries and probabilistic context-free grammars, while allowing the user to fine-tune it by correcting the automatic coding and/or add more features. However, this remains first of all a time-consuming task. Moreover, it is not easy for humans to selec tthe best translation among a set of alternatives, let alone assign them probabilities. Last but not least, the beneficial effect on translation is not guaranteed"
"[Mani et al, 1999] employed rules such as the referencing of pronouns with the most recently mentioned noun phrase. However, this might be inappropriate in MDS, where the use of multiple documents increases the number of possible entities with which an anaphor could be referenced. "
"In the domain of indicative summarization, Kan and McKeown (2002) studied the problem of generating abstracts for bibliographical data which although in a restricted domain has some contact points with the work described here. As in their work we use the abstracts in our corpus to induce the model. They rely on a more or less fixed discourse structure to accommodate the generation process. In our approach the discourse structure is not fixed but predicted for each particular abstract. "
"Related to this is the work by Teufel and Moens (2002) on rhetorical classification for content selection. In cut-and-paste summarization (Jing and mcKeown, 2000), sentence combination operations were implemented manually following the study of a set of professionally written abstracts; however the particular “pasting” operation presented here was not implemented. "
"Previous studies on text-to-text abstracting (Banko et al., 2000; Knight and Marcu, 2000) have studied problems such as sentence compression and sentence combination but not the “pasting” procedure presented here. "
"Recently, a number of machine learning approaches have been proposed (Zettlemoyer and Collins, 2005; Mooney, 2007). However, they are supervised, and providing the target logical form for each sentence is costly and difficult to do consistently and with high quality. "
"The drawback is that the complexity in syntactic processing is coupled with semantic parsing and makes the latter even harder. For ex- ample, when applying their approach to a different domain with somewhat less rigid syntax, Zettlemoyer and Collins (2007) need to introduce new combinators and new forms of candidate lexical Entries. "
"Another major challenge in USP learning is the summation in the likelihood, which is over all possible semantic parses for a given dependency tree. Even an efficient sampler like MC-SAT (Poon and Domingos, 2006), as used in Poon & Domingos (2008), would have a hard time generating accurate estimates within a reasonable amount of time. "
"But in fact, the issue of editing in text summa-rization has usually been neglected, notable exceptions being the works by Jing andMcKeown (2000) and Mani, Gates, and Bloedorn (1999). In our work, we partially ad-dress this issue by enumerating some transformations frequently found in our corpusthat are computationally implementable."
"Jing (2000) proposes a novel algorithmfor sentence reduction that takes into account different sources of information to de-cide whether or not to remove a particular component from a sentence to be includedin a summary. The decision is made based on (1) the relation of the component toits context, (2) the probability of deleting such a component (estimated from a cor-pus of reduced sentences), and (3) linguistic knowledge about the essentiality of thecomponent in the syntactic structure. Sentence reduction is concerned only with theremoval of sentence components, so it cannot explain transformations observed in ourcorpus and in summarization in general, such as the reexpression of domain conceptsand verbs."
"Jing and McKeown (2000) have proposed a rule-basedalgorithm for sentence combination, but no results have been reported. Radev andMcKeown (1998) have already addressed the issue of information fusion in the con-text of multidocument summarization in one specific domain (i.e., terrorism)"
Berger et al (1996) proposed an iterative procedure of adding news features to feature set driven by data. We present a simple and effective approach using some statistical heuristics for feature selection. 
"Nguyen and Horiguchi (2003) describe an ex tension of the decision tree-based compression model (Knight and Marcu, 2002) which allows for word order changes. The key to their approach is that dropped constituents are temporarily stored on a deletion stack, from which they can later be renserted in the tree where required. Although this provides an unlimited freedom for rearranging constituents, it also complicates the task of learning the parsing steps, which might explain why their evaluation results show marginal improvements at best. "
"The inadequacy of the bag-of-words method to the fusion task demonstrates theneed for a more linguistically motivated approach. At the other extreme, previous ap-proaches (Radev and McKeown 1998) have demonstrated that this task is feasible whena detailed semantic representation of the input sentences is available. However, theseapproaches operate in a limited domain (e.g terrorist events), where information ex-traction systems can be used to interpret the source text."
"For Japanese, (Nagao and Mori, 1994) proposed a method of computing an arbitrary length character N-gram, and showed that the character N-gram statistics obtained from a large corpus includes information useful for word extraction. However, they did not report any evaluation of their word extraction method."
"(Teller and Batchelder, 1994) proposed a very naive probabilistic word segmentation method for Japanese, based on character type informationand hiragana bigram frequencies. They claimed 98% word segmentation accuracy, while we claim 94.7%. However, their evaluation method is veryoptimistic, and completely different from ours. They count an error only when the system segmentation violates morpheme boundaries. In other words, they count an error only when the system segmentation is not acceptable to human judgement while we count an error whenever the system segmentation does not exactly match the corpus segmentation, even if it is inconsistent."
"In English taggers, (Weischedel et al., 1993) proposed a statistical model to estimate word output probability p(wi|tl) for an unknown word from spelling information such as inflectional endings, derivational endings, hyphenation, and capitalization. Our word model can be thought of a generalization of their statistical model. One potential benefit of our statistical model and segmentation algorithm is that they are completely independent of the target language and its writing system. "
"Barzilay and Elhadad (1997) proposed lexical chains as an intermediate step in the text summarization process. Attempts to determine the benefit of this proposal have been faced with a number of difficulties. First, previous methods for computing lexical chains have either been manual (Morris and Hirst 1991) or automated, but with exponential efficiency (Hirst and St.-Onge 1997; Barzilay and Elhadad 1997). Because of this, computing lexical chains for documents of any reasonable size has been impossible. "
"Recently, many phrase reordering methods have been proposed, ranging from simple distance based distortion model (Koehn et al., 2003; Ochand Ney, 2004), flat reordering model (Wu, 1997;Zens et al., 2004), lexicalized reordering model (Tillmann, 2004; Kumar and Byrne, 2005), to hierarchical phrase-based model (Chiang, 2005; Setiawan et al., 2007) and classifier-based reordering model with linear features (Zens and Ney, 2006; Xiong et al., 2006; Zhang et al., 2007a; Xiong et al., 2008). However, one of the major limitations of these advances is the structured syntactic knowledge, which is important to global reordering (Li et al., 2007; Elming, 2008), hasnot been well exploited."
"Assuming that the number of feature templates in a given set is n, the algorithm of (Ding and Chang, 2008) requires O(n 2 ) times of training/test routines, it cannot handle a set that consists of hundreds of templates. "
"However, in this paper we clarify a number of details that are omitted in major previous publications concerning tagging with Markov models. As two examples, (Rabiner, 1989) and (Charniak et al., 1993) give good overviews of the techniques and equations used for Markov models and part-of-speech tagging, but they are not very explicit in the details that are needed for their application. "
"For the Penn Treebank, (Ratnaparkhi, 1996) reports an accuracy of 96.6% using the Maximum Entropy approach, our much simpler and therefore faster HMM approach delivers 96.7%. This comparison needs to be re-examined, since we use a ten-fold crossvalidation and averaging of result while Ratnaparkhi only makes one test run "
"For example, the Markov model tagger used in the comparison of (van Halteren et al., 1998) yielded worse results than all other taggers. "
"Co-occurrence statistics extracted from corpora lead to good performance on a wide range of tasks that involve the identification of the semantic relation between two words or concepts (Sahlgren, 2006; Turney, 2006). However, the difficulty of such tasks and the fact that they are apparently unrelated has led to the development of largely ad-hoc solutions, tuned to specific challenges. "
"While the literature suggests that Baum-Welch training can degrade performance on the tagging task (Elworthy, 1994; Merialdo, 1994), we have found in early experiments that agreement between a tagger trained in this way and the tagger from the XTag Project consistently increases with each iteration of Baum-Welch, eventually reaching a plateau, but not Decreasing. "
"Clark (2000) presents a framework which in principle should accommodate lexical ambiguity using mixtures, but includes no evidence that it does so. "
"Schone and Jurafsky (2001) list several universal characteristics of language that can serve as clues in this process, some of which we exploit. However, their use of “perfect” clusters renders some of their algorithmic suggestions problematic. "
"Several models have been used for pair-wise alignment, starting with TER and proceeding with more sophisticated techniques such as HMM models, ITG, and IHMM (Rosti et. al 2007a, Matusov et al 2008, Krakos et al. 2008, He et al. 2008). A major problem with such methods is that each hypothesis is aligned to the backbone independently, leading to sub- optimal behavior."
"While P R W is the first attempt to formalize well known relevance weighting (Sparck Jones, 1972; Salton and McGill, 1983) by probability theory, there are several drawbacks in PRW. "
"A similar argument applies to all other problems in (Robertson and Sparck Jones, 1976) that are caused by having insufficient training cases. "
"The other term weighting scheme frequently used is TFIDF (Term Frequency Inverse Document Frequency) (Salton and Buckley, 1988). However, this technique needs a corpus for computing IDF score, causing the genre-dependent problem for generic text summarization task. "
"This massive interest in speed is bringing rapid progress to the field, but it comes with a certain amount of baggage. Each technique brings its own terminology (from the cubes of (Chiang, 2007) to the lazy lists of (Pust and Knight, 2009)) into the mix. Often, it is not entirely clear why they Work. "
"Unlike many other methods that directly utilize noun phrase (NP) coreference (Nenkova 2008; Mani et al. 1999), we propose a method that employs insertion and substitution of phrases that modify the same chunk in the lead and other sentences."
"As is well known, the extractive summary that has been extensively studied from the early days of summarization history (Luhn, 1958) suffers from various drawbacks. "
"We also tested feature normalization (as described in Section 4.2). While Blitzer et al. (2006) found it necessary to normalize (and scale) the projection features, we did not observe any improvement by normalizing them (actually, it slightly degraded performance in our case). Thus, we found this step unnecessary, and currently did not look at this issue any further. "
"The empirical results show that our instantiation of SCL to parse disambiguation gives promising initial results, even without the many additional extensions on the feature level as done in Blitzer et al. (2006). "
"Our rhetorical analysis, as noted above, is nonhierarchical, in contrast to Rhetorical Structure Theory (RST) (Mann and Thompson 1987; Marcu 1999), and it concerns text pieces at a lower level of granularity. Although we do agree with RST that the structure of text is hierarchical in many cases, it is our belief that the relevance and function of certain text pieces can be determined without analyzing the full hierarchical structure of the text. "
"In the news domain, sentence location is the single most important feature for sentence selection (Brandow, Mitze, and Rau 1995); in our domain, location information, although less dominant, can still give a useful indication.  "
"Sparck Jones (1999) argues that it is crucial for a summarization strategy to relate the large-scale document structure of texts to readers’ tasks in the real world (i.e., to the proposed use of the summaries). We feel that incorporating a robust analysis of discourse structure into a document summarizer is one step along this way. "
"In this case the model hypothesesare not satisfied; e.g., there are strong intra-tag relations in distancesgreater than the model order, idiomatic expressions, language dependentexceptions, etc. A general solution to the variable length and depth ofdependency for HMM has been already proposed (Tao 1992), but has notbeen implemented in taggers"
"Existing treebanks of English ((Marcus et al., 1994), (Sampson, 1995), (Black et al., 1996)) contain conventional phrase structure trees augmented with annotations for discontinuous constituents. As this encoding strategy is not well-suited to a free word order language like German, we have focussed on a less surface-oriented level of description, most closely related to the LFG f-structure, and representationsused in dependency grammar."
"Secondly, our method is related to beam search (Woods, 1985).In b e a m search, incomplete parses of an utterance are pruned or discarded when, on some criterion, they are significantly less plausible than other, competing parses. This pruning is fully interleaved with the parsing process. In contrast, our pruning takes place only at certain points: currently before parsing begins, and between the phrasM and full parsing stages. "
"Thirdly, our method is a generalization of the strategy employed by (McCord, 1993). McCord interleaved parsing with pruning in the same way as us, but only compared constituents over the same span and with the same major category. Our comparisons are more global and therefore can result in more effective pruning. "
"In (Rayner and Samuelsson, 1994), a simple scheme is given, which creates rules corresponding to four possible units: full utterances, recursive NPs, PPs, and non-recursive Nps. A more elaborate scheme is given in (Samuelsson, 1994b), where the chunking criteria are learned automatically by an entropy-minimization method; the results, however, do not appear to improve on the earlier ones. In both cases, the coverage loss due to grammar specialization was about 10 to 12% using training corpora with about 5,000 examples. In practice, this is still unacceptably high for most Applications. "
"To evaluate our WSD program, named LEXAS we tested it on a common data set involving the noun ""interest"" used by Bruce and Wiebe (Bruce and Wiebe, 1994).LEXAS achieves a mean accuracy of 87.4% on this data set, which is higher than the accuracy of 78% reported in (Bruce and Wiebe, 1994). "
"In the results reported in (Bruce and Wiebe, 1994), they used a test set of 600 randomly selected sentences from the 2369 sentences. Unfortunately, in the data set made available in the public domain, there is no indication of which sentences are used as test sentences. "
"There is now a large body of past work on WSD. Early work on WSD, such as (Kelly and Stone, 1975; Hirst, 1987) used hand-coding of knowledge to perform WSD. The knowledge acquisition process is laborious."
"The work of (Miller et al., 1994; Leacock et al., 1993; Yarowsky, 1992) used only the unordered set of surrounding words to perform WSD, and they used statistical classifiers, neural networks, or IR-based Techniques. "
"The work of (Bruce and Wiebe, 1994) used parts of speech (POS) and morphological form, in addition to surrounding words. However, the POS used are abbreviated POS, and only in a window of -b2 words. No local collocation knowledge is used. A probabilistic classifier is used in (Bruce and Wiebe, 1994). "
"The work of (Yarowsky, 1994) is perhaps the most similar to our present work. However, his work used decision list to perform classification, in which only the single best disambiguating evidence that matched a target context is used. In contrast, we used exemplar-based learning, where the contributions of all features are summed up and taken into account in coming up with a classification. We also include verb-object syntactic relation as a feature, which is not used in (Yarowsky, 1994). "
"The work of (Miller et al., 1994) is the only prior work we know of which attempted to evaluate WSD on a large data set and using the refined sense distinction of WORDNET. However, their results show no improvement (in fact a slight degradation in performance) when using surrounding words to perform WSD as compared to the most frequent heuristic. "
"The work of (McRoy, 1992) pointed out that a diverse set of knowledge sources are important to achieve WSD, but no quantitative evaluation was given on the relative importance of each knowledge source. No previous work has reported any such evaluation Either."
"The more similar conditions reported in previous work are those experiments performed on the WSJ corpus: (Brill, 1992) reports 3-4% error rate, and (Daelemans et al., 1996) report 96.7% accuracy. We obtained a 97.39% accuracy with tri- grams plus automatically acquired constraints, and 97.45% when hand written constraints were added.  "
"Since we are using a larger corpus than Padó et al. (2007), who train on the BNC, a fairer comparison might be the one with our alternative models, that are all outperformed by DM by a large margin. "
Unfortunately  this is not the case for such widely used MT evaluation metrics as BLEU  and NIST   applied the parser of  developed for English  to Czech  and found thatthe performance wassubstantially lower when compared to the results for English 
Hanks and  proposed using pointwise mutual information to identify collocations in lexicography  however  the method may result in unacceptable collocations for lowcount pairs Unlike   Smadja  1993  goes beyond the  twoword  limitation and deals with  collocations of arbitrary length  
2This can explain why previous attempts to use WordNet for generating sentencelevel paraphrases  were unsuccessful We have also illustrated that ASIA outperforms three other English systems   even though many of these use more input than just a semantic class name 
For unknown words  SCL gives a relative reduction in error of 195  over   even with 40000 sentences of source domain training data For comparison purposes  we revisit a fullygenerative Bayesian model for unsupervised coreference resolution recently introduced by   discuss its potential weaknesses and consequently propose three modifications to their model  Section 3  
Unfortunately  there is no straightforward generalization of the method of  to the two edge marginal problem Previous literature on GB parsing  Wehrli  1984  Sharp  1985    1986  Kuhns  1986  Abney  1986has not addressed the issue of implementation of the Binding theory  The present paper intends in part to fill this gap 
In general  these authors have found that existing lexicalized parsing models for English  do not straightforwardly generalize to new languages  this typically manifests itself in a severe reduction in parsing performance compared to the results for English 
This method was shown to outperform the class based model proposed in  and can thus be expected to discover better clusters of words Although a rich literature covers bootstrapping methods applied to natural language problems  several questions remain unanswered when these methods are applied to syntactic or semantic pattern acquisition 
It also differs from previous proposals on lexical acquisition using statistical measures such as  which either deny the prior existence of linguistic knowledge or use linguistic knowledge in ad hoc ways In pursuit of better translation  phrasebased models  havesignificantlyimprovedthe quality over classical wordbased models  
Several studies have shown that largemargin methods can be adapted to the special complexities of the task  However  the capacity of these algorithms to improve over stateoftheart baselines is currently limited by their lack of robust dimensionality reduction 
The 746  final accuracy on apartments is higher than any result obtained by   the highest is 741    higher than the supervised HMM results reported by Grenager et al One prominent constraint of the IBM word alignment models  is functional alignment  that is each target word is mapped onto at most one source word 
Although the first three are particular cases where N  1 andor M  1  the distinction is relevant  because most wordbased translation models  eg IBM models   can typically not accommodate general MN alignments 
Insideout alignments   such as the one in Example 13  can not be induced by any of these theories  in fact  there seems to be no useful synchronous grammar formalisms available that handle insideout alignments  with the possible exceptions of synchronous treeadjoining grammars   Bertsch and Nederhof  and generalized multitext grammars   which are all way more complex than ITG  STSG and  22   BRCG 
Bilexical contextfree grammars have been presented in  as an abstraction of language models that have been adopted in several recent realworld parsers  improving stateoftheart parsing accuracy  For the Penn Treebank   reports an accuracy of 966  using the Maximum Entropy approach  our much simpler and therefore faster HMM approach delivers 967  
Although such approaches have been employed effectively   there appears to remain considerable room for improvement In terms of alignment  this wordnumber difference means that multiword connections must be considered  a task which 334 Sue J Ker and Jason S Chang Word Alignment is beyond the reach of methods proposed in recent alignment works based on  Model 1 and 2 
2 Motivation and Prior Work While several authors have looked at the supervised adaptation case  there are less  and especially less successful  studies on semisupervised domain adaptation  
In particular  the model in  failed to generate punctuation  a deficiency of the model 1 Introduction Phrasebased translation models   which go beyond the original IBM translation models  1 by modeling translations of phrases rather than individual words  have been suggested to be the stateoftheart in statistical machine translation by empirical evaluations 
Table 2  Figures about clustering algorithms Algorithm  Sentences   Clusters SHAC 623 CHAC 217 QT 232 EM 416 In fact  table 2 shows that most of the clusters have less than 6 sentences which leads to question the results presented by  who only keep the clusters that contain more than 10 sentences 
These methods go beyond the original IBM machine translation models   by allowing multiword units  phrases  in one language to be translated directly into phrases in another language 1 Introduction Recent works in statistical machine translation  SMT  shows how phrasebased modeling  significantly outperform the historical wordbased modeling  
Our system outperforms competing approaches  including the standard machine translation alignment models  and the stateoftheart Cut and Paste summary alignment technique  For example  we would like to know that if a  JJ  JJ  7We also tried using word clusters  instead of POS but found that POS was more helpful 
This is because their training data  the Penn Treebank   does not fully annotate NP structure Although this Wikipedia gazetteer is much smaller than the English version used by  that has over 2000000 entries  it is the largest gazetteer that can be freely used for Japanese NER 
As the tagger of  can not tag a word lattice  we can not back off to this tagging Pointwise mutual information  PMI  is commonly used for computing the association of two terms   which is defined as  nullnullnull null null  null null nullnullnull nullnullnullnull  nullnull nullnull null null null nullnullnullnullnull However  we argue that PMI is not a suitable measure for our purpose 
Moreover  the parameters of the model must be estimated using averaged perceptron training   which can be unstable This method has the advantage that it is not limited to the model scaling factors as the method described in  They reported that their method is superior to BLEU  in terms of the correlation between human assessment and automatic evaluation 
While the idea of exploiting multiple news reports for paraphrase acquisition is not new  previous efforts  have been restricted to at most two news sources Furthermore  we provide a 638  error reduction compared to IBM Model 4  
With all but two formats IBIIG achieves better FZ  l rates than the best published result in  A maximum entropy approach has been applied to partofspeech tagging before   but the approach s ability to incorporate nonlocal and nonHMMtaggertype evidence has not been fully explored 
If we consider these probabilities as a vector  the similarities of two English words can be obtained by computing the dot product of their corresponding vectors2 The formula is described below  similarity  ei  ej   Nsummationdisplay k  1 p  ei fk  p  ej fk   3  Paraphrasing methods based on monolingual parallel corpora such as  can also be used to compute the similarity ratio of two words  but they dont have as rich training resources as the bilingual methods do 
32 Evaluation Metrics AER  Alignment Error Rate   is the most widely used metric of alignment quality  but requires goldstandard alignments labeled with surepossible annotations to compute  lacking such annotations  we can compute alignment fmeasure instead 
Clustering algorithms have been previously shown to work fairly well for the classification of words into syntactic and semantic classes   but determining the optimum number of classes for a hierarchical cluster tree is an ongoing difficult problem  particularly without prior knowledge of the item classification 
Although  there are various manualautomatic evaluation methods for these systems  eg  BLEU   these methods are basically incapable of dealing with an MTsystem and a wpMTsystem at the same time  as they have different output forms However  reordering models in traditional phrasebased systems are not sufficient to treat such complex cases when we translate long sentences  
Current treebased models that integrate linguistics and statistics  such as GHKM   are not able to generalize well from a single phrase pair Our syntacticrelationbased thesaurus is based on the method proposed by   although Hindle did not apply it to information retrieval 
Both Charniak  and Bikel  were trained using the goldstandard tags  as this produced higher accuracy on the development set than using  s tags The utility of ITG as a reordering constraint for most language pairs  is wellknown both empirically  and analytically   howeverITGsstraight  monotone  andinverted  reverse  rules exhibit strong cohesiveness  which is inadequate to express orientations that require gaps 
This latter point is a critical difference that contrasts to the major weakness of the work of  which uses a topN list of translations to select the maximum BLEU sentence as a target for training  so called local update  
Even the creators of BLEU point out that it may not correlate particularly well with human judgment at the sentence level  1 Introduction In recent years  various phrase translation approaches  have been shown to outperform wordtoword translation models  
Sentencelevel approximations to B exist   but we found it most effective to perform B computations in the context of a setOof previouslytranslated sentences  following Watanabe et al In such a process  original phrasebased decoding  does not take advantage of any linguistic analysis  which  however  is broadly used in rulebased approaches 
However  many of these models are not applicable to parallel treebanks because they assume translation units where either the source text  the target text or both are represented as word sequences without any syntactic structure  2 Statistical Word Alignment Statistical translation models  only allow word to word and multiword to word alignments 
Even the 3 A demo of the parser can be found at httplfgdemocomputingdcuielfgparserhtml creators of BLEU point out that it may not correlate particularly well with human judgment at the sentence level  Our method  extending this line of research with the use of labeled LFG dependencies  partial matching  and nbest parses  allows us to considerably outperform  highest correlations with human judgement  they report 0144 for the correlation with human fluency judgement  0202 for the correlation with human overall judgement   although it has to be kept in mind that such comparison is only tentative  as their correlation is calculated on a different test set 
Unlike wellknown bootstrapping approaches   EM and CE have the possible advantage of maintaining posteriors over hidden labels  or structure  throughout learning  bootstrapping either chooses  for each example  a single label  or remains completely agnostic 
1 Introduction Recent approaches to statistical machine translation  SMT  piggyback on the central concepts of phrasebased SMT  and at the same time attempt to improve some of its shortcomings by incorporating syntactic knowledge in the translation process 
For the results in this paper  we have used Pointwise Mutual Information  PMI  instead of IBM Model 1   since  found it to be as effective on Springer  but faster to compute Numbers in the table correspond to the percentage of experiments in which the condition at the head of the column was true  for example figure in the first row and first column means that for 989 percent of the language pairs the BLEU score for the bidirectional decoder was better than that of the forward decoder  proach   
This provides a compelling advantage over previous dependency language models for MT   whichusea5gramLMonlyduringreranking Experimental results indicate that our model outperforms  coreference model by a large margin on the ACE data sets and compares favorably to a modified version of their model 
This method was preferred against other related methods  like the one introduced in   since it embeds all the available semantic information existing in WordNet  even edges that cross POS  thus offering a richer semantic representation Both the global models  use fairly small training sets  and there is no evidence that their techniques will scale to larger data sets 
Our model improves the baseline provided by    i  accuracy is increased by creating a lexicalised PCFG grammar and enriching conditioning context with parent fstructure features  and  ii  coverage is increased by providing lexical smoothing and fuzzy matching techniques for rule smoothing 
While the model of  significantly outperforms the constrained model of   they both are well below the stateoftheart in constituent parsing By doing so we must emphasize that  as described in the previous section  the BLEU score was not designed to deliver satisfactory results at the sentence level   and this also applies to the closely related NIST score 
 focus on alignment and do not present MT results  while May and Knight  2007  takesthesyntacticrealignmentasaninputtoanEM algorithm where the unaligned target words are insertedintothetemplatesandminimumtemplatesare combinedintobiggertemplates  
But without the global normalization  the maximumlikelihood criterion motivated by the maximum entropy principle  is no longer a feasible option as an optimization criterion   word class   measures polarity using only adjectives  however in our approach we consider the noun  the verb  the adverb and the adjective content words While most parsing methods are currently supervised or semisupervised   they depend on handannotated data which are difficult to come by and which exist only for a few languages 
To analyze our methods on IV and OOV words  we use a detailed evaluation metric than Bakeoff 2006  which includes Foov and Fiv Surprisingly  although JESSCM is a simpler version of the hybrid model in terms of model structure and parameter estimation procedure  JESSCM provides Fscores of 9445 and 8803 for CoNLL00 and 03 data  respectively  which are 015 and 083 points higher than those reported in  for the same configurations 
We presented some theoretical arguments for not limiting extraction to minimal rules  validated them on concrete examples  and presented experiments showing that contextually richer rules provide a 363 BLEU point increase over the minimal rules of  
Our study also shows that the simulatedannealing algorithm  is more effective 1552 than the perceptron algorithm  for feature weight tuning By segmenting words into morphemes  we can improve the performance of natural language systems including machine translation  and information retrieval  
Statistical disambiguation such as  for PPattachment or  for generative parsing greatly improve disambiguation  but as they model by imitation instead of by understanding  complete soundness has to remain elusive 
It can be applied to complicated models such IBM Model4  By 17 0 10 20 30 40 50 60 70 80 90 100 10000 100000 1e06 1e07 Test Set Items with Translations  Training Corpus Size num words unigrams bigrams trigrams 4grams Figure 1 Percent of unique unigrams bigrams trigrams and 4grams from the Europarl Spanish test sentences for which translations were learned in increasingly large training corpora increasing the size of the basic unit of translation phrasebased machine translation does away with many of the problems associated with the original wordbased formulation of statistical machine translation Brown et al  1993
2 21 Word Alignment Adaptation Bidirectional Word Alignment In statistical translation models   only onetoone and moretoone word alignment links can be found The method was intended as a replacement for sentencebased methods  eg     which are very sensitive to noise 
This approach addresses the problematic aspects of both pure knowledgebased generation  where incomplete knowledge is inevitable  and pure statistical bag generation   where the statistical system has no linguistic guidance  In addition  the clustering methods used  such as HMMs and Browns algorithm   seem unable to adequately capture the semantics of MNs since they are based only on the information of adjacent words 
We preferred the loglikelihood ratio to other statistical scores  such as the association ratio  or   2  since it adequately takes into account the frequency of the cooccurring words and is less sensitive to rare events and corpussize  The ubiquitous minimum error rate training  MERT  approach optimizes Viterbi predictions  but does not explicitly boost the aggregated posterior probability of desirable ngrams  
However  work in that direction has so far addressed only parse reranking  
The combination is significantly better than  at a very high level  but more importantly  Shens results  currently representing the replicable stateoftheart in POS tagging  have been significantly surpassed also by the semisupervised Morce  at the 99  confidence level  
a timeconsuming process  Other statistical machine translation systems such as  and  also produce a tree a15 given a sentence a16 Their models are based on mechanisms that generate two languages at the same time  so an English tree a15 is obtained as a subproduct of parsing a16 However  their use of the LM is not mathematically motivated  since their models do not decompose into Pa4a5a2a9a8a3a10a6 and a12a14a4a5a3a7a6 unlike the noisy channel model 
WSD systems have been far more successful in distinguishing coarsegrained senses than finegrained ones   but does that approach neglect necessary meaning differences  Secondly  while most pronoun resolution evaluations simply exclude nonreferential pronouns  recent unsupervised approaches  must deal with all pronouns in unrestricted text  and therefore need robust modules to automatically handle nonreferential instances 
12Poon and Domingos  outperformed  As with similar work   the size of the corpus makes preprocessing such as lemmatization  POS tagging or partial parsing  too costly The size of the development set used to generate 1 and 2  compensates the tendency of the unsmoothed MERT algorithm to overfit  by providing a high ratio between number of variables and number of parameters to be estimated 
 have proposed a rulebased algorithm for sentence combination  but no results have been reported  provides anecdotal evidence that only incorrect alignments are eliminated by ITG constraints 
By segmenting words into morphemes  we can improve the performance of natural language systems including machine translation  and information retrieval  It has been difficult to identify all and only those cases where a token functions as a discourse connective  and in many cases  the syntactic analysis in the Penn TreeBank  provides no help 
For example  10 million words of the American National Corpus  will have manually corrected POS tags  a tenfold increase over the Penn Treebank   currently used for training POS taggers The process of phrase extraction is difficult to optimize in a nondiscriminative setting  many heuristics have been proposed   but it is not obvious which one should be chosen for a given language pair 
While several methods have been proposed to automatically extract compounds   we know of no successful attempt to automatically make classes of compounds Many approaches for POS tagging have been developed in the past  including rulebased tagging   HMM taggers   maximumentropy models   cyclic dependency networks   memorybased learning   etc All of these approaches require either a large amount of annotated training data  for supervised tagging  or a lexicon listing all possible tags for each word  for unsupervised tagging  
Our focus is on the sentence level  unlike  and   we employ a significantly larger set of seed words  and we explore as indicators of orientation words from syntactic classes other than adjectives  nouns  verbs  and adverbs  This additional conditioning has the effect of making the choice of generation rules sensitive to the history of the generation process  and  we argue  provides a simpler  more uniform  general  intuitive and natural probabilistic generation model obviating the need for CFGgrammar transforms in the original proposal of  
1 Introduction The most widely applied training procedure for statistical machine translation IBM model 4  unsupervised training followed by postprocessing with symmetrization heuristics  yields low quality word alignments This is in contrast to purely statistical systems   which are difficult to inspect and modify 
In addition  the semisupervised Morce performs  on single CPU and development data set  77 times faster than the combination and 23 times faster than  In a recent study by   nonlocal information is encoded using an independence model  and the inference is performed by Gibbs sampling  which enables us to use a stateoftheart factored model and carry out training efficiently  but inference still incurs a considerable computational cost  suggests use of an approximation summing over the training data  which does not sum over possible tags   h E f j  2 P    p  ti l hi  f j  hi  ti  i  1 However  we believe this passage is in error  such an estimate is ineffective in the iterative scaling algorithm 
IBM Model1  is a simplistic model which takes no account of the subtler aspects of language translation including the way word order tends to differ across languages A number of studies have investigated sentiment classification at document level  eg    and at sentence level  eg    however  the accuracy is still less than desirable 
Automatic evaluation methods such as BLEU   RED   or the weighted Ngram model proposed here may be more consistent in judging quality as compared to human evaluators  but human judgments remain the only criteria for metaevaluating the automatic methods For comparison purposes  we revisit  fullygenerative Bayesian model for unsupervised coreference resolution  discuss its potential weaknesses and consequently propose three modifications to their model 
The classbased kappa statistic of  can not be applied here  as the classes vary depending on the number of ambiguities per entry in the lexicon While the amount of parallel data required to build such systems is orders of magnitude smaller than corresponding phrase based statistical systems   the variety of linguistic annotation required is greater 
Although this method is comparatively easy to be implemented  it just achieves the same performance as the synchronous binarization method  for syntaxbased SMT systems Among the applications of collocational analysis for lexical acquisition are the derivation of syntactic disambiguation cues Basili et al 1991 1993a Hindle and Rooths 19911993 Sekine 1992 Bogges et al 1992 sense preference Yarowski 1992 acquisition of selectional restrictions Basili et al 1992b 1993b Utsuro et al 1993 lexical preference in generation Smadjia 1991 word clustering Pereira 1993 Hindle 1990 Basili et al 1993c etc In the majority of these papers even though the precedent or subsequent statistical processing reduces the number of accidental associations very large corpora 10000000 words are necessary to obtain reliable data on a large enough number of words
Due to limited variations in the NBest list  the nature of ranking  and more importantly  the nondifferentiable objective functions used for MT  such as BLEU    one often found only local optimal solutions to  with no clue to walk out of the riddles We also compare our performance against  and  and find that we manage higher relative improvement than existing work despite starting from a very competitive baseline CRF Although several methods have already been proposed to incorporate nonlocal features   these present a problem that the types of nonlocal features are somewhat constrained 
The morphological processing in PairClass  is more sophisticated than in  In addition  the performance of the adapted model for Joint ST obviously surpass that of   which achieves an F1 of 9341  for Joint ST  although with more complicated models and features Some are the result of inconsistency in labeling in the training data   which usually reflects a lack of linguistic clarity or determination of the correct part of speech in context 
Therefore  sublanguage techniques such as Sager  and  do not work Mutual information  though potentially of interest as a measure of collocational status  was not tested due to its wellknown property of overemphasising the significance of rare events  
While in traditional wordbased statistical models  the atomic unit that translation operates on is the word  phrasebased methods acknowledge the significant role played in language by multiword expressions  thus incorporating in a statistical framework the insight behind ExampleBased Machine Translation  
Note that the minimum error rate training  uses only the target sentence with the maximum posterior probability whereas  here  the whole probability distribution is taken into account A word order correlation bias  as well as the phrase structure biases in  Models 4 and 5  would be less beneficial with noisier training bitexts or for language pairs with less similar word order 
Although various approaches to SMT system combination have been explored  including enhanced combination model structure   better word alignment between translations  and improved confusion network construction   most previous work simply used the ensemble of SMT systems based on different models and paradigms at hand and did not tackle the issue of how to obtain the ensemble in a principled way 
1 Introduction The field of machine translation has seen many advances in recent years  most notably the shift from wordbased  to phrasebased models which use token ngrams as translation units  Although ITA rates and system performance both significantly improve with coarsegrained senses   the question about what level of granularity is needed remains three models in  are susceptible to the O  n 3  method  cf 
Even with the current incomplete set of semantic templates  the hypertagger brings realizer performance roughly up to stateoftheart levels  as our overall test set BLEU score  slightly exceeds that of   though at a coverage of 96  insteadof98  An alternative method  makes decisions at the end but has a high computational requirement 
Several teams had approaches that relied to varying degrees on an IBM model of statistical machine translation Brown et al  1993 with different improvements brought by different teams consisting of new submodels improvements in the HMM model model combination for optimal alignment etc Several teams used symmetrization metrics as introduced in Och and Ney 2003 union intersection refined most of the times applied on the alignments produced for the two directions sourcetarget and targetsource but also as a way to combine different word alignment systems
In the thriving area of research on automatic analysis and processing of product reviews   little attention has been paid to the important task studied here assessing review helpfulness In such tasks  feature calculation is also very expensive in terms of time required  huge sets of extracted rules must be sorted in two directions for relative frequency calculation of such features as the translation probability p  f e  and reverse translation probability p  e f   
Other statistical systems that address word classification probleans do not emphasize the use of linguistic knowledge and do not deal with a specific word class   or do not exploit as much linguistic knowledge as we do  As one can see in Table 4  the resulting parser ranks among the best lexicalized parsers  beating those of Collins  and Charniak and Johnson  8 Its F1 performance is a 27  reduction in error over  et al 
Our experiments on the Canadian Hansards show that our unsupervised technique is significantly more effective than picking seeds by hand   which in turn is known to rival supervised methods 2 Related Work One of the major problems with the IBM models  and the HMM models  is that they are restricted to the alignment of each sourcelanguage word to at most one targetlanguage word This implies that the complexity of structure divergence between two languages is higher than suggested in literature   report better perplexity results on the Verbmobil Corpus with their HMMbased alignment model in comparison to Model 2 of  
 have implemented a dependency parser with good accuracy  it is almost as good at dependency parsing as Charniak   and very impressive speed  it is about ten times faster than  and four times faster than Charniak   Unlike minimum error rate training   our system is able to exploit large numbers of specific features in the same manner as static reranking systems  
However  to what extent that assumption holds is tested only on a small number of language pairs using hand aligned data  Although evaluated on a different test set  our method also outperforms the correlation with human scores reported in  However  it seems unrealistic to expect a onesizefitsall approach to be achieve uniformly high performance across varied languages  and  in fact  it doesnt Though the system presented in  outperforms the best systems in the 2006 PASCAL challenge for Turkish and Finnish  it still does significantly worse on these languages than English  Fscores of 662 and 665  compared to 794  
For comparison purposes  three additional heuristicallyinduced alignments are generated for each system   1  Intersection of both directions  Aligner  int     2  Union of both directions  Aligner  union    and  3  The previously bestknown heuristic combination approach called growdiagfinal   Aligner  gdf   Brill s results demonstrate that this approach can outperform the Hidden Markov Model approaches that are frequently used for partofspeech tagging   as well as showing promise for other applications 
It has the advantage of naturally capturing local reorderings and is shown to outperform wordbased machine translation  Again the best result was obtained with IOB1  which is an imI  rovement of the best reported F    1 rate for this data set    9203  This is well illustrated by the Collins parser   scrutinized by Bikel  2004   where several transformations are applied in order to improve the analysis of noun phrases  coordination and punctuation 
It is faster and more mnemonic than the one in  1 Introduction Translations tables in Phrasebased Statistical Machine Translation  SMT  are often built on the basis of Maximumlikelihood Estimation  MLE   being one of the major limitations of this approach that the source sentence context in which phrases occur is completely ignored  
In what concerns the evaluation process  although ROUGE  is the most common evaluation metric for the automatic evaluation of summarization  since our approach might introduce in the summary information that it is not present in the original input source  we found that a human evaluation was more adequate to assess the relevance of that additional information 
The program takes the output of char_align   a robust alternative to sentencebased alignment programs  and applies wordlevel constraints using a version of Brown el al s Model 2   modified and extended to deal with robustness issues6 Conclusion Traditional approaches for devising parsing models  smoothing techniques and evaluation metrics are not well suited for MH  as they presuppose 13The lack of head marking  for instance  precludes the use of lexicalized models a la  
The experimental results show that our method outperforms the synchronous binarization method  with over 08 BLEU scores on both NIST 2005 and NIST 2008 ChinesetoEnglish evaluation data sets  produced a corpus of 4000 questions annotated with syntactic trees  and obtained an improvement in parsing accuracy for Bikels reimplementation of the Collins parser  by training a new parser model with a combination of newspaper and question data 
1 Introduction Statistical phrasebased systems  have consistently delivered stateoftheart performance in recent machine translation evaluations  yet these systems remain weak at handling word order changes With these linguistic annotations  we expect the LABTG to address two traditional issues of standard phrasebased SMT  in a more effective manner 
At the same time  we believe our method has advantages over the approach developed initially at IBM  for training translation systems automatically Despite relying on a the same concept  our approach outperforms BE in most comparisons  and it often achieves higher correlations with human judgments than the stringmatching metric ROUGE  
More recent work  has considered methods for speeding up the feature selection methods described in   Ratnaparkhi  1998   and Della Pietra  Della Pietra  and Lafferty  1 Introduction Currently  most of the phrasebased statistical machine translation  PBSMT  models  adopt full matching strategy for phrase translation  which means that a phrase pair  tildewidef  tildewidee  can be used for translating a source phrase f  only if tildewidef  f Due to lack of generalization ability  the full matching strategy has some limitations 
1 Introduction For statistical machine translation  SMT   phrasebased methods  and syntaxbased methods  outperform wordbased methods  To our knowledge no systems directly address Problem 1  instead choosing to ignore the problem by using one or a small handful of reference derivations in an nbest list   or else making local independence assumptions which sidestep the issue  
For example  the statistical word alignment in IBM translation models  can only handle word to word and multiword to word alignments Our graphical representation has two advantages over previous work   unifying sentence relations and incorporating question interactions 
 presented a historybased generation model to overcome some of the inappropriate independence assumptions in the basic generation model of  
Unfortunately  longer sentences  up to 100 tokens  rather than 40   longer phrases  up to 10 tokens  rather than 7   two LMs  rather than just one   higherorder LMs  order 7  rather than 3   multiple higherorder lexicalized reordering models  up to 3   etc all contributed to increased system  s complexity  and  as a result  time limitations prevented us from performing minimumerrorrate training  MERT   for ucb3  ucb4 and ucb5 
Unsupervised methods have been developed for WSD  but despite modest success have not always been well understood statistically  In addition  uniform conditioning on mother grammatical function is more general than the casephenomena specific generation grammar transform of   in that it applies to each and every subpart of a recursive input fstructure driving generation  making available relevant generation history  context  to guide local generation decisions 
Section 5 presents an error analysis for  lexicalized model  which shows that the headhead dependencies used in this model fail to cope well with the flat structures in Negra Even the creators of BLEU point out that it may not correlate particularly well with human judgment at the sentence level   a problem also noted by  and  
Methods like McDonalds  including the wellknown Maximal Marginal Relevance  MMR  algorithm   are subject to another problem  Summarylevel redundancy is not always well modeled by pairwise sentencelevel redundancy 
While other systems  such as   have addressed these tasks to some degree  OPINE is the first to report results Thirdly   deploys the dependency language model to augment the lexical language model probability be1183 tween two head words but never seek a full dependency graph 
METEOR was chosen since  unlike the more commonly used BLEU metric   it provides reasonably reliable scores for individual sentences Our method is a natural extension of those proposed in  and   and overcomes their drawbacks while retaining their advantages Another consequence of not generating posthead conjunctions and punctuation as firstclass words is that they 19 In fact  if punctuation occurs before the head  it is not generated at alla deficiency in the parsing model that appears to be a holdover from the deficient punctuation handling in the model of  
One conclusion that we can draw is that at present the additional word features used in  looking at words more than one position away from the current do not appear to be helping the overall performance of the models In comparison  the 2D model in Figure 2  c  used in previous work  can only model the interaction between adjacent questions 
The problem is typically presented in logspace  which simplifies computations  but otherwise does not change the problem due to the monotonicity of the log function  hm  log hprimem  log p  t s   summationdisplay m m hm  t  s   3  Phrasebased models  are limited to the mapping of small contiguous chunks of text Though taggers based on dependency networks   SVM   MaxEnt   CRF   and other methods may reach slightly better results  their traintest cycle is orders of magnitude longer 
Several papers have looked at higherorder representations  but have not examined the equivalence of synpara distributions when formalized as Markov chains  Of the methods we compare against  only the WordNetbased similarity measures    and  provide a method for predicting verb similarities  our learned measure widely outperforms these methods  achieving a 136  Fscore improvement over the LESK similarity measure 
In order to capture the dependency relationship between lexcial heads  breaks down the rules from head outwards  which prevents us from factorizing them in other ways Besides  our model  as being linguistically motivated  is also more expressive than the formally syntaxbased models of Chiang  and  
String alignment with synchronous grammars is quite expensive even for simple synchronous formalisms like ITG  but Duchi et al 1 Introduction Phrasebased systems  flat and hierarchical alike   have achieved a much better translation coverage than wordbased ones   but untranslated words remain a major problem in SMT 
Such a quasisyntactic structure can naturally capture the reordering of phrases that is not directly modeled by a conventional phrasebased approach  Although generating training examples in advance without a working parser  is much faster than using inference   our training time can probably be decreased further by choosing a parsing strategy with a lower branching factor 
This cost can often be substantial  as with the Penn Treebank  2 Previous work on Sentiment Analysis Some prior studies on sentiment analysis focused on the documentlevel classification of sentiment  where a document is assumed to have only a single sentiment  thus these studies are not applicable to our goal 
Since Czech is a language with relatively high degree of wordorder freedom  and its sentences contain certain syntactic phenomena  such as discontinuous constituents  nonprojective constructions   which can not be straightforwardly handled using the annotation scheme of Penn Treebank   based on phrasestructure trees  we decided to adopt for the PCEDT the dependencybased annotation scheme of the Prague Dependency Treebank PDT  13  give an informal example  but do not elaborate on it 
Like WASP1  the phrase extraction algorithm of PHARAOH is based on the output of a word alignment model such as GIZA     which performs poorly when applied directly to MRLs  Section 32  This restriction is necessary because the problem of optimizing manytomany alignments 5 Our preliminary experiments with ngrambased overlap measures  such as BLEU  and ROUGE   show that these metrics do not correlate with human judgments on the fusion task  when tested against two reference outputs 
Although the authors of  stated that they would discuss the search problem in a followup arti cle  so far there have no publications devoted to the decoding issue for statistical machine translation This strategy is commonly used in MT evaluation  because of BLEUs wellknown problems with documents of small size  
Our system improves over the latent namedentity tagging in   from 61  to 87  The generalized perceptron proposed by  is closely related to CRFs  but the best CRF training methods seem to have a slight edge over the generalized perceptron 2 Previous Work It is helpful to compare this approach with recent efforts in statistical MT Phrasebased models  are good at learning local translations that are pairs of  consecutive  substrings  but often insufficient in modeling the reorderings of phrases themselves  especially between language pairs with very different wordorder 
When compared to other kernel methods  our approach performs better than those based on the Tree kernel   and is only 02  worse than the best results achieved by a kernel method for parsing  Lexical relationships under the standard IBM models  do not account for manytomany mappings  and phrase extraction relies heavily on the accuracy of the IBM wordtoword alignment 
Despite ME theory and its related training algorithm  do not set restrictions on the range of feature functions1  popular NLP text books  and research papers  seem to limit them to binary features 4 Conclusions Compared with other word alignment algorithms   word_align does not require sentence alignment as input  and was shown to produce useful alignments for small and noisy corpora 1 Introduction Statistical phrasebased systems  have consistently delivered stateoftheart performance in recent machine translation evaluations  yet these systems remain weak at handling word order changes 
Our approach not only outperformed a notoriously difficult baseline but also achieved similar performance to the approach of   without requiring their thirdparty data resources While we have shown an increase in performance over a purely syntactic baseline model  the algorithm of    there are a number of avenues to pursue in extending this work 
Presently  there exist methods for learning oppositional terms  and paraphrase learning has been thoroughly studied  but successfully extending these techniques to learn incompatible phrases poses difficulties because of the data distribution Formal complexity analysis has not been carried out  but my algorithm is simpler  at least conceptually  than the variablewordorder parsers of Johnson     and Abramson and Dahl  1989  
These scores are higher than those of several other parsers   but remain behind tim scores of Charniak  2000  who obtains 901  LP and 901  LR for sentences _  40 words In contrast to existing approaches   the context of the whole corpus rather than a single sentence is considered in this iterative  unsupervised procedure  yielding a more reliable alignment 
While minimum error training  has by now become a standard tool for interpolating a small number of aggregate scores  it is not well suited for learning in highdimensional feature spaces 
 and Collins and Duffy  2002  rerank the top N parses from an existing generative parser  but this kind of approach 1Dynamic programming methods  can sometimes be used for both training and decoding  but this requires fairly strong restrictions on the features in the model Ever since its introduction in general  and in computational linguistics   many researchers have pointed out that there are quite some problems in using  eg 
1 Introduction The dominance of traditional phrasebased statistical machine translation  PBSMT  models  has recently been challenged by the development and improvement of a number of new models that explicity take into account the syntax of the sentences being translated 1 Introduction Hierarchical approaches to machine translation have proven increasingly successful in recent years   and often outperform phrasebased systems  on targetlanguage fluency and adequacy 
While we do not have a direct comparison  we note that  performs worse on movie reviews than on his other datasets  the same type of data as the polarity dataset At any rate  regularized conditional loglinear models have not previously been applied to the problem of producing a high quality partofspeech tagger  Ratnaparkhi   Toutanova and Manning   and  all present unregularized models 
The most commonly used metric  BLEU  correlates well over large test sets with human judgments   but does not perform as well on sentencelevel evaluation  
We will show that some achieve significantly better results than the standard minimum error rate training of  Allomorphs  eg  deni and deny  are also automatically identified in   but the general problem of recognizing highly irregular forms is examined more extensively in  
Most recently   published their Semisupervised sequential labeling method  whose results on POS tagging seem to be optically better than   but no significance tests were given and the tool is not available for download  ie for repeating the results and significance testing 
By increasing the size of the basic unit of translation  phrasebased machine translation does away with many of the problems associated with the original wordbased formulation of statistical machine translation   in particular  The Brown et al  examine the FS of the weighted loglikelihood ratio  WLLR  on the movie review dataset and achieves an accuracy of 871   which is higher than the result reported by  with the same dataset 
While both  and  propose models which use the parameters of the generative model but train to optimize a discriminative criteria  neither proposes training algorithms which are computationally tractable enough to be used for broad coverage parsing Turneys method did not work well although they reported 80  accuracy in  
 tried a different generative phrase translation model analogous to IBM wordtranslation Model 3   and again found that the standard model outperformed their generative model The automatically generated patterns in PairClass are slightly more general than the patterns of  
Although previous work  has tackled the bootstrapping approach from both the theoretical and practical point of view  many key problems still remain unresolved  such as the selection of initial seed set Unlike   who found optimal performance when was approximately 104  we observed monotonic increases in performance as dropped 
While these approaches have had som e success to date   their usability as parsers in systems for natural language understanding is suspect 
2 Motivation and Prior Work While several authors have looked at the supervised adaptation case  there are less  and especially less successful  studies on semisupervised domain adaptation 
"Information on WOM content (i.e., message) has generally been unavailable to companies in the past because interpersonal communication such as a chat between friends leaves no record for analysis [20]. As a result, researchers turn to senders and ask whether they really are opinion leaders by a questionnaire survey [50]. "
"The network structure approach also requires knowledge on consumers’ social networks that are often private information [5,43]. Moreover, even when companies acquire information about consumers’ social networks, consumers’ influence over strangers outside their social networks in an online setting is difficult to determine. Therefore, the network structure approach is not suitable for identifying eWOM opinion leadership."
"The self-report method seems to be most popular due to existing scales such as King and Summers’ [50], although the key informant method has also been used in a recent study [59]. In addition, consumer demographics [1] and loyalty [35] are considered in conjunction with surveys to identify opinion leaders. The main findings of the extant literature are that self-reported and peer-nominated opinion leaders influence the choices of their followers. However, self-reported surveys may capture self-confidence rather than opinion leadership [2,42]."
"While individuals can arguably expand their social network to include the strangers, Dunbar’s number (150) suggests a cognitive limitation in the number of social relationships that people can maintain [30]. Existing evidence suggests that the Internet does not remove the cognitive/biological constraints on human communication [36]."
"Consumers do not know all opinion leaders, as consumers only know a limited number of peers [30], and companies cannot directly compare different opinion leaders reported in either the self-report or key informant approaches. However, our approach allows us to identify all opinion leaders among a large number of consumers and compare their relative strengths in opinion leadership. "
"Identifying opinion leaders from observed behaviors such as WOM is the most expensive method, although highly accurate [75]. Fortunately, online user reviews are now available to companies and can serve as a proxy for overall WOM [79]. This approach is consistent with recent research findings that link online consumer behavior with product sales [51]."
"Although some researchers treat all reviewers as opinion leaders [25], we are interested in examining a much smaller set of reviewers because it is costly for a company to recruit all available reviewers [75]. The theoretical basis for considering a subset of reviewers is that opinion leadership is not a dichotomy; rather, it varies in a continuous fashion [22,67]. "
"The extant literature seems to imply that marketers only need to focus on two product effects [21,23]. However, our results, based on eWOM from opinion leaders, suggest the importance of improving all three product effects at the same time. "
Many executives have little idea of how to orchestrate a marketing campaign that exploits the full power of opinion leader eWOM [31]. Part of the challenge is the lack of a proper approach to identify eWOM opinion leaders
Our paper fills the gap by studying opinion leader and eWOM together as the original interpersonal communication theory intends [47].
"By using this new approach, we identify communicative, buzz-generating, and trustworthy opinion leaders and find their eWOM positively associated with product sales, contrary to a prior study that has raised doubts about the influence of opinion leaders [74]. Furthermore"
"Our method is both more accurate than traditional survey methods [35,59] in measuring opinion leadership, and more comprehensive than network structure methods [41,43]. "
" However, unlike the study carried out by Di Marco etÂ al., authors inÂ  [22] did not report results referred to the computational burden required by the analytical model resolution, especially, in the case in which the number of nodes is big. "
"Furthermore, the procedure proposed by Samaras etÂ al.Â inÂ  [22] is restricted to multi-tier single-hop hierarchical networks (i.e., cluster-tree topology), while the work of Di Marco etÂ al.Â  [21] can be extended to other topologies as the mesh one."
" In addition, the Request to Send/Clear to Send (RTS/CTS) mechanism, commonly used in IEEE 802.11-based networksÂ  [27], is also employed right before exchanging data/acknowledgment (ack) messages. Queen-MAC is evaluated by means of computer simulation, showing a good performance in terms of network lifetime and message delivery ratio. Nevertheless, Queen-MAC exhibits some significant shortcomings. One of its main disadvantages lies in the use of the RTS/CTS mechanism."
"Throughput per linkTo calculate the throughput under HT conditions, we take inspiration from the work of Cano etÂ al.Â  [23], where the HT problem in a WSN is undertaken. Unlike the work of Cano etÂ al., we include diverse design premises of the IEEE 802.15.5 standard in the throughput characterization, which provide a more realistic study of the WMSN requirements."
" To this end, the decision makers, through a preliminary study, estimate the expected result for that goal.The idea behind distinguishing GP from other more conventional methods of optimization (i.e., single-objective, sequential-objectives or other MO techniquesÂ  [42-45]) is to introduce flexibility into the objective functions (as opposed to the rigid constraints of the conventional techniques). "
"These kinds of journeys are included in our study. However, we excluded cases where an ini-tially small agile organization grew organically (Maranzato et al., 2012), and discussions focusing on processes or tools without describing organizational change (Lyon and Evans, 2008)."
"To complicate matters, some papers talked about “the team” in singular when referring to the organization (Hodgkins and Hohmann, 2007), making it nontrivial to judge whether the orga-nization met the large scale criteria based on the choice of words of the author."
"Further examples on exclusion by the large scale facet were cases with large organizations but only a single team adopt-ing agile (Fulgham et al., 2011). We considered cases of single teams (although as part of a larger organization) unrelated to this research as our focus is on transformation of the entire (development) organization."
"Also piloting cases that reported only single teams using agile, even though considering the whole organization would meet the large scale criteria, were excluded, e.g. (Scott et al., 2008). Finally, cases where the organization was growing to large scale, but did not meet the size criteria at the start of the transformation, were excluded."
"Instead of using the most complete paper as suggested in the guidelines for systematic literature reviews (Kitchenham, 2007), we combined the results presented in each paper and considered the case as a single unit in our analysis. Keeping in mind the potential bias caused by duplicate publications, we think that including all papers enabled us to get a more in-depth understanding of the individual cases."
"There does not exist similar literature studies on large-scale ag-ile as ours, nor does there exist any surveys specifically on large-scale agile. The studies and surveys that do exist have studied agile in general, not specifically as large-scale nor agile transformations, e.g. Chow and Cao (2008) studied success factors in agile software projects in general. The only studies that touch the topic of this paper are not scientific but done by agile consulting or tool com-panies, e.g. Version One’s State of Agile surveys (VersionOne, Inc, 2016) or Forrester’s surveys (For, 2012; Giudice et al., 2014)."
"This self-consistent estimate converges on the true distribution at a faster rate than traditional binning or kernel density estimation methods (Bernacchia and Pigolotti, 2011).However, during our initial attempts to apply the PDF estimation method of Bernacchia and Pigolotti (2011) (hereafter BP11), we discovered that its computational performance is not practicable. "
"Though this manuscript focuses specifically on the case of using the nonuniform FFT to improve the ECF calculation stage of the Bernacchia and Pigolotti (2011) estimation method, this method should be applicable to other ECF-based methods. We posit that the nonuniform FFT would especially reduce the computational cost of multidimensional ECF calculations: potentially by a factor of íª(100d) for d-dimensional data.If the BP11 method can be extended to multidimensional data, then a nonuniform FFT method could be used to dramatically decrease the computational time of the method. "
"To address this issue, a more concise view of model differences is required that aggregates the atomic operations into composite operation applications such that the intent of the change is becoming explicit. Existing solutions (Hartung et al., 2010; KÃ¼ster et al., 2008; Xing and Stroulia, 2006) only provide language-specific operation detection algorithms. However, due to the plethora of existing modeling languages, this is an unfavorable solution."
"First, there is the approach by Xing and Stroulia (2006) for detecting refactorings in evolving software models which is integrated in UMLDiff. Refactorings are expressed by change pattern queries used to search a diff model obtained by a state-based model comparison. Although, the general goal of UMLDiff is comparable to ours, there are several major differences. First, UMLDiff is tailored to a fixed modeling language, namely a subset of structural UML diagrams while our approach is applicable for any modeling language."
"Second, the approach by Vermolen et al. (2011) copes with the detection of complex evolution steps between different versions of a metamodel. They use a diff model comprising primitive operations as input and calculate on these basis complex operations. The approach is tailored to the core of current metamodeling languages, but follows a similar methodology as UMLDiff. However, a specific feature is the detection of masked operations, i.e., operations hidden by other operations, by defining additional detection rules. Nevertheless, the approach is again dedicated to one single modeling language and does not allow to reuse the operation specifications used for execution for the detection process."
"Third, KÃuster et al. (2008) calculate hierarchical change logs for process models. The authors apply the concept of Single-EntryâSingle-Exit fragments to calculate the hierarchical change logs after computing the correspondences between two process models. Thereby, several atomic changes are hidden behind one compound change. The difference to our work is twofold. First, we consider the detection of composite operations comprising changes cross-cutting the whole model, i.e., we are not restricted to reason only about one hierarchy branch, and second, our approach is language independent, thus we are not restricted to process models.Ontology engineering. "
"Buckley et al. (2005) introduced four aspects of software changes as the basis for software evolution: temporal properties (when), object of change (where does the change occur), system properties (what), and change support (how). The authors neglect the stake-holders (who) and the reason for change (why), which is essential to address the “nature of [the] evolution phenomenon, its drivers and impact” (Lehman and Ramil, 2002). "
"Managing (evolving) functional requirements is a well-studied topic in the literature (Ramesh and Jarke, 2001). In contrast, managing non-functional requirements – especially dependability requirements – for aPS is rarely addressed (Fay et al., 2015; Ladiges et al., 2013). One reason is that the relationship between evolution and dependability of a system is vaguely understood until now because both are very complex challenges (Felici, 2003; Machado et al., 2006; Vogel-Heuser et al., 2014c)."
" The restructuring of a production unit versus a continuous improvement process of these units is another topic in this field (Schuh et. al 2013). Furthermore, continuous integration of the software and the hardware of an aPS is more complicated than in a pure software setting as the changes in hardware are much slower to realize than in software as well as that simulation models of the hardware system are required for automated integration tests."
"A variety of norms, guidelines, recommendations and approaches is available for performing a structured and systematic requirements engineering. However, in industrial practice these approaches are not strictly followed. This was, for example, shown in a survey regarding requirements engineering practice for software projects in industry by Neill and Laplante (2003). Morris et al. (1998), identified the following reasons for missing requirements engineering practices in research and development projects in industry"
"Choices are often made as a result of project team discussions and not by comparing design possibilities with requirements specifications to find the best solution. Decisions are made based on employer experience as well as reusable existing solutions and done intuitively. Reasons named by Bellgran and Säfsten (2010) are mainly high time-pressure and low priority despite of the assumption that a well performed requirements engineering helps reducing time for fixing design and implementation errors. The evolutionary changes usually have to be carried out under even tighter schedules and higher time pressure, because production standstill must be minimized, thus resulting in even worse boundary conditions for a systematic requirements engineering."
"For many aPS in operation, a formal specification has never been provided (Frey and Litz, 2000). This often includes that no assessment of the quality of changes is done because of lack of time, or cost constraints, or missing measurements (Bellgran and Säfsten, 2010). Consequently, a lack of explicit knowledge about the process behavior and its actual occurring properties unfolds."
"While engineering approaches exist in the different disciplines to address these problems (e.g., architectural description languages in software engineering Medvidovic and Taylor (2000), the problems become more difficult to identify and solve if a change in one discipline's parts results in problems in another discipline."
"A formal semantics for automatic verification of structural compatibility has been proposed (Feldmann et al., 2014a), but verifying functional conformance is not considered yet."
"With respect to the repair of inconsistencies, approaches have been proposed to use OCL constraints and the part of the constraint, which has not been satisfied, to create an appropriate repair action (Nentwich et al., 2003)."
"Most approaches addressing safety employ this kind of additional annotations (Giese and Tichy, 2006; Grunske et al., 2005; Papadopoulos et al., 2001). Tribastone and Gilmore (2008) and Becker et al. (2009) propose similar approaches addressing performance.Those approaches are restricted to be used during the design of the system as they only use assumptions of the behavior of the system with respect to its characteristics affecting probabilistic quality attributes, e.g., failure rates, performance of certain parts."
"Despite efforts toward including object-oriented programming aspects within IEC 61131-3 (IEC, 2013), the standard in its current version has not yet been fully established in the industry. Thus, we focus on IEC 61131-3 without object-oriented extension (IEC, 2009), which is mostly used within state of the art industrial applications (Thramboulidis and Frey, 2011)."
" The evolution of aPS especially during operation as discussed in scenario category “V” (Table 1) is performed on code level by suppliers' start-up personnel or customers' operation staff with limited software education, but huge knowledge about the process. This may lead to inconsistencies between implementation and design artifacts as well as to unclear code structure (Katzke et al. 2004; Vogel-Heuser et al. 2014a)"
" In Kormann et al. (2012), the semantics of sequence diagrams are adapted in order to make direct IEC 61131-3 code generation possible. In this way, the modeled test scenarios can be executed directly. In recent years, SysML is increasingly established for supporting the development process of real-time systems by Detommasi et al. (2013). However, investigations on the possibilities to derive test cases from these models or adapting these models are still missing."
"Lochau et al. (2014) propose model-based testing for variant-rich aPS. Based on a 150% UML state chart test model incorporating all variant-specific test models with explicit specification of differences by means of feature annotations, test case generation for the complete system family is applicable in order to test the corresponding PLC control software. In contrast to test case generation for each variant in isolation, their approach exploits the specification of commonality and variability between variants to reuse generated test cases also for other variants. "
"Static code analysis is successfully applied for several programming languages and environments, e.g. Lint for C (Johnson, 1988) and FindBugs for Java (Ayewah et al., 2008). However, static code analysis for IEC 61131-3 is not yet supported sufficiently (Angerer et al., 2013)."
"Finally, the MechatronicUML provides specifically support for systems which self-adapt their behavior at runtime by modeling adaptations by architectural reconfiguration based on graph transformations (Eckardt et al., 2013; Tichy et al., 2008). Verification of the adaptation behavior is based on graph transformation verification approaches (Becker et al., 2006; Ghamaraian et al., 2012). However, the approach does not address the specifics of aPS, e.g., the employed languages."
"Verification of PLC programs – written mainly in the programming languages Sequential Function Charts (Bauer et al., 2004) and Instruction List (Huuck, 2005) – was investigated by means of the model checker UPAAL, but lack in analyzing industrial PLC programs due to size and structure. "
"Arcade supporting model checking to of PLC programs – written for the programming languages ST, IL as well as vendor-specific programming language and their combinations as often applied in IEC 61131 environments – is presented in Biallas et al. (2012). It provides a counterexample-guided abstraction refinement mechanism, which is already applied successfully for verification of embedded systems in Stursberg et al. (2004) and hierarchical predicate abstraction (Biallas et al., 2013) to cope with the challenge of state explosion. Nevertheless, the approach was not applied to industrial PLC software to identify its applicability in practice."
 Greifeneder and Frey (2007) proposed the modeling language called DesLaNAS which can be applied in combination with a probabilistic model checking but an automatic translation of the description model into the form needed for model checking is not available. 
" Variant annotations, e.g., using stereotypes in UML models (Gomaa, 2006) or presence conditions (Czarnecki and Antkiewicz, 2005), define which parts of the model have to be removed to derive a concrete product variant. Annotative variability models become easily very complex and unmanageable for large SPLs with many variants."
" Thüm et al. (2009) present an algorithm computing the differences between two feature models after changes to a feature model. In general, the inputs are the original and the evolved feature models. The algorithm computes deleted or added product variants and scales up to large feature models with thousands of features. However, it only focuses on a homogeneous feature model and does not cover multidisciplinary aspects and features of different granularity."
"Tool support is fully available and is already tested with an Eclipse as a large product line example (Pleuss et al., 2012). But, again, multidisciplinary feature modeling is not considered."
" Wille et al. (2013) apply this idea to block-based diagrams, e.g., as available in Matlab/Simulink. Holthusen et al. (2014)apply it in a prototypical manner to the IEC 61131-3- language FBD. However, an extension to the full language scope of IEC61131 is still missing."
"This cycle of generating code from models and propagating changes on the source code back to the models is known as round-trip engineering (Hettel et al., 2008) in contrast to the one-way forward engineering of source code from models and the one-way reverse engineering from models from source code. A particular issue, which complicates this challenge, is that manual changes in source code often do not have a corresponding equivalent in the model, i.e., the modeling language is not able to express the changed behavior."
"Sim and Vogel-Heuser (2010) proved the benefit of active learning comparing mechatronic engineering students with students of computer science, but still an appropriate education for MDE with a focus on aPS is missing."
"The modeling approaches used in these and other works (Bassi et al., 2011; Hackenberg et al. 2014; Bonfè et al., 2013) enable an integration of software models and physical models into a single consistent syntax. In contrast to these integrated approaches, there are many research works that combine control code (or an executable model thereof) with an object-oriented simulation model of the physical parts of the aPS. Such approaches require to include a separate simulation tool, which is possible for the designer but not for the technician on-site, and are therefore not considered here in more detail."
"Aside from works addressing IEC 61131-3 implementations, event-driven implementations conforming to the IEC 61499 standard (Bianchi et al., 2003; Chhabra and Emami, 2011; Hirsch, 2010; Hirsch et al., 2008; Vyatkin et al., 2009) were proposed. A systems engineering framework based on SysML and IEC 61499 is considered in Hirsch (2010) and Hirsch et al. (2008). However, with the approach of Hirsch (2010) and Hirsch et al. (2008), debugging of automation software directly inside the SysML model is not provided."
"Another approach (Angyal et al., 2008) proposes to use the Abstract Syntax Tree of the generated source code as an intermediate model, which represents the source code as a model, and uses bi-directional incremental model merges to keep the abstract syntax tree consistent with the changed code. However, the approach does not address the challenge to ensure that the abstract syntax tree is consistent with the model. Furthermore, it does not allow the use of template-based code generation."
"Several recent efforts to invert large sparse matrices using a series of computational tricks show promise, though they are still extremely computationally expensive [4,12]"
"it would lead to additional costs and set-up burden, as it would require a Spark cluster [29]"
"“Historians of science are accustomed to call these two traditions in science Cartesian and Baconian, since Descartes was the great unifier and Bacon the great diversifier at the birth of modern science in the seventeenth century.”[1, p. 40].However it is doubtful that Dyson believes these are exact synonyms for his terminology."
" In an adult brain, white and grey matter, despite of their anisotropic (WM) and isotropic (GM) properties, are characterized by similar ADC values [26,27] and cannot be efficiently distinguished."
"In this context, various solutions have been proposed to identify possible malicious apps and behaviours (Aafer etÂ al., 2013; Arp etÂ al., 2014; Google, 2012; Wu etÂ al., 2012). However, most of these mechanisms are based on analysis of permissions granted to apps, i.e., the set of API methods they are allowed to invoke. This type of analysis is not sufficient, because it can result in an over-approximation that rates legitimate apps as malicious (false positives) and it is not able to detect collusion attacks66Attacks that allow apps to indirectly execute operations for which they do not have specific permission."
"In contrast to the refinement rules proposed in Neisse and Doerr (2013), our extension considers also the modification of events in addition to only allowing or denying the execution of activities."
"Approaches like Damopoulos etÂ al. (2014) and Grace etÂ al. (2012) that focus specifically on malware detection is out of the scope of this paper. We present these solutions in chronological order, briefly analysing the functionality proposed, the implementation details, limitations, and contrast to our proposal."
"Kirin, proposed by Enck etÂ al. (2009), is a security service running on the phone which analyses the requested permissions of an app and detects potential security flaws. When an app is about to be installed, Kirin evaluates, using a rule based engine, if there is a match between the set of requested permissions and the signatures defined in the rule engine. Note that the signatures defined in the rule engine represent possible attack vectors, for example, RECORD_AUDIO and INTERNET permissions define a rule in the signature set. Kirin results in a high number of false positives since legitimate apps that follow the defined signature pattern are characterised as malicious. In contrast to our proposal, Kirin does not provide enforcement capabilities, it is only a solution to inform users about possible risks."
"Ongtang et al. (2009) propose Saint as an extension of Kirin. In addition to the analysis performed by Kirin at install time, Saint monitors apps also at the runtime Inter-Component Communication (ICC) flows, e.g., activities initialisation, components binding to services, access to content providers, etc. The policies defined in Saint are static and similarly to our approach define conditions to control the runtime behaviour. For example, when a specific activity can bind with a specific content provider considering the allowed permissions, signature, or package name. Saint is implemented as a modified Android middleware, while our proposal relies on app instrumentation for policy enforcement. Furthermore, we propose a more flexible architecture and expressive policy language with the possibility of deploying and changing policies at runtime without requiring changes to the middleware or instrumented app."
"Orthogonally to our proposal, Dietz etÂ al., 2011) propose QUIRE as a solution to protect android apps manipulation by other malicious apps or services. Their proposal is to enable apps to reason about access to sensitive data through call chain validation. To achieve this goal the authors propose the modification of the underlying OS IPCs mechanism in order to pass the appropriate information between IPCs.In the same direction, Porscha, proposed by Ongtang etÂ al. (2010), introduces a Digital Rights Management (DRM) framework for Android phones that mediates the access to protected content between different Android components. For example, it can regulate the access of an app to the content of an SMS message. The Porscha mediator supports constraints on devices, apps, and on the use (e.g., cardinality) of the protected data. Porscha mediates ICC flows, with extensions including a policy filed, and it has been implemented as a modified Android firmware that is considered to be trusted. Our proposal does not require changes to the Android firmware, therefore, our solution could be adopted straightforwardly in all different firmware versions."
"Bai etÂ al. (2010) propose a context-aware usage control that focus on a user basis mechanism for granting and revoking permissions, similar to the approach introduced in the latest android OS version. To do so, authors enable users to define policies related to application permission grants in order to protect access to users' sensitive resources. However, the use of this approach requires the modification of underlying Android OS services. In a similar direction, Sun etÂ al. (2012) introduce a design that requires the modification of Android sandbox as well, in order to monitor access to sensitive information. In this approach the hook points are installed before the actual permission check occurs by Android OS. On the contrary, SecureDroid (Arena etÂ al., 2013) extends Android OS security manager service to control access also for user defined sensitive URIs."
"Feth and Pretschner (2012) employ information flow tracking as well. Their framework uses an expressive policy language to describe users' preferences to content providers, intents, and certain data sinks like the network, file system and IPC in order to eliminate access to private data. Jung etÂ al. (2013) extend the work of Feth and Pretschner with context-aware policy rules. In this direction, Andriatsimandefitra etÂ al. (2012) introduce an approach for determining data flows, which is an important aspect for realising a policy enforcement tool. In contrast to all these approaches for policy enforcement in Android we are the only ones to propose the use of policy refinement techniques to simplify the management of the security policies by end-users."
"Constroid, introduced by Schreckling etÂ al. (2012), also defines a management framework for employing data-centric security policies of fine granularity. To do this, Constroid adopts the UCONABC model. However, in contrast to our contribution in this paper only the abstract model is detailed and no concrete example of policy is provided."
"Zefferer and Teufl (2013) propose a solution for device security assessment based on user defined preferences. The use of security policies guarantees that each application that integrates the developed service can define and assess its own critical aspects. In order for such a service to be employed in a given device is required by a third party app to integrate the appropriate controls to the app through the corresponding API. However, researchers have shown that programmers are not taking into consideration in most of the cases security features."
"AppGuard, introduced by Backes etÂ al. (2013), is an app instrumentation framework that runs directly in users' device and allows user-centric security policies customisations. AppGuard computes a risk score for each app considering the number of dangerous permissions and provides the option of instrumenting the app to control the access to âdangerousâ calls. For example, in an app with NETWORK permission, a user can choose to enable/disable the corresponding functionality. The solution presented by Bartel etÂ al. (2012) follows the same direction. In contrast to our proposal these solutions do not support context-based policy specification and policy refinement in order to simplify the policy management by end-users."
"DroidForce proposed by Rasthofer etÂ al. (2014) relies on the Soot framework for analysing and instrumenting an app to enforce a security policy. This approach considers PEPs injected in multiple applications with a single PDP running as an app in the phone, addresses information flow intra-app statically and inter-app at runtime, and uses an expressive policy language with cardinality and temporal constraints. Their policies allow or deny an activity, while do not support modification/obfuscation of values. Complementary, Jing etÂ al. (2015) propose DeepDroid, which in contrast to DroidForce performs instrumentation at the native level with the possibility of intercepting system calls in addition to methods invocation to regulate the access to sensitive resources. However, DeepDroid does not consider information flow tracking nor uses any expressive policy language for enforcement."
"More similar to our approach Cotterell etÂ al. (2015) introduce a solution to enable users to install policies for controlling sensitive data; however, the policies are not based on access to sensitive resources. Instead, the authors focus on known malicious activities for defining a policy with a more explicit focus on malware apps."
"Empirical studies showed that, by integrating the advantages of different EAs into one framework, PAP not only provides practitioners a unified approach for solving his/her problem set, but also may lead to better performance than a single EA [17]."
"1. In addition, some benchmarking studies have been undertaken to empirically compare the performance of these various techniques (e.g., Baesens et al., 2003), but they did not focus specifically on how these techniques compare on heavily imbalanced samples, or to what extent any such comparison is affected by the issue of class imbalance. For example, in Baesens et al. (2003) seventeen techniques including both well-known techniques such as logistic regression and discriminant analysis and more advanced techniques such as least square support vector machines were compared on eight real-life credit scoring data sets."
"Our proposed method has some particularities that distinguish it from previous textural analyses in the literature. First, unlike the method in [11] that focuses on particular affine regions, our method considers all pixels to have the same importance. "
"In all cases, the connectivity descriptors outperformed all the other approaches by at least 5% (UIUC), 2% (Outex), and 13% (KTH-TIPS2b) in relative percentages. In KTH-TIPS2b the classification performance was also better than that reported in [16] (76%) using a similar database and protocol."
" It has been reported that these PSO variants have a more diverse search than does the standard PSO [1,3,10,17]"
"Most models derive a chaotic system by transforming the original constraint problem into an unconstrained one with a diffeomorphic function, such as a sigmoid function, and then applying the steepest descent method with a large step-size to the unconstrained problem, for which it is well known that the derived system is chaotic if the step-size is sufficiently large [6]."
"As a countermeasure, a large inertia weight wd of(C1) was selected in [17]; this can reduce the amount of change of the critical value β that is caused by the variation in r.However, this is not an essential solution because the change remains."
"Since in [17] it was reported that the diversity of the search was more significant during the early stages in the numerical experiments for CPSO-VQO, in this model, the initial value of σ(t) is set to be large and is decreased exponentially"
"“What does the user want to see?” and “What do the data want to be?” as well as how these two points mutually enhance one another. In the problem domain of cyber security, Fink et al. (2009) developed a set of visualization design principles with a focus on high-resolution displays and presented proto-types according to these design principles. Goodall et al. (2004) conducted contextual interviews to gain a better understand-ing of the intrusion detection workflow and proposed a three-phased model in which tasks could be decoupled by necessary know-how to provide more flexibility for organizations in train-ing new analysts. However, none of these user-centered studies tackled behavior-based malware pattern analysis."
"On the other hand, there are systems for “Malware Classifica-tion” which support the comparison of many samples to identify the common behavior (e.g., Gove et al., 2014; Han et al., 2014; Long et al., 2014). For example, Wüchner et al. (2014) pro-posed the interactive “DAVAST” system for the detection and analysis of email worm attacks. Additionally, Gove et al. (2014) introduced “SEEM”, which allows the behavioral comparison of a large set of malware samples in relation to the imported DLLs and callback domains. However, none of these approaches covered the full set of requirements as specified by Wagner et al. (2014) during their problem characterization and abstraction. "
"when sequencing molecules, the position where a circular sequence starts can be totally arbitrary. For instance, the linearised human (NC 001807) and chimpanzee (NC 001643) mito- chondrial DNA (mtDNA) sequences do not start in the same region [13] ."
They propose two algorithms. The first one modifies the branch and bound algorithm of Barrachina and Marzal [2] by avoiding exploring ranges known to be lower than the lower bound in the branch and bound computation. The second one modifies the BBA algorithm by preventing searching for distances when it is known that the final result will not improve the current external bound.
"Adaptive systems, however, have one main disadvantage: new unlabeled data has to be robustly included into an already built model. Typical approaches are self-training (e.g., Rosenberg et al., 2005; Li et al., 2007), co-training (e.g., Blum and Mitchell, 1998; Levin et al., 2003), semi-supervised learning (e.g., Goldberg et al., 2008), or the application of oracles1 (e.g., Nair and Clark, 2004; Wu and Nevatia, 2005)."
"Having a large stack assures that the assumption for the negative bag containing at least one negative sample is mostly valid, since the probability that an object stays at one specific location over a longer period of time is very low (Sternig et al., 2010a)."
"This causes short-term drifting in existing classifier grid approaches (e.g., Roth et al., 2009), which is in particular the problem addressed within this paper. From all experiments the benefits of the proposed methods are clearly visible."
"Creating temporary solutions to the code base increases complexity, which makes further development hard and time-consuming (Yli-Huumo et al., 2015a; Yli-Huumo et al., 2014)"
"The metaphor technical debt (TD) has been introduced by Ward Cunningham (Cunningham, 1992). He describes the metaphor as ‘Shipping first time code is like going into debt. A little debt speeds development so long as it is paid back promptly with a rewrite. Objects make the cost of this transaction tolerable. The danger occurs when the debt is not repaid. Every minute spent on not-quite-right code counts as interest on that debt.” (op.cit., p. 29-30). Even though the metaphor was first introduced over twenty years ago, a recent mapping study shows that it has received the attention of researchers and practitioners only in the past few years (Li et al., 2015a)."
"TD is often seen only as a negative concept in software development (Lim et al., 2012;Yli-Huumo et al., 2014). Software developers think that creating shortcuts and non-scalable solutions will increase the complexity within the code base (Yli-Huumo et al., 2014). When the code base starts to accumulate with too much TD that is not fixed afterwards, the development becomes more challenging, because the shortcuts are not designed to work well with other parts of the code base. Complexities in the code base start to reduce the overall quality and productivity goes down when new solutions and features must be implemented to the code base in debt (Yli-Huumo et al., 2015a)."
" The current literature related to TDM has identified and developed some processes and tools (Li et al., 2015a). Managing technical debt (MTD) workshops have gathered multiple studies related to TD and TDM in the past years (Seaman et al., 2015). However, TDM is challenging to implement, and it is hard for managers and developers to estimate and identify what and how much TD the current system has, how it will change, and what effects it will have in the future (Li et al., 2015a)."
"The reduction and repayment of TD are done by refactoring or rewriting the bad solutions (Codabux and Williams, 2013). Refactoring or rewriting can be seen as processes for “changing a software system in such a way that it does not alter the external behavior of the code yet improves its internal structure. It is a disciplined way to clean up code that minimizes the chances of introducing bugs. In essence when you refactor you are improving the design of the code after it has been written” (Fowler et al., 1999, p. 9). However, changing old solutions in the code is not easy, because improving the code base requires a significantly competent developer, and the company cannot just use all development time on refactoring or rewriting the solutions. "
"Even though the current literature has started to tackle and identify the concept and solutions of TDM, the problem is that there is a need for more empirical evidence from real-life software development (Li et al., 2015a)."
"TD was an important discussion topic in most of the development teams. This is not a surprise, considering the popularity of TD research in the past few years (Li et al., 2015a). The biggest issue with TD communication has been the gap between technical and non-technical stakeholders (Klinger et al., 2011). "
"Communication related to TD issues does not often transfer from the development team to the business stakeholders, which leads to TD issues not receiving the required time to get fixed (Yli-Huumo et al., 2014)."
"Without proper tracking and documentation of architectural changes and issues, it is also extremely challenging to quantify TD (Klinger et al., 2011). The inability to quantify TD also creates more challenges to other TDM activities, such as communication, repayment, monitoring, and measurement, due to the lack of TD data."
"However, the challenge in identification is that TD is not just related to simple errors, but especially to the architectural and design issues of software. It is challenging to identify this type of TD with tools. The challenge is how the tools tackle architectural or structural issues and technology gaps (Kruchten et al., 2012a)"
"Ramasubbu et al. (2015) describe TD prioritization with three dimensions: customer satisfaction needs, reliability demanded by the business, and probability of technology disruption. These dimensions are essential for decisions, but quantifying these with exact numbers is extremely difficult."
"A case study does not provide statistical generalizability (Yin, 2003), i.e. a case study with a limited number of cases cannot be generalized over a population. We, however, consider generalization as theoretical (Lee and Baskerville, 2003), i.e. abstraction from concrete events and actions to theoretical constructs. "
"[B] A comparison of the fuzzy and estimated reliabilities in this study and those obtained by Chen [7].Chen [7] used a fuzzy number that was used as a triangular fuzzy number in this study. As stated in [A], the triangular fuzzy number is a special case of the Level (Î»,1)iâv fuzzy number. Therefore, the consideration of fuzziness is superior to that of Chen [7]. Chen did not consider the estimated reliability of the system in the fuzzy sense."
The cumulative distribution of Eq. (11) is shown as the black line in Fig. 12. It can be concluded that the zero parameter model by Angelidou et al. [25] predicts a reasonable but not perfect distribution of enstrophy
"First, we transform the skin probability map using the DSPF space, and then apply the spatial analysis in the transformed skin map. For the spatial analysis we adapted an algorithm based on the distance transform in a combined domain (DTCD) of hue, luminance and skin probability, developed during our earlier study (Kawulok, 2013). Originally, the DTCD algorithm operates in raw skin probability maps, but here it has been adapted to perform the propagation in the DSPF space. The proposed method was compared with another approach proposed by Jiang et al. (2007), who also exploit the textural features, which is followed by the spatial analysis. Not only do the results indicate that our method outperforms the alternative algorithms, but also the gain attributed to the spatial analysis is larger than in case of processing raw skin probability maps."
"In our earlier research (Kawulok, 2010) we introduced an energy-based technique for skin blobs analysis. The pixels are adjoined to the skin regions depending on the amount of the energy which is spread over the image according to the local skin probability..Recently, we proposed to use the distance transform in a combined domain (DTCD) of hue, luminance and skin probability (Kawulok, 2013). The algorithm was proved to be very competitive and outperformed our energy-based method and the method proposed by del Solar and Verschae (2004). We overcame the most significant shortcoming of the latter approach, i.e. misbehaving in case of smooth transitions between skin and non-skin regions, by taking advantage of the cumulative character of the distance transform. "
"The proposed algorithm was compared with the Gabor wavelet-based texture analysis method proposed by Jiang et al. (2007). As it was mentioned in Section 2, the texture map threshold ÎT is set individually for every image presented in the original work. We tried to determine an optimal value of the threshold investigating the range 5â©½ÎTâ©½100, which covers all of the values quoted by the authors, but the overall results were quite poor. We overcame this problem by multiplying the skin probability map by the normalized textural filter response (0 for the maximal value, and 1 for the minimal), instead of applying the threshold ÎT."
"The table also includes the comparison with the DTF method (Kawulok, 2012). Although this earlier approach performs better if the distance transform is not applied (Î´min=20.39% compared with 21.14% for DSPF), the improved BIF extraction scheme offers lower error than DTF with DTCD. Moreover, the computation time is also reduced here due to a smaller number of the basic image features."
"Existing research has demonstrated the social engineering risks posed by such OSINT data (Ball etÂ al., 2012). However, this normally relies on labour intensive manual analysis (Creese etÂ al., 2012), which is impractical and poses a high cost to a potential attacker. Alternatively, such techniques utilise automated conversational agents (Huber etÂ al., 2009), which do not scale and are not very effective due to the challenges of imitating human conversational behaviour. "
"Huber etÂ al. (2009) make use of an organisation's Facebook presence to automatically identify and target its employees. Their tool gathers public information on members from Facebook, then attempts to expand that information through mechanisms like friend requests. Theoretically, their tool then uses Facebook chat to act as a chat-bot, building a rapport before executing a predefined attack (e.g., sending a link). Their evaluation shows that this scheme is impractical due to the overhead associated with imitating a human conversational partner."
"Ball etÂ al. (2012) detail how open source information can be used to construct spear-phishing attacks on an organisation's employees. They manually mine employee information from an organisation's website and gather additional information using the Maltego toolkit, before then using the Simple Phishing Toolkit to create phishing emails based on each employee's interests.The approach of Ball etÂ al. demonstrates the value of OSINT in this domain, but their method still relies on significant manual workload, whereas we focus on methods which can be deployed as part of a completely automated scanner."
"Scheelen etÂ al. (2012) attempt to map out a company's structure from online sources, including gathering information for social engineering. In their method, they first connect to the company on LinkedIn and then crawl LinkedIn for a list of employees, then search Facebook for those employees, matching on name, profile picture and location. They prune multiple matches by sending friend requests from âzombieâ profiles which are designed so as to look relevant to the targeted organisation. Their organisational mapping is based on heuristic processing of self-reported roles in LinkedIn profiles.In contrast to the connections and friend requests utilised by Scheelen etÂ al., our interaction with the target organisation is entirely passive, leaving the target organisation unaware of this stage of information-gathering. While we also resolve the identities of employees, we do this through a more flexible process using a larger and richer set of potential features, as described in Section 4.2.2.2.2"
"Clearly, the sheer amount of data created by mobile devices, the Internet of Things (IoT)  [1], and a myriad of other sources cannot be handled by traditional data processing approaches  [2]."
"More recently, the usage of simulators in the Cloud Computing field has also become widespread, which motivated the development of a number of simulators such as CloudSim   [12], GreenCloud   [13], and iCanCloud   [14]. None of these, however, can effectively model CEP applications."
" Existing approaches  [3,9-14] mainly consist of analysing, at design time, the contextual changes and the generation of the reconfiguration plans to fit the new environmental conditions. Then, the set of valid configurations are pre-calculated, as well as the differences between pairs of configurations and the conditions to adapt the system from one configuration to another one. All this previously calculated information is loaded into the device as part of the knowledge base of the MAPE-K loop. This is a shortcoming which limits the number of possible configurations and prevents the generation of the optimal ones."
 Other existing approaches that generate the configurations at  runtime [15-20] also have limitations in mobile environments as usually most of them demand high computing resources.
"Those DSPL approaches that perform the analysis and derivation of reconfiguration plans at design time are usually based on the definition of a set of eventâconditionâaction (ECA) rulesÂ  [29,20]. An ECA rule includes the event that will trigger a reconfiguration, a condition about the system state that must be evaluated as true, and the reconfiguration plan or actions that have to be executed. The main problem with this approach is that the number of rules could become untreatable, especially if the number of potential configurations is very high."
"4.1Generation of the reconfiguration planAs Brataas etÂ al. show inÂ  [30], the reconfiguration time is divided into three different tasks: (1) analyse the context data; (2) plan (decide) the new configuration and (3) execute the plan in order to deploy the new configuration. They prove that the cost of the first and third tasks can be considered fixed, while it is critical to make the plan task as efficient as possible because it depends on the number of configuration variants. Therefore, the challenge is to find the set of choices for the VSpecs tree (i.e.Â the resolution model) that defines the optimal configuration (the one that provides the highest utility while not exceeding the resources limitations) in a highly efficient way. However, this is an NP-hard problemÂ  [26] and, therefore, it is impossible to use exact techniques to solve this optimization problem for our purpose."
"Shen etÂ al.Â  [29] propose a dynamic reconfiguration approach based on dynamic aspect weaving where the set of valid configurations is also generated at design time and the reconfiguration process is triggered by ECA rules. The variability is specified using FMs, and they propose a meta-model for specifying role models, which are used to bind features to elements of the software architecture. However, their approach relies on the JBoss-AOP framework, which is not available in mobile devices. "
"Although the MUSIC middleware does not focus on mobile devices in particular, it is possible to execute it on mobile devices supporting an implementation of the OSGi platform (e.g.Â Android devices).Brataas etÂ al.Â  [35] propose a mechanism for extending MUSIC with support for specifying the requirements and the utility of the components of a software architecture. They estimate how many hardware operations are generated by each user action and the application response time. To this end, a structure and performance (SP) model is defined, which allows them to evaluate, at runtime, the resource usage of each configuration and therefore decide whether a configuration is appropriate for the current context. We have identified several drawbacks to this approach:"
"Cheng etÂ al.Â  [17] propose a predictive, instead of reactive, adaptation approach, trying to foresee changes in the availability of resources and lower the disruption to the quality of service provided to the user. To this end, they extend the Rainbow frameworkÂ  [37] with a mechanism for predicting resource availability based on data gathered in the past. The adaptation process is based on predefined strategies, which specify the changes that need to be applied to the system under certain conditions and contexts in particular. Furthermore, this approach has been evaluated by applying it to a web service and, therefore, it is not possible to assess the suitability of their approach in the case of mobile applications."
"InÂ  [18], Gomaa etÂ al. propose a DSPL approach which enables the dynamic adaptation of software architectures, but it is exclusively focused on service-oriented architectures. The variability is modelled using FMs, the features of which are mapped to the artefacts of the software architecture. However, it does not provide details on how a configuration of the FM is selected at runtime, although it states that this process is usually human assisted."
"The most similar approach to ours is the work presented inÂ  [19], where an optimization algorithm is also used to improve user interface adaptation at runtime. An important difference is that their work is specific to a user interface architectural model, while our approach is more general because it can be applied to the architectural model of any kind of applications. In their approach, the dynamic variation points are modelled by performing a mapping between the context and the different reconfiguration actions that can be executed at runtime. They use a different optimization algorithm (NSGA-II) although, as in our case, their approach does not depend on a particular optimization algorithm and is designed to work with other algorithms. Finally, the average adaptation time of our approach is considerably lower than the reported inÂ  [19]."
" On the other hand, the proposal of Benavides etÂ al.Â  [38] always finds the optimal configuration using Constraint Satisfaction Problems with exponential-time complexity, making it unsuitable for runtime optimization."
"Church's solvability problem was first raised for specifications in S1S (monadic second order logic of one successor). It inspired the great works of Buchi and Landweber on finite games of infinite duration [2-4] and of Rabin on finite automata over infinite structures [13,14]."
"Pnueli and Rosner extended the question to a setting, where the processes have access to incomplete information [12,10,9,7,1]. They introduced architectures, where the communication from an external environment to working processes and the communication between working processes is through boolean variables. "
"In this article, we show that distributed synthesis is undecidable even for the syntactic safety and reachability fragments of LTL [11,6], of ACTL [6], and of their semantic intersection."
"Different from other incremental models of self-organization that create new neurons at a fixed growth rate (e.g. Fritzke, 1995, 1997), GWR learning creates new neurons whenever the activity of well-trained neurons is smaller than a given threshold. This mechanism creates a larger number of neurons at early stages of the training and then tune the weights through subsequent training epochs. While the process of neural growth of the GWR algorithm does not resemble biologically plausible mechanisms of neurogenesis (e.g., Eriksson et al., 1998; Gould, 2007; Ming & Song, 2011), it is an efficient learning model exhibiting a computationally convenient trade-off between adaptation to dynamic input and learning convergence. For instance, it has been shown that GWR learning is particularly suitable for novelty detection and cumulative learning in robot scenarios (Marsland, Nehmzow, & Shapiro, 2005)."
"For example, Sasama etÂ al. (2009) proposed an error back-propagation neural network model that takes two images as input and produces one binary vector as an output that encodes the angular disparity between the two input images together with a response answer (match/mismatch). Similarly, Inui and Ashizawa (2011) proposed a radial basis function neural network to mentally rotate 3D objects. These models use neural networks but these networks have not been designed to reproduce brain mechanisms suggested to underlie mental rotation in humans. "
"These aspects have indeed been neglected by prior computational models of mental rotation (e.g., Inui & Ashizawa, 2011; Sasama etÂ al., 2009). Although cognitive robotics models of mental simulation have been recently proposed (Di Nuovo, De La Cruz, & Marocco, 2013), these do not directly address mental rotation capabilities, but rather mental simulation for motor planning tasks. They also do not propose hypotheses on the brain mechanisms that may underlie them (e.g., Di Nuovo, Marocco, Cangelosi, De La Cruz, & Di Nuovo, 2012). "
"In this study we propose a new neuro-robotic model of mental rotation that builds upon the prior model proposed in Seepanomwan etÂ al. (2013a, 2013b) and overcomes its limitations discussed above. Specifically, the new model has generalisation capabilities to transfer the mental rotation processes acquired with a small set of 2D visual training stimuli to novel 2D visual objects. "
"The architecture we proposed and its functioning mechanisms represent a further step with respect to previous computational models (e.g., Inui & Ashizawa, 2011; Sasama etÂ al., 2009) as these focused on mental rotation mechanisms without relating them to the other supporting processes such as matching processes and decision making processes (Lamm etÂ al., 2007)."
"The architecture also represents an innovation with respect to previous neuro-robotic models (Seepanomwan etÂ al., 2013a, 2013b) that did not distinguish between the brain areas possibly performing visual and proprioceptive processes and also used abstract monitoring and decision making mechanisms."
"To overcome these drawbacks, Herda et al. [17] introduced a real-time method using an anatomical human model to predict the position of the markers. It is unfortunately very difficult and time consuming to setup such a model."
"Lai et al. [23] noticed that the low-rank property of mocap data has not been explicitly exploited in the previous work, and they proposed to handle the mocap data refinement problem based on low-rank matrix completion theory and algorithm. The key point of their method is that it does not need any training data. Inspired by [23], our model also takes the low-rank structure property into account. Besides, we include two other properties into our design: the temporal stability and the noise effect. Compared with [23], our model does not only take the low-rank structure property into account but also the temporal stability property of motion data and noise effect. Our proposed model can handle both sub-problems of mocap data refinement at the same time, while in [23] they used two separate models to achieve the same goal. Another significant difference between our model and its counterpart in [23] is that we do not need to guess the standard deviation of the noise which is used for solving the de-noising model in [23]. More importantly, the optimization method for solving our model is much faster and robust than SVT [23], which has been proven both in theory and in experiment [33]. "
" In Section 4, we also have observed that our method is not only faster than the work [23] but also outperforms it in the experiments on both synthetic and real data. Meanwhile, we also notice that two low-rank matrix based methods [19,47] have been proposed almost at the same time as ours following the work [23]. "
" From the last two columns of each sub-image in Fig. 11, the shortcoming of SVT [23] is shown that it is unable to correctly predict some missing values. Relatively speaking, we find that Dynammo [30] offers the best performance for all the three motion sequences. However, when some markers are missing for a long period of time, Dynammo [30] also fails to correctly predict the missing values."
"Conventional robust preview control, which has been studied in [5–9], is restricted to small ranges of variations."
"It is clear that choosing the location of each nuclear installation is a balancing act between various factors, most importantly the proximity of the sites to urban areas with significant population (Grimston, Nuttall, & Vaughan, 2014). Whilst the so-called semi-urban installations (for example, at Hartlepool and Heysham in the UK) could reduce operational and transmission costs, they pose a greater risk to the nearby population, and therefore require detailed emergency planning."
"The dose conversion factors for the individual elements could vary significantly depending on their decay energy: the factor for Cs-134 is around 9 times greater than that for Cs-137 (Yoo, Jang, Lee, Noh, & Cho, 2013)."
"These radiation levels are well below the known thresholds for the deterministic effects, and tend to cause stochastic effects on human health (Choi, Costes, & Abergel, 2015), including cancers. "
"Van-der-Aalst and Weske [31] applied a three step Public-to-Pri-vate approach to inter-organisational workflows. In the first step, the partner organisations agree on a common public workflow; in the second step, the common public workflow is divided be-tween the interacting organisations; and in the third step, the organisations create their private workflows autonomously. This approach requires manual negotiations to reach an agreement, which can be very time consuming especially if there are many partners."
"Krukkert [13] proposed a solution in the openXchange project. Two activity diagrams are taken as input and compared to find out all common execution sequences. If any common sequence is found then a common activity diagram is constructed for collabo-ration. For the solution to work, there must be a common activity sequence in the workflows or activity diagrams of the participating organisations. If a common sequence is not found, then collabora-tion cannot proceed, which is a limitation of the system."
"Okutan and Cicekli [15] proposed an event calculus based web service composition and execution (WSCE) system. The system has two phases, namely composition phase and execution phase. In the composition phase, the OWLS process definitions are translated to axioms in event calculus domain. Web services are encoded as ac-tions, web service inputs and outputs as action’s knowledge pre-conditions and knowledge effects, and web service preconditions and effects as action preconditions and effects. The user inputs are substituted as initial condition axioms and the outputs as goals. Based on the domain knowledge, the Abductive Event Calculus Planner generates plans to reach the given goal state. The plans are presented to the user in the form of visual graphs, which can be sorted according to user’s quality of service parameter among execution duration, price, reliability and availability. In the execution phase, the selected graph is transformed to OWLS descriptions and passed to the execution engine. The user enters the actual input values, and the actual web services modelled by the OWLS pro-cesses are invoked.WSCE is a good effort to use event calculus for web service com-position. The main benefit of this system is that it supports concur-rent plans and so it is better suited for solving real world business scenarios. The main issue with this system is that it can compose workflows for a single organsisation only and does not take the generation of collaborative workflows for multiple organizations into account. The work, if extended for solving multi-organisation scenarios, can be a good addition to research."
"Recently, there has been some work on composing workflows for multiple organisations. [1] suggested a Pi-Calculus based ap-proach to compose web services into cross organisational business processes. A cross organisational business processes is modelled as a set of concurrent local processes, which has a global start and a global end activity. The activities in the local processes can receive external start messages. A cross organisational con-troller controls the flow of control and data in the cross organisa-tional process. The limitation with this work is that it uses a manual modelling approach and the web services composition is not automatic."
"Correˆa da Silva et al. [8] presented a lightweight, flexible and user-friendly platform for cross organisational workflow interac-tions. The platform is named JamSession and it can be considered as a meeting point for already existing software components to form new and innovative service systems. JamSession is a user-friendly and light-weight platform, providing an appropriate framework to specify and implement cross organisational work-flow interactions. It uses knowledge-based interaction protocols and predicates to models cross organisational workflows and activ-ities in such away that the workflow definitions are local to the respective workflow management systems, and only the interac-tion protocols are made public. It makes the workflows highly decoupled. While the paper claims that the interaction protocols can be used to specify and execute cross organisational workflows, it only shows examples for the execution of cross organisational workflows. So, it is not possible to deduce whether the interaction protocols can be used for bringing about collaboration among cross organisational workflows at build time."
"Unlike the translation algorithm described in Sirin et al. [27] which translates only the preconditions of atomic processes into the preconditions of SHOP2 operators, Translate-Atomic-Pro-cess(Q) translates both the preconditions and inputs of atomic pro-cesses into the preconditions of SHOP2 operators. This enables the developed framework to use web services that have both inputs and preconditions in workflow generation. Similarly, unlike the translation algorithm described in Sirin et al. [27] which translates only the effects of atomic processes into the post-conditions of SHOP2 operators, Translate-Atomic-Process(Q) translates both the effects and outputs of atomic processes into the post-condi-tions of SHOP2 operators. This enables the presented framework to use web services that have both outputs and effects in workflow generation."
"Unlike the discussed approaches [27,33], the implemented framework is not focussed on finding an execution path for already defined composite processes. We be-lieve that forming an execution path for an already built composite process limits the strength of workflow generation by limiting the automation. Therefore, the composite processes are decomposed to atomic processes and then the atomic processes are used to create a single SHOP2 if-then-else method to guide the composition process."
"Unlike the proposed approach described above, the approach by Sirin et al. [27] does not combine operators to form a method. This means that their system can generate workflows only if the user manually defines the composite processes and passes them to the system. "
"As the system proposed by Sirin et al. [27] targets the creation of workflows for a single organisation only, it has a single SHOP2 domain to begin with. Therefore, they do not present any algo-rithm for collapsing the domains of multiple collaborating organi-sations into a single domain."
"Albanese etÂ al. (2013) present a well-modeled formalism for complex inter-dependencies of missions as a set of tasks. Using numerical scores and tolerances in a holistic approach Albanese etÂ al. focus on cost minimization. Their approach can solely be validated holistically, as involved parameters do not bear local semantics and do not provide bias-free and context-free understandable results. Buckshaw etÂ al. (2005) propose a quantitative risk management by involving various experts and present a score-based assessment based on individual values and a standardization using a weighted sum. Unfortunately, a mathematical foundation is missing and obtained results are only interpretable after deep training of experts in the characteristics of this approach. Buckshaw etÂ al. themselves note that a validation of the proposed model requires large amounts of actual data and ground truth, which both are not available."
"Jakobson (2011) presents a well-understood conceptual framework using interdependencies based on operational capacity at different abstraction layers. In this dependency model, impacts are propagated and reduce the operational capacity, which has a similar intention to our approach. However Jakobson (2011) uses self-defined metrics for propagating impacts through Boolean gates, which cannot provide context- and bias-free understandable results or parametrization. Moreover, an explicit representation of âintra-assetâ dependencies is required, i.e., all individual critical, and non-critical resources must be identified. "
Musman etÂ al. (2011) proposes the use of BPMN models and describes a process for evaluating impacts of cyber-attacks. However Musman etÂ al. (2011) fails to get across any mathematical approaches or formal definitions for impact assessment.
"Further works focuses solely on modeling. For example, Goodall etÂ al. (2009) focus on modeling and available data integration using ontologies but do not address an impact assessment. Another ontology-based approach is presented by Amico etÂ al. (2010), which identifies multiple experts while noting that, e.g., system administrators are not capable of understanding an organization's missions."
"In terms of (probabilistic) approaches toward assessments of impacts caused by vulnerabilities and attacks, probabilistic models have been researched by Wang etÂ al. (2008), Liu and Man (2005), or Xie etÂ al. (2010). However, Wang etÂ al. base their work on attack graphs and do not consider imperfect knowledge, e.g., unknown extents of damage causable by vulnerabilities, uncertainty of specific events and potentially disagreeing information sources as we do. Xiep etÂ al. and Liu etÂ al. are significantly limited by the lack of supporting cyclic dependencies and do not consider any mission impact relations. Chung etÂ al. (2013) consider a probabilistic approach as well to determine the likelihoods of explicit attack paths. However, presented probability theory in Chung etÂ al. (2013) is not sound and voids fundamental principles of probabilistic inference in multiply connected graphs. Other impact propagation approaches, e.g., by Kheir etÂ al. (2009) or Jahnke etÂ al. (2007), claiming to handle details such as disagreeing information sources and cycles, are not probabilistic based and degrade to a handcrafted propagation algorithm with arbitrary scores, where parameters are only assessable by deeply trained experts and obtained results can only be used in a holistic way, as they provide no directly interpretable meaning. "
"To obtain well-defined results, i.e., to obtain a solid and consistent business dependency model from multiple sources and experts, a semantic normalization and merging is required for business dependency model. We deeply discuss and propose a solution to the semantic normalization and merging problem of mission dependency models in Motzek etÂ al. (2016)."
"To date, the managerial and scholarly debate on two-sided mar-kets has followed the logic of the credit card business, where the ab-solute number of merchants accepting a credit card — or the number of applications available in the marketplace — determines the value of the credit card for the end user (see e.g., Chen, 2010; Reuters,2012; Lee, 2015; Smith, 2015). However, this approach considers all appli-cations equal and thus ignores the qualitative aspects of the market dynamics. "
"According to Sun and Tse (2009), this implies that the mar-ket would be able to sustain more than one ecosystem. Overall, our research advances Sun and Tse’s (2009) model of platform compe-tition by emphasizing that multi-homing can manifest differently within a single group of actors in the market."
"As a result, we depart from Sun and Tse (2009) who emphasized the sheer size of the two sides of the market as a decisive factor in platform competition. In addition, our findings differ from the extant research (e.g., Yamakami, 2010; Holzer and Ondrus, 2011; Schultz et al., 2011) that, grounded on network externalities (Katz and Shapiro, 1985), somewhat simplistically argues that a large base of develop-ers leads to a large number of applications that, in turn, leads to an increasing number of end-users, and vice versa. "
"Second, our findings imply that application marketplaces are not used to differentiating their ecosystem from that of competitors. The results of the content analysis show that the content of the most installed applications are similar in the three leading mobile appli-cations ecosystems. This supports the findings by Hyrynsalmi et al. (2013) who did not find differentiation between the consumers nor the application offerings of the ecosystems."
"For example, prior survey research indicates that mobile gaming is not of interest to customers (see e.g., Economides and Grousopoulou, 2009; Bouwman, Carlsson, Castillo, Giaglis, and Walden, 2010; Suominen, Hyrynsalmi, and Knuutila, 2014). However, our results show that games form the majority of the most installed applications. This might be a result of a pattern whereby a user downloads several games, tries them all once, and then removes uninteresting ones from the device."
"A second motivation is related to several software architecture issues that lead to practical difficulties for the functional extension of current software libraries. For instance, WordNet::Similarity [99] and WS4J [121] were designed before the emergence of the intrinsic IC models described in sectionÂ 2.1, thus, these libraries maintain in-memory tables with the concept frequency counts which are interrogated in order to compute the IC values required in a similarity evaluation step; however, their data structures does not provide any proper abstraction layer or software architecture to integrate new intrinsic IC models easily."
"Many works introducing similarity measures or IC models during the last decade have only implemented or evaluated classic IC-based similarity measures, such as the Resnik [108], Lin [70] and Jiang-Conrath [52] measures, avoiding the replication of IC models and similarity measures introduced by other researchers. Some works have not included all the details of their methods, or the experimental setup to obtain the published results, thus, preventing their reproducibility. Most works have copied results published by others"
"The first known IC model is based on corpus statistics and was introduced by Resnik [108], and subsequently detailed in [109]. The main drawback of the corpus-based IC models is the difficulty in getting a well-balanced and disambiguated corpus for the estimation of the concept probabilities. To bridge this gap, Seco etÂ al. [119] introduce the first intrinsic IC model in the literature, whose core hypothesis is that the IC models can be directly computed from intrinsic taxonomical features."
"The pioneering WNSim library was developed in Perl by Pedersen etÂ al. [99], and subsequently migrated to Java by Tedeki Shima, under the name of WS4J [121]. WS4J includes, like its parent library, the most significant path-based similarity measures, the three aforementioned classic IC-based measures and several corpus-based IC models [95]. However, WNSim and WS4J do not include most ontology-based similarity measures developed during the last decade, nor any intrinsic IC model."
"Finally, we have the WNetSS semantic measures library introduced recently by Aouicha etÂ al. [15], which is based on an off-line pre-processing and caching in a MySQL server of WordNet, as well as all WordNet-based topological features and implemented IC models. As we mentioned previously in sectionÂ 1.1.1, the caching strategy used by WNetSS severely impacts its performance and scalability. In addition, WNetSS exhibits two other significant extensibility drawbacks which prevent its use for researching and prototyping of new methods, as follows: (1) the current distribution of WNetSS does not include its source files, thus, their architecture, representation model for taxonomies and implementation details are missing; and (2) the current WNetSS version does not allow any type of functional extension, such as including a new taxonomy parser, as well as a new semantic similarity library or IC model. Finally, despite one of the main motivations of WNetSS being to provide a software implementation for the most recent methods, looking at tablesÂ 2 and 3, you can see that WNetSS [15] neither implements nor cites many recent similarity measures and IC models reported in the literature."
"A long-known but much neglected practice first advocated by Dijkstra in Dijkstra (1982), separation of concerns strives to separate different aspects of software design and implementation to enable separate reasoning and focused specification for each of them. "
"Components and connectors are present in most component-oriented approaches: a wealth of literature discusses their various possible flavours (see for example (Szyperski, 2002; Lau and Wang, 2007; Mehta et al., 2000)). Containers have a much lesser prominence in the literature, perhaps a token of the insufficient penetration of the concept of separation of concerns in component-based software engineering. "
"The nature of our target systems reduces the variety of necessary connectors to a few basic kinds, which are required to perform function/procedure calls, remote message passing or data access (I/O operations on files in safeguard memory). This also means that we do not require an approach for the creation or composition of complex connectors (Spitznagel and Garlan, 2001)."
" As in the HB model, recognition is signalled by a transient synchronization event, and this synchronization is brought about by coupling feature detectors that have nonstationary, pattern-dependent frequency response profiles. However, the synchronization process itself is not implemented using balanced excitation and inhibition among IF cells as in Hopfield and Brody (2001), but is rather described at the level of phase dynamics. This allows us to be equivocal about the details of the neural circuits that generate the oscillations themselves. We see this as a benefit as there are currently a large number of possible candidates for the underlying processes (see next section)."
" We should also bear in mind, however, that our results were obtained by training the system on clean utterances whereas the results in Lee etÂ al. (2011) were obtained from a system trained on noisy utterances. Our results are more in line with those of Rouat, Loiselle, and Molotchnikoff (2011) who obtained word error rates of 78% when training an MFCCâHMM system on clean utterances and testing it on 10Â dB noisy utterances using noise samples from Aurora-2."
"Third, we have shown that the dynamical pattern recognition process can act as a forward model of neuroimaging data. Previous studies in this area (Corchs & Deco, 2004; Husain, Tagamets, Fromm, Braun, & Horwitz, 2004) have used computational models of auditory processing as forward models for fMRI data. Corchs and Deco (2004), for example, have used a neurodynamical model of feature-based attention in combination with a haemodynamic process as a forward model of fMRI activity. "
"The Liquid State Machine (LSM) (Maass, Natschlager, & Markram, 2002), for example, uses OT features and the temporal embedding idea proposed in the HB model, but then applies standard methods for recognizing the resulting static patterns. This results in good pattern discrimination abilities (Verstraeten etÂ al., 2005), though not as accurate as a recent approach based on OT features (Gutig & Sompolinsky, 2009). Further, LSMs do not generate a gamma burst as an integral part of the recognition process, so would not be so appropriate as a forward model for the sort of neuroimaging data addressed here."
"The notion that regions higher up in the auditory cortical hierarchy process information at longer time scales has recently been made use of in a model of auditory sequence recognition based on stable heteroclinic channels (Kiebel, Kriegstein, Daunizeau, & Friston, 2009). Moreover, the approach developed in that work derives from a Bayesian perspective in which cortical hierarchies embody a generative model which is then inverted during the pattern recognition process. Generative models of speech production are, as yet however, still in the early stages of development. This currently limits the ecological validity of such a generative modelling approach."
"While there is a lot of folk wisdom on how to design good algorithms for these highly-threaded machines, in addition to a significant body of work on performance analysis  [16-20], there are no systematic theoretical models to analyze the performance of programs on these machines. "
" Over the years, computer scientists have designed various models to capture important aspects of the machines that we use. The most fundamental model that is used to analyze sequential algorithms is the Random Access Machine (RAM) modelÂ  [21], which we teach undergraduates in their first algorithms class. This model assumes that all operations, including memory accesses, take unit time. While this model is a good predictor of performance on computationally intensive programs, it does not properly capture the important characteristics of the memory hierarchy of modern machines. "
"For parallel computing, the analogue for the RAM model is the Parallel Random Access Machine (PRAM) modelÂ  [30], and there is a large body of work describing and analyzing algorithms in the PRAM model  [31,32]. The PRAM model also ignores the vagaries of the memory hierarchy and assumes that each memory access by the algorithm takes unit time. For modern machines, however, this assumption seldom holds. Therefore, researchers have designed various models that capture memory hierarchies for various types of machines such as distributed memory machines  [33-35], shared memory machines and multi-cores  [36-40], or the combination of the two  [41,42].All of these models capture particular capabilities and properties of the respective target machines, namely shared memory machines or distributed memory machines."
"Many machine and memory models have been designed for various types of parallel and sequential machines. In an early work, Aggarwal etÂ al.  [25] present the Hierarchical Memory Model (HMM) and use it for a theoretical investigation of the inherent complexity of solving problems in RAM with a memory hierarchy of multiple levels. It differs from the RAM model by defining that access to location x takes logx time, but it does not consider the concept of block transfers, which collects data into blocks to utilize spatial locality of reference in algorithms."
" Alpern etÂ al. propose the Memory Hierarchy (MH) FrameworkÂ  [26] that reflects important practical considerations that are hidden by the RAM and HMM models: data are moved in fixed size blocks simultaneously at different levels in the hierarchy, and the memory capacity as well as bus bandwidth are limited at each level. But there are too many parameters in this model that can obscure algorithm analysis. "
"In the parallel case, although widely used, the PRAMÂ  [30] model is unrealistic because it assumes all processors work synchronously and that interprocessor communication is free. "
"Culler etÂ al.Â  [33] offer a new parallel machine model called LogP based on BSP, characterizing a parallel machine by four parameters: number of processors, communication bandwidth, delay, and overhead. It reflects the convergence towards systems formed by a collection of computers connected by a communication network via message passing. Vitter etÂ al.Â  [35] present a two-level memory model and give a realistic treatment of parallel block transfers in parallel machines. But this model assumes that processors are interconnected via sharing of internal memory.More recently, several models have been proposed emphasizing the use of private-cache chip multiprocessors (CMPs). Arge etÂ al.Â  [36] present the Parallel External Memory (PEM) model with P processors and a two-level memory hierarchy, consisting of the main memory as external memory shared by all processors and caches as internal memory exclusive to each of the P processors. Blelloch et al.Â  [37] present a multi-core-cache model capturing the fact that multi-core machines have both per-processor private caches and a large shared cache on-chip. Bender etÂ al.Â  [44] present a concurrent cache-oblivious model. Blelloch etÂ al.Â  [38] also propose a parallel cache-oblivious (PCO) model to account for costs of a wide range of cache hierarchies. Chowdhury etÂ al.Â  [39] present a hierarchical multi-level caching model (HM), consisting of a collection of cores sharing an arbitrarily large main memory through a hierarchy of caches of finite but increasing sizes that are successively shared by larger groups of cores. They inÂ  [42] consider three types of caching systems for CMPs: D-CMP with a private cache for each core, S-CMP with a single cache shared by all cores, and multi-core with private L1 caches and a shared L2 cache. All the models above do not accurately describe highly-threaded, many-core systems, due to their distinctive architectures, i.e.Â the explicit use of many threads for the purpose of hiding memory latency."
" Liu etÂ al.Â  [19] describe a general performance model that predicts the performance of a biosequence database scanning application fairly precisely. Their model incorporates the relationship between problem size and performance, but only targets their biosequence application."
" Govindaraju etÂ al.Â  [45] propose a cache model for efficiently implementing three memory intensive scientific applications with nested loops. It is helpful for applications with 2D-block representations while choosing an appropriate block size by estimating cache misses, but is not completely general."
"Ryoo etÂ al.Â  [46] summarize five categories of optimization mechanisms, and use two metrics to prune the GPU performance optimization space by 98% via computing the utilization and efficiency of GPU applications. They do not, however, consider memory latency and multiple conflicting performance indicators."
" Kothapalli etÂ al. are the first to define a general GPU analytical performance model inÂ  [47]. They propose a simple yet efficient solution combining several well-known parallel computation models: PRAM, BSP, QRQW, but they do not model global memory coalescing."
" Meantime, Baghsorkhi etÂ al.Â  [16] measure performance factors in isolation and later combine them to model the overall performance via workflow graphs so that the interactive effects between different performance factors are modeled correctly. The model can determine data access patterns, branch divergence, and control flow patterns only for a restricted class of kernels on traditional GPU architectures."
" Zhang and OwensÂ  [15] present a quantitative performance model that characterizes an applicationâs performance as being primarily bounded by one of three potential limits: instruction pipeline, shared memory accesses, and global memory accesses. More recently, Sim etÂ al.Â  [48] develop a performance analysis framework that consists of an analytical model and profiling tools. The framework does a good job in performance diagnostics on case studies of real codes. Kim etÂ al.Â  [49] also design a tool to estimate GPU memory performance by collecting performance-critical parameters. Parakh etÂ al.Â  [50] present a model to estimate both computation time by precisely counting instructions and memory access time by a method to generate address traces. All of these efforts are mainly focused on the practical calibrated performance models. No attempts have been made to develop an asymptotic theoretical model applicable to a wide range of highly-threaded machines."
"In his thesis [12], Korp generalizes Definition 17 (cf. [12, Definition 3.10]) by incorporating an auxiliary relation âª°ÏA that may be viewed as a precursor to our relation â«. The modified definition permits smaller automata, which benefits implementations, but is more complicated than Definition 17. The modification also does not add expressive power."
"In this section we show how tree automata can be used to prove termination via match-bounds [9]. To this end, we first recapitulate the basic concepts and important results about match-bounds. Note that match-bounds require special treatment for non-left-linear TRSs (see Example 30)."
"On the analyzer side we have extended the termination tool Image 1 [11] and the confluence tool CSI [19] to produce state-coherent, state-compatible automata. Since both tools use quasi-deterministic automata in their completion process, we apply the construction of Theorem 20 as a post-processing step, resulting in a state-coherent, state-compatible automaton. CeTA can then be used to certify this output. In contrast to the earlier version of CeTA corresponding to [5], CeTA now supports match-bounds for non-left-linear TRSs as well."
" In fact, for some algorithms we just relied on the automatic refinement provided in [16] which turns set operations into operations on trees. For other algorithms we performed manual data refinement. Although the latter approach is more tedious, it has the advantage for the user that we additionally integrated detailed error messages which are displayed if the certifier rejects a proof."
"The Chen system with the parameter set {a,c,b}={35,28,3} is chaotic [12], but it may not be chaotic for some other parameter. The Lu system with the parameter set {a,c,b}={36,20,3} is also chaotic [61] and, likewise, it may not be chaotic for any other parameter.It is easy to see that a generalized system [36,14,37]"
"But the question of importance is what was known about repulsor (or repellor) in the Lorenz system and the dynamics of time-reversal Lorenz system before the works [12,61] were published? Note that in [2,3] there is no discussion of the following important questions for the consideration of the Lorenz system in the backward time or with non positive parameters: the existence of the extension of solutions, the existence of attractors, and the possibility of consideration of invariant sets in the backward time. Some necessary results can be found in [16], [15, p. 35]. "
" For Î± different from 0, there exists only one partial result in [17], in which existence and uniqueness is proved for the equation, under very strong assumptions on the data. However, the small data and non negativity of dh/dt assumptions in [17] are very far from conditions in the present situation. "
"Many researchers have addressed effort estimation and, therefore, consider productivity factors (PFs) (Wang et al., 2009), but they do not address the possibility of potentially inappropriate variables in the UCP algorithm itself, which is important for software size estimation."
"However, the existing use case-based estimation methods have some well-known issues (Diev, 2006). Use cases are written in natural language; consequently, there is no rigorous approach for comparing use case quality or fragmentation. "
" Rosa et al., (2014)investigated whether a linear model based on both size and application type was better than a model based on size only; however, this study did not investigate the effects of each variable nor evaluate additional types of regression models."
"López-Martín (2015) described linear regression models as less accurate than neural networks, but they provided no description of the regression models studied. Moreover, they did not consider the stepwise principle for model construction nor did they investigate whether all the UCP variables contribute to size estimation."
" A discussion of variable significance can be found in Urbanek et al., (2015a). Silhavy et al., (2015a, 2015b) offered a linear model obtained by the least squares approach, in which two prediction coefficients were used to adjust the UAW and the UUCW. These studies did not focus on evaluating of variables for use in regression models, nor did they compare linear and polynomial regression models."
"Urbanek et al., (2015b) described the number of points in the use case scenario as the most significant factor, but the scope of this paper is analytical programing; therefore, this finding is not applicable to MLR."
"The complexity of a use case is based on the number of scenario steps or, sometimes, on the number of transactions it contains (Ochodek et al., 2011a). However, a transaction typically refers to a set of activities, not a simple step in a structured scenario."
"MLR models depend on assumptions, and when those assumptions are violated, data transformation (Christensen, 2006) may be required. However, such transformations also change the method of prediction because the predicted value of the dependent variable does not represent the project size."
"The approaches to motion estimation are biomechanical based [5,7], silhouette based [2,3,6,11,12] and image based [1,4,8–10]. A biomechanical-based approach involves tissue analysis and bone and joint location, which requires expensive devices and equipment. Silhouette-based estimation is analyzed by the silhouette extract from a human image file. It is always challenging when the estimation involves more than one subject."
"A two-stage Monte Carlo Markov Chain (MCMC) inference algorithm was implemented by Zhang et al. [37] using part-based gait estimation. Because some motions such as walking and running only focus on the lower body segment, the gait motion has been essential, with emphasis placed on the lower body segment. However, for motions that also involve the upper and lower body segments, such as dancing and sword playing, gait motion is not a proper option to yield good estimation."
"On the other hand, silhouette-based motion estimation investigates the image silhouette of human motion. Güdükbay et al. [38] demonstrated that the human silhouette can be labeled using a model-based approach. However, Rosenhahn et al. [2] reported that silhouette information is insufficient for estimating the model correctly, as the extracted silhouette is hard to determine."
"Image-based estimation is liable in motion capture and suitable for direct analysis of the image motion data. However, when an individual is in contact with other objects, it is difficult to differentiate between the subject and the objects."
"Recent human motion classification works [18,20–22,54] applied the classification method to the available or captured motion data, but the authors did not classify estimated matching motions."
"It is well known that polynomial fitting approximations have played a central role in numerical analysis. Polynomial fitting is able to generate smooth functions, which are easy to manipulate and evaluate. In addition, polynomial fitting is also able to provide precise accuracy of convergence in the approximation of numeric data. However, polynomial fitting is only able to generate precise approximation in short intervals, where the large interval will cause the approximation to oscillate widely [56]."
"Foot skating refers to the error that occurs in animation whereby the feet slide or float on the ground [64]. Foot skating often occurs when the recorded motion data are applied to different subjects whose position no longer fits the motion of the limbs, which is not applicable to our study. "
"There are of course many limitations in this approach. Firstly it is entirely possible that other articles have been published in OR/MS journals that have analytics-related content, however do not use this term in their abstract or title. Secondly there is the potential that academics in the OR/MS community would pub-lish analytics-orientated research in journals not directly associ-ated with OR/MS (e.g. Coghlan et al. (2010)). "
"Alternative approaches to detecting the OPMD from microscopic images could be to threshold the cell images [5,6], to apply either the graphical model method [7,8] or the contour-based method [9,10], or to open them morphologically using structuring elements to eliminate background objects while preserving the shape of cells [11-15]. However, the shape-based methods [6-15] require considerably complex preprocessing steps, and also would be useful only when the appropriate shapes of images are available.Due to the simplicity and efficiency of the histogram based techniques, the histogram based approaches are widely used for image analysis. "
" Although good performance was achieved by the method of watershed transform (derived from the morphology) using the image of region of interest for non-uniform images [14,15], we keep our approach simple for three main reasons. First, the complexity in selecting constraint parameters, structuring elements, cost functions, etc., was quite high in [5,15]. Second, the initial regional minima was manually defined as highly related to the over segmentation issue in [14,15], and thus the applicability to different microscopic image data, e.g., other data of PCD, is uncertain."
"At the end, using the synthesized features, the framework of HROIT/GP-EM with MDC [37,38] achieved an average accuracy rate of 90.20% with a Std of 3.50% on the OPMD dataset in the ten-fold cross-validation.Using each of the best feature functions with the same MDC, the average accuracy rates over the ten-fold test sets are reported in Table 4 for the CDF-DRVC/GP-EM framework and are compared to the performance of the HROIT/GP-EM framework [37,38]. From Table 4, we can see that the average rate obtained by CDF-DRVC/GP-EM is higher than that obtained by HROIT/GP-EM [37â38], with an improved average rate of 4.3%; the CDF-DRVC/GP-EM also yields a lower Std of 2.39%, compared with a Std of 3.50% obtained by HROIT/GP-EM in the ten-fold cross-validation."
"Developing valuable software is a goal that has long been on the Agile manifesto (Beck et al., 2001). However, it is not an easy goal to achieve. Before adopting CD, some of our teams had been using an Agile method called Kanban (Anderson, 2010); however, due to delivery problems, we still had situations where a team had completed a feature but could not deliver it to production to ob-tain users’ feedback. Consequently, they built additional function-alities on top of that feature, simply assuming it was useful. Un-fortunately, when they finally delivered the software to the users, they found out that the feature was not what the users needed. "
"Many works have reported challenges and solutions in adopt-ing CD (Leppanen et al., 2015; Claps et al., 2015; Olsson et al., 2012; Karvonen et al., 2015; Rissanen and Münch, 2015; Marschall, 2007; Zhu et al., 2015; Debbiche et al., 2014; Puneet, 2011; Noured-dine and Foutse, 2014; Laukkanen et al., 2015; Rogers, 2004; Sek-itoleko et al., 2014; Krusche and Alperowitz, 2014; Adams et al., 2015; Souza et al., 2015; Feitelson et al., 2013; Rahman et al., 2015; Neely and Stolt, 2013), including a very recent systematic literature review (Laukkanen et al., 2017). However, none of the existing lit-erature on CD has included the strategies I reported in this paper."
" These challenges are not reported in the recent Systematic Liter-ature Review (SLR) (Laukkanen et al., 2017), which systematically reviewed the existing literature to identity problems and solutions in adopting CD."
"My previous work (Chen, 2015a) did not include any of the strategies reported in this paper. In terms of challenges, four of the further challenges for research were not included in my pre-vious paper. For those challenges that were mentioned in (Chen, 2015a), I provided new and additional descriptions in this paper."
"However, from an economic perspective, traditional CDNs require significant investments for scaling up, as it requires deployment and management of geographically distributed data centres [54]."
" Early on, it was identified that P2P swarms possess the so-called self-scaling property [73,83] – available capacity increases with the number of users in the swarm, as each user downloading content also adds new capacity by acting as a server for other users. However, obtaining content through self-organised P2P swarms has proved to be unreliable due to availability issues [52], because in selfish swarming protocols such as BitTorrent, users leave the swarm after obtaining the item they need, making it difficult to put together complete copies of content."
"However, this makes peer-to-peer systems prone to unpredictable changes in content availability - a phenomenon called peer churn - when peers frequently and suddenly leave or join the system due to network failures or their own intent [103]."
"For example they may upload and share corrupted, malicious or illegal contents in the system or block nodes from using the service [89]. Indeed, pollution of the system with corrupted and unauthorized contents has been named as a serious problems in peer-to-peer networks [105]. Privacy is yet another issue in decentralized P2P systems: private information of the users including their IP addresses, geographic locations and viewing preferences might be exposed to unauthorized users."
"The accesses of Spotify users [120] feature a vivid diurnal pattern: The lengths of user sessions are the longest during morning hours and are gradually decreasing by the end of the day. The user sessions are also shorter during weekends than during working days, and on mobile devices than on desktop computers [120]."
" Indeed, in  [50], the authors reported that the Top-10% popular videos account for up to 80% of the total traffic in BBC iPlayer. Similarly, in  [42] is reported that the Top-10 videos constituted a significant fraction of CDN traffic in MSN Video. The authors in  [108] reported that, video popularity in online social networks (OSNs) follows the Zipf-like distribution. However, the content popularity would change quite dynamically, for example, due to daily releases of highly popular news and business-related shows [42]."
"The delivery of high quality multimedia contents to the customers is the challenging issue for many network and content delivery services  [14,15,62,79]."
"The simulation results suggest that this approach can reduce the startup delay by up to 2 sec in comparison to the previous results in  [22,42]."
"In summary, our analysis in this section suggests that, the most promising compromise in mixing advantages of peer-assisted and traditional CDNs to improve QoS has been so far found in bootstrapping video streaming with initial chunks downloaded from CDN nodes and delegating the remaining work to peers [39,65]. "
"Similarly, the peer-assisted content delivery system may benefit from redirecting streaming requests to CDNs in emergency cases when no sufficient upload bandwidth is available among the peers to meet playback deadlines [112]."
"The peers inaccessibility problem; when a peer inside a private network can initiate a connection with the peers of public networks, but a reverse connection is often complicated by administrative policies [29], has been discussed in some of the earliest partially centralized PA-CDN articles  [101,102]. "
"The simulation results in  [102] suggest that, the users which are connected outside a firewall have slower downloading rates since they are unable to establish connections with the nodes in private networks, whereas peers behind firewalls exchange more, therefore, leading to an increase in downloading speed."
"It is estimated that, 90% of users in China are located behind the firewalls  [45,61]. Among those, as the user trace collected [61] from LiveSky is revealed, 37% are not available for peer-to-peer distribution and nearly 30% can provide uploading capacity sufficient to serve only one user at a time."
"This includes, among others a need for economically sound mechanisms to incentivize user participation, which is reportedly low in some of the existing systems [124] and is recognized as a significant obstacle factor in the others [38,50]. "
"Das et al. [8] suggested an algorithm for constructing the (unweighted) straight skeleton of monotone polygons, which they claim runs in O(nlogâ¡n) time, where n denotes the number of vertices of the polygon. However, we have simple examples that show that their Lemmas 5, 6, and 7 do not hold for all valid inputs. In particular, their approach hinges upon the assumption that no event introduces a new reflex vertex during the wavefront propagation process, which is clearly incorrect for general input. (See the node marked in green in Fig. 2, on the right-hand side of the lower chain.) Note that a perturbation of the input in order to avoid such a vertex event, as suggested by Das et al. [8], cannot be applied as the straight skeleton changes discontinuously [2]."
"Our algorithm can compute the positively weighted straight skeleton of a monotone polygon in O(nlogâ¡n) time and O(n) space, which constitutes a significant improvement over the O(n17/11+Ïµ) worst-case time and space complexity of the currently best algorithm for arbitrary simple polygons by Eppstein and Erickson [2]. "
"APFA may be useful when there is interest in understanding the dependence structure between the variables, and when this structure is expected to vary over the time interval of the study. The methodology assumes that the variables are measured at common times (or, in the case of genomic data, at common spatial positions) and so is not appropriate when times between transitions vary. It has proven to be well-suited to highly-structured, high-dimensional data such as DNA chip data, but we believe that it may be of more general interest and utility. Note that here, except for a brief mention in Section  6, we do not include explanatory variables in the models; for a way to do this see Edwards and Ankinakatte (in press, Section 8)."
"A set of discrete longitudinal data with  observations of discrete variables  can be represented as a tree in the following way. Starting with the root node, edges branch out to nodes at the first level or stage. (Previous papers Ron et al., 1998, Browning and Browning, 2007b and Edwards and Ankinakatte, in press use level. But since in statistics this term usually refers to the value of a discrete variable, we here choose to use stage.)"
"In Browning (2006) the same dissimilarity score is used but the algorithm is modified in two ways. In Ron et al. (1998) the order in which nodes within a stage are compared and possibly merged was unspecified. Instead, Browning (2006) describes a greedy approach in which dissimilarities between all pairs of nodes at the stage are computed, and the most similar pair  is merged. The scores are re-computed as necessary, and the process is repeated until the resulting nodes are pairwise dissimilar. The second modification is to allow the threshold to depend on the nodes counts  and , using"
"The results of applying the model selection procedure (using the AIC) to the Duroc SNP data set and the Biofam data set are shown in Fig. 4. In Fig. 4(a) it is seen that the APFA for the Duroc SNP data is highly structured, with relatively few, long blocks (particularly for the first 50 or 60 SNPs). This reflects the relatively low haplotype diversity in this region (Edwards, 2013). In contrast, the APFA for the Biofam data in Fig. 4(b) shows a more diffuse structure. In this context it is of interest to display the relative frequencies of the different life courses and this is done here by setting the width of the edges proportional to the edge counts. The red edges represent children staying with their parents. We observe that a large number of children live with their parents till the age of 20 and then the number gradually decreases. The number of those who left home (blue edges) increase correspondingly. The different parts of the plot show the life courses of those that left home without getting married (blue edges), got married (green edges), got divorced (purple edges) and got married, left home and had children (orange edges)."
"Fig. 5 compares the four model selection procedures in terms of rate of convergence to the true model as the sample size increases for the three data sets. The AIC-based procedure and that in Beagle with tuning parameters  and appear to have very similar convergence patterns. The BIC-based procedure is slightly slower, and Beagle with the settings suggested in Browning and Browning (2007a) ( and ) performs substantially worse than the alternatives, particularly at smaller sample sizes"
"where  and  are two APFA (defined on the same variable set), and  and  are the maximum-likelihood (ML) estimates of the probabilities of  under  and . In other words, it is the expectation of  under . This can be shown to non-negative, and equal to zero only when the distributions are identical. It is commonly used to compare non-nested models (Kent, 1986). It is not a true distance, since it is asymmetric in  and  and does not fulfil the triangle inequality."
"Automatic intelligent labelling of fMRI data is by no means a trivial task. Modelling of a fixed haemodynamic response function in the MRI literature [13,15] can be perceived as one attempt to address this issue, yet to the best of our knowledge no pure data-driven approach based on machine learning techniques exists"
"As it can be seen, a simple linear classifier (ldc) was able to achieve an average accuracy of 74.2%, not only vastly outperforming other tested classifiers, but also outperforming the Random Forest ensemble from [17] at the fraction of computations, yet still leaving room for improvement."
"On the other hand, the third and fourth TRs (between 5 and 10 s) appear to be the easiest to classify on average, which is also more or less consistent with the effect of the haemodynamic lag causing the blood oxygenation level to peak around 5 s after stimulus presentation [13]."
"In the future we plan to upgrade the tooling with optimizations for fast and massive slicing (Binkley et al., 2007) and to merge the clustering phase into the slicing to reduce the runtime significantly.Although the clustering and building the visualization data can take a long time for large projects, it is still useful because the clustering only needs to be done once. "
"Moreover, the analysis produces an is-in-the-slice-of relation and graph with even more edges. We have tried several clustering and visualization tools to cluster the is-in-the-slice-of graph for comparison, but most of the tools (such as Gephi Bastian et al., 2009) failed due to the large dataset. "
"Other tools such as CCVisu (Beyer, 2008) which were able to handle the large data set simply produced a blob as a visualization which was not at all useful. The underlying problem is that the is-in-the-slice-of graph is dense and no traditional clustering can handle such dense graphs."
"Various reviews of the literature on the evaluation of participative methods suggest that most of the justifications provided by researchers are based on personal reflections alone (Entwistle et al., 1999; Connell, 2001; Rowe and Frewer, 2004; Sieber, 2006; White, 2006). Clearly, many researchers are highly experienced, so their reflections should not be dismissed out of hand. Nevertheless, unless they think broadly and from different perspectives about the criteria they use to evaluate their participative interventions, they may miss evidence that does not fit their current thinking about what is important (Romm, 1996; Midgley, 2011)."
"We suggest that the use of systemic PSMs is relatively intensive compared with several of the other participative processes investigated by Beierle and Cayford (2002), so this gives us grounds to be cautiously optimistic. However, we cannot take this study as strong evidence because they did not specifically identify systemic PSMs as a category for comparison with other participative approaches."
"There may be socio-economic and ecological systems providing resources that can be used constructively by participants, or these systems may impose limits on what is achievable without incurring negative side-effects (Clayton and Radcliffe, 1996). Economic issues may point to concerns about social justice, which (if present) could influence people’s perceptions of the effects of systemic PSMs: i.e., the use of a particular method may be seen as supportive of just or unjust social relationships (Jackson, 1991, 2006), so it can be useful to look at the effects of socio-economic systems as part of boundary critique"
"While Rowe and Frewer (2004) say that an appropriate response is to set aside the purposes and preferred criteria of diverse stakeholders in favour of a single criterion of ‘acceptability of the method to all parties’, more nuanced findings will be generated by evaluating the method against multiple criteria of relevance to different stakeholders (Murphy-Berman et al., 2000)."
"Even when an academic researcher makes a significant effort to be responsive to stakeholders, there may still be mistrust stemming from expectations of divergent purposes (Adams and McCullough, 2003), and this may affect the evaluation of methods."
"Outcomes may also be longer term in nature, and these are not always predictable or easy to measure (Duignan and Casswell, 1989). "
"This is the approach taken by Bjärås et al. (1991) and Beierle and Konisky (2000). However, while it is useful to identify ‘common denominators’ and assess methods against these, this does not help in evaluating the unique attributes of methods that might make them complementary rather than competing."
"This is arguably one of the most significant limitations in terms of conducting longer-term research based on multiple case studies: it appears that, after around 20 years of relative stability in the number of systemic PSMs that are widely used in practice, systems/OR practitioners are now producing a new generation of methodologies and methods (Rosenhead and Mingers, 2004; Shaw et al., 2006; Franco et al., 2007), and it is important that the questionnaire does not go out of date."
"The authors of [9] provide a mitigation for this drawback. Their solution is rooted in deflation [21,22,15,14]. Unfortunately, deflation causes an undesirable increase in the size of the polynomial system and can be rather costly, particularly for solutions of high multiplicity."
"Generalizations of all three lemmas appear in Appendix A of [24] as consequences of an algebraic version of Sardâs Theorem. Indeed, Lemma 3.2 is proved as Theorem A.6.1 in [24]. Similarly, Lemma 3.4 is proven in more generality as Corollary A.4.19 of [24]."
"Unlike the model introduced by GonzÃ¡lez-Manteiga etÂ al. (2008b) with a common random effect for all the components of the target variable, the new models have multivariate vectors of random effects with the same dimension as the target variable and allowing for different correlation structures. "
Datta etÂ al. (2002) considered current population survey (CPS) estimates of median income of four-person families for states of the US for nine years (1981â1989) to produce estimates for the year 1989. They chose 1989 since the corresponding estimates were available from the census which allowed them to compare their hierarchical Bayes (HB) estimates with a few multivariate HB estimates. This comparison showed that the more complex multivariate HB estimators did not perform better than their univariate HB estimator. Datta etÂ al. (2002) conclusions were thus restricted to their case of study.
"Datta etÂ al. (2002) also recommended the use of univariate methods because they are more simple to implement. This is true. However, once the methods are implemented and available (for example, in R or SAS code), we do not find great usability differences. Our opinion is that univariate models (simpler models in general) are good enough if we have a good set of auxiliary variables. If this is not the case, then more complex models, that takes into account additional data relationships, might provide estimators with a sensible gain of precision.This paper introduces multivariate FayâHerriot models for estimating small area parameters. "
"Neurophysiological data about perceptual experiences like binocular rivalry illustrate this fact (e.g.,  Logothetis, 1998) and neural models have explained, and indeed predicted, key properties of these data (Grossberg, 1987; Grossberg, Yazdanbakhsh, Cao, & Swaminathan, 2008). In this regard, ART predicts that “all conscious states are resonant states”, but the converse statement that “all resonant states are conscious states” is not true. "
"Grossberg (1980) called the problem whereby the brain learns quickly and stably without catastrophically forgetting its past knowledge the stability–plasticity dilemma. ART was introduced to explain how brains solve the stability–plasticity dilemma. Since its introduction in Grossberg (1976a, 1976b), ART has been incrementally developed into a cognitive and neural theory of how the brain autonomously learns to attend, recognize, and predict objects and events in a changing world, without experiencing catastrophic forgetting."
"Spatial and orientational competition prevent this catastrophe from occurring by closing boundary gaps at line ends using end cuts (Grossberg & Mingolla, 1985). These competitive stages thus illustrate hierarchical resolution of uncertainty: They overcome the spatial uncertainty at line ends that is caused by using simple cell receptive fields."
" The spatial competition stage is sensitive to the length of lines, a sensitivity that helps to create end gaps and end cuts. Its cells are often called hypercomplex cells (Hubel & Wiesel, 1968). However, many boundaries would still remain incomplete if boundary processing stopped with hypercomplex cells. "
"This coarseness can be understood as the embodiment within bipole receptive fields of perceptual experiences with nearly collinear and aligned visual stimuli during cortical development (Grossberg & Swaminathan, 2004; Grossberg & Williamson, 2001). However, if all perceptual groupings remained fuzzy, visual perception would be significantly degraded. "
"All of these properties have been reported in experiments on working memory whose recordings were taken from lateral prefrontal cortex and the frontal eye fields (Lundqvist et al., 2016). The authors interpret their results in terms of working memory, and discuss the gamma oscillations in terms of “encoding/decoding events” and “re-activation of sensory information”, whereas beta oscillations are viewed as a “default state interrupted by encoding and decoding” (p. 152). This “default” interpretation does not explain why beta bursts are “interrupted by encoding and decoding” or why the gamma oscillations embody “encoding/decoding events”. "
"However, such extinction can be eliminated if the two events get grouped as a single object, even if the link between the two stimuli is amodally completed behind an occluder (Mattingley, Davis, & Driver, 1997). "
"For example, Herzog, Sayim, Chicherov, and Manassi (2015, p. 1)assert “that the spatial configuration across the entire visual field determines crowding. Only when one understands how all elements of a visual scene group with each other, can one determine crowding strength."
"fMRI data of Burr and Morrone (2011, p. 504) illustrate this subtlety: “We firstly report recent evidence from imaging studies in humans showing that many brain regions are tuned in spatiotopic [head-centered] coordinates, but only for items that are actively attended”. The current theory provides a clear mechanistic explanation of this otherwise potentially confusing assertion."
"Timbres have more complex spatio-temporal spectral distributions that can also include frequency sweeps, different onset and offset times, and different relative amplitudes in different frequency bands (Grey, 1977; McAdams, 2013). "
" A stream-shroud resonance is proposed to play a similar role in sustaining auditory spatial attention and conscious auditory quality, and problems with sustained auditory attention, say due to a parietal lesion, can cause unilateral auditory neglect (e.g.,  Robertson et al., 1997)."
"The position of auditory sources is computed from interaural time differences (ITD) and interaural level differences (e.g.,  Bronkhorst & Plomp, 1988; Darwin & Hukin, 1999; Rayleigh, 1907). As a result, auditory spatial attention in humans does not appear to have a map structure (Kong et al., 2012), but rather seems to be embodied by an opponent process whose neurons are tuned to (e.g.) ITDs (Magezi & Krumbholz, 2010). "
"Because of this shared design, it becomes easier to understand how language in young children can begin to develop in a way that parallels the motor behaviors of adult teachers during mutual play (Bruner, 1975), or how sign language by hearing adults can coordinate signing with speaking (Neville et al., 2002). Both of these activities need much more model development to be fully understood."
"At the time that this circuit was published (Cohen, Grossberg, & Stork, 1988), it was not yet understood how speaker normalization might occur, so these acoustic features were simply said to be “invariant”;—that is, speaker-normalized—at the model’s Invariant Feature Detectors level."
"For example, hypoactivity of amygdala or orbitofrontal cortex can prevent a cognitive–emotional resonance from occurring, thereby causing failures in Theory of Mind processes (Baron-Cohen, 1989; Perner, Frith, Leslie, & Leekam, 1989) in both autism and schizophrenia (Grossberg, 2000b; Grossberg & Seidman, 2006). Such failures include problems with activating motivationally directed goals and intentions."
"These theoretical results suggest that, contrary to Clark and Squire (1998), episodic memory may not be necessary to consciously experience emotions."
"Dennett’s highly cited book called Consciousness Explained (Dennett, 1991) argued against a Cartesian Theater model, a place in the brain where “it all comes together” and generates subjective judgments. Instead, Dennett advocated a Multiple Drafts model where discriminations are distributed in space and time across the brain, a concept that, without further elaboration, is too vague to have explanatory power."
"Perhaps the theory of Damasio (1999) comes closest to theoretically linking brain to mind by providing what is, in effect, a heuristic derivation of the CogEM model to explain his clinical data about cognitive–emotional interactions (Section  19.4). But this theory provided no mechanistic account, could therefore provide no data simulations, and did not situate this heuristic derivation within a larger theory of how brain resonances and consciousness may be linked."
"Datta et al. [11] showcase a demo for skill-based, cohesion-aware team formation that utilizes common citations for establishing a social network. The authors, however, remain silent on specific details about the actual algorithm to find an optimal team."
General research on the formation of groups in large scale social networks [17] helps to understand the involved dynamic aspects but does not provide the algorithms for identifying optimal team configurations
"A prominent example of a graph-based global importance metric is Google's page rank [34]. An extended version [35] yields total ranks by aggregating search-topic-specific ranks. Balog and De Rijke [36] extract a social profile from collaborations within intranets to find suitable experts. The Aardvark search engine by Horowitz and Kamvar [37] leverages social ties for expert finding in a user's extended network. Inspired by the page rank algorithm, Schall [38] applies interaction intensities and skills to rank humans in mixed service-oriented environments. These algorithms and frameworks provide additional means to determine person-centric metrics but do not address the team composition problem per se. The potential of expert finding applications, however, and subsequently the impact on team formation cannot be underestimated. "
"In highly connected networks, we risk having the recommendations overpower the direct interaction links. Especially social networks that lack a rich-club structure (see [18]) are prone to produce compositions of non-connected experts."
"However, investigations of the rich-club phenomenon in scientific collaboration networks (e.g., [19]) have shown that such tight collaborative groups exist only within particular research domains but not beyond. "
"Such comparison can be made by either a ratio or a difference, where amplification (smoothing) is indicated by a ratio larger (smaller) than one, or a difference greater (less) than zero (Cachon, Randall, & Schmidt, 2007). Due to data availability, some empiricists use alternatives such as production quantity, sales and shipments which are easier to observe than orders and demand (Blinder & Maccini, 1991). "
" Wang (2002) extracted data from 46 product items and pinpoints price variation as a contributing factor to the production smoothing phenomenon. However, incorporation of price and seasonal fluctuation does not always generate results in support of production smoothing (Miron & Zeldes, 1988). "
"Using U.S. industry-level data, Cachon et al. (2007) also found that bullwhip primarily appears in the wholesaler, rather than in the retailer or manufacturer, echelon. Dooley, Yan, Mohan, and Gopalakrishnan (2010) studied the bullwhip effect during the 2007–2009 recession and concluded that retailers responded to market changes rapidly and adaptively, whereas wholesalers responded late and drastically."
"Sterman (1989) understood the order volatility from the perspective of bounded rationality and sub-optimal decisions. By analysing Beer Game results he discovered that most participants tend to overlook the on-order inventory (the supply-line or work-in-process) when making replenishment decisions. This phenomenon repeatedly occurred in subsequent experimental studies. This underweighting does not improve when: the supply line is made visible (Wu & Catok, 2006); demand is known and stationary (Croson & Donohue, 2006); or even when demand is known and constant (Croson et al., 2014)."
" In this regard, both Sterman's (1989) and Lee et al. (1997) explanations are inadequate since the cost assumptions in both approaches inherently induce amplification. Hence a lot of questions remain open regarding the emergence of bullwhip in real supply chains"
"Sodhi and Tang (2011) considered an arborescent supply chain and calls for a need to remove structural complexity in order to reduce bullwhip. Chatfield (2013)challenged the opinion that multi-echelon system can be approximated by cascading two-echelon systems, a.k.a. the decomposition assumption. They found that such an assumption leads to underestimated bullwhip measures."
"Lee et al. (1997) suggested that price stabilization or everyday low price (EDLP) helps to mitigate this problem. This strategy has been implemented in several retail chains, such as ASDA and Walmart. However, the validity of this measure remains questionable."
Muhanmmad Irfan Ali discussed another view on reduction of parameters in soft sets [41]
"Contrary to our prediction is the finding that the quality of negotiation outcomes, in terms of contract balance (fairness) and joint utility (efficiency) is lower when negotiators are provided with the history graph compared to those provided with tables. The results indicate that negotiators provided with the history graph followed a non-compensatory strategy. Usually, non-compensatory strategies are used when decision makers face a vast amount of information and balance a strategy's accuracy against its cognitive effort [3,24]"
"When comparing the effects of different information levels provided by the two graphs, we find that negotiation behavior becomes tougher. If negotiators are provided with the utilities of their opponent, then the visualization of offer-ratings according to the preferences of both negotiators makes it impossible to outwit the counterpart. The high level of control of both negotiation partners may actually act as a barrier to deceive the partner. Therefore, negotiators use more hard and soft tactics to substantiate their own position. At the same time, the negotiation dance graph may act as an ex-post monitoring system. When users make a concession, they can easily see whether their counterparts reciprocate, and the dance graph reduces the risk of being exploited. We observe that negotiators provided with the negotiation dance graph offer more unconditional concessions. The effect of these differences in behavior is visible in the quality of outcomes: in contrast to the history graph, the negotiation dance graph facilitates efficient and fair agreements. Nevertheless, it does not make negotiators more satisfied. On the contrary, their holistic assessment of the negotiation outcome is significantly lower compared to the negotiators who have no access to utility values of their opponent. This can be explained by the tougher negotiation process visible through the increased use of hard tactics and by the fact that negotiators compare their individual outcome with the opponent's outcome. Even a small difference in utilities might lead to the feeling of being a loser instead of a winner (e.g. [17,37,77])."
"Itoh etÂ al. [24] proposed to overlay pie-like glyphs over the nodes in a graph to encode multiple categories. Each set is hence represented using disconnected regions that are linked by having the same colour. This causes difficulties with tasks that involve finding relations between sets such as T1, T3 and T4 "
"Riche and Dwyer introduced two Euler diagram-based techniques, ComEd and DupEd, designed to visualize sets using simple regions and to draw individual data elements as text-annotated nodes [35]. Unlike the methods described in detail above, Richeâs and Dwyerâs two methods lay out the network with regard to the set structure. ComEd represents each set as one or more rectangles connected with concave curves, assigned a unique colour. The use of rectangles and irregular curves could impose cognitive difficulties in perceiving the sets, as they might give the impression of different semantics [45]. Another issue with ComEd is the artifacts caused by overlaps between the curves that connect the rectangles. "
"In [25], each node belonged to only one group and, in addition, the groups were all disjoint. This meant that no set overlaps were present. By contrast, for our study the nodes belonged to multiple groups and there were richer relationships between the groups such as subset and intersection"
"Of particular note, though, is that our tasks differed significantly from those used in previous studies [3,25,28]. All of our questions required the participants to access information about both the sets and the network. The tasks used in [3,28] did not include any network or group-network tasks. Thus, our results suggest that, as the complexity of the task increases, the difference in effectiveness of Bubble Sets, KelpFusion and LineSets becomes insignificant."
"They argue that their results show that perception is a top-down process, in contrast to the Ecological bottom-up process, where the readers recognize the genres through the attributes of the layout which forms the basis of document recognition (or perception for recognition), and although Toms and Campbell, like Lakoff (1987), refer to the bottom-up process and suggest that genres may “act as a single gestalt” Toms & Campbell (1999a, p. 2015) they do not explore other possibilities, such as perception for action and how a genre is perceived when the document is displayed to a reader (in all fairness Watt (2009, chap. 8) also fails to explore the perception for recognition concept)."
"The scanpath mirrors clearly the unfolding of visual attention over time and indicates which features or contents in a visual context are attended (Coco, 2009). The movement represented by these scanpaths are not random, rather they reflect the viewer’s frame of mind, expectations and purpose (Yarbus, 1967). "
"The techniques were detected using the methodology employed in Campbell & Maglio (2001, p. 3) and Buscher, Dengel, van Elst, and Mittag (2008), with some modifications. These two papers reported on the detection of skimming and reading techniques, not skimming and scanning techniques. "
"Ulf Grenander was working on abstract mathematical and statistical models and methods for many computer vision problems and presented his findings in books that were not easily understood by mainstream computer vision researchers. As a result, Prof. Grenander's work was seen as esoteric. David Cooper was also vigorously pursuing Bayesian methods for boundary and object recognition [4]."
"Firstly, there has been some considerable work on optimizing resource usage while keeping QoS goals. These papers, however, concentrate on specific subsystems of Large Scale Distributed Systems, such asÂ [11] on the performance of memory systems, or only deal with one or two specific SLA parameters. "
Petrucci etÂ al.Â [12] or Bichler etÂ al.Â [13] investigate one general resource constraint and Khanna etÂ al.Â [14] only focuses on response time and throughput.
"A quite similar approach to our concept is provided by the Sandpiper frameworkÂ [15], which offers black-box and gray-box resource management for VMs. Contrary to our approach, though, it plans reactions just after violations have occurred."
"Also the VCONF model by Rao etÂ al.Â [16] has similar goals as presented in SectionÂ 1, but depends on specific parameters, can only execute one action per iteration and it neglects the energy consumption of executed actions. "
 Hoyer etÂ al.Â [20] also undertake a speculative approach as in our work by overbooking PM resources. 
" [27] viewed the system in four layers (i.e.,Â business, system, network and device) and broke down the SLA into relevant information for each layer, which had the responsibility of allocating required resources. Again, no details on how to achieve this have been given"
"Thirdly, commercial Cloud IaaS platforms such as Amazon EC2Â [29], RackspaceÂ [30] or RightScaleÂ [31] have a very limited choice of preconfigured and static VM resource provisioning types."
"MonitoringCurrent monitoring systems (e.g.,Â gangliaÂ [35]) facilitate monitoring only of low-level systems resources, such as free_disk or packets_sent, but SLA parameters typically are, e.g.,Â storage and outgoing bandwidth."
"As opposed to the CBR approach inÂ [7], the rule-based approach is able to fire more than one action at the same iteration, which inherently increases the flexibility of the system."
" In this framework, a very interesting method has been developed by Evgeniou and Pontil in [10]. They present a preprocessing algorithm that computes clusters of points in each class, based on Euclidean distance, and substitute each cluster with the mass center of the points in the cluster. The algorithm tends to produce large (small) clusters of data points which are far (near) from the boundary between the two classes. These strategies did not focus on both large and imbalanced data learning."
" In [8] the authors proposed a novel classification approach for large data sets using Minimum Enclosing Ball (MEB) clustering: after partitioning the training data via MEB method, the centers of the clusters were used for the first time SVM classification; the algorithm used only the clusters whose centers are support vectors or those clusters which have different classes to perform the second time SVM classification. In this way many data points were recursively removed. However, the above mentioned methods are not helpful for classification of large data sets with imbalanced classes."
"There have been many works in literature that apply different techniques to the SVM framework in order to overcome problems due to imbalance. Most of them assign different error costs to different classes in order to shift the decision boundary and to guarantee that it is better defined [1,12]. Another major category of kernel-based learning research efforts focuses more concretely on the mechanics of the SVM itself; this group of methods is often called kernel modification methods [12]. However these methods could be useless for large data set, because they use all the data for training the classifier. The undersampling techniques are useful for large training set. "
" In [30] the Granular Support Vector Machines - Repetitive Undersampling algorithm (GSVM-RU) was proposed to integrate SVM learning with undersampling methods. This method uses the SVM itself as a mechanism for undersampling in order to sequentially develop multiple subsets with different informative samples, which are later combined to develop a final SVM for classification. Also this method is not tailored for very huge data sets, because the SVM problem should be hard to solve due to the training set size. "
"Although a similar technique was successfully used by Qamar and Sanghi [19], the approach does not appear to have been widely adopted as an alternative to projection techniques. A possible limitation of the method is that it can rapidly become expensive as the number of parameters of interest increases. "
As well as the previously discussed problems with doing this it is also known that a particular set of snapshots may result in POD modes which are only appropriate in a limited parameter space. For example Lieu et al. [15] applied Galerkin projection to a complete F-16 aircraft model which depended only on the angle of attack and free-stream Mach number Mâ
" To avoid this problem and others introduced by projection methods the strategy adopted in this work is to treat POD as a basis for interpolation.POD coefficient interpolation type techniques, as introduced by Ly and Tran [11], have not been widely adopted as an alternative to projection type techniques. "
"It is known that the accuracy of the RBF approximation depends upon the value that is adopted for the parameter c [22]. Rippa [23] introduced an algorithm to find the optimum value for a particular data set but, for all the cases considered here, the additional expense incurred in performing an analysis of this type did not prove to be justified. Instead, the value of c is simply calculated to be the mean distance between the data points."
"In contrast to Sivaraman et al. (2016); Mittal et al. (2016), our research findings show that a chain of NFs requires a global sched-uler to make chain-level decisions, rather than an internal sched-uler that executes local switch policies. To address this gap, we de-signed and implemented the Service Chain Coordinator (SCC)." 
