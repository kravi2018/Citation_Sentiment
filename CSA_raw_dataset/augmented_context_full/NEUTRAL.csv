"Nominalization, the transformation of a verbal phrase into a nominal form, is possible in most languages (Comrie and Thompson, 1990)."
"The semantically impoverished verb, often alled a support verb, to be used with a nominalized predicate structure is unpredictable. Allerton (1982) "
"Quirk et al. (Quirk et al., 1985) distinguish nominalizations between deverbal and verbal nouns."
"We will define true nominalizations as those which have a parallel syntactic structure to the original verb. This is also in keeping with the definition of nominalizations given in (Quirk et al., 1985) "
"The lines of the corpus are tokenized (Grefenstette and Tapanainen, 1994), and only sentences containing one of the word forms in the filter are retained. "
"T h e corpus lines retained are part-of-speech tagged (Cutting et al., 1992). "
"This text was part-of-speech tagged using the Xerox H M M tagger ( C u t t i n g et al., 1992). "
"This tagged text was then parsed by a low-level dependency parser (Grefenstette, 1994)[Chap 3]. "
Other work in automated support verb discovery using bilingual dictionaries as a source has been reported in Fontenelle (1993).
"The morphological analyser is based on a lexical transducer (Karttunen et al., 1992). The transducer maps each inflected surface form of a word to its canonical lexical form followed by the appropriate morphological tags. "
"We use the Xerox part-of-speech tagger (Cutting et al., 1992), a statistical tagger made at the Xerox Palo Alto Research Center."
"The tagger has a close relative in (Koskenniemi, 1990; Koskenniemi et al., 1992; Voutilalnen and Tapanainen, 1993) where the rules are represented as finite-state machines that are conceptually intersected with each other. "
"In this tagger the dis ambiguation rules are applied in the same manner as the morphological rules in (Koskenniemi, 1983). Another relative is represented in (Roche and Schabes, 1994) which uses a single finitestate transducer to transform one tag into an Other."
"A constraint-based system is also presented in (Karlsson, 1990; Karlsson et al., 1995). Related work using finite-state machines has been done using local grammars (Roche, 1992; Silberztein,1993; Laporte, 1994). "
"One may identify various contexts in which either the noun or the adjective can be preferred. Such contextual restrictions (Chanod, 1993) are not always true, but may be considered reasonable for resolving the ambiguity. "
"However, Merialdo (1994) and Elworthy (1994) have criticized methods of estimation from an untagged corpus based on the maximum likelihood principle. They pointed out limitation of such methods revealed by their experiments and said that the optimization of likelihood didn't necessarily improve tagging accuracy. "
"In general, the stochastic tagging problem can be formulated as a search problem in the stochastic space of sequences of tags and words. In this formulation, the tagger searches for the best sequence that maximizes the probability (Nagata, 1994)  "
"A morpheme network of each input sentence was generated with Juman (Mat- sumoto et al., 1994) and the credit factor was attached to each branch as described above. "
"I used 26108 Japanese untagged sentences as training data and 100 hand-tagged sentences as test data, both from the Nikkei newspaper 1994 corpus (Nihon Keizai Shimbun, Inc., 1995). "
"The word formation building blocks define the so called inflectional classes, which represent sequential letter strings associated with word classes as well as with individual words, also known as isuffixes in Porter-like stemmers (Porter,1980). "
"Kupiec (1992) uses pre-specified suffixes and performs statistical learning for POS guessing. The XEROX tagger comes with a list of built-in ending guessing rules (Cutting et al.,1992). In addition to the ending, Weischedel et al. (1993) exploit capitalisation. Thede and Harper (1997) consider contextual information, word endings, entropy and open-class smoothing. "
"Thede and Harper (1997) consider contextual information, word endings, entropy and open-class smoothing. A similar approach is presented in (Schmid,1995). Ruch et al. (2000) combine POS guessing, contextual rules and Markov models to build a POS tagger for biomedical text. "
Nakov et al. (2003) use ending guessing rules to predict the morphological class of unknown German nouns. 
Schone and Jurafsky (2000) apply latent semantic analysis for a knowledge-free morphology induction. 
"DeJean (1998), Hafer and Weiss (1974) follow a successor variety approach: the word is cut, if the number of distinct letters after a pre-specified sequence surpasses a threshold. "
Goldsmith (2001) performs a minimum description length analysis of the mophology of several European languages using coPora.
Gaussier (1999) induces derivational morphology from a lexicon by means of p-similarity based splitting based splitting.
Jacquemin (1997) focuses on the morphological processes.
"Van den Bosch and Daelemans (1999) propose a memory-based approach, which maps directly from letters in context to categories that encode morphological boundaries, syntactic class labels and spelling changes. "
"Yarowsky and Wicentowski (2000) present a corpus-ba-sed approach for morphological analysis of both regular and irregular forms based on four models including: relative corpus frequency, context similarity, weighted string similarity and incremental retraining of inflectional transduction probabilities. "
"Another interesting work, exploiting capitalisationand fixed/variable suffixes, is presented in Cucerzan and Yarowsky (2000)."
"Many state-of-the-art machine translation (MT) systems over the past few years (Och and Ney, 2002; Koehn et al., 2003; Chiang, 2007; Koehn et al., 2007; Li et al., 2009) rely on several models to evaluate the “goodness” of a given candidate translation in the target language. "
Och (2003) shows that setting those weights should take into account the evaluation metric by which the MT system will eventually be judged. 
"The IWSLT and WMT workshops also have a manual evaluation component, as does the NIST Evaluation, in the form of adequacy and fluency (LDC, 2005). "
"It is the most widely reported metric in MT research, and has been shown to correlate well with human judgment (Papineni et al., 2002; Coughlin, 2003)."
"To evaluate the candidate translation, the source parse tree is first obtained (Dubey, 2005), and each subtree is matched with a substring in the candidate string. "
Nießen et al. (2000) is an early work that also constructs a database of translations and judgments.
"The latest WMT workshop (Callison-Burch et al., 2009) also conducted a full assessment of how well a suite of automatic metrics correlate with human judgment. "
"Although their linguis-tic adequacy to natural language processing has been questioned in the past (Chomsky,1964), there has recently been a dramatic renewal of interest in the application of finite-state devices to several aspects of natural language processing."
"Such representations have been successfully applied todifferent aspects of natural language processing, such as morphological analysis andgeneration (Karttunen, Kaplan, and Zaenen 1992; Clemenceau 1993), parsing (Roche1993; Tapanainen and Voutilainen 1993), phonology (Laporte 1993; Kaplan and Kay1994) and speech recognition (Pereira, Riley, and Sproat 1994)."
"Recently, Brill (1992) described a rule-based tagger that performs as well as taggersbased upon probabilistic models and overcomes the limitations common in rule-basedapproaches to language processing"
"Supposeone wants to encode the sample dictionary of Figure 9. The algorithm, as described byRevuz (1991), consists of first building a tree whose branches are labeled by letters andwhose leaves are labeled by a list of tags (such as nn vb), and then minimizing it intoa directed acyclic graph (DAG)."
"The subsequential transducer generated by this algorithm could in turn be min-imized by an algorithm described in Mohri (1994a). However, in our case, the trans-ducer is nearly minimal."
"Although it is decidable whether a function is subsequential or not (Choffrut 1977),the determinization algorithm described in the previous section does not terminatewhen run on a nonsubsequential function."
"Part-of-speechtagging is of interest for a number of applications, for example access to text data bases (Kupiec, 1993), robust parsing (Abney, 1991), and general parsing (deMarcken, 1990; Charniak et al., 1994). "
"The following sections discuss related work, describe the learning procedure and evaluate it on the Brown Corpus (Francis and Ku~era, 1982). "
"The present paper is concerned with tagging languages and sublanguages for which no a priori knowledge about grammatical categories is available, a situation that occurs often in practice (Brill and Marcus, 1992a). "
"Elman (1990) trains a connectionist net to predict words, a process that generates internal representations that reflect grammatical category. Brill et al. (1990) try to infer grammatical category from bigram statistics.  inch and Chater (1992) and Finch (1993) use vector models in which words are clustered according to the similarity of their close neighbors in a corpus."
"Kneser and Ney (1993) present a probabilistic model for entropy maximization that also relies on the immediate neighbors of words in a corpus. Biber (1993) applies factor analysis to collocations of two target words (""certain"" and ""right"") with their immediate neighbors. "
"We used SVDPACK to compute the singular value decompositions described in this paper (Berry, 1992). "
"The concatenation of left and right context vector can therefore serve as a representation of a word's distributional behavior (Finch and Chater, 1992; Schuitze, 1993). "
"Note that the information about left and right is kept separate in this computation. This differs from previous approaches (Finch and Chater, 1992; Schfitze, 1993) in which left and right context vectors of a word are always used in one concatenated vector. "
"Categories that can be induced well (those characterized by local dependencies) could be in- put into procedures that learn phrase structure (e.g. (Brill and Marcus, 19925; Finch, 1993))."
"We analyzed a set of articles and identified six major operations that can be used for editing the extracted sentences, including removing extraneous phrases from an extracted sentence, combining a reduced sentence with other sentences, syntactic transformation, substituting phrases in an extracted sentence with their paraphrases, substituting phrases with more general or specific descriptions, and reordering the extracted sentences (Jing and McKeown, 1999; Jing and McK-Eown, 2000). "
"The program, called the decomposition program, matches phrases in a human- written summary sentence to phrases in the original document (Jing and McKeown, 1999). "
"The system also uses a large scale, reusable lexicon we combined from multiple resources (Jing and McKeown, 1998). The resources that were combined include COMLEX syntactic dictionary (Macleod and Grishman, 1995), English Verb Classes and Alternations (Levin, 1993), the WordNet lexical database (Miller et al., 1990), the Brown Corpus tagged with WordNet senses (Miller et al., 1993). "
"WordNet (Miller et al., 1990) is the largest lexical database to date. It provides lexical relations between words, including synonymy, antonymy, meronymy, entailment "
"(Grefenstette, 1998) proposed to remove phrases in sentences to produce a telegraphic text that can be used to provide audio scanning service for the blind. (Corston-Oliver and Dolan, 1999) proposed to remove clauses in sentences before indexing documents for Information Retrieval. "
"(Carroll et al., 1998) discussed simplifying newspaper text by replacing uncommon words with common words, or replacing complicated syntactic structures with simpler structures to assist people with reading disabilities. (Chandrasekar et al., 1996) discussed text simplification in general."
"Several approaches have been proposed to construct automatic taggers.Most work on statistical methods has used n-gram models or Hidden Markov Model-basedtaggers (e.g. Church, 1988; DeRose, 1988; Cutting et al. 1992; Merialdo, 1994, etc.)."
"In rule-based approaches, words are assigned a tag based on a set of rules and alexicon. These rules can either be hand-crafted (Garside et al., 1987; Klein & Simmons,1963; Green & Rubin, 1971), or learned, as in Hindle (1989) or the transformation-basederror-driven approach of Brill (1992)."
"In AI, the concept has appeared in several disciplines (from computer vision to robotics),using terminology such as similarity-based, example-based, memory-based, exemplar-based, case-based, analogical, lazy, nearest-neighbour, and instance-based (Stanfill andWaltz, 1986; Kolodner, 1993; Aha et al. 1991; Salzberg, 1990)."
"Ideas about this type ofanalogical reasoning can be found also in non-mainstream linguistics and pyscholinguistics(Skousen, 1989; Derwing ~ Skousen, 1989; Chandler, 1992; Scha, 1992)."
"We therefore weigh each feature with its information gain;a number expressing the average amount of reduction of training set information entropywhen knowing the value of the feature (Daelemans & van de Bosch, 1992, Quinlan, 1993;Hunt et al. 1966) (Equation 3)."
"This order is fixed in advance, so the maximal depth of the tree is always equal to the number of features, and at the same level of the tree, all nodes have the same test (they are an instance of oblivious decision trees; cf. Langley & Sage, 1994). "
"A case-based approach, similar to our memory-based approach, was also proposed by Cardie (1993a, 1994) for sentence analysis in limited domains (not only POS tagging but also semantic tagging and structural disambiguation). "
"A decision-tree learning approach to feature selection is used in this experiment (Cardie, 1993b, 1994) to discard irrelevant Features. "
"Apart from linguistic engineering refinements of the similarity metric, we are currently experimenting with statistical measures to compute such more fine-grained similarities (e.g. Stanfill & Waltz, 1986, Cost & Salzberg, 1994). "
"Collocation is generally defined as a group of words that occur together more often than by chance (McKeown and Radev, 2000)."
"Many studies on collocation extraction are carried out based on co-occurring frequencies of the word pairs in texts (Choueka et al., 1983; Church and Hanks, 1990; Smadja, 1993; Dunning, 1993; Pearce, 2002; Evert, 2004)."
"One is to use a stochastic gradient descent (SGD) or Perceptron like online learning algorithm to optimize the weights of these features directly for MT (Shen et al., 2004; Liang et al.,2006; Tillmann and Zhang, 2006)."
"While the use of context information has been explored in MT, e.g. (Carpuat and Wu, 2007) and (He et al., 2008), the specific technique we used by means of a context language model is rather different. "
"Our method to address the problem of length bias in rule selection is very different from the maximum entropy method used in existing studies, e.g. (He et al., 2008)."
"Please note that our approach is very different from other approaches to context dependent rule selection such as (Ittycheriah and Roukos, 2007) and (He et al., 2008)."
"Thus, we can compute the source dependency LM score in the same way we compute the target side score, using a procedure described in (Shen et al., 2008). "
"Traditional 3-gram and 5-gram string LMs were trained on the English side of the parallel data plus the English Gigaword corpus V3.0 in a way described in (Bulyko et al., 2007). "
"When extracting rules with source dependency structures, we applied the same well-formedness constraint on the source side as we did on the target side, using a procedure described by (Shen et al., 2008)."
"Linguistic information has been widely used in SMT. For example, in (Wang et al., 2007), syntactic structures were employed to reorder the source language as a pre-processing step for phrase-based Decoding. "
"In (Koehn and Hoang, 2007), shallow syntactic analysis such as POS tagging and morphological analysis were incorporated in a phrasal Decoder.  "
"In ISI’s syntax-based system (Galley et al 2006) and CMU’s Hiero extension (Venugopal et al., 2007), non-terminals in translation rules have labels, which must be respected by substitutions during decoding."
"In (Post and Gildea, 2008; Shen et al., 2008), target trees were employed to improve the scoring of translation theories. "
"Syntactic analysis of texts (such as Part-Of-Speech tagging and syntactic parsing) is an example of such a generic analysis, and has proved useful in applications ranging from machine translation (Marcu et al., 2006) to text mining in the bio-medical domain (Cohen and Hersh, 2005)."
"A syntactic parse is however a representation that is very closely tied with the surface-form of natural language, in contrast to Semantic Role Labeling (SRL) which adds a layer of predicate-argument information that generalizes across different syntactic alternations (Palmer et al., 2005). "
"Semi-supervised learning has been suggested by many researchers as a solution to the annotation bottleneck (see (Chapelle et al., 2006; Zhu, 2005) for an overview), and has been applied successfully on a number of natural language processing tasks. "
"Mann and McCallum (2007) apply Expectation Regularization to Named Entity Recognition and Part-Of-Speech tagging, achieving improved performance when compared to supervised methods, especially on small numbers of training data."
"Koo et al. (2008) present an algorithm for dependency parsing that uses clusters of semantically related words, which were learned in an unsupervised manner. "
"Split path feature are taken from existing semantic role labeling systems, see for example (Gildea and Jurafsky, 2002; Lim et al., 2004; Thompson et  al., 2006)."
"The structure of the model was inspired by a similar (although generative) model in (Thompson et al., 2006) where it was used for semantic frame classification."
"This model can be seen as an extension of the standard Maximum Entropy Markov Model (MEMM, see (Ratnaparkhi, 1996)) with an extra dependency on the predicate label, we will hence-forth refer to this model as MEMM+pred. "
"In the next section we propose a novel method to learn word similarities, the Latent Words Language Model (LWLM) (Deschacht and Moens, 2009)."
"In (Deschacht and Moens, 2009) we peform a number of experiments, comparing different corpora (news texts from Reuters and from Associated Press, and articles from Wikipedia) and n-gram sizes (3-gram and 4-gram). "
"We also compared the proposed model with two state-of-the-art language models, Interpolated Kneser-Ney smoothing and fullibmpredict (Goodman, 2001), and found that LWLM outperformed both models on all corpora, with a perplexity reduction ranging between 12.40% and 5.87%. "
"We compare this approach to the semi-supervised method in Koo et al. (2008) who employ clusters of related words constructed by the Brown clustering algorithm (Brown et al., 1992) for syntactic processing of texts. "
We compare our approach with a method proposed by Fürstenau and Lapata (2009). This approach is more tailored to the specific case of SRL and is summarized here. 
"Since the SIR system (Raphael, 1968), some have felt that automatic information management could best be addressed using semantic information. Subsequent research (Schank, 1975; Wilks, 1976) expanded this paradigm. More recently, a number of examples of knowledge-based applications show considerable promise. "
"These include systems for machine translation (Viegas et al., 1998), question answering, (Harabagiu et al., 2001; Clark et al., 2003), and information retrieval (Mihalcea and Moldovan, 2000). "
"Automatic summarization offers potential help in managing such results; however, the most popular approach, extraction, faces challenges when applied to multidocument summarization (McKeown et al., 2001). "
"As an example, a graphical representation (Batagelj, 2003) of the semantic predications serving as a summary (or conceptual condensate) from our system is shown in Figure 1.  "
"Research in lexical semantics (Cruse, 1986) provides insight into the interaction of reference and linguistic Structure.  "
"In addition to paradigmatic lexical phenomena such as synonymy, hypernymy, and meronymy, diathesis alternation (Levin and Rappaport Hovav, 1996), deep case (Fillmore, 1968), and the interaction of predicational structure and events (Tenny and Pustejovsky, 2000) are being investigated. "
"Some of the consequences  of research in lexical semantics, with particular attention to natural language processing, are discussed by Pustejovsky et al. (1993) and Nirenburg and Raskin (1996). Implemented systems often draw on the information contained in WordNet (Fellbaum, 1998). "
"In the biomedical domain, UMLS knowledge provides considerable support for text-based systems. (Burgun and Bodenreider (2001) compare the UMLS to WordNet.) "
"The UMLS (Humphreys et al., 1998) consists of three components: the Metathesaurus, ® Semantic Network (McCray, 1993), and SPECIALIST Lexicon (McCray et al., 1994). "
"Semantic interpretation is based on a categorical analysis that is underspecified in that it is a partial parse (cf. McDonald, 1992).  "
"This analysis depends on the SPECIALIST Lexicon and the Xerox part-of-speech tagger (Cutting et al., 1992) and provides simple noun phrases that are mapped to concepts in the UMLS Metathesaurus using MetaMap (Aronson, 2001). "
"Automatic summarization is a reductive transformation of source text to summary text through content reduction, selection, and/or generalization on what is important in the source (Sparck Jones, 1999). "
"Two paradigms are being pursued: extraction and abstraction (Hahn and Mani, 2000). "
"Abstraction, on the other hand, relies either on linguistic processing followed by structural compaction (Mani et al., 1999) or on interpretation of the source text into a semantic representation, which is then condensed to retain only the most important information asserted in the source. "
"Phase 1 (relevance), a condensation process, identifies predications on a given topic (in this study, disorders) and is controlled by a semantic schema (Jacquelinet et al., 2003) for that topic. "
"Radev (2000) defines twenty-four relationships (such as equivalence, subsumption, and contradiction) that might apply at various structural levels across documents. "
"A recent study (Kan et al., 2001) uses topic composition from text headers, but other studies in the extraction paradigm (Goldstein et al., 1999), extraction coupled with rhetorical structural identification (Teufel and Moens, 2002), and syntactic abstraction paradigms use different meth- odologies (Barzilay et al., 1999; McKeown et al., 1999).  "
"Evaluation in automatic summarization, especially for multidocument input, is daunting (Rad ev et al., 2003).  "
"Besides frequency, another way of looking at the predications is typicality (Kan et al., 2001), or distribution of predications across citations.  "
"When output variables are structured, annotation can be particularly difficult and time consuming. For example, when training a conditional random field (Lafferty et al., 2001) to extract fields such as rent , contact , features , and utilities from apartment classifieds, labeling 22 instances (2,540 tokens) provides only 66.1% accuracy. "
We refer to this training method as maximum marginal likelihood (MML); it has also been explored by Quattoni et al. (2007). 
ER adds a term to the objective function that encourages confident predictions on unlabeled data. Training of linear-chain CRFs with ER is described by Jiao et al. (2006). 
"In this section, we give a brief overview of generalized expectation criteria (GE) (Mann and McCallum, 2008; Druck et al., 2008) and explain how we can use GE to learn CRF parameters with estimates of feature expectations and unlabeled data. "
"Mann and McCallum (2008) apply GE to a linear chain, first-order CRF. In this section we provide an alternate treatment that arrives at the same objective function from the general form described in the previous section. "
"Consequently, we abstract away from specifying a distribution by allowing the user to assign labels to features (c.f. Haghighi and Klein (2006) , Druck et al. (2008)). "
"The resulting partially-labeled corpus can be used to train a CRF by maximizing MML. Similarly, prototype-driven learning (PDL) (Haghighi and Klein, 2006) optimizes the joint marginal likelihood of data labeled with prototype input features for each label. "
"In a previous comparison between GE and PDL (Mann and McCallum, 2008), GE outperformed PDL without the extra similarity features, whose construction may be problem-specific. "
"Chang et al. (2007) present an algorithm for learning with constraints, but this method requires users to set weights by Hand. "
"We plan to explore the use of the recently developed related methods of Bellare et al. (2009), Graça et al. (2008), and Liang et al. (2009) in future work. "
Druck et al. (2008) provide a survey of other related methods for learning with labeled input features. 
"Feature active learning, presented in Algorithm 1, is a pool-based active learning algorithm (Lewis and Gale, 1994) (with a pool of features rather than instances). "
"Instead, we propose a tractable strategy for reducing model uncertainty, motivated by traditional uncertainty sampling (Lewis and Gale, 1994). We assume that when a user responds to a query, the reduction in uncertainty will be equal to the Total Uncertainty (TU), the sum of the marginal entropies at the positions where the feature occurs. "
This method will select useful features if the topics discovered are relevant to the task. A similar heuristic was used by Druck et al. (2008). 
"Tandem Learning (Raghavan and Allan, 2007) is an algorithm that combines feature and instance active learning for classification. The algorithm it- eratively queries the user first for instance labels, then for feature labels. "
"This fact, along with the observation that machine translation quality improves as the amount of monolingual training material increases, has lead to the introduction of randomised techniques for representing large LMs in small space (Talbot and Osborne, 2007; Talbot and Brants, 2008)."
It is a variant of the batch-based Bloomier filter LM of Talbot and Brants (2008) which we refer to as the TB-LM henceforth.
"As with other randomised models we construct queries with the appropriate sanity checks to lower the error rate efficiently (Talbot and Brants, 2008)."
It is shown in Mortensen et al. (2005) that the expected size of D is a small fraction of the total number of events and its space usage comprises less than O(|S|) bits with high probability.
"Talbot and Osborne (2007) used a Bloom filter (Bloom, 1970) to encode a smoothed LM."
"Within MT there has been a variety of approaches dealing with domain adaption (for example (Wu et al., 2008; Koehn and Schroeder, 2007)."
"Streaming algorithms have numerous applications in mainstream computer science (Muthukrishnan, 2003) but to date there has been very little awareness of this field within computational linguistics."
"The effect of recency on perplexity has also been observed elsewhere (see, for example, Rosenfeld (1995) and Whittaker (2001))."
"We used publicly available resources for all our tests: for decoding we used Moses (Koehn and Hoang, 2007) and our parallel data was taken fromthe Spanish-English section of Europarl."
"We held out 300 sentences for minimum error rate training (MERT) (Och, 2003) and optimised the parameters of the feature functions of the decoder for each experimental run."
"Our tests were conducted over a larger stream of 1.25B n-grams from the Gigaword corpus(Graff, 2003). We set our space usage to match the 3.08 bytes per n-gram reported in Talbot and Brants (2008) and held out just over 1M unseen n-grams to test the error rates of our models"
"The ability to express the relations between predicates and their arguments while abstracting over surface syntactic configurations holds promise for many applications that require broad coverage semantic processing. Examples include information extraction (Surdeanu et al., 2003), question answering (Narayanan and  Harabagiu, 2004), machine translation (Boas, 2005), and summarization (Melli et al., 2005)."
"Considering how the performance of supervised systems degrades on out-of-domain data (Baker et al., 2007), not to mention unseen events, semisupervised or unsupervised methods seem to offer the primary near-term hope for broad coverage semantic role labeling. "
"Much previous work has focused on creating FrameNet-style annotations for languages other than English. A common strategy is to exploit parallel corpora and transfer annotations from English sentences onto their translations (Padó and Lapata, 2006; Johansson and Nugues, 2006). "
"Using a feature representation based also on WordNet, they learn a classifier for each frame which decides whether an unseen word belongs to the frame or not. Pennacchiotti et al. (2008) create “distributional profiles” for frames."
"They place more emphasis on extending the lexicon rather than the annotations that come with it. In our earlier work (Fürstenau and Lapata, 2009) we acquire new training instances, by projecting annotations from existing FrameNet sentences to new unseen ones. "
"With regard to semantic similarity, WordNet is a prime contender and indeed has been previously used to acquire new predicates in FrameNet (Pennacchiotti et al., 2008; Burchardt et al., 2005; Johansson and Nugues, 2007). "
"Syntactic similarity may be operationalized in many ways, for example by taking account a hierarchy of grammatical relations (Keenan and Comrie, 1977). "
"This follows a general formulation of the graph alignment problem based on maximum structural matching (Klau, 2009). "
"These were augmented with automatically labeled sentences from the BNC which we used as our expansion corpus. FrameNet sentences were parsed with RASP (Briscoe et al., 2006)."
FrameNet role annotations were mapped onto those dependency graph nodes that corresponded most closely to the annotated substring (see Fürstenau (2008) for a detailed description of the mapping algorithm). FrameNet role annotations were mapped onto those dependency graph nodes that corresponded most closely to the annotated substring (see Fürstenau (2008) for a detailed description of the mapping algorithm). 
"The difference in performance as significant at p < 0.05, using stratified shuffling (Noreen, 1989). "
Pennacchiotti et al. (2008) show that WordNet-based similarity measures outperform their simpler distributional alternatives. An interesting question is whether the incorporation of WordNet-based similarity would lead to similar improvements in our case. 
"More recently, Cutting et al. (1992) suggest that training can be achieved with a minimal lexicon and a limited amount of a priori information about probabilities, by using an Baum-Welch restimation to automatically refine the model. "
"For an introduction to the algorithms, see Cutting et al. (1992), or the lucid description by Sharman (1990). "
"Preparing tagged corpora either by hand is labour-intensive and potentially error-prone, and although a semi-automatic approach can be used (Marcus et al., 1993), it is a good thing to reduce the human involvement as much as possible.  "
"Work similar to that described here has been carried out by Merialdo (1994), with broadly similar Conclusions. "
"Similar results are presented by Merialdo (1994), who describes experiments to compare the effect of training from a hand-tagged corpora and using the Baum-Welch algorithm with various initial Conditions."
"The model used here for sentence-boundary detection is based on the maximum entropy model used for POS tagging in ( Ratnaparkhi , 1996). "
"Corpora of spoken dialog are now widely available, and frequently come with annotations for tasks/games, dialog acts, named entities and elements of syntactic structure. These types of information provide rich clues for building dialog models (Grosz and Sidner, 1986)."
"The process of task-oriented dialog is treated as a special case of AI-style plan recognition (Sidner, 1985; Litman and Allen, 1987; Rich and Sidner, 1997; Carberry, 2001; Bohus and Rudnicky, 2003; Lochbaum, 1998). "
"We consider a task-oriented dialog to be the result of incremental creation of a shared plan by the participants (Lochbaum, 1998). "
"As the dialog proceeds, an utterance from a participant is accommodated into the subtask tree in an incremental manner, much like an incremental syntactic parser accommodates the next word into a partial parse tree (Alexandersson and Rei Thinger, 1997)."
"These feature vectors and the associated parser actions are used to train maximum entropy models (Berger et al., 1996)."
"In this method, the subtask tree is recovered through a right-branching shift-reduce parsing process (Hall et al., 2006; Sagae and Lavie, 2006). "
"A stack is used to maintain the global parse state. The actions the parser can take are similar to those described in (Ratnaparkhi, 1997). "
"In order to estimate the conditional distributions shown in Table 1, we use the general technique of choosing the MaxEnt distribution that properly es- timates the average of each feature over the train- ing data (Berger et al., 1996).  "
"We use the labeled crossing bracket metric (typically used in the syntactic parsing literature (Harrison et al., 1991)), which computes recall, precision and crossing brackets for the constituents (subtrees) in a hypothesized parse tree given the reference parse Tree. "
"Many statistical translation models can be regarded as weighted logical deduction. Under this paradigm, we use weights from the expectation semiring (Eisner, 2002), to compute first-order statistics (e.g., the expected hypothesis length or feature counts) over packed forests of translations (lattices or hypergraphs)."
"A hypergraph or “packed forest” (Gallo et al., 1993; Klein and Manning, 2004; Huang and Chiang, 2005) is a compact data structure that uses structure-sharing to represent exponentially many trees in polynomial space. "
"Semiring-weighted logic programming is a general framework to specify these algorithms (Pereira and Warren, 1983; Shieber et al., 1994; Goodman, 1999; Eisner et al., 2005; Lopez, 2009). Goodman (1999) describes many useful semirings (e.g., Viterbi, inside, and Viterbi-n-best). "
"In this paper, we apply the expectation semiring (Eisner, 2002) to a hypergraph (or packed forest) rather than just a lattice. "
"We use a specific tree-based system called Hiero (Chiang, 2007) as an example, although the discussion is general for any systems that use a hypergraph to represent the hypothesis space. "
"Semiring parsing (Goodman, 1999) is a general framework to describe such algorithms. "
"For example, Eisner (2002) uses finite-state operations such as composition, which do combine weights entirely within the expectation semiring before their result is passed to the forward-backward algorithm."
"For example, in variational decoding for machine translation (Li et al., 2009b), p is a distribution represented by a hypergraph, while q, represented by a finite state automaton, is an approximation to p. "
Smith and Eisner (2006) instead propose a dif ferentiable objective that can be optimized by gradient descent: the Bayes risk R(p) of (7). 
"We built a translation model on a corpus for IWSLT 2005 Chinese-to-English translation task (Eck and Hori, 2005), which consists of 40k pairs of sentences. "
" entence-level combination methods directly select hypotheses from original outputs of single SMT systems (Sim et al., 2007; Hildebrand and  ogel, 2008), while phrase-level or word–level combination methods are more complicated and could produce new translations different from any translations in the input (Bangalore et al., 2001; Jayaraman and Lavie, 2005; Matusov et al., 2006; Sim et al.2007). "
"Although various approaches to SMT system combination have been explored, including enhanced combination model structure (Rosti et al., 2007), better word alignment between translations (Ayan et al., 2008; He et al., 2008) and improved confusion network construction (Rosti et al.2008), most previous work simply used the ensemble of SMT systems based on different models and paradigms at hand and did not tackle the issue of how to obtain the ensemble in a principled way. "
"Lopez and Resnik (2006) also showed that feature engineering could be used to overcome deficiencies of poor alignment. To illustrate the usefulness of feature subspace in the SMT task, we start with the example shown in Table 1. In the example, the Chinese source sentence is translated with two settings of a hierarchical phrase-based system (Chiang, 2005)."
"This method can also be viewed to be a hypotheses reranking model since we only use the existing translations instead of performing decoding over a confusion network as done in the word-level combination method (Rosti et al., 2007). "
"Parameters were tuned with MERT algorithm (Och, 2003) on the NIST evaluation set of 2003 (MT03) for both the baseline systems and the system combination model.  "
"In our experiments, two in-house developed systems are used to validate our method. The first one (SYS1) is a system based on the hierarchical phrase-based model as proposed in (Chiang, 2005). "
The second one (SYS2) is a reimplementation of a phrase-based decoder with lexicalized reordering model based on maximum entropy principle proposed by Xiong et al. (2006). 
"The reason for the small optimal n-best list size could be that the low-rank hypotheses might introduce more noises into the combined translation candidate pool for sentence-level combination (Hasan et al., 2007; Hildebrand and Vogel, 2008) "
"Word-sense disambiguation, a problemthat once seemed out of reach for systems without a great deal of handcrafted lin-guistic and world knowledge, can now in some cases be done with high accuracywhen all information is derived automatically from corpora (Brown, Lai, and Mercer1991; Yarowsky 1992; Gale, Church, and Yarowsky 1992; Bruce and Wiebe 1994)."
"This algorithm has been applied to anumber of natural language problems, including part-of-speech tagging, prepositionalphrase attachment disambiguation, and syntactic parsing (Brill 1992; Brill 1993a; Brill1993b; Brill and Resnik 1994; Brill 1994)."
The technique employed by the learner is somewhat similar to that used in decisiontrees (Breiman et al. 1984; Quinlan 1986; Quinlan and Rivest 1989).
"When automated part-of-speech tagging was initially explored (Klein and Sim-mons 1963; Harris 1962), people manually engineered rules for tagging, sometimeswith the aid of a corpus."
"As large corpora became available, it became clear that simpleMarkov-model based stochastic taggers that were automatically trained could achievehigh rates of tagging accuracy (Jelinek 1985)."
Almost all recent work in developingautomatically trained part-of-speech taggers has been on further exploring Markov-model based tagging (Jelinek 1985; Church 1988; Derose 1988; DeMarcken 1990; Meri-aldo 1994; Cutting et al. 1992; Kupiec 1992; Charniak et al. 1993; Weischedel et al. 1993;Schutze and Singer 1994).
"we list the first twenty transformations learned from training on thePenn Treebank Wall Street Journal Corpus (Marcus, Santorini, and Marcinkiewicz1993)."
"In Weischedel et al. (1993), results are given when training and testing a Markov-model based tagger on the Penn Treebank Tagged Wall Street Journal Corpus."
"In Weischedel et al. (1993), a statistical approach to tagging unknown words isShown."
"In DeMarcken (1990) and Weischedel et al. (1993), k-best tags are assignedwithin a stochastic tagger by returning all tags within some threshold of probabilityof being correct for a particular word."
"This learning approach has also been applied to a numberof other tasks, including prepositional phrase attachment disambiguation (Brill andResnik 1994), bracketing text (Brill 1993a) and labeling nonterminal nodes (Brill 1993c)."
"The last years have seen a boost of work devoted to the development of machine learning based coreference resolution systems (Soon et al., 2001; Ng & Cardie, 2002; Kehler et al., 2004, inter alia). Similarly, many researchers have explored techniques for robust, broad coverage semantic parsing in terms of semantic role labeling (Gildea & Jurafsky, 2002; Carreras & Màrquez, 2005, SRL Henceforth). "
"On the other hand, the literature emphasizes since the very beginning the relevance of world knowledge and inference (Charniak, 1973)."
"This layer of semantic context abstracts from the specific lexical expressions used, and therefore represents a higher level of abstraction than predicate argument statistics (Kehler et al., 2004) and Latent Semantic Analysis used as a model of world knowledge (Klebanov & Wiemer-Hastings, 2002)."
"The system was initially prototyped using the MUC-6 and MUC-7 data sets (Chinchor & Sundheim, 2003; Chinchor, 2001), using the standard partitioning of 30 texts for training and 20-30 texts for testing. Then, we developed and tested the system with the ACE 2003 Training Data corpus (Mitchell et al., 2003) "
"In our experiments we use the ASSERT parser (Pradhan et al., 2004), an SVM based semantic role tagger which uses a full syntactic analysis to automatically identify all verb predicates in a sentence together with their semantic arguments, which are output as PropBank arguments (Palmer et al., 2005). "
"We report in the following tables the MUC score (Vilain et al., 1995). "
"A translation model consists of two distinct elements: an unweighted ruleset, and a parameterization (Lopez, 2008a; 2009)."
Germann et al. (2004) identify two types of translation system error: model error and search Error
A hierarchical model can translate discontiguous groups of words as a unit. A phrase-based model cannot. Lopez (2008b) gives indirect experimental evidence that this difference affects performance.
Forced translation was implemented by Schwartz (2008) who ensures that hypothesis are a prefix of the reference to be generated.
"Our hierarchical system is Hiero (Chiang, 2007), modified to construct rules from a small sample of occurrences of each source phrase in training as described by Lopez (2008b)."
"Ayan and Dorr (2006) showed that under certain conditions, this constraint could have significant impact on system performance. "
"We also extend the work of Zollmann et al. (2008) on Chinese-English, performing the analysis in both directions and providing a detailed qualitative explanation"
"Accounting for sparsity explicitly has achieved significant improvements in other areas such as in part of speech tagging (Goldwater and Griffiths, 2007)."
"In this writingsystem not all the vowels are represented, several letters represent both consonantsand different vowels, and gemination is not represented at all (Ornan 1986, 1991)."
"A much simpler problem occurs in English, where for some wordsthe correct syntactic tag is necessary for pronunciation (Church 1988)."
"This last result was in accordance with the previous acknowledgment (Callison-Burch et al., 2006) that systems of too differing structure could not be compared reliably with BLEU. "
"The basic setup is identical to the one described in (Dugast et al., 2007). "
"Weights for these separate models were tuned through the Mert algorithm provided in the Moses toolkit (Koehn et al., 2007), using the provided news tuning set. "
"In a statistical translation model, trimming of the phrase table had been shown to be beneficial (Johnson et al., 2007). "
"Deleted and spurious content is a well known problem for statistical models (Chiang et al., 2008). "
"We present and evaluate empirically statistical models for both mention detection and entity tracking problems. For mention detection we use approaches based on Maximum Entropy (MaxEnt henceforth) (Berger et al., 1996) and Robust Risk Minimization (RRM henceforth) (Zhang et al., 2002). "
"Good performance in many natural language processing tasks, such as part-of-speech tagging, shallow parsing and named entity recognition, has been shown to depend heavily on integrating many sources of information (Zhang et al., 2002; Jing et al., 2003; Ittycheriah et al., 2003). "
"The segmentation model is similar to the one presented by Lee et al. (2003), and obtains an accuracy of about 98%. "
"Simple as it seems, the mention-pair model has been shown to work well (Soon et al., 2001; Ng and Cardie, 2002). "
"Although we also use a mention-pair model, our tracking algorithm differs from Soon et al. (2001), Ng and Cardie (2002) in several aspects. "
"Third, this probabilistic framework allows us to search the space of all possible entities, while Soon et al. (2001), Ng and Cardie (2002) take the “best” local hypothesis. "
"To transform the problem into a classification task, we use the IOB2 classification scheme (Tjong Kim Sang and Veenstra, 1999).  "
"If we view MT as a machine learning problem, features and formalisms imply structural independence assumptions, which are in turn exploited by efficient inference algorithms, including decoders (Koehn et al., 2003; Yamada and Knight, 2001)."
"Hence a tension is visible in the many recent research efforts aiming to decode with “non-local” features (Chiang, 2007; Huang and Chiang, 2007)."
"A major difference between the phrase features used in this work and those used elsewhere is that we do not assume that phrases segment into disjoint parts of the source and target sentences (Koehn et al., 2003); they can overlap."
"Additionally, since phrase features can be any function of words and alignments, we permit features that consider phrase pairs in which a target word outside the target phrase aligns to a source word inside the source phrase, as well as phrase pairs with gaps (Chiang, 2005; Ittycheriah and Roukos, 2007)."
"For example, Quirk et al. (2005) use features involving phrases and source-side dependency trees and Mi et al. (2008) use features from a forest of parses of the source sentence."
"In phrase-based systems, reordering is accomplished both within phrase pairs (local reordering) as well as through distance-based distortion models (Koehn et al., 2003) and lexicalized reordering models (Koehn et al., 2007)."
"This is a problem with other direct translation models, such as IBM model 1 used as a direct model rather than a channel model (Brown et al., 1993). "
"Optimization problems of this form are by now widely known in NLP (Koo and Collins, 2005), and have recently been used for machine translation as well (Blunsom et al., 2008)."
"The summation over target word sequences and alignments given fixed t bears a resemblance to the inside algorithm, except that the tree structure is fixed (Pereira and Schabes, 1992)."
"Our approach permits an alternative to minimum error rate training (MERT; Och, 2003); it is discriminative but handles latent structure and regularization in more principled ways."
"We evaluate translation output using case-insensitive BLEU (Papineni et al., 2001), as provided by NIST, and METEOR (Banerjee and Lavie, 2005), version 0.6, with Porter stemming and WordNet synonym matching."
"These probabilities are estimated on the training corpus parsed using the Stanford factored parser (Klein and Manning, 2003)."
"[Halliday & Hasan, 1976] offer a clear definition for text cohesion"
"Using multiple documents to generate a summary further complicates the situation. As contended by [Goldstein et al, 2000] a multi-document summary may contain redundant messages, since a cluster of news articles tends to cover the same main point and shared background. "
"RST can be used in sentence selection for single document summarization [Marcu, 1997]. However, it cannot be applied to MDS. "
"To contrast, [Harabagiu, 1999] concentrated on the derivation of a model that can establish coherence relations in a text without relying on cue phrases. "
"[Hovy, 1993] summarized previous work that focused on the automated planning and generation of multi-sentence texts using discourse relationships "
"The systems discussed in [Hovy, 1993] relied on a knowledge base and a representation of discourse Structure. "
"[Barzilay et al, 2001] evaluated three algorithms for sentence ordering in multi-document Summaries. "
"Here, a CST-enhancement procedure [Zhang et al, 2002] may take place, ensuring that interdependent sentences appear together in a summary. "
The MEAD summarizer [Radev et al 2002] is based on sentence extraction and uses a linear combination of three features to rank the sentences in the source documents. 
"Mani (2001) defines an abstract as “a summary at least some of whose material is not present in the input”. In a study of professional abstracting, Endres-Niggemeyer (2000) concluded that professional abstractors produce abstracts by “cut-and-paste” operations, and that standard sentence patterns are used in their production. "
Montesi and Owen (2007) observe that the revision of abstracts is carried out to improve comprehensibility and style and to make the abstract Objective.
"Montesi and Owen (2007) noted that professional abstractors prepend third person  ingular verbs in present tense and without subject to the author abstract, a phenomenon related – yet different – from the problem we are investigating in this paper. "
" Abstractive techniques in text summarization include sentence compression (Cohn and Lapata, 2008), headline generation (Soricut and Marcu, 2007), and canned-based generation (Oakes and Paice, 2001). Close to the problem studied here is Jing and McKeown’s (Jing and McKeown, 2000) cut-and-paste method founded on Endres- Niggemeyer’s observations. "
"Each electronic version of the abstract was processed using the freely available GATE text analysis software (Cunningham et al., 2002)."
"Where the classification algorithm is concerned, we have decided to use Support Vector Machines which have recently been used in different tasks in natural language processing, they have been shown particularly suitable for text categorization (Joachims, 1998). "
"We have tried other machine learning algorithms such as Decision Trees, Naive Bayes Classification, and Nearest Neighbor from the Weka toolkit (Witten and Frank, 1999), but the support vector machines gave us the best classification accuracy  "
"Cohesion information has been used in rhetorical-based parsing for summarization (Marcu, 1997) in order to decide between “list” or “elaboration” relations and also in content selection for summarization (Barzilay and Elhadad, 1997)."
"Predicates such as “to present” and “to include” have the tendency of appearing towards the very beginning or the very end of the abstract been therefore predicted by position-based features (Edmundson, 1969; Lin and Hovy, 1997). "
Liddy (1991) produced a formal model of the informational or conceptual structure of abstracts of empirical research. 
"Related to our classification experiments is work on semantic or rhetorical classification of “structured” abstracts (Saggion, 2008) from the MEDLINE abstracting database where similar features to those presented here were used to identify in abstracts semantic categories such as objective, method, results, and Conclusions. "
"This contrasts with semantic role labeling (Carreras and Marquez, 2004) and other forms of shallow semantic processing, which do not aim to produce complete formal meanings. "
"Unsupervised approaches have been applied to shallow semantic tasks (e.g., paraphrasing (Lin and Pantel, 2001), information extraction (Banko et al., 2007)), but not to semantic parsing. "
"This in turn allows it to correctly answer many more questions than systems based on TextRunner (Banko et al., 2007) and DIRT (Lin and Pantel, 2001)."
"In the approach of Zettle moyer and Collins (2005), the training data consists of sentences paired with their meanings in lambda form."
"For example, DIRT (Lin and Pantel, 2001) learns paraphrases of binary relations based on distributional similarity of their arguments; TextRunner (Banko et al., 2007) automatically extracts relational triples in open domains using a self-trained extractor; SNE applies relational clustering to generate a semantic network from TextRunner triples (Kok and Domingos, 2008). "
"In general, this is a difficult open problem that only recently has started to receive some attention (Mohammad et al., 2008). Resolving this is not the focus of this paper, but we describe a general heuristic for fixing this problem. "
"We omit the proof here but point out that it is related to the unordered subtree matching problem which can be solved in linear time (Kilpelainen, 1992)."
"A serious challenge in unsupervised learning is the identifiability problem (i.e., the optimal parameters are not unique) (Liang and Klein, 2008). This problem is particularly severe for log-linear models with hard constraints, which are common in MLNs. "
"We built a system for knowledge extraction and question answering on top of USP. It generated Stanford dependencies (de Marneffe et al., 2006) from the input text using the Stanford parser, and then fed these to USP-Learn 11 , which produced an MLN with learned weights and the MAP semantic parses of the input sentences."
SparckJones and Endres-Niggemeyer (1995) stated the need for a research program in text
Rowley (1982)proposes the following typology of different types of document condensations
"In our research, we are concerned only with summaries of technical articles, whichare called abstracts. In this context, two main types of abstracts are considered (ANSI1979; ERIC 1980; Maizell, Smith, and Singer 1971)"
This methodological process was established af-ter studying procedures for abstract writing (Cremmins 1982; Rowley 1982) and someinitial observations from our corpus.
"Kupiec, Ped-ersen, and Chen (1995) report on the semiautomatic alignment of 79 of sentences ofprofessional abstracts in a corpus of 188 documents with professional abstracts."
"Teufel and Moens (1998) report on a similar work, but this time onthe alignment of sentences from author-provided abstracts."
Sharp (1989) reports onexperiments carried out with abstractors in which it is shown that introductions andconclusions provide a basis for producing a coherent and informative abstract.
"According to Cremmins (1982), the last step in the human production of the sum-mary text is the “extracting” into “abstracting” step in which the extracted informa-tion will be mentally sorted into a preestablished format and will be “edited” usingcognitive techniques."
"Bernier (1985) states that redundancy, repetition, andcircumlocutions are to be avoided. He gives a list of linguistic expressions that can besafely removed from extracted sentences or reexpressed in order to gain conciseness."
"Mathis and Rush (1985) indicate thatsome transformations in the source material are allowed, such as concatenation, trun-cation, phrase deletion, voice transformation, paraphrase, division, and word deletion.Rowley (1982) mentions the inclusion of the lead or topical sentence and the use ofactive voice and advocates conciseness."
This study was motivated by the need to answer to the question of contentselection in text summarization (Sparck Jones 1993).
Jing and McKeown (2000) and Jing (2000)propose a cut-and-paste strategy as a computational process of automatic abstractingand a sentence reduction strategy to produce concise sentences.
Knight and Marcu (2000) propose a noisy-channel model and adecision-based model for sentence reduction also aiming at conciseness
The sources of information we use for implementing our system are a POS tag-ger (Foster 1991)
"Other techniques exist for boosting thescore of longer phrases, such as adjusting the score of the phrase by a fixed factor thatdepends on the length of the phrase (Turney 1999)."
"Our approach is based on the empirical examination of abstracts published by sec-ond services and on assumptions about technical text organization (Paice 1991; Bhatia1993; Jordan 1993, 1996)."
Theproblem of anaphoric expressions in technical articles has been extensively addressedin research work carried out under the British Library Automatic Abstracting Project(BLAB) (Johnson et al. 1993; Paice et al. 1994).
"The quality of human-produced abstractshas been examined in the literature (Grant 1992; Kaplan et al. 1994; Gibson 1993),using linguistic criteria such as cohesion and coherence, thematic structure, sentencestructure, and lexical density; in automatic text summarization, however, such detailedanalysis is only just emerging."
The eval-uations can be made in intrinsic or extrinsic fashions as defined by Sparck Jones andGalliers (1995).
Variables measured canbe the number of correct answers and the time to complete the task. Recent experi-ments (Jing et al. 1998) have shown how different parameters such as the length ofthe abstract can affect the outcome of the evaluation.
This method of evaluation has already been used in other summarizationevaluations such as Edmundson (1969) and Marcu (1997).
Ad-ditional evaluations of SumUM using sentence acceptability criteria and content-basedmeasures of indicativeness have been presented in Saggion and Lapalme (2000b) andSaggion (2000).
"Our implementation of patterns for information extraction is similar to Black’s(1990) implementation of Paice’s (1981) indicative phrases method, but whereas Blackscores sentences based on indicative phrases contained in the sentences, our methodscores the information from the sentences based on term distribution."
"Semantic analysis is an open research field in natural language processing. Two major research topics in this field are Named Entity Recognition (NER) (N. Wacholder and Choi, 1997; Cucerzan and Yarowsky, 1999) and Word Sense Disambiguation (WSD) (Yarowsky, 1995; Wilks and Steven Son, 1999)."
"A major issue in MaxEnt training is how to select proper features and determine the feature targets (Berger et al., 1996; Jebara and Jaakkola, 2000)."
"The task of sentence compression (or sentence reduction) can be defined as summarizing a single sentence by removing information from it (Jing and McKeown, 2000). "
"One of the applications is in automatic summarization in order to com- press sentences extracted for the summary (Lin, 2003; Jing and McKeown, 2000). Other applications include automatic subtitling (Vandeghinste and Tsjong Kim Sang, 2004; Vandeghinste and Pan, 2004; Daelemans et al., 2004) and displaying text on devices with very small screens (Corston Oliver, 2001). "
"A more restricted version defines sentence compression as dropping any subset of words from the input sentence while retaining important information and grammaticality (Knight and Marcu, 2002). "
"This formulation of the task provided the basis for the noisy-channel en decision tree based algorithms presented in (Knight and Marcu, 2002), and for virtually all follow-up work on data-driven sentence compression (Le and Horiguchi, 2003; Vandeghinste and Pan, 2004; Turner and Charniak, 2005; Clarke and Lapata, 2006; Zajic et al., 2007; Clarke and Lapata, 2008)  "
"Subtitles can be presented at a rate of 690 to 780 characters per minute, while the average speech rate is considerably higher (Vandeghinste and Tsjong Kim Sang, 2004). "
"It was originally collected and processed in two earlier research projects Atranos and Musa – on automatic subtitling (Van- deghinste and Tsjong Kim Sang, 2004; Vandegh- inste and Pan, 2004; Daelemans et al., 2004).  "
"Pairs of similar syntactic nodes – either words or phrases – were aligned and labeled according to a set of five semantic similarity relations (Marsi and Krahmer, 2007). "
"More powerful compression models may draw on existing NLG methods for text revision (Inui et al., 1992) to accommodate full Paraphrasing. "
"First, splitting and merging of sentences (Jing and McKeown, 2000), which seems related to content planning and aggregation. "
"On the other hand, redundancy can be exploited to identify important and accurateinformation for applications such as summarization and question answering (Maniand Bloedorn 1997 Radev and McKeown 1998 Radev, Prager and Samn 2000 ClarkeCormack and Lynam 2001 Dumais et al. 2002 Chu-Carroll et al. 2003)."
"A straightforward approach for approximating sentence fusion can be found in theuse of sentence extraction for multidocument summarization (Carbonell and Goldstein1998; Radev, Jing, and Budzikowska 2000; Marcu and Gerber 2001; Lin and Hovy2002)."
"Evalua-tion involving human judges revealed that Simfinder identifies similar sentences with49.3 precision at 52.9 recall (Hatzivassiloglou, Klavans, and Eskin 1999)."
"To identify themes, Simfinder extracts linguistically motivated features for eachsentence, including WordNet synsets (Miller et al. 1990) and syntactic dependencies,such as subject–verb and verb–object relations."
"The first two of these scores are produced by Simfinder, and the salience score iscomputed using lexical chains (Morris and Hirst 1991; Barzilay and Elhadad 1997) asdescribed below."
Lexical chains—sequences of semantically related words—are tightly connected tothe lexical cohesive structure of the text and have been shown to be useful for determin-ing which sentences are important for single-document summarization (Barzilay andElhadad 1997; Silber and McCoy 2002).
"To increase the coherence of the output text, we identify blocks of topicallyrelated themes and then apply chronological ordering on blocks of themes using themetime stamps (Barzilay, Elhadad, and McKeown 2002)."
A representation which fits these requirements is adependency-based representation (Melcuk 1988).
"In fact, we have developed a rule-basedcomponent that transforms the phrase structure output of Collins’s (2003) parser intoa representation in which a node has a direct link to its dependents."
"Our manual analysis of paraphrased sen-tences (Barzilay 2003) revealed that such alignments most frequently occur in pairs ofnoun phrases (e.g., faculty member and professor) and pairs including verbs with parti-cles (e.g., stand up, rise)."
Weautomatically constructed the paraphrasing dictionary from a large comparable newscorpus using the co-training method described in Barzilay and McKeown (2001).
"As previously observedin the literature (Mani, Gates, and Bloedorn 1999; Jing and McKeown 2000), such com-ponents include a clause in the clause conjunction, relative clauses, and some ele-ments within a clause (such as adverbs and prepositions)."
"While the ordering of many sentenceconstituents is determined by their syntactic roles, some constituents, such as time,location and manner circumstantials, are free to move (Elhadad et al. 2001)."
We trained a trigram model with Good–Turing smoothingover 60 megabytes of news articles collected by Newsblaster using the second versionCMU–Cambridge Statistical Language Modeling toolkit (Clarkson and Rosenfeld 1997).
"In the previous version of thesystem (Barzilay, McKeown, and Elhadad 1999), we performed linearization of afusion dependency structure using the language generator FUF/SURGE (Elhadadand Robin 1996)."
"In our previous work, we evaluated the overall summarization strategy of MultiGenin multiple experiments, including comparisons with human-written summaries inthe Document Understanding Conference (DUC) 11 evaluation (McKeown et al. 2001;McKeown et al. 2002) and quality assessment in the context of a particular informa-tion access task in the Newsblaster framework (McKeown et al. 2002)"
"From theoverlap data, we computed weighted recall and precision based on fractional count(Hatzivassiloglou and McKeown 1993)."
"In addition to sentence fusion, compressionalgorithms (Chandrasekar, Doran, and Bangalore 1996; Grefenstette 1998; Mani, Gates,and Bloedorn 1999; Knight and Marcu 2002; Jing and McKeown 2000; Reizler et al. 2003)and methods for expansion of a multiparallel corpus (Pang, Knight, and Marcu 2003)are other instances of such methods."
"While earlier approaches for text compression werebased on symbolic reduction rules (Grefenstette 1998; Mani, Gates, and Bloedorn 1999),more recent approaches use an aligned corpus of documents and their human writtensummaries to determine which constituents can be reduced (Knight and Marcu 2002;Jing and McKeown 2000; Reizler et al. 2003)."
Knight and Marcu (2000) treat reduction as a translation process using a noisy-channel model (Brown et al. 1993).
"The alignment method described in Section 3 falls into a class of tree comparisonalgorithms extensively studied in theoretical computer science (Sankoff 1975; Findenand Gordon 1985; Amir and Keselman 1994; Farach, Przytycka, and Thorup 1995)and widely applied in many areas of computer science, primarily computational bi-ology (Gusfield 1997)."
"In the NLP context, this class of algorithms has been used previously in example-based machine translation, in which the goal is to find an optimal alignment betweenthe source and the target sentences (Meyers, Yangarber, and Grishman 1996)."
"Forexample, all summary sentences may contain the full description of a named entity(e.g., President of Columbia University Lee Bollinger), while the use of shorter descriptionssuch as Bollinger or anaphoric expressions in some summary sentences would in-crease the summary’s readability (Schiffman, Nenkova, and McKeown 2002; Nenkovaand McKeown 2003)."
"The application of the word segmenter is described elsewhere (Nagata, 1996)."
"We then describe the word segmentation algorithm and the new word extraction method, with their derivation as an approximation of a generalization of the Forward-Backward algorithm (Baum, 1972).  "
"The underlying idea of the replacement is the same as Turing's estimates in back-off smoothing (Katz, 1987). "
"For Chinese, (Sproat et al., 1994) used the word unigram model in their word segmenter based on weighted finite-state transducer.  "
"Our translation system makes use of a hierarchical phrase-based translation model (Chiang, 2007), which we argue is a strong baseline for these language pairs."
"First, such a system makes useof lexical information when modeling reordering (Lopez, 2008), which has previously been shown to be useful in German-to- nglish translation (Koehn et al., 2008)."
The rule feature values were computed online during decoding using the suffix array method described by Lopez (2007).
"Since the official evaluation criterion for WMT09 is human sentence ranking, we chose to minimize a linear combination of two common evaluation metrics, BLEU and TER (Papineni et al., 2002; Snover et al., 2006), during system development and tuning "
"Doing so enables us to use possibly inaccurate approaches to guess the segmentation of compound words, allowing the decoder to decide which to use during translation. This is a further development of our general source-lattice approach to decoding (Dyer et al., 2008)."
"To tune the model parameters, we selected a set of compound words from a subset of the German development set, manually created a linguistically plausible segmentation of these words, and used this to select the parameters of the log-linear model using a lattice minimum error training algorithm to minimize WER (Macherey et al., 2008)."
"For the test data, we created a lattice of every possible segmentation of any word 6 characters or longer and used forward-backward pruning to prune out low probability segmentation paths (Sixtus and Ortmanns, 1999)."
"Although during minimum error training we assume a decoder that uses the maximum derivation decision rule, we find benefits to translating using a minimum risk decision rule on a test set (Kumar and Byrne, 2004)."
"The emerging technology of information extraction (Appelt and Israel 1997, Hearst 1999) provides a means of gaining access to this information."
"Considerable interest in information extraction has concentrated on identifying named entities in text pertaining to current events (for example, Wacholder et al. 1997, Voorhees and Harman 1998, and MUC-7); however, several recent efforts have been directed at biomolecular data (Blaschke et al. 1999, Craven and Kumlien 1999, and Rindflesch et al. 2000)"
"The SPECIALIST minimal commitment parser relies on the SPECIALIST Lexicon as well as the Xerox stochastic tagger (Cutting et al. 1992). The output produced is in the tradition of partial parsing (Hindle 1983, McDonald 1992, Weischedel et al. 1993) and concentrates on the simple noun phrase, what Weischedel et al. (1993)"
"Several approaches provide similar output based on statistics (Church 1988, Zhai 1997, for example), a finite-state machine (AitMokhtar and Chanod 1997), or a hybrid approach combining statistics and linguistic rules (Voutilainen and Padro 1997)."
"The SPECIALIST parser is based on the notion of barrier words (Tersmette et al. 1988), which indicate boundaries between phrases. "
"As the first step in the process, an existing program, MetaMap, (Aronson et al. 1994) attempts to map each simple noun phrase to a concept in the UMLS Metathesaurus."
In this context such a word is often an acronym not defined locally and indicates the presence of a binding term (Fukuda et al. 1998).
"Rindflesch et al. (1999) use the term ""macro-noun phrase"" to refer to structures that include reduced relative clauses (commonly introduced by prepositions or participles) as well as appositives."
"For English, we obtained results comparable with the results presented in (Merialdo, 1992) as well as in (Church, 1992). "
"Given the comparability of the accuracy of the rule-based part-of-speech (POS) tagger (Brill, 1992) with the accuracy of the stochastic tagger and given the fact that a rule-based POS tagger has never been used for a Slavic language we have tried to apply rule-based methods even for Czech. "
"Note especially, that Czech nouns are divided into four classes according to gender (Sgall, 1967) and into seven classes according to ease. "
"We have used the basic source channel model (de-scribed e.g. in (Merialdo, 1992))."
"(Schiller, 1996) describes the general architecture of the tool for noun phrase mark-up based on finitestate techniques and statistical part-of-peech disambiguation for seven European languages. "
The first step is the extraction of important concepts from the source text by building an intermediate representation of some sort. The second step uses this intermediate representation to generate a summary (Sparck Jones 1993). 
"In the research presented here, we concentrate on the first step of the summarization process and follow Barzilay and Elhadad (1997) in employing lexical chains to extract important concepts from a document. "
"The concept of lexical chains was first introduced by Morris and Hirst. Basically, lexical chains exploit the cohesion among an arbitrary number of related words (Morris and Hirst 1991). "
"Considering the size of most documents, the linear nature of this algorithm makes it usable for generalized summarization of large documents (Silber and McCoy 2000). "
"Our method affords a considerable speedup for these smaller documents. For instance, a document that takes 300 seconds using Barzilay and Elhadad’s method takes only 4 seconds using ours (Silber and McCoy 2000). "
"The first, scientific documents with abstracts, represents a readily available class of summaries often discussed in the literature (Marcu 1999).  "
"The ITC-irst system (Chen et al., 2005) is based on a log-linear model which extends the original IBM Model 4 (Brown et al., 1993) to phrases (Koehn et al., 2003; Federico and Bertoldi, 2005). "
"While feature functions exploit statistics extracted from monolingual or word-aligned texts from the training data, the scaling factors λ of the log-linear model arestimated on the development data by applying a minimum error training procedure (Och, 2004). "
"Hence, either the best translation hypothesis is directly extracted from the word graph and output, or an N-best list of translations is computed (Tran et al., 1996). "
"Starting from the parallel training corpus, provided with direct and inverted alignments, the so called union alignment (Och and Ney, 2003) is Computed. "
"Target language models (LMs) used by the decoder and rescoring modules are, respectively, estimated from 3-gram and 4-gram statistics by applying the modified Kneser-Ney smoothing method (Goodman and Chen, 1998). "
"The Arabic-to-English system has been trained with the data provided by the International Work- shop on Spoken Language Translation 2005 The context is that of the Basic Traveling Expression Corpus (BTEC) task (Takezawa et al., 2002). "
"Phrase-based method (Koehn et al., 2003; Och and Ney, 2004; Koehn et al., 2007) and syntax- based method (Wu, 1997; Yamada and Knight, 2001; Eisner, 2003; Chiang, 2005; Cowan et al., 2006; Marcu et al., 2006; Liu et al., 2007; Zhang et al., 2007c, 2008a, 2008b; Shen et al., 2008; Mi and Huang, 2008) represent the state-of-the-art technologies in statistical machine translation (SMT). "
"In this paper, we bring forward the first idea by studying the issue of how to utilize structured syntactic features for phrase reordering in a phrase-based SMT system with BTG (Bracketing Transduction Grammar) constraints (Wu, 1997).  "
"In syntax-based method, word reordering is implicitly addressed by translation rules, thus the performance is subject to parsing errors to a large extent (zhang et al 2007a) and the impact of syntax on reordering is difficult to single out (Li et al., 2007)."
"This makes the phrase based method particularly weak in handling global phrase reordering. From machine learning viewpoint (Vapnik, 1995), it is computationally infeasible to explicitly generate features involving structured information in many NLP applications. "
"Indeed, using tree kernel methods to mine structured knowledge has shown success in some NLP applications like parsing (Collins and Duffy,2001), semantic role labeling (Moschitti, 2004 Zhang et al., 2007b), relation extraction (Zhang et al., 2006), pronoun resolution (Yang et al.,2006) and question classification (Zhang and Lee, 2003). "
"In this paper, phrase reordering is recast as a classification issue as done in previous work (Xiong et al., 2006 & 2008; Zhang et al., 2007a). "
"In total, we arrive at 13 features, including 8 boundary word features, 4 (kinds of) internal word features and 1 LM feature. The first 12 features have been proven useful (Xiong et al., 2006; Zhang et al., 2007a) to phrase reordering. "
"In convolution tree kernel (Collins and Duffy, 2001), a parse tree T is implicitly represented by a vector of integer counts of each sub-tree type (regardless of its Ancestors)"
"The composite kernel Kcis a linear combination of the two individual kernels, where the coefficient α is set to its default value 0.3 as that in Moschitti (2004)’s implementation. "
"The Stanford parser (Klein and Manning, 2003) is used to parse Chinese sentences on the training, dev and test sets."
"Besides the the case-sensitive BLEU-4 (Papineni et al., 2002) used in the two experiments, we design another evaluation metrics Reordering Accuracy (RAcc) for forced decoding evaluation. "
"However, while researchers have shown that it is sometimes possible to annotate corpora that capture features of interpretation, to provide empirical support for theories, as in (Eugenio et al., 2000), or to build classifiers that assist in dialogue reasoning, as in (Jordan and Walker, 2005), it is rarely feasible to fully annotate the interpretations themselves. "
"Our specific approach is based on contribution tracking (DeVault, 2008), a framework which casts linguistic inference in situated, task-oriented dialogue in probabilistic terms. "
"As in the experiments of Clark and Wilkes- Gibbs (1986) and Brennan and Clark (1996), one of the players, who plays the role of director, instructs the other player, who plays the role of matcher, which object is to be added next to the Scene. "
"COREF treats interpretation broadly as a problem of abductive intention recognition (Hobbs et al., 1993). "
Interpretations are constructed abductively in that the initial actions in the sequence need not be directly tied to observable events; they may be tacit in the terminology of Thomason et al. (2006). 
"When a dialogue act is preceded by tacit actions in an interpretation, the speaker of the utterance implicates that the earlier tacit actions have taken place (DeVault, 2008)."
"We chose to train maximum entropy models (Berger et al., 1996). Our learning framework is described in Section 4.1; the results in Section 4.2  "
"We used the MALLET maximum entropy classifier (McCallum, 2002) as an off-the-shelf, trainable maximum entropy model. Each run involved two steps."
"Our work adds to a body of research learning deep models of language from evidence implicit in an agent’s interactions with its environment. It shares much of its motivation with co-training (Blum and Mitchell, 1998) in improving initial models by leveraging additional data that is easy to obtain. "
"Closer in spirit is AI research on learning vocabulary items by connecting user vocabulary to the agent’s perceptual representations at the time  of utterance (Oates et al., 2000; Roy and Pentland, 2002; Cohen et al., 2002; Yu and Ballard, 2004; Steels and Belpaeme, 2005)."
"In general, dialogue coherence is an important source of evidence for all aspects of language, for both human language learning (Saxton et al., 2005) as well as machine models. For example, Bohus et al. (2008) use users’ confirmations of their spoken requests in a multi-modal interface to tune the system’s ASR rankings for recognizing subsequent utterances. "
"Thus, even when spoken language interfaces use probabilistic inference for dialogue management (Williams and Young, 2007), new techniques may be needed to mine their experience for correct interpretations."
"Word alignment is a critical component in training statistical machine translation systems and has received a significant amount of research, for example, (Brown et al., 1993; Ittycheriah and Roukos, 2005; Fraser and Marcu, 2007), including work leveraging syntactic parse trees, e.g., (Cherry and Lin, 2006; DeNero and Klein, 2007; Fossum et al., 2008)."
"Word alignment is also a required first step in other algorithms such as for learning sub-sentential phrase pairs (Lavie et al., 2008) or the generation of parallel treebanks (Zhechev and Way, 2002)."
"We finally also include as alignment candidates those word pairs that are transliterations of each other to cover rare proper names (Hermjakob et al., 2008), which is important for language pairs that don’t share the same alphabet such as Arabic and English"
"The test set includes only sentences for which our English parser (Soricut and Marcu, 2003) could produce a parse tree, which effectively excluded a few very long Sentences."
"In the first set of experiments, we compare two settings of our UALIGN system with other aligners, GIZA++ (Union) (Och and Ney, 2003) and LEAF (with 2 iterations) (Fraser and Marcu, 2007). The GIZA++ aligner is based on IBM Model 4 (Brown et al., 1993)."
"These corpus-based models can be represented e.g. as collocational matrices (Garside et al. (eds.) 1987: Church 1988), Hidden Markov models (cf. Cutting et al. 1992), local rules (e.g. Hindle 1989) and neural networks (e.g. Schmid 1994).  "
"One doubt concerns the notion 'correct analysis"". For example Church (1992) argues that linguists who manually perform the tagging task using the double blind method disagree about the correct analysis in at least 3% of all words even after they have negotiated about the initial disagreements. "
"However, ""Vbutilainen (1995) has shown that EngCG combined with a syntactic parser produces morphologically unambiguous output with an accuracy of 99.3%, a figure clearly better than that of the statistical tagger in the experiments below (however. the test data was not the same). "
"The stochastic tagger was trained on a sample of 357,000 words from the Brown University Corpus of Present-Day English (Francis and Kucera 1982) that was annotated using the EngCG tags. "
"The tag N-gram probabilities, and both the scheme and its application to these two tasks are described in detail in (Samuelsson 1996), where it was also shown to compare favourably to (deleted) interpolation, see (Jelinek and Mercer 1980), even when the back-off weights of the latter were optimal. "
"Generally speaking, a joint system is slower than a pipeline system in training. (Xue and Palmer, 2004) found out that different features suited for different sub-tasks of SRL, i.e. argument identification and classification. "
"Though aiming at Chinese SRL, (Xue, 2006) reported that their experiments show that simply adding the verb data to the training set of NomBank and extracting the same features from the verb and noun instances will hurt the overall performance. "
"From the results of CoNLL-2008 shared task, the top system by (Johansson and Nugues, 2008) also used two different subsystems to handle verbal and nominal predicates, respectively."
"A word-pair classification is used to formulate semantic dependency parsing as in (Zhao and Kit, 2008). "
"Note that this pruning algorithm is slightly different from that of (Xue and Palmer, 2004), the predicate itself is also included in the argument candidate list as the nominal predicate sometimes takes itself as its argument. "
"The concept of support verb was broadly used (Toutanova et al., 2005; Xue, 2006; Jiang and Ng, 2006) 4 , we here extend it to nouns and prepositions. "
"As an optimal feature template subset cannot be expected to be extracted from so large a set by hand, a greedy feature selection similar to that in (Jiang and Ng, 2006; Ding and Chang, 2008) is applied."
"Though the time complexity of the algorithm given by (Jiang and Ng, 2006) is also linear, it should assume all feature templates in the initial selected set ‘good’ enough and handles other feature template candidates in a strict incremental way. "
Chen et al. (2008) used features derived from short dependency pairs based on large-scale auto-parsed data to enhance dependency parsing.
Koo et al. (2008) presentednew features based on word clusters obtained from large-scale unlabeled data and achieved large improvement for English and Czech. 
Nivre and McDonald (2008) presented an integrating method to provide additional information for graph-based and transition-based parsers. 
"They were chosen among the 20 participating systems either because they held better results (the first four participants) or because they used some joint learning techniques (Henderson et al., 2008). "
"The results of (Titov et al., 2009) that use the similar joint learning technique as (Henderson et al., 2008) are also Included . "
"Among the statistical approaches, the Maximum Entropy framework has a very strong position. Nevertheless, a recent independent comparison of 7 taggets (Zavrel and Daelemans, 1999) has shown that another approach even works better "
"Additionally, we present results of the tagger on the NEGRA corpus (Brants et al., 1999) and the Penn Treebank (Marcus et al., 1993). The Penn Treebank results reported here for the Markov model approach are at least equivalent to those reported for the Maximum Entropy approach in (Ratnaparkhi, 1996). For a comparison to other taggers, the reader is referred to (Zavrel and Daelemans, 1999) "
"A theoretical motivated argumentation uses the standard deviation of the m a x i m u m likelihood probabilities for the weights 0i (Samuelsson, 1993) This leaves room for interpretation."
"The processing time of the Viterbi algorithm (Rabiner, 1989) can be reduced by introducing a beam Search. "
"The German NEGRA corpus consists of 20,000 sentences (355,000 tokens) of newspaper texts (Frank-furter Rundschau) that are annotated with parts-of-speech and predicate-argument structures (Skut et al., 1997)."
"According to current tagger comparisons (van Halteren et al., 1998; Zavrel and Daelemans, 1999), and according to a comparsion of the results pre- sented here with those in (Ratnaparkhi, 1996), the Maximum Entropy framework seems to be the only other approach yielding comparable results to the one presented here. "
"The literature on word space models (Sahlgren, 2006) has focused on taxonomic similarity (synonyms, antonyms, co-hyponyms. . . ) and general association (e.g., finding topically related words), exploiting the idea that taxonomically or associated words will tend to occur in similar contexts, and thus share a vector of cooccurring words. "
"The literature on relational similarity, on the other hand, has focused on pairs of words, devising various methods to compare how similar the contexts in which target pairs appear are to the contexts of other pairs that instantiate a relation of interest (Turney, 2006; Pantel and Pennacchiotti, 2006). "
"Beyond these domains, purely corpus-based methods play an increasingly important role in modeling constraints on composition of words, in particular verbal selectional preferences – finding out that, say, children are more likely to eat than apples, whereas the latter are more likely to be eaten (Erk, 2007; Padó et al., 2007). "
"In all of our experiments, the same normalization method and classification algorithm is used with the default parameters: First, a TF-IDF feature weighting is applied to the cooccurrence matrix (Salton and Buckley, 1988). "
"For other pattern recognition related coding (e.g., cross validation, scaling, etc.) we made use of the Matlab PRTools (Duin, 2001).  "
"This task, introduced by Landauer and Dumais (1997), consists of 80 multiple choice questions in which a word is given as the stem and the correct choice is the word which has the closest meaning to that of the stem, among 4 candidates. "
"Linguists have long been interested in the semantic constraints that verbs impose on their arguments, a broad area that has also attracted computational modeling, with increasing interest in purely corpus-based methods (Erk, 2007; Padó et al., 2007). "
"As has been stressed at least since Chomsky’s early work (Chomsky, 1957), no matter how large a corpus is, if a phenomenon is productive there will always be new well-formed instances that are not in the corpus. "
"An end result of the project is ConceptNet 3, a large scale semantic network consisting of relations between concept pairs (Havasi et al., 2007).  "
"Intuitively, AUC is the probability that a randomly picked positive instance’s estimated posterior probability is higher than a randomly picked negative instance’s estimated posterior probability (Fawcett, 2006). "
"It is well-known that such distributions can represent meaning reasonably well, at least for meaning-comparison purposes (Landauer and Dumais, 1997)."
"Distributional information has uses beyond part of speech induction. For example, it is possible to augment a fixed syntactic or semantic taxonomy with such information to good effect (Hearst and Schütze, 1993). "
"System combination for machine translation (MT) has emerged as a powerful method of combining the strengths of multiple MT systems and achieving results which surpass those of each individual system (e.g. Bangalore, et. al., 2001, Matusov, et. al., 2006, Rosti, et. al.,2007a)."
"Confusion networks allow word-level system combination, which was shown to outperform sentence re-ranking methods and phrase-level combination (Rosti, et. al. 2007a)."
"The choice is performed to maximize a scoring function using a set of features and a log-linear model (Matusov, et. al 2006, Rosti, et al. 2007a)."
"Incremental alignment methods have been proposed to relax the independence assumption of pair-wise alignment (Rosti et al. 2008, Li et al. 2009). Such methods align hypotheses to a partially constructed CN in some order."
"Each backbone produces a separate CN and the decision of which CN to choose is taken at a later decoding stage, but this still restricts the possible orders and alignments greatly (Rosti et al. 2008, Matusov et al. 2008)."
"In the scenario that N-best lists are available from individual systems for combination, the weight of each hypothesis can be computed based on its rank in the N-best list (Rosti et. al. 2007a). "
The word posterior feature is the same as the one proposed by Rosti et. al. (2007a). i.e
The second feature we used is a bi-gram voting feature proposed by Zhao and He (2009)
"To compute scores for word pairs, we perform pair-wise hypothesis alignment using the indirect HMM (He et al. 2008) for every pair of input hypotheses."
Decoding is based on a beam search algorithm similar to that of the phrase-based MT decoder (Koehn 2004b).
"In order to prevent the garbage collection problem where many words align to a rare word at the other side (Moore, 2004), we further impose the limit that if one word is aligned to more than T words, these links are sorted by their alignment score and only the top T links are kept. "
"Results are reported in case insensitive BLEU score in percentages (Papineni et. al., 2002)."
"Rule ordering issue has been discussedby Voutilainen(1994), but he has recently indicated 1that insensitivity to rule ordering is not a propertyof their system (although Voutilainen(1995a) statesthat it is a very desirable property) but rather isachieved by extensively testing and tuning the rules."
"Onthe other hand, in languages like Turkish or Finnishwith very productive agglutinative morphology, itis possible to produce thousands of forms (or evenmillions (Hankamer, 1989)) from a given root wordand the kinds of ambiguities one observes are quitedifferent than what is observed in languages like En-Glish"
"Furthermore, Turkish allows very productive derivational processes and the information about the derivational structure of a word form is usually crucial for disambiguation (Oflazer and Tiir, 1996). "
"The preprocessor module also performs a number of additional functions such as grouping of lexicalizcd and non-lexicalized collocations, compound verbs, etc., (Ofiazer and Kurubz, 1994; Oflazer and Tiir, 1996). "
"This is very similar to Brill's use of contexts to induce transformation rules for his tagger (Brill, 1992; Brill, 1995), but instead of generating transformation rules from a training text, we gather statistics and apply them to parses in the text being disambiguated. "
"The proposed approach is also amenable to an efficient implementation by finite state transducers (Kaplan and Kay, 1994). "
"The larger P(cldi) a document di has, the more probably it will be categorized into category c. This is called the Probabilistic Ranking Principle (PRP) (Robertson, 1977). Several strategies can be used to assign categories to a document based on PRP (Lewis, 1992). "
"There are several ways to calculate P(c[d). Three representatives are (Robertson and Sparck Jones, 1976), (Kwok, 1990), and (Fuhr, 1989). "
"Term weighting for target documents would also be necessary for sophisticated information retrieval (Fuhr, 1989; Kwok, 1990). "
"Furthermore, Fuhr (1989) pointed out that transformation, as in Eq. (6), is not monotonic of P(cld ). It follows then, that C T does not satisfy the probabilistic ranking principle ( P R P ) any More. "
"In the following experiments we used this binary estimation method, but non binary estimates could be used as in (Fuhr, 1989). "
"In particular, because of problem 3, P(cld) would become an illegitimate value. In our experiments, as well as in Lewis' experiments (1992), P(cld ) ranges from 0 to more than 101°. "
"Lewis proposed the proportional assignment strategy based on the probabilistic ranking principle (Lewis, 1992). "
"The best known measures for evaluating text categorization models are recall and precision, calculated by the following equations (Lewis, 1992): "
The superiority of proportional assignment over the other strategies has already been reported by Lewis (1992). 
"Thus, most of recent works in this research area are based on extraction (Goldstein et al., 1999). "
"Subsequent works have demonstrated the success of Luhn’s approach (Buyukkokten et al., 2001; Lam-Adesina and Jones, 2001; Jaruskulchai et al., 2003). Edmunson (1969) proposed the use of other features such as title words, sentence locations, and bonus words to improve sentence extraction. Goldstein et al. (1999) presented an extraction technique that assigns weighted scores for both statistical and linguistic features in the sentence."
"Recently, Salton et al. (1999) have developed a model for representing a document by using undirected graphs.  "
"Chuang and Yang (2000) studied several algorithms for extracting sentence segments, such as decision tree, naive Bayes classifier, and neural network."
"The graph G is called the text relationship map of D (Salton et al., 1999). "
"The typical approach for testing a summarization system is to create an “ideal” summary, either by professional abstractors or merging summaries provided by multiple human subjects using methods such as majority opinion, union, or intersection (Jing et al., 1998). "
"In this paper, we attempt to bring some clarity to the situation by taking a closer look at one of these existing methods. Specifically, we cast the popular technique of cube pruning (Chiang, 2007) in the well-understood terms of heuristic search (Pearl, 1984). "
"We show how this insight enables us to easily develop faster and exact variants of cube pruning for tree-to-string transducer-based MT (Galley et al., 2004; Galley et al., 2006; DeNero et al., 2009). "
"This should be regarded neither as a surprise nor a criticism, considering cube pruning’s origins in hierarchical phrase-based MT models (Chiang, 2007), which have only a small number of distinct Nonterminals."
"But the situation is much different in treeto-string transducer-based MT (Galley et al., 2004; Galley et al., 2006; DeNero et al., 2009). "
"Binarizing the grammars (Zhang et al., 2006) further increases the size of these sets, due to the introduction of virtual nonterminals. "
"A* has nice guarantees (Dechter and Pearl, 1985), but it is space-consumptive and it is not anytime. For a use case where we would like a finer- rained speed/quality tradeoff, it might be useful to consider an anytime search algorithm, like depth-first branch-and-bound (Zhang and Korf, 1995). "
"Annotations should not be influenced by theory-specific considerations. Nevertheless, different theory-specific representations shMl be recoverable from the annotation, cf. (Marcus et al., 1994). "
"The scheme must provide representational means for all phenomena occurring in texts. Disambiguation is based on human processing skills (cf. (Marcus et at., 1994), (Sampson, 1995), (Black et al. , 1996)). "
"Due to the substantial differences between existing models of constituent structure, tile question arises of how the theory , requirement can be satisfied. At this point the importance of the underlying argument : is emphasised (cf. (Lehmaim et al., 1996), (Marcus et al., 1994), (Sampson, 1995)). "
"This requirement speaks against the traditional sort of dependency trees, in which heads are represented as non-terminal nodes, cf. (Hudson, 1984). "
"As keyboard input is more efficient than mouse input (cf. (Lehmalm et al., 1995)) mnost effort has been put in developing an efficient keyboard interLace. "
"Grammatical functions are assigned using standard statistical part-of-speech tagging methods (cf. e.g. (Cutting et al., 1992) and (Feldweg, 1995)). "
"Actually, our dependency structure alignment is almost the same as that of Filippova and Strube (2008), and our lead sentence plays the role of a basis tree in the Barzilay and McKeown approach (2005). "
"This revision strategy was employed by the human reviser mentioned in section 2, and we consider this to be effective because our target document has a so-called inverse pyramid structure (Robin and McKeown 1996), in which the first sentence is elaborated by the following sentences. "
"Notice that the term coreferential is used in an extended way as it is usually used to describe the phenomena in noun group pairs (Mitkov, 2002).   "
"The articles were morphologically analyzed by Mecab (Kudo et al., 2003) and syntactically parsed by Cabocha (Kudo and Matsumoto, 2002). "
"We planned a linguistic evaluation like DUC2005 (Hoa Trang, 2005). "
"An attempt was made in the CoNLL 2007 shared task to apply SCL to non-projective dependency parsing (Shimizu and Nakagawa, 2007), however, without any clear conclusions. "
"Therefore, whenever we have ac cess to a large amount of labeled data from some “source” (out-of-domain), but we would like a model that performs well on some new “target” domain (Gildea, 2001; Daumé III, 2007), we face the problem of domain adaptation. "
"For example, the performance of a statistical parsing system drops in an appalling way when a model trained on the Wall Street Journal is applied to the more varied Brown corpus (Gildea, 2001) "
"The problem itself has started to get attention only recently (Roark and Bacchiani, 2003; Hara et al., 2005; Daumé III and Marcu, 2006; Daumé III, 2007; Blitzer et al., 2006; McClosky et al., 2006; Dredze et al., 2007).  "
"We distinguish two main approaches to domain adaptation that have been addressed in the literature (Daumé III, 2007): supervised and semi-supervised. "
"In supervised domain adaptation (Gildea, 2001; Roark and Bacchiani, 2003; Hara et al., 2005; Daumé III, 2007), besides the labeled source data, we have access to a comparably small, but labeled amount of target data. "
"In contrast, semi-supervised domain adaptation (Blitzer et al., 2006; McClosky et al., 2006; Dredze et al., 2007) is the scenario in which, in addition to the labeled source data, we only have unlabeled and no labeled target domain Data. "
"Thus, one conclusion from that line of work is that as soon as there is a reasonable (often even small) amount of labeled target data, it is often more fruitful to either just use that, or to apply simple adaptation techniques (Daumé III, 2007; Plank and van Noord, 2008). "
"In contrast, Dredze et al. (2007) report on “frustrating” results on the CoNLL 2007 semi-supervised adaptation task for dependency parsing, i.e. ”no team was able to improve target domain performance substantially over a state of the art baseline”. "
"So far, most previous work on domain adaptation for parsing has focused on data-driven systems (Gildea, 2001; Roark and Bacchiani, 2003; McClosky et al., 2006; Shimizu and Nakagawa, 2007), i.e. systems employing (constituent or de endency based) treebank grammars (Charniak, 1996)."
"The system just ended up at rank 7 out of 8 teams. However, based on annotation differences in the datasets (Dredze et al., 2007) and a bug in their system (Shimizu and Nakagawa, 2007), their results are inconclusive. 1 Thus, the effectiveness of SCL is rather unexplored for parsing. "
"The output of the parser is dependency structure based on the guidelines of CGN (Oost Dijk, 2000). "
"The Maximum Entropy model (Berger et al., 1996; Ratnaparkhi, 1997; Abney, 1997) is a conditional model that assigns a probability to every possible parse ω for a given sentence s."
"The parameters (weights) θj can be estimated efficiently by maximizing the regularized conditional likelihood of a training corpus (Johnson et al., 1999; van Noord and Malouf, 2005)"
"SCL (Structural Correspondence Learning) (Blitzer et al., 2006; Blitzer et al., 2007; Blitzer, 2008) is a recently proposed domain adaptation technique which uses unlabeled data from both source and target domain to learn correspondences between features from different Domains."
"Intuitively, if we are able to find good correspondences among features, then the augmented labeled source domain data should transfer better to a target domain (where no labeled data is available) (Blitzer et al., 2006)."
"As pointed out by Blitzer et al. (2006), each instance will actually contain features which are totally predictive of the pivot features (i.e. the pivot itself)."
"While SCL has been successfully applied to PoS tagging and Sentiment Analysis (Blitzer et al., 2006; Blitzer et al., 2007), its effectiveness for parsing was rather unexplored. "
"In the part-of-speech hterature, whether taggers are based on a rule-based approach (Klein and Simmons, 1963), (Brill, 1992), (Voutilainen, 1993), or on a statistical one (Bahl and Mercer, 1976), (Leech et al., 1983), (Merialdo, 1994), (DeRose, 1988), (Church, 1989), (Cutting et al., 1992), there is a debate as to whether more attention should be paid to lexical probabilities rather than contextual ones. "
"Consider, for example, a word like cover as discussed by Voutilainen (Karlsson et al., 1995): in the Brown and the LOB Corpus (Johansson, 1980), the word ""cover"" is a noun 40% of the occurrences and a verb 60% of the other, but in the context of a car maintenance manual, it is a noun 100~0 of the time. "
"We also argue against Church's position, supporting the claim that more attention needs to be paid to contextual information for part-of-speech disambiguation (Tzoukermann et ai., 1995). "
"this technique generally consists of using a small tagged corpus to train a system and having the system tag another subset of the corpus that gets disambiguated later. (Derouault and Merialdo, 1986) have used these techniques but the necessary human effort is still considerable. "
"In some sense, this approach is similar to the notion of ""ambiguity classes"" explained in (Kupiec, 1992) and (Cutting et al., 1992) where words that belong to the same part-of-speech figure together. "
"(Chanod and Tapanainen, 1995) compare two tagging frameworks for tagging French, one that is statistical, built upon the Xerox tagger (Cutting et al., 1992), and another based on linguistic constraints only. "
"We have used a toolkit developed at AT&T Bell Laboratories (Pereira et al., 1994) which manipulates weighted and unweighted finite-state machines (acceptors or transducers). "
"The morphological FST is generated automatically from a large dictionary of French of about 90,000 entries and on-line corpora, such as Le Monde Newspapers (ECI, 1989 and 1990). "
"The highest ranking material can then be extracted and displayed verbatim as extracts (Luhn 1958; Edmundson 1969; Paice 1990; Kupiec, Pedersen, and Chen 1995). Extracts are often useful in an information retrieval environment since they give users an idea as to what the source document is about (Tombros and Sanderson 1998; Mani et al. 1999), but they are texts of relatively low quality.  "
"Because of this, it is generally accepted that some kind of postprocessing should be performed to improve the final result, by shortening, fusing, or otherwise revising the material (Grefenstette 1998; Mani, Gates, and Bloedorn 1999; Jing and McKeown 2000; Barzilay et al. 2000; Knight and Marcu 2000). "
An extrinsic evaluation (Teufel 2001) shows that the output of our system is al-ready a useful document surrogate in its own right.
"Research is often described as a problem-solving activity (Jordan 1984; Trawinski 1989; Zappen 1983). Three information types can be expected to occur in any research article: problems (research goals), solutions (methods), and results. In many disciplines, particularly the experimental sciences, this problem-solution structure has been crystallized in a fixed presentation of the scientific material as introduction, method, result and discussion (van Dijk 1980). "
"In contrast to the view of science as a disinterested fact Factory, researchers like Swales (1990) have long claimed that there is a strong social aspect to science, because the success of a researcher is correlated with her ability to convince the field of the quality of her work and the validity of her arguments. Authors construct an argument that Myers (1992) calls the “rhetorical act of the paper” "
"They are useful indicators of overall importance (Pollock and Zamora 1975); they can also be relatively easily recognized with information extraction techniques (e.g., regular expressions).  "
"Citation indexes are constructs that contain pointers between cited texts and citing texts (Garfield 1979), traditionally in printed form. When done on-line (as in CiteSeer [Lawrence, Giles, and Bollacker 1999], or as in Nanba and Okumura’s [1999] work), citations are presented in context for users to browse. "
"Citations may vary in many dimensions; for example, they can be central or perfunctory, positive or negative (i.e., critical); apart from scientific reasons, there is also a host of social reasons for citing (“politeness, tradition, piety” [Ziman 1969]).  "
"As our immediate goal is to select important content from a text, we also need a second set of gold standards that are defined by relevance (as opposed to rhetorical status). Relevance is a difficult issue because it is situational to a unique occasion (Saracevic 1975 Sparck Jones 1990 Mizzaro 1997) "
"Humans perceive relevance differently from each other and differently in different situations. Paice and Jones (1993) report that they abandoned an informal sentence selection experiment in which they used agriculture articles and experts in the field as participants, as the participants were too strongly influenced by their personal research interest.  "
"Rath, Resnick, and Savage (1961) report that six participants agreed on only 8 of 20 sentences they were asked to select out of short Scientific American texts and that five agreed on 32 of the Sentences. "
Edmundson and Wyllys (1961) find similarly low human agreement for research articles. More recent experimentsreporting more positive results all used news text (Jing et al. 1998; Zechner 1995).
"Recently, researchers have been looking for more objective definitions of relevance. Kupiec, Pedersen, and Chen (1995) define relevance by abstract similarity "
"In our case, neither assumption holds. First, the experiments in Teufel and Moens (1997) showed that in our corpus only 45of the abstract sentences appear elsewhere in the body of the document (either as a close variant or in identical form), whereas Kupiec, Pedersen, and Chen report a figure of 79.  "
"Author summaries tend to be less systematic (Rowley 1982) and more “deep generated,” whereas sum- maries by professional abstractors follow an internalized building plan (Liddy 1991) and are often created through sentence extraction (Lancaster 1998).  "
"The annotation experiment described here (and in Teufel, Carletta, and Moens [1999] in more detail) tests the rhetorical annotation scheme presented in section "
"We use the kappa coefficient K (Siegel and Castellan 1988) to measure stability and reproducibility, following Carletta (1996).  "
We now describe an automatic system that can perform extraction and classification of rhetorical status on unseen text (cf. also a prior version of the system reported in Teufel and Moens [2000] and Teufel [1999]). 
"To test if such a simple approach would be enough, we performed a text categorization experiment, using the Rainbow mplementation of a naı̈ve Bayes term requency times inverse document frequency (TF*IDF) method (McCallum 1997) and onsidering each sentence as a document. "
"This is not surprising, given that the definition of our task has little to do with the distribution of “content-bearing” words and phrases, much less so than the related task of topic segmentation (Morris and Hirst 1991; Hearst 1997; Choi 2000), or Saggion and Lapalme’s (2000) approach to the summarization of scientific articles, which relies on scientific concepts and their relations."
"We use a naı̈ve Bayesian model as in Kupiec, Pedersen, and Chen’s (1995) experiment (cf. Figure 9)."
"Rhetorical zones appear in typical positions in the article, as scientific argumentation follows certain patterns (Swales 1990)."
"Kupiec, Pedersen, and Chen (1995) report sentence length as a useful feature for text extraction. "
Sentences containing many “content-bearing” words have been hypothesized to be good candidates for text extraction. Baxendale (1958) extracted all words except those on the stop list from the title and the headlines and determined for each sentence whether or not it contained these words.  
How content-bearing a word is can also be measured with frequency counts (Salton and McGill 1983). 
"Linguistic features like tense and voice often correlate with rhetorical zones; Biber (1995) and Riley (1991) show correlation of tense and voice with prototyp- ical section structure (“method,” “introduction”). "
"In order to avoid a full Viterbi search of all possibilities, we perform a beam search with width of three among the candidates of the previous sentence, following Barzilay et al. (2000). "
We measured associations using the log-likelihood measure (Dunning 1993) for each combination of target category and semantic class by converting each cell of the contingency into a 2×2 contingency table. 
"Table 10 shows the results in terms of three overall measures: kappa, percentage accuracy, and macro-F (following Lewis [1991]).  "
"When looking at the numerical values, however, one should keep in mind that macroaveraging results are in general numerically lower (Yang and Liu 1999). "
An extrinsic evaluation additionally showed that the end result provides considerable added value when compared to sentence extracts (Teufel 2001)
"Further work can be done on the semantic verb clusters described in section 4.2. Klavans and Kan (1998), who use verb clusters for document classification according to genre, observe that verb information is rarely used in current practical natural language Applications. "
"There are good reasons for using such a hand-crafted, genre-specific verb lexicon instead of a general resource such as WordNet or Levin’s (1993) classes "
"In our experiments, we used the Hidden Markov Model (HMM) tagging method described in [Cutting et al, 1992]."
"The HMM training and tagging programs in our experiment [Wilkens and Kupiec, 1995] are based on bigrams, i.e. only the immediate context of a word is taken into account."
"For the part-of-speech tagging problem, it is known that assigning the most common part of speech for each lexical item gives a baseline of 90% accuracy [Brill, 1992]. "
"Corpus-based information can be represented e.g. as neural networks (Eineborg and Gamback 1994; Schmid 1994), local rules (Brill 1992), or collocational matrices (Garside 1987). "
"Syntactic analysis is carried out in another reductionistic parsing framework known as Finite State Intersection Grammar (Koskenniemi 1990; Koskenniemi, Tapanainen and Voutilainen 1992; Tapanainen 1992; Voutilainen and Tapanainen 1993; Voutilainen 1994). "
"Several taggers based on rules, stochastic models, neural networks, and hybridsystems have already been presented for Part-of-speech (POS) tagging. Rule-basedtaggers (Brill 1992; Elenius 1990; Jacobs and Zernik 1988; Karlsson 1990; Karlsson etal. 1991; Voutilainen, Heikkila, and Antitila 1992; Voutilainen and Tapanainen 1993)use POS-dependent constraints defined by experienced linguists."
"In the case where additionally raw untagged text isavailable, the Maximum Likelihood training can be used to reestimate the parametersof H M M taggers (Merialdo 1994)."
Connectionist models have been used successfully for lexical acquisition (Eineborgand Gamback 1993; Elenius 1990; Elenius and Carlson 1989; Nakamura et al. 1990).
"In taggers thatare based on hidden Markov models (HMM), parameters of the unknown words areestimated by taking into account morphological information from the last part of theword (Dermatas and Kokkinakis 1994; Maltese and Mancini 1991)."
"Similar results have beenreported by Maltese and Mancini (1991) for the Italian language. Weischedel et al.(1993) have used four categories of word morphology, such as inflectional endings,derivational endings, hyphenation, and capitalization."
"When the training text is adequate to estimate the tagger parameters, moreefficient stochastic taggers (Dermatas and Kokkinakis 1994; Maltese and Mancini 1991;Weischedel et al. 1993) and training methods can be implemented (Merialdo 1994)."
"The first-(Rabiner 1989) and second- (He 1988) order Viterbi algorithms have been presentedelsewhere. Recently, Tao (1992) described the Viterbi algorithm for generalized HMMs."
The optimum tag tio is estimated using the probabilities of the forward-backwardalgorithm (Rabiner 1989)
The probabilities in equation 4 are estimated recursively for the first- (Rabiner1989) and second-order HMM (Watson and Chung 1992).
The scaling processintroduced in this case multiplies the forward and backward probabilities by a scalingfactor at selective word events in order to keep the computations within the floating-point dynamic range of the computer (Rabiner 1989).
"Currently, most of the POS tagger accuracy reports are based on the experiments involving Penn Treebank data (Marcus, 1993). "
"For example, while the TnT tagger performs at 97% accuracy on known words in the Treebank, the accuracy drops to 89% on unknown words (Brandts, 2000). "
"The LT POS tagger is reported to perform at 93.6-94.3% accuracy on known words and at 87.7-88.7% on unknown words using a cascading unknown word “guesser” (Mikheev, 1997). "
"The only way to avoid it, is to anonymize the notes prior to POS tagging which in itself is a difficult and expensive process (Ruch et al. 2000).  "
"In order to test some of our assumptions regarding how the differences between general English language and the language of clinical notes may affect POS tagging, we have trained the HMM-based TnT tagger (Brandts, 2000) with default parameters at the tri-gram level both on Penn Treebank and the clinical notes data. "
"Since keyboard input is most efficient for assigning categories to words and phrases, cf. (Lehmann et al.,- 1996; Marcus et al., 1994), and structural manipulations are executed most efficiently using the mouse, both an elaborate keyboard and optical interface is Provided. "
"Since broad-coverage parsers for German, especially robust parsers that assign predicate-argument structure and allow crossing branches, are not available, or require an annotated traing corpus (cf. (Collins, 1996), (Eisner, 1996))."
"The task can be performed by a chunk parser that is equipped with an appropriate finite state grammar (Abney, 1996). "
"The contexts are smoothed by linear interpolation of unigrams, bigrams, and trigrams. Their weights are calculated by deleted interpolation (Brown et al., 1992). "
"For example, (Briscoe and Carroll, 1993) train an LR parser based on a general grammar to be able to distinguish between likely and unlikely sequences of parsing actions; (Andry et al., 1994) automatically infer sortal constraints, that can be used to rule out otherwise grammatical constituents; and (Grishman et al., 1984) describes methods that reduce the size of a general grammar to include only rules actually useful for parsing the training corpus. "
"Constituent pruning is a bottom-up approach, and is complemented by a second, top-down, method based on Explanation-Based Learning (EBL; (Mitchell et al., 1986; van Harmelen and Bundy, 1988)). "
"The scheme is fully implemented within a version of the Spoken Language Translator system (Rayner et al., 1993; Agniis et al., 1994), and is normally applied to input in the form of small lattices of hypotheses produced by a speech recognizer. "
"That is, an assumption of full statistical dependence (Yarowsky, 1994), rather than the more common full independence, is made  "
"The basic corpus used was a set of 16,000 utterances from the Air Travel Planning (ATIS; (Hemphill et al., 1990)) domain. "
"Care was taken to ensure not just that the utterances themselves, but also the speakers of the utterances were disjoint between test and training data; as pointed out in (Rayner et al., 1994a), failure to observe these precautions can result in substantial spurious improvements in test data results. "
"The 16,000 sentence corpus was analysed by the SRI Core Language Engine (Alshawi (ed), 1992), using a lexicon extended to cover the ATIS domain (Rayner, 1994). "
"The four sets of outputs from the parser were then translated into Swedish by the SLT transfer and generation mechanism (Agn et al., 1994). "
"Making such an assumption is reasonable since POS taggers that can achieve accuracy of 96% are readily available to assign POS to unrestricted English sentences (Brill, 1992; Cutting et al., 1992)."
"To evaluate the performance of LEXAS, we conducted two tests, one on a common data set used in (Bruce and Wiebe, 1994), and another on a larger data set that we separately collected. "
"One exception is the sense-tagged data set used in (Bruce and Wiebe, 1994), which has been made available in the public domain by Bruce and Wiebe. "
"Note that the sense definitions used in this data set are those from Longman Dictionary of Contemporary English (LDOCE) (Procter, 1978). "
"Bruce and Wiebe also performed a separate test by using a subset of the ""interest"" data set with only 4 senses (sense 1, 4, 5, and 6), so as to compare their results with previous work on WSD (Black, 1988; Zernik, 1990; Yarowsky, 1992), which were tested on 4 senses of the noun ""interest"". However, the work of (Black, 1988; Zernik, 1990; Yarowsky, 1992) were not based on the present set of sentences, so the comparison is only suggestive. "
"Previous work on using the unordered set of surrounding words have used a much larger window, such as the 100-word window of (Yarowsky, 1992), and the 2-sentence context of (Leacock et al., 1993). "
"Our experimental finding, t h a t local collocations are the most predictive, agrees with past observa- tion that humans need a narrow window of only a few words to perform WSD (Choueka and Lusignan, 1985). "
"To get an idea of how the sense assignments of our d a t a set compare with those provided by WoRDNET linguists in SEMCOR, the sense-tagged subset of Brown corpus prepared by Miller et al. (Miller et al., 1994), we compare a subset of the occurrences that overlap. "
"This should not be too surprising, as it is widely believed that sense tagging using the full set of refined senses found in a large dictionary like WORDNET involve making subtle human judgments (Wilks et al., 1990; Bruce and Wiebe, 1994), such that there are many genuine cases where two humans will not agree fully on the best sense assignments."
"This default strategy has been advocated as the baseline performance level for comparison with WSD programs (Gale et al., 1992)."
"The work of (Cardie, 1993) used a case-based approach that simultaneously learns part of speech, word sense, and concept activation knowledge, although the method is only tested on domain-specific texts with domain-specific word senses. "
"We also implemented a version of Hobbs’s (1978) well-known pronoun interpretation algorithm as a baseline, in which no machine learning is involved. "
"first, the linguistic ap-proach, in which the model is written by a linguist,generally in the form of rules or constraints (Vouti-lainen and Jgrvinen, 1995). Second, the automaticapproach, in which the model is automatically ob-tained from corpora (either raw or annotated) 1 , andconsists of n-grams (Garside et al., 1987; Cuttinget ah, 1992), rules (Hindle, 1989) or neural nets(Schmid, 1994)."
"Thehigh level data trend acquires more sophisticated in-formation, such as context rules, constraints, or de-cision trees (Daelemans et al., 1996; M/~rquez andRodriguez, 1995; Samuelsson et al., 1996)."
"The ac-quisition methods range from supervised-inductive-learning-from-example algorithms (Quinlan, 1986; A h a et al., 1991) to genetic algorithm strategies(Losee, 1994), through the transformation-basederror-driven algorithm used in (Brill, 1995), Stillanother possibility are the hybrid models, which tryto join the advantages of both approaches (Vouti-lainen and Padr6, 1997)."
"The constraint language is able to express thesame kind of patterns than the Constraint Gram-mar formalism (Karlsson et al., 1995), although in adifferent formalism."
"Decision trees,recently used in NLP basic tasks such as taggingand parsing (McCarthy and Lehnert, 1995: Daele-mans et al., 1996; Magerman, 1996), are suitable forperforming this task."
"Itconstructs the trees in a top - down way, guided bythe distributional information of the examples, butnot on the examples order (Quinlan, 1986)."
"There exist two main families of attribute-selecting functions: information-based (Quinlan, 1986: Ldpez, 1991) and statistically--based (Breiman et al., 1984; Mingers, 1989). "
"Some s.vs terns perform a previous recasting of the attributes in order to have only binary-valued attributes and to deal with binary trees- (Magerman, 1996)."
"Experimental tests (M&rquez and Rodriguez, 1995) have shown that the pruning process reduces tree sizes at about 50% and improves their accuracy in a 2-5%. "
"The compatibility value for each constraint is the mutual information between the tag and the context (Cover and Thomas, 1991). "
Usual tagging algorithms are either n - g r a m oriented -such as Viterbi algorithm (Viterbi. 1967)- or a d - hoc for every case when they must deal with more complex information.
"The algorithm has been applied to part-of-speech tagging (Padr6, 1996), and to shallow parsing (Voutilainen and Padro. 1997).  "
"In addition, M L indicates a baseline model conraining no constraints (this will result in a most- likely tagger) and H M M stands for a hidden Markov model bigram tagger (Elworthy, 1992). "
"The two systems we use are E N G C G (Karlsson et al., 1994) and the Xerox Tagger (Cutting et al., 1992). We discuss problems caused by the fact that these taggers use different tag sets, and present the results obtained by applying the combined taggers to a previously unseen sample of text. "
"The English Constraint Grammar Parser, ENGCG  Voutilainen et al., 1992; Karlsson el al., 1994), is based on Constraint Grammar, a parsing framework proposed by Fred Karlsson (1990). "
"The ENGTWOL lexicon is based on the two-level  model (Koskenniemi, 1983). "
"The Xerox Tagger 1, XT, (Cutting et al., 1992) is a statistical tagger made by Doug Cutting, Julian Kupiec, Jan Pedersen and Penelope Sibun in Xerox PARC. It was trained on the untagged Brown Corpus (Francis and Kubera, 1982). "
"The tagger itself is based on the Hidden Markov Model (Baum, 1972) and word equivalence classes (Kupiec, 1989). "
"The system was tested against 26,711 words of newspaper text from The Wall Street Journal, The Economist and Today, all taken from the 200-million word Bank of English corpus by the COBUILD team at the University of Birmingham, England (see also (J/irvinen, 1994)). "
"The differences were jointly examined by the judges to see whether they were caused by inattention or by a genuine difference of opinion that could not be resolved by consulting the documentation that outlines the principles adopted for this grammatical representation (for the most part documented in (Karlsson et al., 1994)).  "
"We could use partly disambiguated text (e.g. the output of parsers D1, D2 or D3~) and disambiguate the result using a knowledge-based syntactic parser (see experiments in (Vou- tilainen and Tapanainen, 1993)).  "
"Corpus-derived distributional semantic spaces have proved valuable in tackling a variety of tasks, ranging from concept categorization to relation extraction to many others (Sahlgren, 2006; Turney, 2006; Padó and Lapata, 2007). "
"From a cognitive angle, corpus-based models hold promise as simulations of how humans acquire and use conceptual and linguistic information from their environment (Landauer and DuMais, 1997). "
"a database of interconnected concepts and properties (Rogers and McClelland, 2004), adapting the information stored there to the task at hand "
Such tasks will require an extension of the current framework of Turney (2008) beyond evidence from the direct cooccurrence of target word pairs. 
"Concept similarity is often measured by vectors of co-occurrence with context words that are typed with dependency information (Lin, 1998; Curran and Moens, 2002). "
"Detecting whether a pair expresses a target relation by looking at shared connector patterns with model pairs is a common strategy in relation extraction (Pantel and Pennacchiotti, 2008). "
"We computed the weight (strength of association) for all the tuples extracted in this way using the local MI measure (Evert, 2005), that is theoretically justified, easy to compute for triples and robust against overestimation of rare events. "
"The myPlain model implements a classic “flat” co-occurrence approach (Sahlgren, 2006) in which we keep track of verb-to-noun co-occurrence within a window that can include, maximally, one intervening noun, and noun-to-noun co-occurrence with no more than 2 intervening nouns. "
"The myHAL model uses the same co-occurrence window, but, like HAL (Lund and Burgess, 1996), treats left and right co-occurrences as distinct features. "
"Like in the DV model of Padó and Lapata (2007), only pairs connected by target links are preserved, but the links themselves are not part of the model. "
"Much work in computational linguistics and related fields relies on measuring similarity among words/concepts in terms of their patterns of co-occurrence with other words/concepts (Sahlgren, 2006). "
"In the example, the patterns of co-occurrence suggest that objects of killing are rather similar to subjects of dying, hinting at the classic cause(subj,die(obj)) analysis of killing by Dowty (1977) and many others. "
We took 232 causative/inchoative verbs and 170 non- alternating transitive verbs from Levin (1993).  
In  it was observed that a significant percent of the queries made by a user in a search engine are associated to a repeated search Output sequence optimization Rather than basing classifications only on model parameters estimated from cooccurrences between input and output symbols employed for maximizing the likelihood of pointwise singlelabel predictions at the output level  classifier output may be augmented by an optimization over the output sequence as a whole using optimization techniques such as beam searching in the space of a conditional markov models output  or hidden markov models  
Dredze et al yielded the second highest score1 in the domain adaptation track  The IBM models  search a version of permutation space with a onetomany constraint  propose the use of language models for sentiment analysis task and subjectivity extraction 
In training process  we use GIZA   4 toolkit for word alignment in both translation directions  and apply growdiagfinal method to refine it  The models in the comparative study by  did not include such features  and so  again for consistency of comparison  we experimentally verified that our maximum entropy model  a  consistently yielded higher scores than when the features were not used  and  b  consistently yielded higher scores than nave Bayes using the same features  in agreement with  
 and  et al We used the WordNet   Similarity package  to compute baseline scores for several existing measures  noting that one word pair was not processed in WS353 because one of the words was missing from WordNet 
We use MER  to tune the decoders parameters using a development data set The training set is extracted from TreeBank  section 1518  the development set  used in tuning parameters of the system  from section 20  and the test set from section 21 For nonlocal features  we adapt cube pruning from forest rescoring   since the situation here is analogous to machine translation decoding with integrated language models  we can view the scores of unit nonlocal features as the language model cost  computed onthefly when combining subconstituents 
31 Agreement for Emotion Classes The kappa coefficient of agreement is a statistic adopted by the Computational Linguistics community as a standard measure for this purpose  ITGs translate into simple  22   BRCGs in the following way  see  for a definition of ITGs 
This may be because their system was not tuned using minimum error rate training  However  most of the existing models have been developed for English and trained on the Penn Treebank   which raises the question whether these models generalize to other languages  and to annotation schemes that differ from the Penn Treebank markup 
Following   we used sections 018 of the Wall Street Journal  WSJ  corpus for training  sections 1921 for development  and sections 2224 for final evaluation 
In   the authors provide some sample subtrees resulting from such a 1000word clustering We took part the Multilingual Track of all ten languages provided by the CoNLL2007 shared task organizer  To set the weights  m  we carried out minimum error rate training  using BLEU  as the objective function 
Use of global features for structured prediction problem has been explored by several NLP applications such as sequential labeling  and dependency parsing  with a great deal of success It is important to realize that the output of all mentioned processing steps is noisy and contains plenty of mistakes  since the data has huge variability in terms of quality  style  genres  domains etc  and domain adaptation for the NLP tasks involved is still an open problem  
They are also used for inducing alignments  In recent work   proposed a general framework for including morphological features in a phrasebased SMT system by factoring the representation of words into a vector of morphological features and allowing a phrasebased MT system to work on any of the factored representations  which is implemented in the Moses system 
2 Architecture of the system The goal of statistical machine translation  SMT  is to produce a target sentence e from a source sentence f It is today common practice to use phrases as translation units  and a log linear framework in order to introduce several models explaining the translation process  e    argmaxp  e f   argmaxe LCB exp  summationdisplay i ihi  e  f   RCB  1  The feature functions hi are the system models and the i weights are typically optimized to maximize a scoring function on a development set  
1 Introduction Sentiment analysis have been widely conducted in several domains such as movie reviews  product reviews  news and blog reviews  Their approaches include the use of a vectorbased information retrieval technique   binbash  line 1  a  command not found Our do  mains are more varied  which may results in more recognition errors 
The corpus was aligned with GIZA    and symmetrized with the growdiagfinaland heuristic  BLEU  was devised to provide automatic evaluation of MT output Statistics in linguistics  Oxford  Basil Blackwell rawString citation citation validtrue authors author N Chinchor author authors title Evaluating message understanding systems  an analysis of the third Message Understanding Conference  MUC3 title date 1993 date journal Computational Linguistics journal volume 19 volume pages 409  449 pages marker Chinchor  1993 marker rawString Chinchor  N  et al  1993 
Note that it is straightforward to calculate these expected counts using a variant of the insideoutside algorithm  applied to the  dependencyparsing data structures  for projective dependency structures  or the matrixtree theorem  for nonprojective dependency structures 
Following   we consider an anaphoric reference  NPi  correctly resolved if NPi and its closest antecedent are in the same coreference chain in the resulting partition This algorithm adjusts the loglinear weights so that BLEU  is maximized over a given development set 
We discriminatively trained our parser in an online fashion using a variant of the voted perceptron  In fact  we found that it doesnt do so badly at all  the bitag HMM estimated by EM achieves a mean 1to1 tagging accuracy of 40   which is approximately the same as the 413  reported by  for their sophisticated MRF model  Motivation and Prior Work While several authors have looked at the supervised adaptation case  there are less  and especially less successful  studies on semisupervised domain adaptation  
There are other types of variations for phrases  for example  insertion  deletion or substitution of words  and permutation of words such as view point and point of view are such variations    search engines   uses the Altavista web browser  while we consider and combine the frequency information acquired from three web search engines 
Many methods have been proposed to measure the cooccurrence relation between two words such as 2   mutual information   ttest   and loglikelihood  It has been argued that the reliability of a coding schema can be assessed only on the basis of judgments made by naive coders  
to the pairwise TER alignment described in  We obtain aligned parallel sentences and the phrase table after the training of Moses  which includes running GIZA     growdiagonalfinal symmetrization and phrase extraction  
From the above discussion  we can see that traditional tree sequencebased method uses single tree as translation input while the forestbased model uses single subtree as the basic translation unit that can only learn treetostring  rules 
Our baseline method for ambiguity resolution is the Collins parser as implemented by Bikel  We then built separate EnglishtoSpanish and SpanishtoEnglish directed word alignments using IBM model 4   combined them using the intersect  grow heuristic   and extracted phraselevel translation pairs of maximum length 7 using the alignment template approach  
For instance  both Pang and Lee  and   consider the thumbs upthumbs down decision  is a film review positive or negative  Binarizing the syntax trees for syntaxbased machine translation is similar in spirit to generalizing parsing models via markovization  4 Options from the Translation Table Phrasebased statistical machine translation methods acquire their translation knowledge in form of large phrase translation tables automatically from large amounts of translated texts  
For example   collected reviews from a movie database and rated them as positive  negative  or neutral based on the rating  eg  number of stars  given by the reviewer The earliest work in this direction are those of          and   established that it is important to tune  the tradeoff between Precision and Recall  to maximize performance 
Moreover  under this view  SMT becomes quite similar to sequential natural language annotation problems such as partofspeech tagging and shallow parsing  and the novel training algorithm presented in this paper is actually most similar to work on training algorithms presented for these task  eg the online training algorithm presented in  and the perceptron training algorithm presented in  Feature weights vector are trained discriminatively in concert with the language model weight to maximize the BLEU  automatic evaluation metric via Minimum Error Rate Training  MERT   
 and Wiebe  2000  focused on learning adjectives and adjectival phrases and Wiebe et al A number of alignment techniques have been proposed  varying from statistical methods  to lexical methods  
Even though there are some studies that compare the results from statistically computed association measures with word association norms from psycholinguistic experiments  there has not been any research on the usage of a digital  networkbased dictionary reflecting the organization of the mental lexicon to our knowledge 
Finally  we use as a feature the mappings produced in  of WordNet senses to Oxford English Dictionary senses 3 Semantic Representation 31 The Need for Dependencies Perhaps the most common representation of text for assessing content is BagOfWords or BagofNGrams  First  we adopt an ONTOLOGICALLY PROMISCUOUS representation  that includes a wide variety of types of entities 
For each training direction  we run GIZA     specifying 5 iterations of Model 1  4 iterations of the HMM model   and 4 iterations of Model 4 To set the weight vector w  we train twenty averaged perceptrons  on different shuffles of data drawn from sections 0221 of the Penn Treebank 
In the refined model 2  alignment probabilities a  ilj  l  m  are included to model the effect that the position of a word influences the position of its translation 6 Conclusions and Future Directions In previous work  statistical NLP computation over large corpora has been a slow  of ine process  as in KNOWITALL  and also in PMIIR applications such as sentiment classi cation  
The first solution might also introduce errors elsewhere As  already noted  ` While this automatic derivation process introduced a small percentage of errors on its own  it was the only practical way both to provide the amount of training data required and to allow for fullyautomatic testing  1 To train their system  R&M used a 200kword chunk of the Penn Treebank Parsed Wall Street Journal  tagged using a transformationbased tagger  and extracted base noun phrases from its parses by selecting noun phrases that contained no nested noun phrases and further processing the data with some heuristics  like treating the possessive marker as the first word of a new base noun phrase  to flatten the recursive structure of the parse 
This can either be semisupervised parsing  using both annotated and unannotated data  or unsupervised parsing  training entirely on unannotated text In   as well as other similar works   only lefttoright search was employed 
This iterative optimiser  derived from a word disambiguation technique   finds the nearest local maximum in the lexical cooccurrence network from each concept seed This model is related to the averaged perceptron algorithm of  
a22 a14 is the sufficient statistic of a16 a14 Then  we can rewrite a2a24a3 a10a27 a42a7 a25 as  a5a7a6a9a8a11a10 a23 a3 a10 a7 a15 a27 a25a18a17a26a25 a12a28a27 a5a7a6a29a8a30a10 a23 a3 a10 a7 a15 a27 a25a18a17 3 Loss Functions for Label Sequences Given the theoretical advantages of discriminative models over generative models and the empirical support by   and that CRFs are the stateoftheart among discriminative models for label sequences  we chose CRFs as our model  and trained by optimizing various objective functions a31 a3 a10a36 a25 with respect to the corpus a36 The application of these models to the label sequence problems vary widely 
Pustejovsky confronted with the problem of automatic acquisition more extensively in  PropBank encodes propositional information by adding a layer of argument structure annotation to the syntactic structures of the Penn Treebank  
Jiao et al propose semisupervised conditional random fields  that try to maximize the conditional loglikelihood on the training data and simultaneously minimize the conditional entropy of the class labels on the unlabeled data  report extracting database records by learning record field compatibility Unfortunately  a counterexample illustrated in  shows that the max function does not produce valid kernels in general 
2 Detecting DiscourseNew Definite Descriptions 21 Vieira and Poesio Poesio and Vieira  carried out corpus studies indicating that in corpora like the Wall Street Journal portion of the Penn Treebank   around 52  of DDs are discoursenew   and another 15  or so are bridging references  for a total of about 6667  firstmention The distinction between lexical and relational similarity for word pair comparison is recognized by   hecallstheformer attributional similarity   though the methods he presents focus on relational similarity 
The POS disambiguation has usually been performed by statistical approaches mainly using hidden markov model  HMM    et al  1992  Kupiec As a baseline model we used a maximum entropy tagger  very similar to the one described in  
We assign tags of partofspeech  POS  to the words with MXPOST that adopts the Penn Treebank tag set  Such coarsegrained inventories can be produced manually from scratch  or by automatically relating  or clustering  existing word senses  a contextual word cw that occurs in the paragraphs of bc  a loglikelihood ratio  G2  test is employed   which checks if the distribution of cw in bc is similar to the distribution of cw in rc  p  cw bc   p  cw rc   null hypothesis  
In this paper we use the socalled Model 4 from  We would expect the opposite effect with handaligned data  Extensions to Hiero Several authors describe extensions to Hiero  to incorporate additional syntactic information   or to combine it with discriminative latent models  The other form of hybridization   a statistical MT model that is based on a deeper analysis of the syntactic 33 structure of a sentence   has also long been identified as a desirable objective in principle  consider   
272 Similaritybased estimation was first used for language modeling in the cooccurrence smoothing method of Essen and Steinbiss   derived from work on acoustic model smoothing by Sugawara et al Following the setup in   we initialize the transition and emission distributions to be uniform with a small amount of noise  and run EM and VB for 1000 iterations 
Our method uses assumptions similar to  et al 1996 but is naturally suitable for distributed parallel computations The agreement on identifying the boundaries of units  using the statistic discussed in   was  9  for two annotators and 500 units   the agreement on features  2 annotators and at least 200 units  was as follows  UTYPE   76  VERBED   9  FINITE   81 
In comparison we introduce 28 several metrics coefficients reported in Albrecht and Hwa  including smoothed BLEU   METEOR   HWCM   and the metric proposed in Albrecht and Hwa  using the full feature set This is one manifestation of what is commonly referred to as the data sparseness problem  and was discussed by  as a sideeffect of specificity 
Techniques for weakening the independence assumptions made by the IBM models 1 and 2 have been proposed in recent work  C3BTC5 and CCCDCA were used in  and   respectively 
In this work we will use structured linear classifiers  This is the best automatically learned partofspeech tagging result known to us  representing an error reduction of 44  on the model presented in   using the same data splits  and a larger error reduction of 121  from the more similar best previous loglinear model in Toutanova and Manning  
The template we use here is similar to   but we have added extra context words before the X and after the Y Our morphological processing also differs from  
The algorithm employs the OpenNLP MaxEnt implementation of the maximum entropy classification algorithm  to develop word sense recognition signatures for each lemma which predicts the most likely sense for the lemma according to the context in which the lemma occurs 
Alternatively  order is modelled in terms of movement of automatically induced hierarchical structure of sentences  Parameters used to calculate P  D  are trained using MER training  on development data 
Chiang  distinguishes statistical MT approaches that are syntactic in a formal sense  going beyond the nitestate underpinnings of phrasebased models  from approaches that are syntactic in a linguistic sense  ie taking advantage of a priori language knowledge in the form of annotations derived from human linguistic analysis or treebanking1 The two forms of syntactic modeling are doubly dissociable  current research frameworks include systems that are nite state but informed by linguistic annotation prior to training  eg     and also include systems employing contextfree models trained on parallel text without bene t of any prior linguistic analysis  eg 
This can be done in a supervised   a semisupervised  or a fully unsupervised way  Equation  3  reads If the target noun appears  then it is distinguished by the majority The loglikelihood ratio  decides in which order rules are applied to the target noun in novel context 
Statisticbased algorithms based on Belief Network  such as HiddenMarkovModel  HMM     Lexicalized HMM  and MaximalEntropy model  use the statistical information of a manually tagged corpus as background knowledge to tag new sentences One way of resolving query ambiguities is to use the statistics  such as mutual information   to measure associations of query terms  on the basis of existing corpora  
The wn   similarity package  to compute the Jiang & Conrath  J&C  distance  as in   propose using a statistical word alignment algorithm as a more robust way of aligning  monolingual  outputs into a confusion network for system com2  construct lattices over paraphrases using an iterative pairwise multiple sequence alignment  MSA  algorithm 
One of the first large scale hand tagging efforts is reported in   where a subset of the Brown corpus was tagged with WordNet July 2002  pp 42 Experiments To build all alignment systems  we start with 5 iterations of Model 1 followed by 4 iterations of HMM   as implemented in GIZA    
The usual recall and precision metrics  eg  how many of the interesting bits of information were detected  and how many of the found bits were actually correct  require either a test corpus previously annotated with the required information  or manual evaluation  
The CRF tagger was implemented in MALLET  using the original feature templates from  Given a set of terms with unknown sentiment orientation   then uses the PMIIR algorithm  to issue queries to the web and determine  for each of these terms  its pointwise mutual information  PMI  with the two seed words across a large set of documents 
This is similartothegraphconstructionmethodof  and Rao et al We distinguish two main approaches to domain adaptation that have been addressed in the literature   supervised and semisupervised 
SmadjaFrank1993The experimental results in  show a negative impact on the parsing accuracy from too long dependency relation k   P  A  P  E   3  1P  E   suggests that the units over which the kappa statistic is computed affects the outcome 
Among the chunk types  NP chunking is the first to receive the attention   than other chunk types  such as VP and PP chunking  This model shares some similarities with the stochastic inversion transduction grammars  SITG  presented by Wu in  
  makes a similar point  noting that for reviews  the whole is not necessarily the sum of the parts   Identifying transliteration pairs is an important component in many linguistic applications which require identifying outofvocabulary words  such as machine translation and multilingual information retrieval  
 and Chan et al Based on these grammars  a great number of SMT models have been recently proposed  including stringtostring model  Synchronous FSG    treetostring model  TSGstring    stringtotree model  stringCFGTSG    treetotree model  Synchronous CFGTSG  DataOriented Translation   and so on 
We use a standard maximum entropy classifier  implemented as part of MALLET  8412 only PTB  baseline  8358 1st  8342 2nd  8338 3rd  8308 third row lists the three highest scores of the domain adaptation track of the CoNLL 2007 shared task 
Johnson 1997 notes that this structure has a higher probability than the correct flat structure given counts taken from the treebank for a standard PCFGWe used a loglinear model with no Markov dependency between adjacent tags 3 and trained the parameters of the model with the perceptron algorithm  with averaging to control for overtraining  
In Turneys work  the cooccurrence is considered as the appearance in the same window  Named entities also pose another problem with the  coreference model  since it models only the heads of NPs  it will fail to resolve some references to named entities   Ford Motor Co  Ford   while erroneously merging others   Ford Motor Co  Lockheed Martin Co  
For the constituentbased models  constituent information was obtained from the output of  for English and Dubeys parser  2004  for German 
This finding has been previously reported  among others  in  In order increase the likelihood that 909 only true paraphrases were considered as phraselevel alternations for an example  extracted sentences were clustered using completelink clustering using a technique proposed in  
As reported in   parameter averaging can effectively avoid overfitting Several representations to encode region information are proposed and examined  
One important application of bitext maps is the construction of translation lexicons  and  as discussed  translation lexicons are an important information source for bitext mapping This method is described hereafter  while the subsequent steps  that use deeper  rulebased  levels of knowledge  are implemented into the ARIOSTO_LEX lexical learning system  described in  
This can be the base of a principled method for detecting structural contradictions  6 Related Work Several works attempt to extend WordNet with additional lexical semantic information  
In order to improve sentencelevel evaluation performance  several metrics have been proposed  including ROUGEW  ROUGES  and METEOR  
Besides the the casesensitive BLEU4  used in the two experiments  we design another evaluation metrics Reordering Accuracy  RAcc  for forced decoding evaluation 
2 Syntacticoriented evaluation metrics We investigated the following metrics oriented on the syntactic structure of a translation output  POSBLEU The standard BLEU score  calculated on POS tags instead of words  POSP POS ngram precision  percentage of POS ngrams in the hypothesis which have a counterpart in the reference  POSR Recall measure based on POS ngrams  percentage of POS ngrams in the reference which are also present in the hypothesis  POSF POS ngram based Fmeasure  takes into account all POS ngrams which have a counter29 part  both in the reference and in the hypothesis 
4 Building Noun Similarity Lists A lot of work has been done in the NLP community on clustering words according to their meaning in text  
In other words   4b  can be used in substitution of  4a   whereas  5b  can not  so easily 41n   a value of K between 8 and I indicates good agreement  a value between 6 and 8 indicates some agreement 
For example   have studied synchronous context free grammar 
 Peter F Brown  Vincent J Della Pietra  Petere V deSouza  Jenifer C Lai  and Robert L Mercer 
Table 1 reports values for the Kappa  K  coefficient of agreement  for Forward and Backward Functions 6 The columns in the tables read as follows  if utterance Ui has tag X  do coders agree on the subtag  
When we have a junction tree for each document  we can efficiently perform belief propagation in order to compute argmax in Equation  1   or the marginal probabilities of cliques and labels  necessary for the parameter estimation of machine learning classifiers  including perceptrons   and maximum entropy models  
   concordancing for bilingual lexicography   computerassisted language learning  corpus linguistics  Melby 
In particular  previous work  has investigated the use of Markov random fields  MRFs  or loglinear models as probabilistic models with global features for parsing and other NLP tasks 
The model consists of a set of wordpair parameters p  t  s  and position parameters p  j  i     in model 1  IBM1  the latter are fixed at 1   1  1   as each position  including the empty position 0  is considered equally likely to contain a translation for w Maximum likelihood estimates for these parameters can be obtained with the EM algorithm over a bilingual training corpus  as described in  
The candidates of unknown words can be generated by heuristic rules  or statistical word models which predict the probabilities for any strings to be unknown words  
For each differently tokenized corpus  we computed word alignments by a HMM translation model  and by a word alignment refinement heuristic of growdiagfinal  
On the other hand  purely statistical systems  extract discriminating MWUs from text corpora by means of association measure regularities 
The chunker is trained on the answer side of the Training corpus in order to learn 2 and 3word collocations  defined using the likelihood ratio of  
The f are trained using a heldout corpus using maximum BLEU training  
Standard CI Model 1 training  initialised with a uniform translation table so that t  ejf  is constant for all sourcetarget word pairs  f  e   was run on untagged data for 10 iterations in each direction  
According to the document  it is the output of Ratnaparkhis tagger  
1 Introduction Current methods for largescale information extraction take advantage of unstructured text available from either Web documents  or  more recently  logs of Web search queries  to acquire useful knowledge with minimal supervision 
6 The Experimental Results We used the Penn Treebank  to perform empirical experiments on this parsing model 
Given a source sentence f  the preferred translation output is determined by computing the lowestcost derivation  combination of hierarchical and glue rules  yielding f as its source side  where the cost of a derivation R1 Rn with respective feature vectors v1   vn Rm is given by msummationdisplay i  1 i nsummationdisplay j  1  vj  i Here  1   m are the parameters of the loglinear model  which we optimize on a heldout portion of the training set  using minimumerrorrate training  
Also related are the areas of word alignment for machine translation   induction of translation lexicons   and crosslanguage annotation projections to a second language  
In Table 6 we report our results  together with the stateoftheart from the ACL wiki5 and the scores of   PairClass  and from Amac Herdagdelens PairSpace system  that was trained on ukWaC 
The features used in this study are  the length of t  a singleparameter distortion penalty on phrase reordering in a  as described in   phrase translation model probabilities  and 4gram language model probabilities logp  t   using KneserNey smoothing as implemented in the SRILM toolkit 
We have computed the BLEU score  accumulated up to 4grams    the NIST score  accumulated up to 5grams    the General Text Matching  GTM  Fmeasure  e  12    and the METEOR measure  
As   we adopted an evaluation of mutual information as a cohesion measure of each cooccurrence 
The last two counts  CAUS and ANIM  were performed on a 29million word parsed corpus  gall Street Journal 1988  provided by Michael Collins   
The tagger used is thus one that does not need tagged and disambiguated material to be trained on  namely the XPOST originally constructed at Xerox Parc  
We perform word alignment using GIZA     symmetrize the alignments using the growdiagfinaland heuristic  and extract phrases up to length 3 
Congress of the Italian Association for Artificial Intelligence  Palermo  1991 B Boguraev  Building a Lexicon  the Contribution of Computers  IBM Report  TJ Watson Research Center  1991 M Brent  Automatic Aquisition of Subcategorization frames from Untagged Texts  in  N Calzolari  R Bindi  Acquisition of Lexical Information from Corpus  in  K W   P Hanks  Word Association Norms  Mutual Information  and Lexicography  Computational Linguistics  vol 
4 Maximum Entropy To explain our method  we l  riefly des   ribe the con   ept of maximum entrol  y Recently  many al  lnoaches l  ased on the maximum entroi  y lnodel have t  een applied to natural language processing  
Note that this early discarding is related to ideas behind cube pruning   which generates the top n most promising hypotheses  but in our method the decision not to generate hypotheses is guided by the quality of hypotheses on the result stack 
We build a subset S C   incrementally by iterating to adjoin a feature f E   which maximizes loglikelihood of the model to S This algorithm is called the Basic Feature Selection  
From this point of view  some of the measures used in the evaluation of Machine Translation systems  such as BLEU   have been imported into the summarization task 
For the multilingual dependency parsing track  which was the other track of the shared task  Nilsson et al achieved the best performance using an ensemble method  
However  by exploiting the fact that the underlying scores assigned to competing hypotheses  w  e  h  f   vary linearly wrt changes in the weight vector  w   proposed a strategy for finding the global minimum along any given search direction 
11 However  modeling word order under translation is notoriously difficult   and it is unclear how much improvement in accuracy a good model of word order would provide 
 present a system called BABAR that uses contextual role knowledge to do coreference resolution 
The system described in  also makes use of syntactic heuristics 
     Dave et al 
Our MT baseline system is based on Moses decoder  with word alignment obtained from GIZA    
1 Motivation A major component in phrasebased statistical Machine translation  PBSMT   is the table of conditional probabilities of phrase translation pairs 
     Dave et al 
12 Related Work Recently  discriminative methods for alignment have rivaled the quality of IBM Model 4 alignments  
The disambiguation algorithms also require that the semantic relatedness measures WordNet   Similarity  be installed 
An analysis of the alignments shows that smoothing the fertility probabilities significantly reduces the frequently occurring problem of rare words forming garbage collectors in that they tend to align with too many words in the other language  
Moses provides BLEU  and NIST   but Meteor  and TER  can easily be used instead 
In an evaluation on the PENN treebank   the parser outperformed other unlexicalized PCFG parsers in terms of labeled bracketing fscore 
Since the word support model and triple context matching model have been proposed in our previous work  at the SIGHAN bakeoff 2005  and 2006   the major descriptions of this paper is on the WBT model 
612 ROUGE evaluation Table 4 presents ROUGE scores  of each of humangenerated 250word surveys against each other 
Two are conditionalized phrasal models  each EM trained until performance degrades  CJPTM3 as described in  Phrasal ITG as described in Section 41 Three provide alignments for the surface heuristic  GIZA   with growdiagfinal  GDF  Viterbi Phrasal ITG with and without the noncompositional constraint We use the Pharaoh decoder  with the SMT Shared Task baseline system  
These words and phrases are usually compiled using different approaches  Hatzivassiloglou and McKeown  1997  Kaji and Kitsuregawa  2006   and Nasukawa  2006  Esuli and Sebastiani  2006  Breck et al  2007  Ding  Liu and Yu 
3 Online Learning Again following   we have used the single best MIRA   which is a variant of the voted perceptron  for structured prediction 
CPSTM  i   l This metric corresponds to the STM metric presented by  
Our experiments created translation modules for two evaluation corpora  written news stories from the Penn Treebank corpus  and spoken taskoriented dialogues from the TRAINS93 corpus  
CIT  
31 A Note on StateSplits Recent studies  suggest that categorysplits help in enhancing the performance of treebank grammars  and a previous study on MH  outlines specific POStags splits that improve MH parsing accuracy 
Given this  the mutual information ratio  is expressed by Formula 1 
In addition to sentence fusion  compression algorithms  and methods for expansion of a multiparallel corpus  are other instances of such methods 
by diagand symmetrization  
There are many research directions  eg  sentiment classification  classifying an opinion document as positive or negative    subjectivity classification  determining whether a sentence is subjective or objective  and its associated opinion    featuretopicbased sentiment analysis  assigning positive or negative sentiments to topics or product features   Hu and Liu 2004  Popescu and Etzioni  2005  Carenini et al  2005  Ku et al  2006  Kobayashi  Inui and Matsumoto  2007  Titov and  
23 Online Learning Again following   we have used the single best MIRA   which is a margin aware variant of perceptron  for structured prediction 
Reported work includes improved model variants  and applications such as web data extraction   scientific citation extraction   word alignment   and discourselevel chunking  
They are not used in LN  but they are known to be useful for WSD  
3 Building the CatVar The CatVar database was developed using a combination of resources and algorithms including the Lexical Conceptual Structure  LCS  Verb and Preposition Databases   the Brown Corpus section of the Penn Treebank   an English morphological analysis lexicon developed for PCKimmo  Englex    NOMLEX   Longman Dictionary of Contemporary English 2For a deeper discussion and classification of Porter stemmers errors  see  
POS tag the text using the tagger of  
The learning algorithm used for each stage of the classification task is a regularized variant of the structured Perceptron  
An alternative is to create an automatic system that uses a set of training questionanswer pairs to learn the appropriate questionanswer matching algorithm  
 compared two Bayesian inference algorithms  Variational Bayes and what we call here a pointwise collapsed Gibbs sampler  and found that Variational Bayes produced the best solution  and that the Gibbs sampler was extremely slow to converge and produced a worse solution than EM 
  Pereira and Tishby   and Pereira  Tishby  and Lee  propose methods that derive classes from the distributional properties of the corpus itself  while other authors use external information sources to define classes  Resnik  uses the taxonomy of WordNet    uses the categories of Roget s Thesaurus  Slator  and Liddy and Paik  use the subject codes in the LDOCE  Luk  uses conceptual sets built from the LDOCE definitions 
For the Brown corpus  we based our division on  
Much of the work in subjectivity analysis has been applied to English data  though work on other languages is growing  eg  Japanese data are used in   Chinese data are used in   and German data are used in  
Morphosyntacticinformationhas in fact been shown to significantlyimprove the extractionresults  
For the chunk part of the code  we adopt the Inside  Outside  and Between  IOB  encoding originating from  
Following   Iusevariational Bayes EM  during the Mstep for the transition distribution  l 1 j i  f  E  ni  j   i  f  E  n i   C i   3  f  v   exp   v    4  60  v   braceleftBigg g  v 1 2  ifv  7  v  1  1v ow 
Our results are similar to those for conventional phrasebased models  
While Kazama and Torisawa used a chunker  we parsed the definition sentence using Minipar  
Perhaps the most wellknown method is maximum marginal relevance  MMR    as well as crosssentence informational subsumption   mixture models   subtopic diversity   diversity penalty   and others 
Before training the classifiers  we perform feature ablation by imposing a count cutoff of 10  and by limiting the number of features to the top 75K features in terms of log likelihood ratio  
32 The parsers The parsers that we chose to evaluate are the C&C CCG parser   the Enju HPSG parser   the RASP parser   the Stanford parser   and the DCU postprocessor of PTB parsers   based on LFG and applied to the output of the Charniak and Johnson reranking parser 
Training via the voted perceptron algorithm  or using a maxmargin criterion also correspond to the first option  eg McCallum and Wellner   Finley and Joachims   
Furthermore  early work on classbased language models was inconclusive  
Measures of crosslanguage relatedness are useful for a large number of applications  including crosslanguage information retrieval   crosslanguage text classification   lexical choice in machine translation   induction of translation lexicons   crosslanguage annotation and resource projections to a second language  
Related Work The recent availability of large amounts of bilingual data has attracted interest in several areas  including sentence alignment   word alignment   alignment of groups of words   and statistical translation  
51 The baseline System used for comparison was Pharaoh   which uses a beam search algorithm for decoding 
1 Introduction Base noun phrases  baseNPs   broadly the initial portions of nonrecursive noun phrases up to the head   are valuable pieces of linguistic structure which minimally extend beyond the scope of named entities 
As modern systems move toward integrating many features   resources such as this will become increasingly important in improving translation quality 
Metrics in the Rouge family allow for skip ngrams   Kauchak and Barzilay  take paraphrasing into account  metrics such as METEOR  and GTM  calculate both recall and precision  METEOR is also similar to SIA  in that word class information is used 
We then piped the text through a maximum entropy sentence boundary detector  and performed text normalization using NSW tools  
For English  we have used sections 0306 of the WSJ portion of the Penn Treebank  distributed by the Linguistic Data Consortium  LDC   which have frequently been used to evaluate sentence boundary detection systems before  compare Section 7 
In the concept extension part of our algorithm we adapt our concept acquisition framework  to suit diverse languages  including ones without explicit word segmentation 
  or  ST    where no labeled target domain data is available  eg 
Moreover  rather than predicting an intrinsic metric such as the PARSEVAL Fscore  the metric that the predictor learns to predict can be chosen to better fit the final metric on which an endtoend system is measured  in the style of  
A possible solution to his problem might be the use of more general morphological rules like those used in partofspeech tagging models  eg  1 2 3 4 530 40 50 60 70 80 90 100 level error RAND BASE Boost_S NNtfidf NB Boost_M Figure 6  Comparison of all models for a129 a48a51a95a66a97a98a97a180a222    where all suffixes up to a certain length are included 
4 Experiments The experiments described here were conducted using the Wall Street Journal Penn Treebank corpus  
Such studies follow the empiricist approach to word meaning summarized best in the famous dictum of the British 3 linguist JR Firth You shall know a word by the company it keeps Firth 1957 p 11 Context similarity has been used as a means of extracting collocations from corpora eg by Church & Hanks 1990 and by Dunning 1993 of identifying word senses eg by Yarowski 1995 and by Schutze 1998 of clustering verb classes eg by Schulte im Walde 2003 and of inducing selectional restrictions of verbs eg by Resnik 1993 by Abe & Li 1996 by Rooth et al
For both experiments  we used dependency trees extracted from the Penn Treebank  using the head rules and dependency extractor from Yamada and Matsumoto  2003  
411 Lexical cooccurrences Lexical cooccurrences have previously been shown to be useful for discourse level learning tasks  
Our evaluation metric is BLEU  
Similar to  eg    we use a Naive Bayes algorithm trained on word features cooccurring with the subjective and the objective classifications 
44 Experiment 2   s Words We also conducted translation on seven of the twelve English words studied in   
Researchers extracted opinions from words  sentences  and documents  and both rulebased and statistical models are investigated  
Since there is no wellagreed to definition of what an utterance is  we instead focus on intonational phrases   which end with an acoustically signaled boundary lone 
Running words 1864 14437 Vocabulary size 569 1081 Table 2  ChineseEnglish corpus statistics  using Phramer   a 3gram language model with KneserNey smoothing trained with SRILM  on the English side of the training data and Pharaoh  with default settings to decode 
These tags are drawn from a tagset which is constructed by 363 extending each argument label by three additional symbols a80a44a81a83a82a84a81a86a85  following  
The first SMT systems were developed in the early nineties  
Document level sentiment classification is mostly applied to reviews  where systems assign a positive or negative sentiment for a whole review document  
A description of the flat featurized dependencystyle syntactic representation we use is available in   which describes how the entire Penn Treebank  was converted to this representation 
According to this model  when translating a stringf in the source language into the target language  a string e is chosen out of all target language strings e if it has the maximal probability given f   e  arg maxe LCB Pr  e f  RCB  arg maxe LCB Pr  f e  Pr  e  RCB where Pr  f e  is the translation model and Pr  e  is the target language model 
 evaluates both estimation techniques on the Bayesian bitag model  Goldwater and Griffiths  emphasize the advantage in the MCMC approach of integrating out the HMM parameters in a tritag model  yielding a tagging supported by many different parameter settings 
Conjunctions are a major source of errors for English chunking as well  9  and we plan to address them in future work 
21 The Evaluator The evaluator is a function ptt s which assigns to each targettext unit t an estimate of its probability given a source text s and the tokens t which precede t in the current translation of s 1 Our approach to modeling this distribution is based to a large extent on that of the IBM group Brown et al  1993 but it differs in one significant aspect whereas the IBM model involves a noisy channel decomposition we use a linear combination of separate predictions from a language model ptlt  and a translation model ptls 
 shows that setting those weights should take into account the evaluation metric by which the MT system will eventually be judged 
1 word w 2 word bigram w1w2 3 singlecharacter word w 4 a word of length l with starting character c 5 a word of length l with ending character c 6 spaceseparated characters c1 and c2 7 character bigram c1c2 in any word 8 the first  last characters c1  c2 of any word 9 word w immediately before character c 10 character c immediately before word w 11 the starting characters c1 and c2 of two consecutive words 12 the ending characters c1 and c2 of two consecutive words 13 a word of length l with previous word w 14 a word of length l with next word w Table 1 Feature templates for the baseline segmentor 2 The Baseline System We built a twostage baseline system using the perceptron segmentation model from our previous work Zhang and Clark 2007 and the perceptron POS tagging model from Collins 2002
A number of systems for automatically learning semantic parsers have been proposed  
791 and score the alignment template models phrases  
The pervading method for estimating these probabilities is a simple heuristic based on the relative frequency of the phrase pair in the multiset of the phrase pairs extracted from the wordaligned corpus  
Metrics based on syntactic similarities such as the headword chain metric  HWCM   
4 Filtering with the CFG Rule Dictionary We use an idea that is similar to the method proposed by Ratnaparkhi  for partofspeech tagging 
 used the BaseNP tag set as presented in   I for inside a BaseNP  O for outside a BaseNP  and B for the first word in a BaseNP following another BaseNP 
After maximum BLEU tuning  on a heldout tuning set  we evaluate translation quality on a heldout test set 
SEPepsilon aA # epsilon  # aepsilon aepsilon bepsilon bB UNKepsilon cC bepsilon cBC e   E epsilon   depsilon depsilon epsilonepsilon bAB # bA # B # e   DE cepsilon dBCD e   DE Figure 1  Illustration of dictionary based segmentation finite state transducer 31 Bootstrapping In addition to the model based upon a dictionary of stems and words  we also experimented with models based upon character ngrams  similar to those used for Chinese segmentation  
The results evaluated by BLEU score  is shown in Table 2 
2 Evaluating Heterogeneous Parser Output Two commonly reported shallow parsing tasks are NounPhrase  NP  Chunking  and the CoNLL2000 Chunking task   which extends the NPChunking task to recognition of 11 phrase types1 annotated in the Penn Treebank 
There are many possible methods for combining unlabeled and labeled data   but we simply concatenate unlabeled data with labeled data to see the effectiveness of the selected reliable parses 
We ran the decoder with its default settings and then used Moses implementation of minimum error rate training  to tune the feature weights on the development set 
55 Dependency validity features Like   we extract the dependency path from the question word to the common word  existing in both question and sentence   and the path from candidate answer  such as CoNLL NE and numerical entity  to the common word for each pair of question and candidate sentence using Stanford dependency parser  
Assuming that the parameters P  etk fsk  are known  the most likely alignment is computed by a simple dynamicprogramming algorithm1 Instead of using an ExpectationMaximization algorithm to estimate these parameters  as commonly done when performing word alignment   we directly compute these parameters by relying on the information contained within the chunks 
We use binary Synchronous ContextFree Grammar  bSCFG   based on Inversion Transduction Grammar  ITG    to define the set of eligible segmentations for an aligned sentence pair 
1 Introduction Over the past decade  researchers at IBM have developed a series of increasingly sophisticated statistical models for machine translation  
Given phrase p1 and its paraphrase p2  we compute Score3  p1  p2  by relative frequency   Score3  p1  p2   p  p2 p1   count  p2  p1  P pprime count  pprime  p1   7  People may wonder why we do not use the same method on the monolingual parallel and comparable corpora 
These include cube pruning   cube growing   early pruning   closing spans   coarsetofine methods   pervasive laziness   and many more 
In recent years  sentiment classification has drawn much attention in the NLP field and it has many useful applications  such as opinion mining and summarization  
To counteract this  we introduce two brevity penalty measures  BP  inspired by BLEU  which we incorporate into the loss function  using a product  loss  1PrecBP  BP1  exp  1max  1  rc    6  BP2  exp  1max  cr  rc   where r is the reference length and c is the candidate length 
129 5 Active learning Whereas a passive supervised learning algorithm is provided with a collection of training examples that are typically drawn at random  an active learner has control over the labeled data that it obtains  
The Penn Treebank documentation  defines a commonly used set of tags 
One of the main directions is sentiment classification  which classifies the whole opinion document  eg  a product review  as positive or negative  
Finally  we are investigating several avenues for using this system output for Machine Translation  MT  including   1  aiding word alignment for other MT system   and  2  aiding the creation various MT models involving analyzed text  eg   
Hence we use a beamsearch decoder during training and testing  our idea is similar to that of  who used a beamsearch decoder as part of a perceptron parsing model 
22 Corpus occurrence In order to get a feel for the relative frequency of VPCs in the corpus targeted for extraction namely 0 5 10 15 20 25 30 35 40 0 10 20 30 40 50 60 70 VPC types  Corpus frequency Figure 1 Frequency distribution of VPCs in the WSJ Tagger correctextracted Prec Rec Ffl1 Brill 135135 1000 0177 0301 Penn 667800 0834 0565 0673 Table 1 POSbased extraction results the WSJ section of the Penn Treebank we took a random sample of 200 VPCs from the Alvey Natural Language Tools grammar Grover et al  1993 and did a manual corpus search for each
However  the pb features yields no noticeable improvement unlike in prefect lexical choice scenario  this is similar to the findings in  
 gave a systematic examination of the efficacy of unigram  bigram and trigram features drawn from different representations surface text  constituency parse tree and dependency parse tree 
Consider the lexical model pw  ry rx   defined following   with a denoting the most frequent word alignment observed for the rule in the training set 
Statistical Model In SIFTs statistical model  augmented parse trees are generated according to a process similar to that described in  
35 Regularization We apply lscript1 regularization Ng 2004 Gao et al 2007 to make learning more robust to noise and control the effective dimensionality of the feature spacebysubtractingaweightedsumofabsolutevalues of parameter weights from the loglikelihood of the training data w  argmaxw LLw summationdisplay i Ciwi 6 We optimize the objective using a variant of the orthantwise limitedmemory quasiNewton algorithm proposed by Andrew & Gao 20073 All values Ci are set to 1 in most of the experiments below although we apply stronger regularization Ci  3 to reordering features
Our method does not suppose a uniform distribution over all possible phrase segmentationsas  since each phrase tree has a probability 
After this conversion  we had 1000 positive and 1000 negative examples for each domain  the same balanced composition as the polarity dataset  
The MT community has developed not only an extensive literature on alignment   but also standard  proven alignment tools such as GIZA    
51 Evaluation of Translation Translations are evaluated on two automatic metrics  Bleu  and PER  position independent errorrate  
These methods have been used in machine translation   terminology research and translation aids   bilingual lexicography   collocation studies   wordsense disambiguation  and information retrieval in a multilingual environment  
For instance  for Maximum Entropy  I picked  for the basic theory   for an application  POS tagging in this case   and  for more advanced topics such as optimization and smoothing 
In comparison with shallow semantic analysis tasks such as wordsense disambiguation Ide and Jeaneronis 1998 and semantic role labeling Gildea and Jurafsky 2002 Carreras and M`arquez 2005 which only partially tackle this problem by identifying the meanings of target words or finding semantic roles of predicates semantic parsing Kate et al  2005 Ge and Mooney 2005 Zettlemoyer and Collins 2005 pursues a more ambitious goal  mapping natural language sentences to complete formal meaning representations MRs where the meaning of each part of a sentence is analyzed including noun phrases verb phrases negation quantifiers and so on
The kappa value  was used to evaluate the agreement among the judges and to estimate how difficult the evaluation task was 
Table lookup using an explicit translation lexicon is sufficient and preferable for many multilingual NLP applications  including ` crummy  MT on the World Wide Web   certain machineassisted translation tools  eg 
The local dependencies between sentiment labels on sentences is similar to the work of  where soft local consistency constraints were created between every sentence in adocument and inference wassolved using a mincut algorithm 
Training Set  Labeled English Reviews   There are many labeled English corpora available on the Web and we used the corpus constructed for multidomain sentiment classification  9  because the corpus was largescale and it was within similar domains as the test set 
Atthefinestlevel  thisinvolvesthealignment of words and phrases within two sentences that are known to be translations  
The parser is coupled with an online averaged perceptron  as the learning method 
Statistical parsers have been developed for TAG   LFG   and HPSG   among others 
For the give source text  S  it finds the most probable alignment set  A  and target text  T  Aa SaTpSTp       1  Brown  proposed five alignment models  called IBM Model  for an EnglishFrench alignment task based on equa68 tion  1  
Large volumes of training data of this kind are indispensable for constructing statistical translation models   acquiring bilingual lexicon   and building examplebased machine translation  EBMT  systems  
We use the default configuration of the measure in WordNet   Similarity012 package   and  with a single exception  the measure performed below Gic  see BP in table 1 
The previous studies  with the exception of   used smaller gazetteers than ours 
This approach is similar to that of seed words  eg    or hook words  eg    in previous work 
The features we use are shown in Table 2  which are based on the features used by  and Uchimoto et al 
Consequently  here we employ multiple references to evaluate MT systems like BLEU  and NIST  
31 A simple solution  suggests that in order to have an ITG take advantage of a known partial structure  one can simply stop the parser from using any spans that would violate the structure 
Similarly   propose a relative distortion model to be used with a phrase decoder An extension to WordNet was presented by  Discovering orientations of context dependent opinion comparative words is related to identifying domain opinion words  
Given a weight vector w  the score wf  x  y  ranks possible labelings of x  and we denote by Yk  w  x  the set of k top scoring labelings for x We use the standard B  I  O encoding for named entities  
To use the data from NANC  we use selftraining  
2 Related work  recently advocated the need for a uniform approach to corpusbased semantic tasks 
Similar to work in image retrieval   we cast the problem in terms of Machine Translation  given a paired corpus of words and a set of video event representations to which they refer  we make the IBM Model 1 assumption and use the expectationmaximization method to estimate the parameters      m j ajm jvideowordpl Cvideowordp 1    1     1  This paired corpus is created from a corpus of raw video by first abstracting each video into the feature streams described above 
In this paper  sentence pairs are extracted by a simple model that is based on the socalled IBM Model1  
The piecewise linearity observation made in  is no longer applicable since we can not move the log operation into the expected value 
Table 4 shows the linguistic features of the resulting model compared to the models of Carroll and Rooth     and Charniak  2000  
2 Related Work There has been a large and diverse body of research in opinion mining  with most research at the text   sentence  or word  level 
A hierarchical alignment algorithm is a type of synchronous parser where  instead of constraining inferences by the production rules of a grammar  the constraints come from word alignments and possibly other sources  
1 Introduction on measures for interrater reliability   on frameworks for evaluating spoken dialogue agents  and on the use of different corpora in the development of a particular system  The CarnegieMellon Communicator  Eskenazi et al 
In the future  we will experiment with semantic  rather than positional  clustering of premoditiers  using techniques such as those proposed in  
Our learning method is an extension of Collinss perceptronbased method for sequence labeling  
These feature vectors and the associated parser actions are used to train maximum entropy models  
prime 1 1 1 05 05 1 05 05 01 01 01 00001 00001 01 00001 00001 Further  we ran each setting of each estimator at least 10 times  from randomly jittered initial starting points  for at least 1000 iterations  as  showed that some estimators require many iterations to converge 
We use the discriminative perceptron learning algorithm  to train the values of vectorw 
7 Experiments To show the effectiveness of crosslanguage mention propagation information in improving mention detection system performance in Arabic  Chinese and Spanish  we use three SMT systems with very competitive performance in terms of BLEU11  
The tagger described in this paper is based on the standard Hidden Markov Model architecture  
We measured associations using the loglikelihood measure  for each combination of target category and semantic class by converting each cell of the contingency into a 22 contingency table 
32 Rare Word Accuracy For these experiments  we use the Wall Street Journal portion of the Penn Treebank  
 extracts rules from nonanaphoric noun phrases and noun phrases patterns  which are then applied to test data to identify existential noun phrases 
2 Previous Approaches  method of estimating phrasetranslation probabilities is very simple 
The form of the maximum entropy probability model is identical to the one used in   k f   wiwi1  wi2  at  ri  YIj  I Otj p  wilwil  wi2  attri   Z  Wil  wi2  attri  k to t j  l where wi ranges over V t3 stop 
Instead  researchers routinely use automatic metrics like Bleu  as the sole evidence of improvement to translation quality 
Rapp     but using cosine rather than cityblock distance to measure profile similarity 
32 Maximum Entropy ME models implement the intuition that the best model will be the one that is consistent with the set of constrains imposed by the evidence  but otherwise is as uniform as possible  
The first model  referred to as Maxent1 below  is a loglinear combination of a trigram language model with a maximum entropy translation component that is an analog of the IBM translation model 2  
In Yarowsky s experiment   an average of 3936 examples were used to disambiguate between two senses 
Err510
For Hw6  students compared their POS tagging results with the ones reported in  
A statistical language model a lexicalized PCFG  is derived from the analysis grammar by processing a corpus using the same grammar with no statistical model and recording frequencies of substructures built by each rule 
We use GIZA    to do mton wordalignment and adopt heuristic growdiagfinaland to do refinement 
The surface heuristic can define consistency according to any word alignment  but most often  the alignment is provided by GIZA    
154 2 Translation Models 21 Standard Phrasebased Model Most phrasebased translation models  rely on a preexisting set of wordbased alignments from which they induce their parameters 
Overall  agreement among judges for 250 propositions 601 A commonly used metric for evaluating interrater reliability in categorization of data is the kappa statistic  
A more optimistic view can be found in   they argue that a near100  interjudge agreement is possible  provided the partofspeech annotation is done carefully by experts 
From the extracted ngrams  those with a flequc ` ncy of 3 or more were kept  other approaches get rid of ngrams of such low frequencies   
This was expected  as it has been observed before that very simple smoothing techniques can perform well on large data sets  such as web data  
On the other hand   proposed an algorithm  borrowed to the field of dynamic programming and based on the output of their previous work  to find the best alignment  subject to certain constraints  between words in parallel sentences 
21 Minimum Error Rate Training The predominant approach to reconciling the mismatch between the MAP decision rule and the evaluation metric has been to train the parameters of the exponential model to correlate the MAP choice with the maximum score as indicated by the evaluation metric on a development set with known references  
In earlier work  only singletons were used as seed words  varying their number allows us to test whether multiple seed words have a positive effect in detection performance 
stituent alignments  
Many strategies have been proposed to integrate morphology information in SMT  including factored translation models   adding a translation dictionary containing inflected forms to the training data   entirely replacing surface forms by representations built on lemmas and POS tags   morphemes learned in an unsupervised manner   and using Porter stems and even 4letter prefixes for word alignment  
Manually defined heuristics are used to automatically annotate each tree in the treebank with partially specified HPSG derivation trees Headargumentmodifier distinctions are made for each node in the tree based on Magerman 1994 and Collins 1997 336 ODonovan et al LargeScale Induction and Evaluation of Lexical Resources the whole tree is then converted to a binary tree heuristics are applied to deal with phenomena such as LDDs and coordination and to correct some errors in the treebank and finally an HPSG category is assigned to each node in the tree in accordance with its CFG category
A synchronous 363 binarization method is proposed in  whose basic idea is to build a leftheavy binary synchronous tree  with a lefttoright shiftreduce algorithm 
This hypothesized relationship between distributional similarity and semantic similarity has given rise to a large body of work on automatic thesaurus generation  
The details of the algorithm can be found in the literature for statistical translation models  such as  
For comparing the sentence generator sample to the English sample  we compute loglikelihood statistics  on neighboring words that at least cooccur twice 
To support distributed computation   we further split the Ngram data into shards by hash values of the first bigram 
See  for an application of the boosting approach to named entity recognition  and Walker  Rambow  and Rogati  for the application of boosting techniques for ranking in the context of natural language generation 
The first stage parser is a bestfirst PCFG parser trained on sections 2 through 22  and 24 of the Penn WSJ treebank  
35 The Experiments We have ran LexTract on the onemillionword English Penn Treebank  and got two Treebank grammars 
The results are comparable to other results reported using the InsideOutside method   see Table 7 
Meanwhile  it is common for NP chunking tasks to represent a chunk  eg  NP  with two labels  the begin  eg  BNP  and inside  eg  INP  of a chunk  
Portage is a statistical phrasebased SMT system similar to Pharaoh  
Like   we used mutual information to measure the cohesion between two words 
Workshop Towards GenreEnabled Search Engines booktitle pages 13  20 pages editor In G Rehm and M Santini  editors editor contexts context ork on an intradocument  or page segment level because a single document can contain instances of multiple genres  eg  contact information  list of publications  CV  see  
Again  we find the clearest patterns in the graphs for precision  where Malt has very low precision near the root but improves with increasing depth  while MST shows the opposite trend  
Fortunately  using distributional characteristics of term contexts  it is feasible to induce partofspeech categories directly from a corpus of suf cient size  as several papers have made clear  
Abney  notes important problems with the soundness of the approach when a unificationbased grammar is actually determining the derivations  motivating the use of loglinear models  for parse ranking that Johnson and colleagues further developed  
6 Discourse Context  pointed out that the sense of a target word is highly consistent within any given document  one sense per discourse  
3 Model As an extension to commonly used lexical word pair probabilities p  f e  as introduced in   we define our model to operate on word triplets 
To derive the joint counts c   s   t  from which p   s  t  and p   t  s  are estimated  we use the phrase induction algorithm described in   with symmetrized word alignments generated using IBM model 2  
We utilise the automatic annotation algorithm of  to derive a version of PennII where each node in each tree is annotated with an LFG functional annotation  ie an attribute value structure equation  
Most previous work with CRFs containing nonlocal dependencies used approximate probabilistic inference techniques  including TRP  and Gibbs sampling  
We tokenized sentences using the standard treebank tokenization script  and then we performed partofspeech tagging using MXPOST tagger  
Training of the phrase translation model builds on top of a standard statistical word alignment over the training corpus of parallel text  for identifying corresponding word blocks  assuming no further linguistic analysis of the source or target language 
Section 7 considers recent efforts to induce effective procedures for automated sense labeling of discourse relations that are not lexically marked  
However  most of them fail to utilize nonsyntactic phrases well that are proven useful in the phrasebased methods  
Words are encoded through an automatic clustering algorithm  while tags  labels and extensions are normally encoded using diagonal bits 
4 Features For our experiments we use the features proposed  motivated and described in detail by  
 presented a thorough discussion on the Yarowsky algorithm 
In the field of statistical analysis of natural language data  it is common to use measures of lexical association  such as the informationtheoretic measure of mutual information  to extract useful relationships between words  eg   
We hence chose transformationbased learning to create this  shallow  segmentation grammar  converting the segmentation task into a tagging task  as is done in 85   inter alia  
In particular  previous work  has investigated the use of Markov random fields  MRFs  or loglinear models as probabilistic models with global features for parsing and other NLP tasks 
 reports results for different numbers of hidden states but it is unclear how to make this choice a priori  while Goldwater & Griffiths  leave this question as future work 
1 Introduction Word Sense Disambiguation  WSD  competitions have focused on general domain texts  as attested in the last Senseval and Semeval competitions  
Historybased models for predicting the next parser action  3 
33 Features Similar to the default features in Pharaoh   we used following features to estimate the weight of our grammar rules 
Previous research has addressed revision in singledocument summaries   and has suggested that revising summaries can make them more informative and correct errors 
2 Three New Features for MT Evaluation Since our sourcesentence constrained ngram precision and discriminative unigram precision are both derived from the normal ngram precision  it is worth describing the original ngram precision metric  BLEU  
This concept of alignment has been also used for tasks like authomatic vocabulary derivation and corpus alignment  
We use the Stanford parser  with its default Chinese grammar  the GIZA    alignment package with its default settings  and the ME tool developed by  
The idea is that the translation of a sentence x into a sentence y can be performed in the following steps1   a  If x is small enough  IBMs model 1  is employed for the translation 
1 A cept is defined as the set of target words connected to a source word  
Measures of attributional similarity have been studied extensively  due to their applications in problems such as recognizing synonyms   information retrieval   determining semantic orientation   grading student essays   measuring textual cohesion   and word sense disambiguation  
Note that the algorithm from  was designed for discriminatively training an HMMstyle tagger 
In this years shared task we evaluated a number of different automatic metrics  Bleu  Bleu remains the de facto standard in machine translation evaluation 
In computational linguistics  our pattern discovery procedure extends over previous approaches that use surface patterns as indicators of semantic relations between nouns or verbs   inter alia  
SMT has evolved from the original wordbased approach  into phrasebased approaches  and syntaxbased approaches  
Various clustering techniques have been proposed  which perform automatic word clustering optimizing a maximumlikelihood criterion with iterative clustering algorithms 
   and Lee   Wilson et al 
The approach is able to achieve 94  precision and recall for base NPs derived from the Penn Treebank Wall Street Journal  
orgpubscitations  j ournalstoms1986 12 2  p154meht a  Mutual Information Given the definition of Mutual Information   I  x  y   log 2 P  x  y  P  x  P  y   we consider the distribution of a window word according to the contingency table  a  in Table 4 
Parse selection constitutes an important part of many parsing systems  
To generate phrase pairs from a parallel corpus  we use the ` diagand  phrase induction algorithm described in   with symmetrized word alignments generated using IBM model 2  
In NLP community  it has been shown that having more data results in better performance  
Such a coding procedure covers  for example  how segmentation of a corpus is performed  if multiple tagging is allowed and if so  is it unlimited or are there just certain combinations of tags not allowed  is look ahead permitted  etc For further information on coding procedures we want to refer to  and for good examples of coding books see  for example      or  
2 Statistical Word Alignment According to the IBM models   the statistical word alignment model can be generally represented as in Equation  1  
2 Data 21 The US Congressional Speech Corpus The text used in the experiments is from the United States Congressional Speech corpus   which is an XML formatted version of the electronic United States Congressional Record from the Library of Congress1 
Among the four steps  the hypothesis alignment presents the biggest challenge to the method due to the varying word orders between outputs from different MT systems  
 has proposed a bootstrapping method for word sense disambiguation 
Language modeling   nounclustering   constructing syntactic rules for SMT   and finding analogies  are examples of some of the problems where we need to compute relative frequencies 
This has been now an active research area for a couple of decades  
Giza   is a freely available implementation of IBM Models 15  and the HMM alignment   along with various improvements and modifications motivated by experimentation by Och & Ney  
Although the BLEU  score from Finnish to English is 218  the score in the reverse direction is reported as 130 which is one of the lowest scores in 11 European languages scores  
 for English  but not identical to strictly anaphoric ones5   since a nonanaphoric NP can corefer with a previous mention 
We report case sensitive Bleu  scoreBleuCforallexperiments 
Using GIZA   model 4 alignments and Pharaoh   we achieved a BLEU score of 03035 
Making such an assumption is reasonable since POS taggers that can achieve accuracy of 96  are readily available to assign POS to unrestricted English sentences  
3 OverviewofExtractionWork 31 English As one mightexpect  the bulk of the collocation extractionwork concernsthe English language    amongmany others1 
6 Experiments We evaluated the translation quality of the system using the BLEU metric  
The loglinear model feature weights were learned using minimum error rate training  MERT   with BLEU score  as the objective function 
Sentiment classification at the sentencelevel has also been studied  
This difference was highlighted in the 3http    w3msivxusejhamaltparser  studyof   whichshowed that the difference is reflected directly in the error distributions of the parsers 
43 Relaxing Length Restrictions Increasing the maximum phrase length in standard phrasebased translation does not improve BLEU  
The importance of including single nonheadwords is now also uncontroversial   and the current paper has shown the importance of including two and more nonheadwords 
We annotated with the BIO tagging scheme used in syntactic chunkers  
We follow the approach of bootstrapping from a model with a narrower parameter space as is done in  eg Och and Ney  and  
6 Results We trained on the standard Penn Treebank WSJ corpus  
For instance  BLEU and ROUGE  are based on ngram precisions  METEOR  and STM  use wordclass or structural information  Kauchak  2006  leverages on paraphrases  and TER  uses editdistances 
Rulesize and lexicalization affect parsing complexity whether the grammar is binarized explicitly  or implicitly binarized using Earlystyle intermediate symbols  
Many methods for calculating the similarity have been proposed  
This idea of employing ngram cooccurrence statistics to score the output of a computer system against one or more desired reference outputs has its roots in the BLEU metric for machine translation  and the ROUGE  metric for summarization 
Nakagawa  and   also showed the effectiveness of global features in improving the accuracy of graphbased parsing  using the approximate Gibbs sampling method and a reranking approach  respectively 
One interesting approach to extending the current system is to introduce a statistical translation model  to filter out irrelevant translation candidates and to extract the most appropriate subpart from a long English sequence as the translation by locally aligning the Japanese and English sequences 
294 Fraser and Marcu Measuring Word Alignment Quality for Statistical Machine Translation 22 Measuring Translation Performance Changes Caused By Alignment In phrasedbased SMT  the knowledge sources which vary with the word alignment are the phrase translation lexicon  which maps source phrases to target phrases using counts from the word alignment  and some of the word level translation parameters  sometimes called lexical smoothing  
Such techniques are currently being applied in many areas  including language identification  authorship attribution   text genre classification   topic identification   and subjective sentiment classification  
or cooking  which agrees with the knowledge presented in previous work  
This ITG constraint is characterized by the two forbidden structures shown in Figure 1  
One is distortion model  which penalizes translations according to their jump distance instead of their content 
 ii  Apply some statistical tests such as the Binomial Hypothesis Test  and loglikelihood ratio score  to SCCs to filter out false SCCs on the basis of their reliability and likelihood 
Second  it can be applied to control the quality of parallel bilingual sentences mined from the Web  which are critical sources for a wide range of applications  such as statistical machine translation  and crosslingual information retrieval  
For example  the word alignment computed by GIZA   and used as a basis to extract the TTS templates in most SSMT systems has been observed to be a problem for SSMT   due to the fact that the wordbased alignment models are not aware of the syntactic structure of the sentences and could produce many syntaxviolating word alignments 
Baseline We use the Moses MT system  as a baseline and closely follow the example training procedure given for the WMT07 and WMT08 shared tasks4 In particular  we perform word alignment in each direction using GIZA     apply the growdiagfinaland heuristic for symmetrization and use a maximum phrase length of 7 
2 Related Work This method is similar to blockorientation modeling  and maximum entropy based phrase reordering model   in which local orientations  leftright  of phrase pairs  blocks  are learned via MaxEnt classifiers 
The straightforward way is to first generate the best BTG tree for each sentence pair using the way of   then annotate each BTG node with linguistic elements by projecting sourceside syntax tree to BTG tree  and finally extract rules from these annotated BTG trees 
Measurement of Beliability The Kappa Statistic Following Jean   we use the kappa statistic  to measure degree of agreement among subjects 
As  point out  WordNet does not encode antonymy across partofspeech  for example  legallyembargo  
In comparison  we deployed the GIZA   MT modeling tool kit  which is an implementation of the IBM Models 1 to 4  
This is the shared task baseline system for the 2006 NAACLHLT workshop on statistical machine translation  and consists of the Pharaoh decoder   SRILM   GIZA     mkcls   Carmel 1 and a phrase model training code 
 claimed that this approximation achieved essentially equivalent performance to that obtained when directly using the loss as the objective  O  lscript 
There also have been prior work on maintaining approximate counts for higherorder language models  LMs     operates under the model that the goal is to store a compressed representation of a diskresident table of counts and use this compressed representation to answer count queries approximately 
Therefore the probability of alignment aj for position j should have a dependence on the previous alignment position O j_l  P    j    j1  A similar approach has been chosen by  and  
54 Domain Adaptation 541 FeatureBased Approaches Onewayofadaptingalearnertoanewdomainwithout using any unlabeled data is to only include features that are expected to transfer well  
Our test set is 3718 sentences from the English Penn treebank  which were translated into German 
Given a set of evidences E over all the relevant word pairs  in   the probabilistic taxonomy learning task is defined as the problem of finding the taxonomy hatwideT that maximizes the 67 probability of having the evidences E  ie  hatwideT  arg max T P  E T  In   this maximization problem is solved with a local search 
Interannotator agreement was measured using the kappa  K  statistics  on 1502 instances  three Switchboard dialogues  marked by two annotators who followed specific written guidelines 
 noted that the unigram unpredictable might have a positive sentiment in a movie review  eg unpredictable plot   but could be negative in the review of an automobile  eg unpredictable steering  
To prune away those pairs  we used the loglikelihoodratio algorithm  to compute the degree of association between the verb and the noun in each pair 
Probabilistic generative models like IBM 15 Brown et al 1993 HMM Vogel et al 1996 ITG Wu 1997 and LEAF Fraser and Marcu 2007 define formulas for Pf  e or Pe f with okvoon ororok sprok atvoon bichat dat erok sprok izok hihok ghirok totat dat arrat vat hilat okdrubel okvoon anok plok sprok atdrubel atvoon pippat rrat dat okvoon anok drok brok jok atvoon krat pippat sat lat wiwok farok izok stok totat jjat quat cat lalok sprok izok jok stok wat dat krat quat cat lalok farok ororok lalok sprok izok enemok wat jjat bichat wat dat vat eneat lalok brok anok plok nok iat lat pippat rrat nnat wiwok nok izok kantok okyurp totat nnat quat oloat atyurp lalok mok nok yorok ghirok clok wat nnat gat mat bat hilat lalok nok crrrok hihok yorok zanzanok wat nnat arrat mat zanzanat lalok rarok nok izok hihok mok wat nnat forat arrat vat gat Figure 1 Word alignment exercise Knight 1997
Still  however  such techniques often require seeds  or prototypes  cf    which are used to prune search spaces or direct learners 
As resolving direct anaphoric descriptions  the ones where anaphor and antecedent have the same head noun  is a much simpler problem with high performance rates as shown in previous results   these heuristics should be applied first in a system that resolves definite descriptions 
et al  2004  CollinsThompson and Callan  2005   and Ramage  2007  
We benchmark our results against a model  Hiero  which was directly trained to optimise BLEUNIST using the standard MERT algorithm  and the full set of translation and lexical weight features described for the Hiero model  
Although to a lesser extent  measures of word relatedness have also been applied on other languages  including German   Chinese   Dutch  and others 
This method is very similar to some ideas in domain adaptation   but we argue that the underlying problems are quite different 
This wrong translation of content words is similar to the incorrect omission reported in   which both hurt translation adequacy 
1 Introduction In global linear models  GLMs  for structured prediction   eg     the optimal label y for an input x is y  arg max yY  x  w f  x  y   1  where Y  x  is the set of possible labels for the input x  f  x  y  Rd is a feature vector that represents the pair  x  y   and w is a parameter vector 
We use the same preprocessing steps as Turian and Melamed   during both training and testing  the parser is given text POStagged by the tagger of   with capitalization stripped and outermost punctuation removed 
2 Phrasebased SMT We use a phrasebased SMT system  Pharaoh    which is based on a loglinear formulation  
To quickly  and approximately  evaluate this phenomenon  we trained the statistical IBM wordalignment model 4  1 using the GIZA   software  for the following language pairs  ChineseEnglish  Italian English  and DutchEnglish  using the IWSLT2006 corpus  for the first two language pairs  and the Europarl corpus  for the last one 
Alignment spaces can emerge from generative stories   from syntactic notions   or they can be imposed to create competition between links  
1 Introduction Many different statistical tests have been proposed to measure the strength of word similarity or word association in natural language texts  
For example  it has been observed that texts often contain multiple opinions on different topics   which makes assignment of the overall sentiment to the whole document problematic 
In each experiment  performance IMutu   d Information provides an estimate of the magnitude of the ratio t  ctw    n the joint prol  ability P  verbnoun 1  reposition   and the joint probability a  suming indcpendcnce P  verbnoun  P  prcl  osition  s      
We use the IBM Model 1   uniform distribution  and the Hidden Markov Model  HMM  firstorder dependency    to estimate the alignment model 
The parameters  j  were trained using minimum error rate training  to maximize the BLEU score  on a 150 sentence development set 
For these first SMT systems  translationmodel probabilities at the sentence level were approximated from wordbased translation models that were trained by using bilingual corpora  
 binarize grammars into CNF normal form  while  allow only GriebachNormal form grammars 
Equation  10  is of interest because the ratio p  C v  r   p  C r  can be interpreted as a measure of association between the verb v and class C This ratio is similar to pointwise mutual information  and also forms part of Resniks association score  which will be introduced in Section 6 
Recently  it has gained renewed attention as empirical methods in parsing have emphasized the importance of relations between words  see  eg     which is what dependency grammars model explicitly  but contextfree phrasestructure grammars do not 
It is clear that Appendix B contains far fewer true noncompositional phrases than Appendix A 7 Related Work There have been numerous previous research on extracting collocations from corpus  eg   and  
model reranking has also been established  both for synchronous binarization  and for targetonly binarization  
From wordlevel alignments  such systems extract the grammar rules consistent either with the alignments and parse trees for one of languages   or with the the wordlevel alignments alone without reference to external syntactic analysis   which is the scenario we address here 
A remedy is to aggressively limit the feature space  eg to syntactic labels or a small fraction of the bilingual features available  as in   but that reduces the benefit of lexical features 
 Classification allows a word to align with a target word using the collective translation tendency of words in the same class 
51 The Prague Dependency Tree Bank  PDT in the sequel   which has been inspired by the buildup of the Penn Treebank   is aimed at a complex annotation of  a part of  the Czech National Corpus  CNC in the sequel   the creation of which is under progress at the Department of Czech National Corpus at the Faculty of Philosophy  Charles University  the corpus currently comprises about 100 million tokens of word forms  
The overall POS tag distribution learned by EM is relatively uniform  as noted by   and it tends to assign equal number of tokens to each tag label whereas the real tag distribution is highly skewed 
1 Introduction Since 1995  a few statistical parsing algorithms  demonstrated a breakthrough in parsing accuracy  as measured against the University of Pennsylvania TREEBANK as a gold standard 
The results are quite promising  our extraction method discovered 89  of the WordNet cousins  and the sense partitions in our lexicon yielded better values  than arbitrary sense groupings on the agreement data 
We use the following features for our rules  sourceand targetconditioned neglog lexical weights as described in Koehn et al  2003b  neglog relative frequencies lefthandsideconditioned targetphraseconditioned sourcephraseconditioned  Counters no rule applications no target words  Flags IsPurelyLexical ie  contains only terminals IsPurelyAbstract ie  contains only nonterminals IsXRule ie  nonsyntactical span IsGlueRule 139  Penalties rareness penalty exp1  RuleFrequency unbalancedness penalty MeanTargetSourceRatio  no source words no target words 4 Parsing Our SynCFG rules are equivalent to a probabilistic contextfree grammar and decoding is therefore an application of chart parsing
Algorithm 1 SCL  1  Select m pivot features 
 applies this approach to the socalled IBM Candide system to build context dependent models  compute automatic sentence splitting and to improve word reordering in translation 
The techniques examined are Structural Correspondence Learning  SCL   and Selftraining  
The perceptron has been used in many NLP tasks  such as POS tagging   Chinese word segmentation  and so on 
Presently  many systems        focus on online recognition of proper nouns  and have achieved inspiring results in newscorpus but will be deteriorated in special text  such as spoken corpus  novels 
For MCE learning  we selected the reference compression that maximize the BLEU score    argmax rR BLEU  r  R r   from the set of reference compressions and used it as correct data for training 
In addition to adapting the idea of Head Word Chains   we also compared the input sentences argument structures against the treebank for certain syntactic categories 
312 Kappa Kappa  is an evaluation measure which is increasingly used in NLP annotation work  
1 Introduction Texttotext generation is an emerging area of research in NLP  
Having a single  canonical tree structure for each possible alignment can help when flattening binary trees  as it indicates arbitrary binarization decisions  
We use GIZA    to train generative directed alignment models  HMM and IBM Model4  from training recordtext pairs 
 introduced a transformationbased learning method which considered chunking as a kind of tagging problem 
The Penn Treebank annotation  was chosen to be the first among equals  it is the starting point for the merger and data from other annotations are attached at tree nodes 
We can then use this newly identified set to   1  use  s method to find the orientation for the terms and employ the terms and their scores in a classifier  and  2  use  s method to find the orientation for the terms and add the new terms as additional seed terms for a second iteration As opposed to    we do not use the web as a resource to find associations  rather we apply the method directly to indomain data 
The second uses Lin dependency similarity  a syntacticdependency based distributional word similarity resource described in  9 
A monotonous segmentation copes with monotonous alignments  that is  j  k aj  ak following the notation of  
We also can not use prior graph construction methods for the document level  such as physical proximity of sentences  used in   at the word sense level 
The model scaling factors are optimized on the development corpus with respect to mWER similar to  
For these classications  we calculated a kappa statistic of 0528  
Feature weights of both systems are tuned on the same data set3 For Pharaoh  we use the standard minimum errorrate training   and for our system  since there are only two independent features  as we always fix  1   we use a simple gridbased lineoptimization along the languagemodel weight axis 
In most statistical machine translation  SMT  models   some of measure words can be generated without modification or additional processing 
The resulting corpus contains 385 documents of American English selected from the Penn Treebank   annotated in the framework of Rhetorical Structure Theory 
We solve SAT analogies with a simplified version of the method of  
Also  the aspect of generalizing features across different products is closely related to fully supervised domain adaptation   and we plan to combine our approach with the idea from Daume III  2007  to gain insights into whether the composite backoff features exhibit different behavior in domaingeneral versus domainspecific feature subspaces 
1 Introduction The task of sentence compression  or sentence reduction  can be defined as summarizing a single sentence by removing information from it  
Because of these kinds of results  the vast majority of statistical parsing work has focused on parsing as a supervised learning problem  
5 The SemCor collection  is a subset of the Brown Corpus and consists of 352 news articles distributed into three sets in which the nouns  verbs  adverbs  and adjectives have been manually tagged with their corresponding WordNet senses and partofspeech tags using Brills tagger  
There are also approaches to anaphora resolution using unsupervised methods to extract useful information  such as gender and number   or contextual roleknowledge  
 discuss the influence of bias towards highor lowfrequency items for different tasks  correlation with WordNetderived neighbor sets and pseudoword disambiguation   and it would not be surprising if the different highfrequency bias were leading to different results 
Some of these have been previously employed for various tasks by Gabrilovich and Markovitch    Overell and Ruger     and Suchanek et al 
Model 4 of  is also a firstorder alignment model  along the source positions  like the HMM  trot includes also fertilities 
 ie   ll  Lj   maz  zi  j  u   i  I where xi  j  u  E Qi and max  xi  j  u   is the highest score in the line of the matrix Qi which corresponds to the head word sense j n is the number of modifiers of the head word h at the current tree level  and k i Lj  j  l Lj where k is the number of senses of the head word h The reason why gj  I0  is calculated as a sum of the best scores  ll   rather than by using the traditional maximum likelihood estimate   Gah eta  
SIGHAN  the Special Interest Group for Chinese Language Processing of the Association for Computational Linguistics  conducted three prior word segmentation bakeoffs  in 2003  2005 and 2006   which established benchmarks for word segmentation and named entity recognition 
We show translation results in terms of the automatic BLEU evaluation metric  on the MT03 ArabicEnglish DARPA evaluation test set consisting of a212a89a212a89a87 sentences with a98a89a212a161a213a89a214a89a215 Arabic words with a95 reference translations 
Wu and Weld  and   calculate the overlap between contexts of named entities and candidate articles from Wikipedia  using overlap ratios or similarity scores in a vector space model  respectively 
In our approach  equation  1  is further normalized so that the probability for different lengths of F is comparable at the word level  m m j n i ijm eft l EFP  1 10    1  1        2  The alignment models described in  are all based on the notion that an alignment aligns each source word to exactly one target word 
3http    wwwopenofficeorg Another corpora based method due to Turney and Littman  tries to measure the semantic orientation O  t  for a term t by O  t   summationdisplay tiS  PMI  t  ti  summationdisplay tjS PMI  t  tj  where S  and S are minimal sets of polar terms that contain prototypical positive and negative terms respectively  and PMI  t  ti  is the pointwise mutual information  between the terms t and ti 
To evaluate sentence automatically generated with taking consideration word concatenation into by using references varied among humans  various metrics using ngram precision and word accuracy have been proposed  word string precision  for summarization through word extraction  ROUGE  for abstracts  and BLEU  for machine translation 
3 Evaluation We trained our model parameters on a subset of the provided dev2006 development set  optimizing for caseinsensitive IBMstyle BLEU  with several iterations of minimum error rate training on nbest lists 
Levin  assumes that the syntactic realization of a verb s arguments is directly correlated with its meaning  cf 
The fstructures are created automatically by annotating nodes in the gold standard WSJ trees with LFG functional equations and then passing these equations through a constraint solver  
First  we trained a finitestate shallow parser on base phrases extracted from the Penn Wall St Journal  WSJ  Treebank  
 Termbased versions of this premise have motivated much sentimentanalysis work for over a decade   
In particular  knowing a little about the structure of a language can help in developing annotated corpora and tools  since a little knowledge can go a long way in inducing accurate structure and annotations  
As shown in   using this representation  a linear classifier can not distinguish sentences sampled from a trigram and real sentences 
Researchers have focused on learning adjectives or adjectival phrases  and verbs   but no previous work has focused on learning nouns 
Probabilistic translation models generally seek to find the translation string e that maximizes the probability Pra5 ea6fa7  given the source string f  
joint likelihood JL productdisplay i p parenleftBig xiyi  vector parenrightBig conditional likelihood CL productdisplay i p parenleftBig yi  xivector parenrightBig classification accuracy Juang and Katagiri 1992 summationdisplay i yi yxi expected classification accuracy Klein and Manning 2002 summationdisplay i p parenleftBig yi  xivector parenrightBig negated boosting loss Collins 2000  summationdisplay i p parenleftBig yi  xivector parenrightBig1 margin Crammer and Singer 2001  st bardbl vectorbardbl  1iy negationslash yi vector  vectorfxiyi   vectorfxiy   expected local accuracy Altun et al  2003 productdisplay i productdisplay j p parenleftBig lscriptjY   lscriptjyi   xivector parenrightBig Table 1 Various supervised training criteria
Most work on discriminative training for SMT has focussed on linear models  often with margin based algorithms   or rescaling a product of submodels  
Bikel and Chiang  in fact contains two parsers  one is a lexicalized probabilistic contextfree grammar  PCFG  similar to   the other is based on statistical TAG  
Finally  it should be noted that in the current implementation  we have not applied any of the possible optimizations that appear in the literature  to speed up normalization of the probability distribution q These improvements take advantage of a models structure to simplify the evaluation of the denominator in  1  
1510 5 Related Work In recent years  many research has been done on extracting relations from free text  eg     however  almost all of them require some languagedependent parsers or taggers for English  which restrict the language of their extractions to English only  or languages that have these parsers  
To compare the performance of system  we recorded the total training time and the BLEU score  which is a standard automatic measurement of the translation qualit  
Typically  a small set of seed polar phrases are prepared  and new polar phrases are detected based on the strength of cooccurrence with the seeds  
The basic LCS has a problem that it does not differentiate LCSes of different spatial relations within their embedding sequences    
3 Parse Tree Features We tagged each candidate transcription with  1  partofspeech tags  using the tagger documented in   and  2  a full parse tree  using the parser documented in Collins  1999  
"When HTC introduced the Windows-based smartphone, it recruited 1000 T-Mobile or AT&T customers to write product reviews and Facebook and Twitter posts, reaching more than 234,000 consumers and significantly increasing the brand awareness [12]. When Dunkin’ Donuts launched Latte Lite, it used 3000 consumers to spread the word about the new beverage, reaching 111,272 consumers over twelve weeks and increasing sales by 26% in test markets [13]. Both examples illustrate that electronic or online word of mouth (eWOM) has become an important factor in consumer buying decisions [37]. "
"Consumers trust eWOM more than advertisements, as they regard their peers as more reliable than companies [65]. "
"Although eWOM is implemented by consumers, companies can initiate eWOM campaigns for marketing communications [35]. "
"To launch an effective eWOM campaign, companies need to identify a small number of disseminators known as opinion leaders who exert personal influence upon other people [68]. "
"Identification of opinion leaders relies on the “two-step flow of communications” theory: as senders, opinion leaders cultivate their knowledge from a variety of sources including mass media in the first step, and then spread their opinions (messages) to the general public (receivers) via WOM in the second step [47]. Thus, sender, message, and receiver are key components in the WOM process [6,20], and provide three important bases for searching for opinion leaders."
"However, a survey may capture self-confidence rather than opinion leadership for two reasons [61,68]."
"Opinion leader WOM has long been used to promote products or to criticize competitors’ offerings [44,47]; its positive impact on new product introduction was first reported by Arndt [3]. "
"One of the most important capabilities of the Internet is interactive communication at a larger scale: “for the first time in human history, individuals can make their personal thoughts, reactions, and opinions easily accessible to the global community of Internet users”, and the interactive communication provides an online feedback mechanism to serve multiple functions, including brand building and customer acquisition, product development and quality control, and supply chain quality assurance [26]."
Electronic commerce performs better than the traditional market in acquiring customers [77]. 
"To be effective in viral marketing campaigns, companies must identify opinion leaders properly and then let them communicate information to their followers [43]."
Opinion leaders are consumers who provide information to others that influences their consumption decisions [22] by obtaining key information through research and shaping their own opinions earlier than the general public.
"Opinion leaders in women’s fashion, for example, acquire fashion knowledge from fashion magazines first and then spread it to followers via WOM [70]."
Rogers and Cartano [68] noted that sender-based surveys are “dependent upon the accuracy with which respondents [senders] can assess and report their self-images on opinion leadership”.
"The network structure method has been widely used by marketers and network analysis researchers [41,43]. Network analysis determines opinion leaders by identifying those who connect with many people (i.e., hubs) and those who connect two clusters of densely connected people (i.e., bridges) in a social network [41]. "
"Since the advent of the Internet in the 1990s, WOM is no longer restricted to personal influence networks because the Internet allows one to reach strangers at a larger scale [26]."
Godes & Mayzlin [35] adopted the King and Summers scale to measure how many followers an opinion leader reaches.
"Buzz is generated around a product when a large number of followers receive eWOM [11,31]. Previous studies suggest that opinion leaders are progressive attention-seekers [70] and fulfill their self-enhancement motivation via buzz creation [33]. "
"Trust is an important issue in electronic commerce and eWOM studies (e.g., [26,80]), as it is one of the main reasons for followers to seek advice from opinion leaders."
"An indirect approach to measure trustworthiness is analyzing the structural, lexical, and semantic aspects of eWOM, which are found to be associated with trustworthiness [10]. "
"The amount of eWOM influences consumers in two ways: eWOM increases exposure to a product and therefore increases consumer awareness of its existence [54]; and a large amount of eWOM suggests a product’s popularity [17,79]. Previous studies reveal that the amount of eWOM created by the general public drives sales [21,27,29,54]."
"Consumers communicate their satisfaction using online user ratings [18,71]. Positive ratings created by the general public can improve consumer attitude, while negative ratings created by the general public can worsen consumer attitude [54]. Customer satisfaction among the general public has been found to have a positive impact on future sales [4,78]."
"They are also motivated to contribute their knowledge to other consumers [19,39]."
" According to Childers [22], opinion leadership is product category specific. The more a consumer purchases and consumes within the same product category, the more likely the consumer is to acquire complex category knowledge. "
"Our dataset contains a sample of 350,122 book, music, video and DVD titles, which, as experience goods, have qualities difficult to ascertain before consumption, making user reviews helpful for consumers [60,63]. "
The overlap between different types of opinion leaders is consistent with extant literature [43].
"Previous research suggests that an opinion leader needs to have both knowledge and influence [47,57]. Since opinion leaders have different breadths of product category knowledge, it is important to examine which kinds of opinion leaders can be more effective in driving sales [34]. Knowledge and influence, the two components of opinion leadership, are not independent. "
"Katz and Lazarsfeld [47] argue that personal influence does not flow from highly interested individuals to less interested individuals, but rather between those with shared interests. Therefore, if general knowledge implies general interest, an opinion leader with broad product category knowledge will attract more followers [58]."
"Fashion industry companies use celebrities such as Madonna as opinion leaders [76], while the pharmaceutical industry uses physicians on editorial boards/scientific committees and with prestigious academic appointments [32,38,49]. When consumers are not celebrities or experts, practitioners use sender-based and receiver-based methods."
"For example, the eWOM company BzzAgent measures engagement as the number of likes, comments, and retweets a consumer’s post receives [9]. The opinion leaders identified through this process are similar to the buzz-generating opinion leaders in our paper."
"We recommend that companies implement online user review systems using Amazon’s patented design [8], and identify the top 1% of communicative, buzz-generating, and trustworthy eWOM opinion leaders among users by measuring eWOM volume, feedback received, and helpful votes received. "
This article presents a new opinion leader identification method rooted in the interpersonal communication theory that inspired sender- and receiver-based methods [47]. 
"By demonstrating how to identify opinion leaders from a large number of consumers, this article shows that companies can collect useful business intelligence from increasingly large amounts of data available to them—a central theme emerging in big data analytics [16]."
"Wireless Mesh Sensor Networks (WMSNs) comprise a technological field that has arisen as the natural evolution from the conventional Wireless Sensor Networks (WSNs), composed of only a few nodes, to large-scale networks in which sources and destinations are interconnected through different paths of intermediate nodes forming a mesh layoutÂ  [1]."
"To this end, new mesh capabilities such as multi-hop mesh routing, scalability, robustness, reliability, self-organization or energy efficiency must be satisfiedÂ  [2]. "
" To achieve this purpose, ASES integrates, in a single solution, an efficient duty-cycle strategy for saving energy along with most of the above mesh capabilities. This fact makes IEEE 802.15.5 standard and its ASES mechanism advantageous with respect to other WMSN approachesÂ  [2], such as Zigbee ProÂ  [3], the International Engineering Task Force (IETF) with its solution IPv6 over Low power Wireless Personal Area Networks (6LoWPAN)Â  [4], WirelessHARTÂ  [5] or ISA SP100.11aÂ  [6]."
" Second, physical-space constraints (e.g.,Â obstacles such as walls, and pillars)Â  [38,39] impose strict restrictions in the application management, such as the impossibility of using relay/sink mobility to improve the network performance. Similarly, Assumption (ii) copes with the premises of data monitoring in the aforementioned domain-driven applications, where physical sensing information is collected and reported periodically. "
"Despite these known problems related to large-scale agile, there is an industry trend towards adopting agile methodologies in-the-large ( VersionOne,Inc, 2016; Paasivaara et al., 2013, 2014; Dings yr and Moe, 2014)."
"According to the latest survey ( VersionOne, Inc, 2016), 62% of the almost 4000 respondents had more than a hundred people in their software organization and 43% of all the respondents worked in development organizations where more than half of the teams were agile"
"However, this indicates that there seems to exist a large number of companies that have taken or are taking agile into use in large-scale settings ( VersionOne, Inc,2016)."
"In two recent workshops on large-scale agile development organized in XP2013 and XP2014 conferences, adoption of agile methods was one of the highlighted themes needing more research ( Dings yr and Moe,2013,2014)."
"Traditional methods focus on up-front planning and strict management of change, but agile methods were designed to accept and efficiently manage change ( Highsmith and Cockburn, 2001; Cockburn and Highsmith, 2001)."
"Agile methods have been both criticized and advocated, and research has shown that accommodating change may be a factor in both success and failure ( Boehm, 2002)"
"It has been shown that agile methods have improved satisfaction of both customers and developers, but on the other hand there is evidence that agile methods may not be a good fit for large undertakings ( Dyb  and Dings yr, 2009)"
"For example, one paper would highlight the viewpoint of the developers ( Fry and Greene, 2007), and another would consider the transformation from the user experience designers point of view ( Federoff and Courage, 2009)."
"Surveys on challenges and success factors for agile projects in general have been conducted, eg( Chow and Cao, 2008)"
" The traditional methods for estimating PDFs, e.g. binning methods and kernel density techniques, require specification of a bandwidth parameter that heavily influences the shape of the resulting PDF (Silverman, 1986; Wilks, 2006; Srihera and Stute, 2011). "
"While methods exist for estimating an optimal bandwidth, these methods usually require some assumption about the shape of the underlying PDF (Wand and Jones, 1995; Srihera and Stute, 2011; Bernacchia and Pigolotti, 2011). Given that our application requires an unbiased determination of the normality of the velocity increments, estimation methods utilizing such assumptions would not be suitable for our analysis. "
"Kernel density estimation is a widely used method for estimating the probability distribution function (PDF) of a given dataset (e.g., Silverman, 1986, Wilks, 2006), in which the PDF is approximated as a normalized sum of kernel functions K(Ï) centered on each data point Ïj"
"The convolution part of this algorithm requires íª(Nâq) calculations and the FFT portion requires íª(MâlogM) (Cooley and Tukey, 1965). Simple algebraic manipulation can show that if q<M and logMâªN, then Nâq+MâlogM<NâM, and so the nuFFT is theoretically faster than the direct DFT calculation"
"We also note that we implemented the selective frequency filter, In, in a slightly different manner than Bernacchia and Pigolotti (2011). They show that the self-consistent density estimate converges on the true density provided the filter In is set to 1 for some subset of the frequencies for which C is above the estimate stability threshold given by |Cn|2â¥4â(Nâ1)âNâ2 and set to 0 for all other frequencies. Whereas they choose the subset based on a frequency cut-off tâ such that C is above the stability threshold for half of the frequencies within [âtâ,tâ], we choose a cut-off frequency based on the occurrence of three consecutive C values below the stability threshold. In our implementation In=0 for all n>nâ, where nâ is the index of the lowest frequency for which Cnâ+1,Cnâ+2, and Cnâ+3 are below the stability threshold. We choose this criterion because it is fast to implement and we find that it avoids an occasional, spurious leakage of high-frequency components that manifests as high-frequency waves superimposed on the density estimate."
" We use a version of CAM4 that includes the Model for Prediction Across Scales atmospheric (MPAS-A) dynamical core, which predicts the evolution of the atmosphere by evaluating conservation laws (e.g., conservation of mass, momentum, etc.) on a centroidal Voronoi tessellation of the sphere (Rauscher etÂ al., 2013; Skamarock etÂ al., 2012)."
"As every software artifact, also software models ( B zivin,2005) are subject to continuous evolution"
"Knowing the operations applied between two successive versions of a model is not only crucial for helping developers to efficiently understand the model's evolution ( Koegel et al., 2010), but it is also a major prerequisite for model management tasks, such as model co-evolution ( Herrmannsdoerfer et al., 2009; Mens,2008) and model versioning ( Brosch et al., 2010; Koegel et al., 2010)"
"The second category comprises composite operations ( Suny et al.,2001) consisting of a set of cohesive atomic operations, which are applied within one transaction to achieve one common goal"
The most prominent class of such composite operations are refactorings introduced by Opdyke (1992)
"As reported in Herrmannsdoerfer et al.(2009), Mens (2008), Brosch et al. (2010) and Koegel et al.(2010), the detection of applied refactorings is a crucial prerequisite for automating model management tasks"
"However, composite operations are not limited to refactorings; they may be used to implement any kind of in-place model transformation for a specific purpose, such as model completion ( Sen et al.,2010), refinement ( Ruhroth and Wehrheim, 2012), and evolution ( Meyers and Vangheluwe, 2011)."
"One way to acquire the set of applied composite operations is to use operation recording ( Herrmannsdoerfer and K gel, 2010; Lippe and Oosterom, 1992); that is, the execution of operations is tracked within the modeling environment while they are performed"
"A set of manually applied atomic operations, having together the intent of a composite operation, which is indeed frequently happening in practice ( Murphy-Hill et al.,2009), cannot be identified, because no explicit command has been issued in the modeling environment"
"In the absence of an operation log, the applied operations have to be detected a posteriori using state-based model comparison approaches using either generic model comparison algorithms ( Brun and Pierantonio, 2008; Kelter et al., 2005; Lin et al., 2007; Schmidt and Gloetzner, 2008) or language-specific comparison algorithms ( Kolovos, 2009; Xing and Stroulia, 2005)"
"In an endeavor to establish a commonly accepted conceptual framework for the rapidly growing number of domain-specific modeling environments, the Object Management Group (OMG) released the specification for Model Driven Architecture (MDA) ( Object Management Group, 2005), standardizing the definition and usage of (meta-)metamodels resulting in a metamodeling stack as depicted in Fig1"
Hartung et al.(2010) present an approach for generating so called semantically enriched evolution mappings between two versions of an ontology
"For limiting the search space of combinable composite operations, we may use the critical pair analysis comparable to how it has been done in Mens (2006)."
"Modern trends in manufacturing are defined by mass customization, small lot sizes, high variability of product types, and a changing product portfolio during the lifecycle of an automated production system (aPS) (Lüder et al., 2005; Rzevski, 2003). These trends imply more complex aPS (Mcfarlane and Bussmann, 2000), which support changes in the physical layout of the aPS including extensive technical updates."
"Since the proportion of system functionality that is realized by software is increasing (Thramboulidis, 2010), concepts for supporting automation engineers in handling this complexity are strongly required. "
"Software as well as software engineering in this domain need to fulfill specific requirements, e.g. regarding real-time and reliability (Vogel-Heuser et al., 2014a, 2014b, 2014c). "
"aPS are comprised of mechanical parts, electrical and electronic parts (automation hardware) and software, all closely interwoven, and thus represent a special class of mechatronic systems (Bonfè and Fantuzzi, 2003; Rzevski, 2003) and consist of mechatronic sub-systems like sensors and actuators."
 Lehman (1980) defined laws of software evolution and – among others – identified that systems are subject to dynamics causing continuing changes of software resulting into increasing complexity.
" Lehman (1980) defined laws of software evolution and – among others – identified that systems are subject to dynamics causing continuing changes of software resulting into increasing complexity. Evolution might be triggered in each phase of an aPS's life. Hence, due to their evolution and the long-living character of aPs, their life is characterized as a cycle (Birkhofer et al., 2010). "
" Software maintainability is “the ease with which a software system can be modified to correct faults, improve performance or other attributes or adapt to change environments” (IEC, 1990)."
"Accordingly, maintenance can either involve repair or modification actions, which in turn can be adjustments to the environment (referred to as adaptive maintenance) or augmentation of a system's function (Avizienis et al., 2004). "
"In the context of maintainability, obsolescence management, i.e., managing, mitigating and resolving the impact of (sub-)component obsolescence, is one important issue regarding long-living systems (ISO/IEC 25010, 2011)."
 An approach to explicitly modeling changes of aPS' physical structures and for analyzing their effects on the system's functions is proposed in Göring and Fay (2013) alongwith the physical causes of these changes.
"According to Wiendahl et al. (2007), changeability “is defined as characteristics to accomplish early and foresighted adjustments of the factory's structure and processes”. In contrast, reconfigurability is defined as the “ability of a manufacturing or assembly system to switch with minimal effort and delay to a particular family of workpieces or subassemblies through the addition or removal of functional elements” (Wiendahl et al., 2007)."
"Drivers for evolution are manifold (Westkämper, 2003), and basically result in changes of requirements on the aPS (Legat et al., 2013)."
"aPS are typically designed-to-order, i.e. they are unique systems, which are designed and implemented once a customer has awarded a contract to an aPS supplier (Birkhofer et al., 2010)"
"The specification and implementation is carried out in the form of a project (VDI/VDE 3695 Part 2, 2010)."
"The development of these (partial) solutions follows a typical product development workflow (cp. upper part of Fig. 1), as described e.g. in VDI/VDE 2206 (2004), and is decoupled from the projects on the timeline. "
"The categorization is based on and extends the one introduced in Ladiges et al. (2013), Vogel-Heuser et al. (2014c) and (2014d)."
" Anticipated software changes in accordance to Buckley et al. (2005) are software changes, which “can be foreseen during the initial development of the systems and, as such, can be accommodated in the design decision taken”. "
"Unanticipated software changes according to Buckley et al. (2005) are not foreseen during the development phase, but they are frequently undertaken at short notice during commissioning and operation in order to, e.g., clear defects in other disciplines, such as an unexpected behavior of the mechanics. "
"The PPU performs a manufacturing (discrete) process and handles, stamps and sorts different kinds of workpieces (Fig. 3) (Vogel-Heuser et al., 2014d)."
"Regarding Buckley et al.'s (2005) criteria temporal change, also history of change is introduced with the characteristic sequential and parallel synchronous and parallel asynchronous changes."
" Accordingly, the development of requirements on a production plant is an integrated part of the production design (Blanchard, 2004; Buede, 2000). In the development process model described in (VDI/VDE 2206, 2004) (cp. Section 2) for the design of mechatronic systems, such as production plants, the phases of identification and documentation of requirements are explicitly foreseen at the beginning of the process. "
"Similarly, other common development process models like the spiral model (Sage and Rouse, 2009) (for software-intensive systems) or the waterfall model include requirements elicitation and specification as a (repetitive) action during engineering. "
" Quality requirements, also called non-functional requirements or extra-functional requirements, are usually more general and include, among others, performance requirements expressed in Key Performance Indicators (KPIs) (see e.g. ISO 22400-2, 2012), flexibility requirements, reliability and availability requirements, safety and security requirements, and maintainability requirements (ISO/IEC 25010, 2011)."
"Unfortunately, the adaptation of the formal specification is often omitted, especially when changes need to be implemented within a short time window and during operation (Vogel-Heuser et al., 2014a)."
"The main requirements to be considered are reliability, performance efficiency, compatibility, maintainability and portability (Frank et al., 2011)."
"General sources of requirements are usually stakeholders, documents, and systems in operation (Pohl and Rupp, 2011). "
" Haubeck et al. (2013), for example, derive four stages in the requirement hierarchy of a single system. "
"Similar hierarchies can be found in the other literatures, like Blanchard (2004) and IEEE-Standard 1220 (1999). However, the hierarchy presented by Haubeck also refers to the process of formalizing requirements."
"Formalization may be a first refinement of the informal requirements (VDI/VDE 3694, 2008). However, informal requirements sometimes need to be defined on several or each hierarchical level (IEEE-Standard 1233, 1998)."
"Formalization usually results in a set of models, and the variety of used models is huge, and the choice of the right model type depends on the domain, industry branch, as well as the specific plant to be designed (VDI/VDE 3681, 2005). The right model choice also depends on the degree of granularity and the current refinement stage."
"These properties can be operationalized in a way that they are measurable by the available online measurements (Ladiges et al., 2013). "
"Feldmann et al. (2014b) presented a light weight notational approach (cp. 3.3) to model requirements for testing and evaluated it with experts from industry showing that the modeling approach provides the means to systematically, comprehensively and efficiently specify mechatronic sub-systems as sensors and actuators. "
"To increase the quality of requirements Rösch et al. (2015) and Teufl and Hackenberg (2015) introduced description of behavior based on MSCs included in the MIRA (Broy and Stølen, 2001) approach and UML SD."
"An important part of the system design is the definition of components and interfaces, which in software engineering is called the architectural configuration (Barais et al., 2008; Medvidovic and Taylor, 2000). During evolution, the engineers working on the different parts of the architecture will evolve their respective system parts."
"The complete system design covers different aspects (Goldschmidt et al., 2012). "
" In a multiple-case embedded system case study in 5 large software companies (Martini et al., 2014), different factors for architectural technical debt have been identified, like pressure to deliver, prioritization of features over products, non-complete refactoring, or technology evolution."
"CONSENS (Anacker et al., 2011) – CONceptual design Specification technique for ENgineering of complex Systems – is a method and specification language that targets the overall, discipline independent system design. "
"The MechatronicUML (Heinzemann et al., 2013) is a modeling language and process for the engineering of mechatronic systems. It is based on a rigorous specification of structure and behavior based on a refinement of the Unified Modeling Language (UML) (Object Management Group, 2011). "
"SysML4Mechatronics (Kernschmidt and Vogel-Heuser, 2013) is a language for interdisciplinary modeling, which addresses mechanical, electrical/electronic and software aspects in product automation and for aPS explicitly."
 Shah et al. (2010) present a multi-discipline modeling framework based on SysML. 
"A formal modeling framework for verifying aPS' engineering models has been proposed in (Hackenberg et al., 2014). The formal models contain the necessary aspects for verifying the system's correctness after evolution changes."
"ne major language for the definition of consistency constraints is the Object Constraint Language OCL (Object Management Group, 2014), which allows for the specification of constraints based on first-order predicate logic. "
 A rule based approach to identify structural inconsistencies during evolution of aPS is presented in Strube et al. (2011).
"Dynamic semantics is concerned with behavior of the model. Here, approaches can be used, for example, to compare whether two finite automata are consistent by identifying whether one simulates the other or both simulate each other (bisimulation) (Clarke et al., 1999). "
"More complex relations (called refinement notions) between automata exist, which can be used to define and check various degrees of consistency (Baier and Katoen, 2008; Heinzemann and Henkler, 2011; Jensen et al., 2000; Weise and Lenzkes, 1997)."
" In the area of model transformations, Bidirectional Model Transformations like JTL (Cicchetti et al., 2010) and Triple Graph Grammars (TGG) (Hermann et al., 2014; Hildebrandt et al., 2013; Schürr, 1995) have been developed to transform between models. "
"Transformation languages that support incremental change propagation (Giese and Wagner, 2009) are particularly interesting as they enable to only propagate single changes from the source, i.e., they change only those parts of the target model which are affected by the change in the source. "
" Similarly, Song et al. (2013) propose a compositional approach for the probability reachability analysis of Discrete Time Markov Chains that decomposes the system into strongly connected components or even parts of them, analyze them using Gauss–Jordan Elimination (Althoen and McLaughlin, 1987), and afterwards use value iteration to compute the result for the complete model based on the individually analyzed parts."
"One specific type of architectural technical debt (Martini et al., 2014) is non-compliance between architectural guidelines and the system architecture. "
"Architectural guidelines, patterns and styles have been presented in Buschmann et al. (1996). "
" specific architectural pattern for embedded systems is the Operator Controller Module (OCM) (Burmester et al., 2008), which defines concrete layers and interfaces between them for different parts of the embedded software – feedback controllers, hard real-time communication and reconfiguration of the feedback controllers, and a soft real-time layer."
Herold et al. (2013) present an approach to automatically check the conformance of the architecture against guidelines and architectural styles. 
A complementary approach has been proposed in Herold and Mair (2014)which uses a meta-heuristic to efficiently search for violations of architectural compliance rules and to propose a sequence of repair actions to remove the identified violations automatically.
"Since the proportion of system functionality that is realized by software is increasing (Thramboulidis, 2010), concepts for supporting automation engineers in handling this complexity are required."
"“Reusable artefacts are mostly fine-grained and have to be modified on different positions, so that errors are probable and important reuse potential is wasted” (Maga et al., 2011). Reuse is mostly achieved through copy, paste and modification actions (Katzke et al., 2004). Feldmann et al. (2012) identified as reasons for this situation: the multitude of disciplines involved (such as software engineering, automation engineering, mechanical engineering, electrical engineering, safety engineering, perhaps also chemical engineering), and the interdependencies of software modules with mechanical and electrical modules (Jazdi et al., 2011). "
"But also in traditional software engineering, the main technique to derive adapted or improved implementation versions of software functionality is clone and own (Ray and Kim, 2012). The main idea of clone and own is to copy existing code and modify the copy until the desired functionality is realized."
"Code clones have been known for more than two decades as serious flaw that may impede evolution. For instance, Juergens et al. (2009) have shown that code clones are more prone to introduce errors. Moreover, Harder and Tiarks (2012) have shown, that error-correcting tasks tend to be incorrect in the presence of clones. "
"Beyond code clones, a broader notion of code smells has been introduced (Fowler, 1999) and extended by the notion of anti-patterns (Brown et al., 1998). "
" Particularly, Abbes et al. (2011) have shown that the presence of two anti-patterns impedes the performance of developers."
"While other studies show that perception of code smells may differ between developers (Yamashita et al., 2012), it is generally admitted that code smells hinder evolution because they impede extending and maintaining the underlying system."
"Design patterns such as those proposed by Gamma et al. (1994), are means in classical software engineering to support code modularity and evolution. "
" Amaptzoglou et al. (2011) reveal that using design patterns increases reusability, and thus, supports the evolution of software systems. However, other studies also indicate that design pattern may also impede evolution and maintenance and have to be applied properly (Khomh et al., 2008, 2009)."
Fuchs et al. (2014) conducted a detailed analysis of IEC 61131-3 code from machine manufacturing industry and introduced an analysis and visualization approach
 Feldmann et al. (2012) and Fuchs et al. (2012) analyzed and refactored the software structure on IEC code level in an industrial case study in a world market leading plant manufacturing company.
"For IEC 61499-based applications (IEC, 2011), common solutions and guidelines were proposed for hierarchical automation solutions (Zoitl and Prähofer, 2013), failure management (Serna et al., 2010) and portable automation projects (Dubinin and Vyatkin, 2012)."
"Even software design patterns for IEC 61499 programs, e.g., Distributed Application, Proxy and Model-View-Controller, were defined (Christensen, 2000) and evaluated (Strömman et al., 2005). However, although IEC 61499 runtimes on state of the art controllers exist (Vyatkin, 2011), “IEC 61499 has a long way in order to be seriously considered by the industry” (Thramboulidis, 2013)."
" As such, refactoring has been established and numerous studies have demonstrated its usefulness (Fanta and Raijlich, 1999; Kegel and Steinmann, 2008). However, certain challenges such as proper tool support and side-effect freeness of refactorings are still open research questions."
"The interdisciplinary dependencies as well as the complexity of aPS lead to the risk of unpredictable side effects of evolution in the resulting system (Jaeger et al., 2011). A detection of these side effects becomes necessary and should be carried out automatically to avoid much effort, as explained in Braun et al. (2012)"
"Management of aPS evolution has to deal with diverging evolution cycles of the involved disciplines (Li et al., 2012). "
"Because of the high complexity of the automation software and the plant itself, it is usually not obvious how the evolution in one part of the system affects other parts or the whole process (Jaeger et al., 2011). "
"Instead of analyzing the state space of a model within a certain time frame, formal techniques aim at analyzing models exhaustively with respect to all reachable states (Bérard et al., 2001; Clarke et al., 1999). For example, Lahtinen et al. (2012) state that in the nuclear engineering domain, automatic verification is beneficial compared to simulation because it can – besides exploring all reachable states – be applied earlier in the design phase."
Hametner et al. (2010) and Hussain and Frey (2006) identify useful diagrams for modeling and deriving test cases from UML for the field of automation software development and especially for IEC 61499 implementations.
" Interaction diagrams are recommended for the extraction of test sequences. In Hussain and Frey (2006), the extraction of test sequences from state charts using round-trip path coverage is shown."
Hametner et al. (2011) show a first implementation of the recommended test case generation process using state chart diagrams especially for IEC 61499 applications. Hametner et al. (2010) also mention the timing diagram of the UML as a useful diagram for test case generation but no implementation is shown in their work. 
"Rösch et al. (2014) realize this test case generation, but focus especially on testing machine's reaction to faults by using fault injection."
" In Kumar et al. (2011), UML test case generation approaches from state charts are combined with the aim of making them executable by mapping them to the Testing and Control Notation (TTCN-3). "
"In order to integrate the requirements and test specification, a template for test cases has been developed by Feldmann et al. (2014b). "
Ulewicz et al. (2014) present a first approach for selecting and reusing existing test cases based on changes within the control program for efficient testing of changes.
"Nevertheless, in Fuchs et al. (2014) and Prähofer et al. (2012) the benefits of static code analysis for IEC 61131-3 software quality improvement are highlighted and an approach for improving compliance to programming conventions and guidelines is proposed,"
"The compositional approach supports multiple refinement notions to guarantee different types of system behavior (Heinzemann and Henkler, 2011). "
"Sünder et al. (2013) propose an approach on verifying PLC programs by means of model checking, taking predefined modifications of the software into account. "
Gourcuff et al. (2008) presented an approach for verifying cyclically executed PLC software. 
Witsch and Vogel-Heuser (2011) proposed an approach to implement PLC software based on UML state charts (so called PLC state charts) and described its behavioral semantics and application for model checking.
 Mertke and Frey (2001) proposed an approach on formal verification based on Petri nets by using the SPIN model checker. 
Machado et al. (2006) investigated the impact of applying a model of the physical plant additionally to the formal specification of controller behavior. 
"An integrated approach to verify conformance of aPS designs is presented in Vyatkin et al. (2009). It considers the overall behavior of mechatronic components in a plant model, i.e. the behavior resulting from the combination of different disciplines. "
"aPS are highly diverse. They may differ in the processes which they are designed to execute, but also in the mechanical and electrical/electronic parts and software parts used to complete their tasks. The inherent variability in such systems results in a huge number of possible variants which may be described by a product family, e.g., a software product line (SPL) (Clements and Northrop, 2001; Pohl et al., 2005). "
"Feldmann et al. (2012) analyzed the approaches and challenges for modularity, variant and version management in aPS."
"For variability management from an electrical engineering viewpoint, tools like EPLAN Engineering Center7 as well as the Siemens Platform COMOS8 exist, and for mechanical engineering, Design Structure Matrices are most popular (Kortler et al., 2011). "
Feldmann et al. (2012) showed that module structures in different disciplines differ. 
"Variability modeling in the problem space defines the commonality and variability of a product family in order to specify the valid configuration space i.e., specifying the set of valid software variants (Czarnecki and Eisenecker, 2000). "
"In the context of feature-oriented domain analysis, Kang et al. (1990) introduce feature models for capturing the commonality and variability of variant-rich systems by means of features and their interrelations."
Pohl et al. (2005) proposed the orthogonal variability model also for capturing the commonality and variability of variant-rich software systems in a graphical way by means of variation points.
"Another variability modeling approach is decision modeling introduced by the Synthesis project (Synthesis, 1993)."
"Model transformations are, for instance, applied in the common variability language (Haugen et al., 2008) where the variability of a base model is described by rules how modeling elements of the base model have to be substituted in order to obtain a particular product model. "
"Delta modeling has so far been applied to represent variability of software architectures (Haber et al., 2011), Java programs (Schaefer, 2010) and a multi-perspective modeling approach for manufacturing systems (Kowal et al., 2014)."
 Elsner et al. (2010) consider evolving software product lines and focus on variability in time.
"Schubanz et al. (2013)developed their own specific modeling approach, which is integrated in a prototypical tool chain and focuses on a high level of abstraction, i.e., evolution of development goals and requirements. "
The approach by Dhungana et al. (2010) uses model fragments for capturing the solution space variability following the principle that smaller models are better to understand.
"While model-driven engineering shows an increase in effectiveness and quality and is used in the embedded software industry (Liebel et al., 2014) by exploiting domain knowledge, it also introduces general challenges and challenges specific to aPS."
"Typically, the models that are developed during system design are in later phases converted to source code by code generation. This process is called Forward Engineering (Sendall and Küster, 2004). "
As Vogel-Heuser et al. (2012) showed usability of MDE approaches is strongly related to students' basic skills and appropriate fade out training approach.
" Furthermore, in Vepsalainen et al. (2010), it was identified that the modeling of user-defined control logic is required in addition to the application of predefined control blocks. For acceptance of novel concepts, those have to be easily applicable and reproducible for other researchers (Goldberg, 2012). "
" Obermeier et al. (2014) introduce a domain specific UML for aPS (modAT4rMS based on plcML) supporting the quality of structural modeling for the creation of new models, the reuse as well as the evolution (building new variants)."
 Duschl et al. (2014) attempt to reveal the reasons behind the errors especially in module creation in the above mentioned study conducting interviews after the experiments.
"As a basis to describe different aspects of aPS at different hierarchical levels, adaptations of UML and SysML are proposed in Bassi et al. (2011), Bonfè et al. (2005) and Secchi et al. (2007). Concepts for supplying object-oriented models with a formal basis, e.g., in order to apply methods for verifying modeled system requirements (Sünder et al., 2013), have been proposed by Secchi et al. (2007)."
"The integration of MDE, i.e. plcML being a UML dialect, into a PLC programming environment is realized by Witsch and Vogel-Heuser (2011) and available for state chart and class diagrams in CODESYS V39."
"Code generation from plcML to Siemens S710 platform has been introduced in Tikhonov et al. (2014). For hybrid models combining closed loop control and interlocking, Bayrak et al. (2012) and Schneider et al. (2014)"
"Trace links can also be created using information-retrieval techniques (Cleland-Huang et al., 2007). The authors give an overview on two techniques for automatic generation of trace link candidates from requirements and other system artifacts based on probabilistic network models (Antoniol et al., 2002) and vector space models (Cleland-Huang et al., 2005). "
"Berkovich et al. (2011) investigate tracing on interdisciplinary Product-Service Systems (PSS), e.g. mechatronic products including services. "
" Wolfenstetter et al. (2015) analyzed and evaluated different traceability techniques from a requirements engineering perspective regarding ten criteria, e.g., variability and configuration management, version management, simultaneous development of different views, with the result provides sufficient support."
"In a Banach space setting, the source inequality (30) has already been used in [36,50] to derive convergence rates for Tikhonov regularization with convex functionals and in [34] for multiparameter regularization. Eq. (29) is an alternate for the missing triangle inequality in the non-metric case."
"In the case that the operator F is linear and R is convex, (2) and (3) are basically equivalent, if is chosen according to Morozov discrepancy principle (see [39], Chap.3])."
"While the theory of Tikhonov regularization has received much attention in the literature (see for instance [1,14,21,22,33,37,45,49,52,56,58]), the same cannot be said about the residual method"
"In [21,50], where convergence and stability of Tikhonov regularization have been investigated, the stability results are of the following form: for every sequence (yk)k N y and every sequence of minimizers xk argmin{ F(x)-yk 2+ R(x)} there exists a subsequence of (xk)k N that converges to a minimizer of F(x)-y 2+ R(x)"
"In this paper we prove similar results for the residual method but with a different notation using a type of convergence of sets (see, for example, [41, Section 29])"
"If X satisfies the first axiom of countability, then x Lim sup k k, if and only if there exists a subsequence ( kj)j N of ( k)k N and a sequence of elements xj kj such that x j x (see [41, Section 29.IV])"
"From Proposition 4.3 we now obtain that (xk)k N weakly converges to x and xk pp x pp_ Thus, in fact, the sequence (xk)k N strongly converges to x (see [44, Corrollory 5.2.19])"
This r-coercivity has already been applied in [2] for the minimization of Tikhonov functionals in Banach spaces
"The spaces X = L p( , ) for p (1, 2] and some -finite measure space ( , ) are examples of 2-convex Banach spaces (see [42, p.81, Remarks following Theorem 1.f.1.])"
"In a finite dimensional setting with p = 1, the minimization problem (40) has received a lot of attention during the last years under the name of compressed sensing (see [7,8,10,16 18,20,26,57])"
"however, for many LDS’s, the parameters are unknown and must be estimated in a process often called system identification[17]."
Reduced-rank LDS models can partially address this problem by reducing the number of latent states. [7]
"An alternative is to use subspace identification methods such as N4SID and PCA-ID, which give asymptotically unbiased closed-form solutions [8,27]."
"The Amari error [2], which is another permutation-invariant measure of similarity, is also provided."
"Mohammad et al. have discussed the impact of autocorrelation on functional connectivity, which also provides some direction for extension [3]."
"Diversifiersare people whose passion is to explore details. They are in love with the heterogeneity of nature […]They are happy if they leave the universe a little more complicated than they found it.”[1, chap. 3, p. 44]"
"Dyson’s treatment of this is relatively short, at just one 18-page chapter [1]. It is therefore important to moderate our contemporary biases, if we are to understand what he intended."
"The field was spawned from the dreams of Artificial Intelligence, though the reality has encompassed a far broader scope of study than originally envisioned at the Dartmouth Conference [2]."
"“Therefore, my success as a man of science, whatever this may have amounted to, has been determined, as far as I can judge, by complex and diversified mental qualities and conditions. Of these the most important have been […]industry in observing and collecting facts, and a fair share of invention as well as of common-sense.”[3, p. 144]"
"The first academic city in the world was Athens, and the first industrial city was Manchester, so I like to use the names of Athens and Manchester as symbols of the two styles of scientific thinking.”[1, p. 37].He clarifies later, “The science of Athens emphasises ideas and theories; it tries to find unifying concepts which tie the universe together. The science of Manchester emphasises facts and things; it tries to explore and extend our knowledge of nature’s diversity.”[1, p. 40]. To clarify, he is not stating that all academics are unifiers, nor that all of industries are diversifiers."
"“Science belongs to both worlds, but the style of academic science is different from the style of industrial science. The science of the academic world tends to be dominated by unifiers, while the science of the industrial world tends to be dominated by diversifiers.”[1, p. 36]."
"The Manchester exemplar is particularly illuminating in this respect, given a deeper look at its historical context. Manchester, situated in the North of England, was the birthplace of the Industrial Revolution, and the growth of its intellectual capital is well documented by Thackray [4]."
"“Science did flourish in Manchester during the crucial formative years of the industrial revolution, but […]did not arise in response to the needs of industrial production. The driving force of the Manchester scientific renaissance were not technological and utilitarian; they were cultural and aesthetic.”[1, p. 38]"
"In later a communication he states that only “roughly speaking, unifiers are following the tradition of Descartes, diversifiers are following the tradition of Bacon”[7]."
A controversial idea of the past century in philosophy has been the distinction between analytic and synthetic statements [8]. 
"Kant [8] referred to analytic statements as clarifying or explicating our knowledge, or in other words, making explicit what was once implicit. "
 A modern perspective on this divide is given by Jogalekar [9].
"So here, Dyson explicitly thinks of himself in a unifying role. In another communication [11] he recounts his discussions on Quantum ElectroDynamics with Richard P. Feynman, who was apparently obsessed with finding a unifying theory of the large (gravity) and small (nuclear forces). In contrast, Dyson was comfortable with more than one set of equations, each useful at different scales. Referencing Gödel’s theorem says:"
“a beautiful or elegant theory is more likely to be right than a theory that is inelegant.”[12]
"The only instance even vaguely like this (that I know of) is the Boosting family of algorithms—the existence of which was predicted by studies in computational complexity theory [14], and discovered later by Schapire [15]. "
"Though this article is focused on Pattern Recognition, it is fair to note that similarly vague work appears in related communities [18]. "
Kuhn [20] presents a treatise on the nature and reasons behind revolutions in scientific understanding.
"Whilst we may never have a truly “unified theory of inference”, there are a number of technical elements of our field which could benefit from a little unification; in classic papers, Breiman [22] and Langley [17] present ideas along this line. "
Breiman [22] discusses two cultures of statistical modelling: data modelling versus algorithmic modelling.
"Langley [17] wrote a striking editorial for an issue of Machine Learning Journal, on the topic of unifying machine learning as it stood in the late 1980s."
"Essentialism is the view that entities in the world have inherent, essential and immutable properties, by which they can be described. Pelillo and Scantamburlo [25] discuss how dissimilarity measures in PR sit in direct opposition to this philosophical view, in that an entity is best described by its relation (similarities) to other entities."
 It could be argued that such devices are at prototype stage already with neuromorphic computing [26].
The methods for automated tumor detection evolved from the approaches that are used for normal tissue detection on medical images. These include a fuzzy set based algorithm [3] that allows the detection of healthy brain substructures with a precision of 95% measured with the Dice similarity coefficient (DSC) [4].
"Another method designed for the same task, but based on self-organizing maps technique (SOM) [5], demonstrates an accuracy of 80% measured with the Tanimoto index (TI). Another popular method of healthy brain segmentation based on active contours [6] results in an F-measure of 90%."
"Publicly available methods for automated tumor detection on MRI images may be divided into two groups with respect to the dimensionality of input data. The first group consists of methods operating on one sequence only. Examples of such methods include: template moderate classification proposed by Kaus etÂ al. [7], a method based on fluid vector flow by Lee etÂ al. [8] or a generative model based method proposed by Prastawa etÂ al. [9]. This group of methods gives accuracy at 50â70% DSC. The second group consists of algorithms that use information stored in multiple modalities/sequences. The most popular and successful examples of such approaches are: a technique based on support vector machines (SVMs) proposed by Verma etÂ al. [10], a random forest classifier by Tusiom etÂ al. [11] and an algorithm based on the fuzzy sets theorem proposed by Fletcher etÂ al. [12]. The methods based on multimodal segmentation in a set of different MRI sequences result in average of 80% DSC obtained on independent validation sets. While performing a literature review, one can notice several other techniques, some related to the famous BRATS challenge held annually by MICAII [13]. The accuracy of methods published in the newest BRATS report varies from 57-82% DSC."
"The diffusion of water molecules depends mostly on the temperature and tissue structure, and is independent of patient gender and age [14â19]."
"DWI allows for the quantitative measurement of water molecules activity, in clinical practice known as the apparent diffusion coefficient value (ADC) [18,20]"
"The DWI image translation into an ADC map was performed with the MITK framework [28], with the original DWIâs additionally filtered with the non-local means filter [29,30]."
" To efficiently filter the skull signal, the T2 FLAIR sequence together with the FSL brain extraction tool (FSL-BET) [31] were used."
"To neglect the problem of cerebrospinal fluid (CSF) distortion, OTSU based CSF filtration routines [32] were applied giving a binary CSF mask."
"In the second step of MiMSeg algorithm, k-means clustering [36] was applied to find the groups of GMM components similar in mean value, variance and weight"
The clustering procedure was repeated for number of clusters ranging from 1 to 10 with intial conditions set according to [37]. The optimal number of clusters was determined by Dunnâs cluster consistency criterion [38]. 
"a neural network based technique named Self-Organizing-Map (SOM) [39] methods proposed by Murakami etÂ al. [40], and by Kang etÂ al. [41]."
The unconstrained version of classical GMM technique was used in a comparison study [42].
"According to the WHO, the family of astrocytomas tumours, a subgroup of gliomas originated from abnormal astrocytic cells, is divided into the following four groups with respect to the increasing malignancy [44]"
"However, the absence of enhancement does not necessarily imply a histopathologic diagnosis of low-grade tumor: one-third of nonenhancing diffuse gliomas in adults are high-grade tumors [45]."
"In addition, ADC changes due to the presence of cystic, necrotic, and/or hemorrhagic areas and the influence of artifacts caused by inhomogeneous structures such as the skull base bone and sinus air must be considered [48]."
"To avoid the influence of susceptibility artifacts or ADC changes, regions with infratentorial components or gross hemorrhage should be excluded from measurement or during data analysis by applying robust and efficient pre-processing techniques [32]"
"Android is the dominant operating system for mobile devices; it currently has the largest installed base ( IDC,2013) mainly because (a) it supports a huge variety of different devices such as watches, tablets, TV sets, etc., and (b) it provides end-users with a large variety of applications (a.k.a)"
"We have performed a complete isolated evaluation of the policy evaluation delay and scalability issues in a previous publication by some of the co-authors ( Neisse et al.,2015)."
"According to Callaham (2014) users have an average of 95 apps installed in their Android phones, and according to Au et al.(2012) there are around 750 API methods associated with the available permissions in Android version 4.3.1"
"This average number of apps and possible API methods would be equivalent to around 71 thousand policies, which according to the PDP evaluation results published in Neisse et al. (2015) would correspond to a response time of around 100 ms"
"Kirin, proposed by Enck et al.(2009), is a security service running on the phone which analyses the requested permissions of an app and detects potential security flaws"
"Batyuk et al.(2011) introduced Androlyzer, a server based solution that focuses mainly on informing users about apps potential security and privacy risks"
"Complementary, AppFence proposed by Hornyack et al.(2011) ., also implemented as a modified OS on the basis of TaintDroid, shadows and ex-filtrates users' private data according to their preferences."
"SEDalvik, introduced by Bousquet et al. (2013), proposes a MAC mechanism to regulate information flows between apps objects building on the advantages of Dalvik internal debugger"
"AppGuard, introduced by Backes et al. (2013), is an app instrumentation framework that runs directly in users' device and allows user-centric security policies customisations"
"We also plan to launch a community-based release of our tool where users can contribute with abstract policies and refinement models of privacy sensitive activities, for example, integrating the results of other Android security approaches (eg Rasthofer et al.,2014) that cannot be easily reused at the moment."
" The objective of PAP is more realistic than seeking an approach that performs the best for all problem instances, since such an approach may not even exists because of the no-free-lunch theorem [26]."
" In case that the number of candidate EAs is huge and direct enumeration becomes prohibitive, Eq. (4) can still be integrated with some existing search method, say forward selection [12], to yield at least a sub-optimal set of constituent algorithms within an acceptable time period."
"Despite the promising preliminary results, both empirical and theoretical analysis revealed that the performance of PAP is sensitive to its constituent EAs [17]"
"Although the accuracy of such an estimate is by no means guaranteed, it has been commonly employed in many other scenarios, eg, fine-tuning the parameters of an EA [4], where the performance of an EA needs to be estimated."
"In case that the number of candidate EAs is huge and direct enumeration becomes prohibitive, Eq (4) can still be integrated with some existing search method, say forward selection [12], to yield at least a sub-optimal set of constituent algorithms within an acceptable time period."
"In the context of ensemble learning, the term different is referred to as diversity and a lot of studies have been carried out to investigate how diversity can be quantitatively defined and be utilized to construct good ensembles [22]"
"Neural networks (NN) are mathematical representations modelled on the functionality of the human brain (Bishop, 1995). "
"For example, in Yobas, Crook, and Ross (2000), the authors found that linear discriminant analysis (LDA) outperformed neural networks in the prediction of loan default, whereas in Desai, Crook, and Overstreet (1996), neural networks were reported to actually perform significantly better than LDA"
"The logistic regression model then takes the form: (1)logit( ) log 1- = + Tx,where is the intercept parameter and T contains the variable coefficients ( Hosmer & Stanley, 2000)."
"The least square support vector machine (LS-SVM) proposed by Suykens, Van Gestel, De Brabanter, De Moor, and Vandewalle (2002) is a further adaptation of Vapnik original SVM formulation which leads to solving linear KKT (Karush Kuhn Tucker) systems (rather than a more complex quadratic programing problem)"
"The Nemenyi post hoc test states that the performances of two or more classifiers are significantly different if their average ranks differ by at least the critical difference (CD), given by (12)CD=q , ,KK(K+1)12D.In this formula, the value q , , K is based on the studentised range statistic ( Nemenyi, 1963)"
"Note that, even though the differences between the classifiers are small, it is important to note that in a credit scoring context, an increase in the discrimination ability of even a fraction of a percent may translate into significant future savings ( Henley & Hand, 1997)."
"The analysis of texture images has played a fundamental role in computer vision and pattern recognition during the last decades and the importance of this area of study is illustrated by the number of applications appearing in diverse areas such as Engineering [12], Medicine [1] and Physics [6]."
"Examples are the theory of textons [17], local patterns [14], local affine regions [11], invariants of scattering transforms [15], Fast Features Invariant to Rotation and Scale of Texture [16] and others [3]"
"In simple terms, it is a generalisation to the three-dimensional Euclidean space of the local connected fractal dimension of binary images previously reported in [10,18]."
The parameter rmax was set to 5 in all tests based on the observation in [5] that larger radii severely increase the computational cost and number of descriptors without a significant gain in the classification performance
"In order to improve the diversity of the search in the PSO, various kinds of models have been investigated [4,19]."
"Moreover, in [17], Tatsumi etÂ al. presented a sufficient condition for chaoticity of the system used in CPSO-VQO and showed that the parameter values in the system can be selected by utilizing its bifurcation diagram."
" Moreover,another popular method based on the same idea,which is called the self-organizing hierarchical PSO with time-varying acceleration coefficients(HPSO-TVAC)[13],was proposed."
"Besides,there is another method of generating a chaotic sequence, the gradient model with sinusoidal perturbations (GP) [16]."
"Recently, a combination of the PSO and GP methods, CPSO-VQO,was proposed [17]."
"The chaoticity of system (C1) is guaranteed by the property that a system derived by using the steepest descent method with a perturbation can be chaotic [16], and it is summarized."
It is well-known that there are an infinite number of orbits which are repelled by the snap-back repeller but which are attracted to its neighborhood [12].
"Then,we have the following theorem with respect to chaos in the sense of Li–Yorke[9]"
"In this subsection, we show the results of numerical experiments in which the standard PSO(PSO), the four existing improved PSOs,PSO-IWA,CEPSOA[1],CPSO-VQO[17]andHPSO-TVAC[13],and the five proposed PSOs,PSO-TPC,PSO-SPC,PSO-SDPC,IPSO-SPCandIPSO-SDPC,were applied to the following 50,200,and 400-dimensional benchmark problems."
"For the standard PSO,we used (w,c1,c2)=(0.729,1.494,1.494),as suggested in[5]. "
"These values were also used for the standard updating system (SP)used in CPSO-VQO, PSO-TPC, PSO-SPC, and PSO-SDPC. For HPSO-TVAC, we set (cu,cl)= (2.5,0.5) based on the paper [13]."
"For CPSO-VQO, we used (wd,α,β,m,rmin,dF)=(0.9,0.35,0.045,30,4.0,40.0),as shown in [17],where the width of the rectangular feasible region of each problem in Table1 is scaled into the same constant dF."
"In the SADE, the parameters F and CR were randomly selected from (0.1,1.0) and (0,1), respectively, and we used DE/rand/1/bin strategy,which was based in the paper [2]."
"The users are malware analysts (domain experts) whose main tasks are to select different rules, categorize them by their task and store them in the database as well as manual adaption and/or tuning of found rules ( Wagner et al.,2014)."
"In a preliminary study, we found that malware analysts preferred visualization concepts containing multiple views and arc-diagrams or word trees ( Wagner et al.,2014)."
"By externalizing and storing of the experts' implicit knowledge ( Chen et al., 2009), it can be made system-internally available as computerized knowledge to support further analysis or other analysts."
"Other than that, Pretorius and Van Wijk(2009) point out that it is important for visualization designers to ask themselves: What does the user want to see? and What do the data want to be? as well as how these two points mutually enhance one another"
Goodall et al. (2004) conducted contextual interviews to gain a better understanding of the intrusion detection workflow and proposed a three-phased model in which tasks could be decoupled by necessary know-how to provide more flexibility for organizations in training new analysts
Conti (2007) dedicated a part of his book to visualization for malware detection but focused on the network level
"On the one hand, there are systems for Individual Malware Analysis which support the analysis of a single malware sample to gain new insights (eg, Donahue et al., 2013; W chner et al.,2014)"
"Additionally, Gove et al. (2014) introduced SEEM , which allows the behavioral comparison of a large set of malware samples in relation to the imported DLLs and callback domains."
"Some problem-oriented projects (eg, Mistelbauer et al.,2012) and general frameworks (eg, Tominski,2011) integrate explicit knowledge in visualization but not in a form similar to malware behavior patterns."
"Therefore, we set up a design study project to find a visualization solution that followed a user-centered design process ( Sharp et al.,2007)"
"Furthermore, the graphical summary is based on the Gestalt principle of similarity ( Johnson,2014) in order to support the analyst in quickly recognizing related call combinations"
"It lasted one hour on average and encompassed five analysis tasks, the system usability scale questionnaire (SUS) ( Brooke,1996), and a semi-structured interview"
"The SUS is a standardized, technology-independent questionnaire to evaluate the usability of a system ( Brooke,1996)"
"These comments were also apparent concerning SUS question ten: I needed to learn a lot of things before I could get going with this system ( Brooke,1996), with a score of 50%"
"Categorization of KAMAS: If we categorize KAMAS along the Malware Visualization Taxonomy ( Wagner et al.,2015), we can see that KAMAS can be categorized on the one hand as a Malware Forensics tool which regards to the analysis of malicious software execution traces"
"On a general level, the workflow for knowledge generation and extraction is mostly similar and always includes the system user as an integral part of the loop ( Endert et al.,2014)"
"It can be defined as the minimal total cost of a sequence of elementary edit operations to transform one sequence into the other; for a se- quence x of length m and a sequence y of length n , it can be com- puted in time O(mn ) [8] ."
"In [21] , the authors show that computing the edit distance can be used to classify handwritten digits, where the contours of the digits are represented with an 8-direction chain-code [11] ; a sequence over an eight-letter alphabet, representing the eight cardinal directions that the contour faces when following the outline of an image in a clockwise motion."
Example applications where image retrieval is required include digital libraries and multimedia editing [28] .
Few exact algorithms exist which are able to compute the cyclic edit distance between x and y . Maes designed an elegant divide- and-conquer algorithm which runs in time O(mn log m ) [18] .
Several heuristic approaches exist for approximating the cyclic edit distance. One of the first ones is the Bunke and Buhler ( BBA ) algorithm [6] .
"The ex- tended Bunke and Buhler method ( EBBA ) computes an estimation of the upper bound for the exact cyclic edit distance, also in time O(mn ) [22] ."
"Informally, the q -gram similarity, de- fined as a distance in [29] , is the number of q -grams shared by the two sequences."
The rotation of x that minimises a generalisation of the q -gram distance between x and y is computed using the algorithm in [13]
"We begin with a few definitions, following Crochemore et al. [8] . We think of a string x of length m as an array x [0 . .m −1] , where every x [ i ], 0 ≤i < m , is a letter drawn from some fixed alphabet  of size |  | = O(1) . We refer to any string x ∈  q as a q-gram ."
"In many applications, we only want to count the number of edit operations, considering the cost of each to be 1 [17] . This distance is known as Levenshtein distance , a special case of edit distance where unit costs apply."
"The cyclic edit distance , denoted by δCE ( x, y ), is defined as δCE (x, y ) = min i ( min j δE (x i , y j )) = min i δE (x i , y ) [18] ."
"We give some further definitions following Ukkonen [29] . The q-gram profile of a string x is the vector G q ( x ), where q > 0 and G q ( x )[ v ] denotes the total number of occurrences of q -gram v ∈  q in x ."
Jokinen and Ukkonen [15] showed the following bound which is directly applicable to the Levenshtein distance. Lemma 1 [15] . Let x and y be strings with Levenshtein distance k. Then at least | x | + 1 −(k + 1) q of the | x | −q + 1 q-grams of x occur in y.
"For a given integer parameter β≥1, Grossi et al. [13] defined a generalisation of the q -gram distance by partitioning x and y in βblocks as evenly as possible, and computing the q -gram distance between each pair of blocks, one from x and one from y . The ratio- nale is to enforce locality in the resulting overall distance."
"The algorithm is based on constructing the suffix array [19] forstringxxyandas- signing a rank to the prefix with length q of each suffix with length at least q , based on its order in the suffix array."
"Myers bit-vector algorithm is used to compute the edit distance when using unit costs for insertion, deletion, and substitution [24] ."
Myers bit-vector algorithm was implemented using the SeqAn library [9] .
"DNA datasets were simulated using INDELible [10] , which produces sequences in a (Multi)FASTA file."
"Handwritten digits from the MNIST database [16] were also used and sorted into ten sets. Each image was placed in one of ten datasets, depending on the value of the drawn digit."
"Normalising the chain-code allows the image to be treated as a circular sequence of minimum magnitude. This produces a sequence independent of the rotation of the image. This was calculated by identifying the number of direction changes between two adjacent elements of the chain-code in an anticlockwise direction (see [12] , for details)."
"Assuming a stationary camera, which is a reasonable constraint for most applications, using scene specific information can help to reduce the number of false alarms (e.g., Hoiem et al., 2006)"
"To further improve the classification power and to further reduce the number of required training samples an adaptive classifier using an on-line learning algorithm can be applied (Nair and Clark, 2004; Javed et al., 2005; Wu and Nevatia, 2007a)."
"The complexity object detection can be even further reduced by using classifier grids. The main idea of classifier grids (Grabner et al., 2007; Roth et al., 2009) is to exploit the prior knowledge, that the camera is fixed."
"To overcome this problem, at time t fixed updates (Grabner et al., 2007) can be applied for updating a classifier Ci,t−1. Given a set of representative positive (hand) labeled"
The second problem was addressed in Stalder et al. (2009) and in Sternig et al. (2010b). Stalder et al. (2009) introduced context-based classifier grids to extract additional positive information from a specific scene
Multiple instance learning (MIL) was first introduced by Dietterich et al. (1997).
"Most of these approaches are based on popular supervised learning algorithms such as SVM (Andrews et al., 2003) or boosting (Viola et al., 2005), that are adapted in order to incorporate the MIL constraints."
"Building on (Roth et al., 2009) the model describing the object class (the positive model) is pre-calculated by off-line boosting for feature selection and only negative updates are performed."
"However, in our case we apply the approximated median background model (McFarlane and Schofield, 1995)."
"To demonstrate the benefits of the proposed approach, we run five experiments considering two tasks, namely pedestrian and car detection. We first give an illustrative comparison between the original grid approach (e.g., Roth et al., 2009) and the proposed method."
"For this experiment we used a sequence from the publicly available PETS 2006 dataset consisting of 308 frames (720 × 576 pixels), which contains 1714 pedestrians. We compare our approach to other state-of-the-art person detectors, namely the deformable part model (Felzenszwalb et al., 2008) (FS) and the Histograms of Oriented Gradients approach (Dalal and Triggs, 2005) (DT)."
"In addition, we compared our method to the classifier grid (CG) approach (Roth et al., 2009)"
"Finally, we want to demonstrate that the long-term stability, which was already shown for the original classifier grid approach in Roth et al. (2009), also holds for the IMIL extension"
"In particular, as in Roth et al. (2009) we kept the positive representation fixed and generated a bag of negative samples from an estimated background model."
"In the European Union, the population share of persons older than 60 was 17 percent in 1980 and increased to 22 percent in 2004/5 (it is expected to reach 32 percent in 2030). Life expectancy of men (women) has risen from 68 (76) years to 74 (80) years during the same time period (European Commission, 2007)."
"In addition, many elderly people prefer to grow old in the privacy of their homes rather than in a nursing home. On the other hand, willingness for informal care by relatives is decreasing. This is partly due to the fact that women and men are both working (Tarricone & Tsouros, 2008)."
"Therefore, organizations providing home care services are inclined to optimize their activities in order to meet the constantly increasing demand for home care (Koeleman, Bhulai, & van Meersbergen, 2012)"
"The latter aspect concerns, e.g. penalties for deviations from preferred visit times or from the set of preferred nurses. Trautsamwieser et al. (2011) consider seven different terms in the objective function and Hiermann et al. (2015) consider as many as 13 (see Table 1, column “# OF terms”)."
"Besides the daily routing and scheduling problem, authors have also addressed the long term problem. Nickel, Schröder, and Steeg (2012) look at weekly schedules and link them to the operational planning problem."
"Weekly home care scheduling problems are also addressed in, e.g., Borsani, Matta, Beschi, and Sommaruga (2006), Gamst and Jensen (2012), Cappanera and Scutellà (2013), Maya Duque, Castro, Sörensen, and Goos (2015) and Trautsamwieser and Hirsch (2014), while Nowak, Hewitt, and Nataraj (2013) investigate planning horizons of two to three months, anticipating future requests."
"Successful implementations of home health care scheduling tools are described, e.g., in Eveborn et al. (2006, 2009) or Begur et al. (1997)"
"An overview of home care routing and scheduling and related problems can be found in Castillo-Salazar, Landa-Silva, and Qu (2015). More information on home care worker scheduling is provided in the survey by Gutiérrez, Gutiérrez, and Vidal (2013) and on personnel scheduling in general by Van den Bergh, Beliën, De Bruecker, Demeulemeester, and De Boeck (2013) and De Bruecker, Van den Bergh, Beliën, and Demeulemeester (2015)."
"We then design a metaheuristic solution framework that is based on multi-directional local search (Tricoire, 2012) to solve instances of realistic size (see Section 4)."
"Using the notation of Vidal, Crainic, Gendreau, and Prins (2015), the scheduling problem may be described as in (43)"
"For a detailed description of these concepts and their underlying principles, the reader is referred to Ehrgott and Gandibleux (2002, 2004) and Ehrgott (2005)."
"The algorithm is based on the multi-directional local search framework (Tricoire, 2012) and uses large neighborhood search (LNS) as a subheuristic."
"Multi-directional local search (MDLS) is a recently proposed meta-heuristic framework for multi-objective optimization problems (Tricoire, 2012)"
"Large neighborhood search (LNS) is a metaheuristic which was first introduced by Shaw (1998). It uses the concept of ruin and recreate to define an implicit, large neighborhood of a current solution as the set of solutions that may be attained by destroying a large part of the solution and subsequently rebuilding the resulting partial solution."
"In recent years, many routing problems have been successfully solved using LNS-based methods. For details and an overview of recent developments on LNS, the reader is referred to Pisinger and Ropke (2010)."
"The structure of our MDLS algorithm is very similar to the structure described by Tricoire (2012), although in our case a single-objective local search procedure may result in more than one new solution as is discussed in Section 4.2."
"Several standard removal and insertion operators from the LNS literature (Pisinger & Ropke, 2007; Ropke & Pisinger, 2006; Shaw, 1998) are adapted to the specific problem context of the BIHCRSP"
The implementation of the operator is the same as in Shaw (1998). Initially a job is removed randomly.
"The minimal overtime cost is used instead of the actual overtime cost as the latter would require resolving the scheduling problem when removing a job while the former may be calculated in constant time (Vidal et al., 2015)."
"As in Shaw (1998) and Ropke and Pisinger (2006), a parameter P ≥ 1 is used in all worst removal operators to introduce some randomness in the selection of jobs, thereby avoiding the same jobs to be removed over and over again."
"For a detailed discussion of these operators, the reader is referred to Ropke and Pisinger (2006) and Pisinger and Ropke (2007)."
"To diversify the search, a noise term may be added to the objective functions of the insertion heuristics (Pisinger & Ropke, 2007; Ropke & Pisinger, 2006)."
"However, the parameter values that have been applied are based on both real-life data of two Viennese companies and real-life-based benchmark data for a related problem (Hiermann et al., 2015)."
Four types of instances may be distinguished based on the travel cost and travel time matrices used. The first three types are based on the travel time matrices for car and public transportation provided by Hiermann et al. (2015) and are generated using OpenStreetMap.
"Several quality indicators have been proposed in the literature to evaluate approximations of the Pareto frontier generated by heuristic solution procedures (Knowles, Thiele, & Zitzler, 2006; Zitzler, Thiele, Laumanns, Fonseca, & Grunert da Fonseca, 2003)."
"The hypervolume indicator (IH(A)), introduced by Zitzler and Thiele (1999), measures the portion of the objective space that is weakly dominated by an approximation set A."
"Technical debt (TD) is a metaphor used to describe a situation in software development, where a shortcut or workaround is used in a technical decision (Kruchten et al., 2012b)."
"TD has also similarities to three aspects of financial debt: repayment, interest, and in some cases high cost (Allman, 2012)."
"In software development, a shortcut or workaround can give the company a benefit in the short term with quicker release to the customer and an advantage in time-to-market over the competition (Kruchten et al., 2012a; Yli-Huumo et al., 2015a). "
"However, if these shortcuts and workarounds are not repaid, TD can accumulate and hurt the overall quality of the software and the productivity of the development team in the long term (Zazworka et al., 2011b)."
"The current literature has identified and developed some tools and practices to conduct TDM. However, according to a recent mapping study, the problem is the lack of empirical evidence about TDM in a real-life software development environment (Li et al., 2015a)."
"For data collection and analysis, we used the eight TDM activities identified by Li et al. (2015a) in semi-structured interviews to gather empirical data about TDM in the selected software development teams."
"We used the exploratory case study method (Robson, 2002) to answer the following main research question:"
"Technical debt management can be separated into the following activities: identification, measurement, prioritization, prevention, monitoring, repayment, representation/documentation, and communication (Li et al., 2015a). "
"There are a number of possible methods, models, practices or tools for every TDM activity (Li et al., 2015a). They have been developed and suggested in the literature, but they lack empirical evidence of their usability and functionality (ibid.). "
"Some software development teams may use more time on TDM, while some development teams may not pay much attention to it (Power, 2013). Therefore, it is important to understand if it is possible to distinguish between different maturities of TDM, similarly as in the capability maturity model (CMM)(Paulk et al., 1993). The results of this study can be used to develop a similar maturity model for TDM, which researchers and practitioners could use to conduct more research, or to improve companies’ internal and external practices."
"The TD metaphor was first associated with compromises on the code level of software (Cunningham, 1992). In addition, terms like code smells (Fowler et al., 1999) have described situations where poor technical choices in software development have caused problems in code quality and architectural soundness. However, the TD metaphor has been rapidly expanded after the initial concept on the code level, and it has been associated with other stages of the software development lifecycle as well (Tom et al., 2013; Alves et al., 2014)."
"The TD metaphor was first associated with compromises on the code level of software (Cunningham, 1992). In addition, terms like code smells (Fowler et al., 1999) have described situations where poor technical choices in software development have caused problems in code quality and architectural soundness."
" However, the TD metaphor has been rapidly expanded after the initial concept on the code level, and it has been associated with other stages of the software development lifecycle as well (Tom et al., 2013; Alves et al., 2014)."
"The current literature identifies such terms as requirements (Brown et al., 2010), design (Zazworka et al., 2011b; Zazworka et al., 2011a), architectural (Nord et al., 2012), test (Brown et al., 2010), process (Lim et al., 2012), documentation (Kruchten et al., 2012a), and people debt (Kruchten et al., 2012b) to demonstrate the same effect of shortcuts or workarounds happening in the other stages of the software development lifecycle."
"Shortcuts and workarounds in software development usually happen for intentional reasons, such as for business deadlines and development complexity (Yli-Huumo et al., 2015a)."
"Time-to-market and customer feedback are important factors for companies’ success, and it is essential to deliver solutions on time (Lim et al., 2012). This is the reason why business stakeholders are often more focused on deadlines and customers than the actual quality of the software, which is more in the developers’ interest area (Barney et al., 2008; Boehm, 2006). Therefore, strict deadlines may sometimes force the development team to create solutions with second-tier quality to meet the requirements within the deadlines set by the business stakeholders (Yli-Huumo et al., 2014)."
"TD can also occur unintentionally (McConnell, 2007). The reason for unintentional TD can be lack of competence, a need to upgrade existing technologies, or a customer or market -induced need for change"
"When taking TD, companies are able to speed up the release cycles to the customer, which can increase customer satisfaction and provide advantage in the market. Another benefit for companies is customer feedback (Yli-Huumo et al., 2015b). Companies are able to adjust the product and its business model based on faster customer feedback"
"According to some scholars, TD should be associated only with intentional decisions happening in the code base, and messy code should not be counted as TD, while some think that old technologies in legacy software should also be counted as TD (Norton, 2009; Fowler, 2009)"
"Even though concepts such as social debt (Tamburri et al., 2013) and people debt (Alves et al., 2014) describe similar phenomena of having shortcuts and non-optimal solutions in software development and organization, we believe that they should be categorized as sources for TD rather than as actual TD"
 Li et al. (2015b) have also developed similar TD list management for architectural technical debt (ATD).
" Implementing coding standards to the development process can prevent TD, when the developers have a cohesive way to produce a similar style code, which makes it readable and modifiable (Green and Ledgard, 2011)."
"Code reviews can be used to check other developers' solutions before the release to catch possible TD issues in the design (Mantyla and Lassenius, 2009). Also simple practices in agile methodologies, such as the Definition of Done practice can reduce TD in the early stages of development (Davis, 2013)."
" A similar approach has also been used by other researchers to identify and document TD issues in order to make TD easier to manage (Zazworka et al., 2013)."
"Klinger et al. (2011) interviewed four experienced software architects to understand how decision-making regarding TD was conducted in an enterprise environment. The results showed that the decisions related to TD issues were often informal and ad hoc, which led to a lack of tracking and quantifying the decisions and issues."
"In addition, an interpretive case study aims at understanding phenomena through the participants’ interpretation of their context (Runeson and Höst, 2008)."
"We decided to use semi-structured interviews (Charmaz, 2014) for data collection, which makes this research a flexible study (Runeson and Höst, 2008). Semi-structured interviews include a mixture of open-ended and specific questions, designed to elicit not only the information foreseen, but also unexpected types of information (Seaman, 1999)."
The analysis of causes and effects of TD is available as a separate publication by Yli-Huumo et al. (2014). The second reason for focusing more on TDM was the publication of the TDM mapping study by Li et al. (2015a).
"In exploratory case studies, the technique for the analysis of qualitative data is hypothesis generation (Seaman, 1999). "
"The data coding and analysis were completed in various steps, guided by the work of Robson (2002). "
Similar practices identified in a study by Codabux and Williams (2013) were reengineering and repackaging.
" A set of other practices for TD prevention have been identified in other studies (Codabux et al., 2014; Krishna and Basu, 2012). These practices include approaches such as education and training, pair programming, test-driven development, refactoring, continuous integration, conformance to process and standards, tools, and customer feedback (Codabux et al., 2014). "
"Developers may also value documentation differently, and they document only issues that they personally think are important (Lethbridge et al., 2003). "
"The literature has suggested approaches for TD prioritization (Eisenberg, 2012; Seaman et al., 2012; Theodoropoulos et al., 2011; Zazworka et al., 2011a). "
" Prioritization can also be based on customer needs, but this can leave the most important TD from the technical perspective out of sight (Codabux and Williams, 2013). These prioritization issues exist also in requirements prioritization (Lehtola and Kauppinen, 2006)."
"When a development team considers for example the criteria in SonarQube (2015) as TD, in can guide TD management and other TD activities. However, TD can also be considered to consist of issues of a larger scale, such as architectural or structural issues and technology gaps (Kruchten et al., 2012a). "
"The goal of TDM is to provide practices and tools to manage and reduce TD during software development (Li et al., 2015a). "
"External validity is concerned with ‘to what extent it is possible to generalize the findings, and to what extent the findings are of interest to other people outside the investigated case’ (Runeson and Höst, 2008, p. 154). "
"On the basis of our findings we believe that TDM in software development has similarities to the characteristics of the capability maturity model (CMM) (Paulk et al., 1993). "
"A similar maturity model to CMM is also adaptable in TDM, where development teams have different TDM maturities in activities and practices. This kind of maturity as a concept has been applied to other processes and domains as well (De Bruin et al., 2005)."
" Chen [7] used fuzzy numbers to determine the fuzzy reliability of the two systems, whereas Singer [12] used LR-type fuzzy numbers to consider the fuzzy reliability problem."
"Therefore, the triangular fuzzy numbers used by Chen [7] are R A=R B=(0.00888,0.02,0.03112;1)R C=R D=(0.75552,0.80,0.84448;1)R E=(0.94434,1.00,1.05566;1)R F=R G=(0.04722,0.05,0.05278;1)R H=(0.00944,0.01,0.10560;1) Using Definition 8, we defuzzified R X(1) and obtained the estimated reliability of the system in the fuzzy sense as dR X(1),0 =0.137755"
[C] Comparison of this study with that of Cheng and Mon [6].Cheng and Mon [6] considered the following:
"Existing models predict different vortex number densities and they are only valid for the inertial subrange of the energy spectrum of turbulent flows. No model is available which is valid for a wider range of flow situations [7,8]. "
" In addition the vortex number distributions should be used to derive expressions relating to the fractional rate of surface renewal and mass transfer coefficients across gasâliquid and solidâliquid interfaces [9]. For these reasons, a systematic evaluation of available models on vortex number density is required as a foundation for further investigations."
"Several Eulerian methods for flow structure visualization are proposed in the literature. They are generally formulated in terms of the invariants of the velocity gradient tensor. These criteria includes the iso-surfaces of vorticity, stream lines, Helicity, Q-criterion, complex eigenvalues of the velocity gradient tensor, Î»2, swirl strength and pressure minimum [21]. "
"The applications are of a wide range and significance, including gesture recognition and human computer interaction ( Bilalet al., 2012; Radlak and Smolka, 2012; Nalepa et al., 2014), objectionable content filtering ( Lee et al.,2007), image retrieval ( Kruppa et al., 2002), image coding using regions of interest ( Chen et al., 2003), and many more."
An approach introduced by Hsu et al. (2002) takes advantage of common skin color properties in nonlinearly transformed YCbCr color space using an elliptical skin color model
"Skin color can be modeled using a number of techniques, including the Gaussian mixture model ( Greenspan et al., 2001) and the Bayesian classifier ( Jones and Rehg, 2002)"
An approach for adapting the segmentation threshold in the probability map based on the assumption that a skin region is coherent and should have homogenous textural features was introduced by Phung et al. (2003)
"Analysis of facial regions for effective adaptation of the skin model to local conditions was investigated by Fritsch et al.(2002); Stern and Efros, 2002; Kawulok, 2008; Kawulok et al., 2013 and Yogarajah et al., 2012."
"Simple textural features were used to boost the performance of a number of skin detection techniques and classifiers, including the ANN ( Taqa and Jalab,2010), non-parametric density estimation of skin and non-skin classes ( Zafarifar et al., 2010 ), Gaussian mixture models ( Ng and Pun, 2011), and many more ( Forsyth and Fleck,1999, Conci et al., 2008, Fotouhi et al., 2009, Abin et al.,2009)"
"A number of skin segmentation techniques emerged based on this observation: Kruppa et al. (2002) assumed that the skin blobs are of an elliptical shape, a threshold hysteresis was applied by Argyros and Lourakis (2004) and recently by Baltzakis et al. (2012)"
"In the case of the DSA ( del Solar and Verschae, 2004), the ROC curves were obtained by applying different values of the diffusion threshold ( ), explained earlier in Section 2"
"In order to provide a thorough comparison, we also investigated how the wavelet-based method behaves when the textural features are extracted from the skin probability maps rather than from the luminance as proposed in the original work of Jiang et al. (2007)."
The analysis of skin probability map domain for skin segmentation using a controlled diffusion was proposed by del Solar and Verschae (2004)
"Although the color-based skin models can be efficiently adapted to a given image, it was proved by Zhu et al.    (2004)  that it is hardly possible to separate skin from non-skin pixels using such approaches"
Conditional random fields were used by Chenaoua and Bouridane (2006) to exploit spatial properties of skin regions
An approach based on the cellular automata for determining skin regions was proposed by Abin et al. (2009).
"Although empirical experiments have shown that it is practically usable on a wide range of programs ( Fraser and Arcuri, 2012), Genetic Algorithms are a global search technique, which tend to induce macro-changes on the test suite"
"For example, to generate tests for specific branches to achieve branch coverage of a program a common fitness function ( McMinn, 2004) integrates the approach level (number of unsatisfied control dependencies) and the branch distance (estimation of how close a branching condition is to being evaluated as desired)"
"However, there remain several different parameters ( El-Mihoub et al., 2006): How often to apply the individual learning? On which individuals should it be applied? How long should it should be performed? In EvoSuite, the choice of how often to apply local learning depends on two parameters: Rate: The application of individual learning is only considered every r generations"
"As the problem is too complex to perform a theoretical runtime analysis (e.g., such as that presented by Arcuri (2009)), we therefore aim to empirically answer the following research questions"
"This is different from the result of our initial experiment ( Fraser et al.,2013), where the best configuration used seeding, small population size (five individuals), low rate of local search (every 75 generations), and a small budget of five fitness evaluations for local search"
"A particular aspect of this real-life, unbiased sample of classes is that the problems it represents are quite different to those considered as difficult search problems ( Fraser and Arcuri,2012): for example, a large share of the classes have environmental dependencies that make high coverage with EvoSuite impossible"
"Methods by which such OSINT data may be used to increase effectiveness in this manner include (but are not limited to): selection of vulnerable personalities, inclusion of ploys personally attractive to the target, and impersonation of a person in authority ( Huber et al.,2009)."
"The nearest approximation we are aware of is Scheelen et al.(2012), who investigated a single company by connecting with followers on LinkedIn, where the social media structure is based around employment."
"Irani et al. (2009) suggest that a record-matching approach to the problem can be fruitful, with identifiers like last name, birth year and country unlikely to change across records"
"Working with a wider range of features, Malhotra et al. (2012) design an ensemble classifier, with subclassifiers relying on individual features such as profile pictures and usernames."
"Goga et al.(2013) exploit innocuous social media profile information such as time-stamps, geographical location and writing styles to match user profiles, demonstrating that even where usernames and other traditional identifiers are disguised, users can still be identified based on their usage of the media"
"The general identity resolution approach of pairwise comparison is supported even more broadly by similar approaches in other domains of security analytics ( Zhang et al., 2016)."
"Each interviewee held internationally recognised certification in the domain area, reaching a minimum level of CREST CRT , allowing level of expertise to be compared and verified to international standards ( Knowleset al.,2016)"
"Furthermore, Kotson and Schulz (2015) and Dewan et al (2014) propose that organisations could use natural language processing techniques to maintain awareness of their online footprint, to be compared with received phishing emails, allowing identification of collection of OSINT by an attacker, and potential early warning of an Advanced Persistent Threat (APT)"
neither general MV-algebras [19] nor even the special case of Łukasiewicz MV-algebra are considered in this paper.
"According to the most commonly accepted definition, Big Data is characterized by the 4 Vs  [3]: volume, velocity, variety, and veracity"
"From the business perspective, the goal of these technologies is to process fast input streams, obtain real-time insights, and enable prompt reaction to them  [4]."
The resurgence of interest in CEP and SP systems has been accompanied by the use of cloud environments as their runtime platform. Clouds are leveraged to provide the low latency and scalability needed by modern applications  [5–7].
"First, cloud environments are subject to variations that make it difficult to reproduce the environment and conditions of an experiment  [9]"
"Simulators have been used in many different fields to overcome the difficulty of executing repeatable and reproducible experiments. Early research into distributed systems  [10] and grid computing  [11] used simulators, as well as the more recent field of cloud computing  [12–14]."
CEPSim extends CloudSim   [12] using a query model based on Directed Acyclic Graphs (DAGs) and introduces a simulation algorithm based on a novel abstraction called event sets.
"This article significantly extends the authors’ previous work  [15] by improving the discussion about CEPSim’s goals and assumptions, by introducing the event set concept, by presenting detailed descriptions of all simulation algorithms and a thorough evaluation of CEPSim."
"The basis of Complex Event Processing (CEP) was established by the work of Luckham on Rapide   [10], a distributed system simulator.Later on, the concepts were generalized and applied to the enterprise context in another study by Luckham  [16]. At about the same time, the database community developed the first classical Stream Processing (SP) systems such as Aurora   [17] and STREAM   [18]. CEP and SP technologies share related goals, as both are concerned with processing continuous data flows coming from distributed sources to obtain timely responses to queries  [19]."
"In CEP systems, users create queries (or rules) that specify how to process input event streams and derive “complex events”. These queries have usually been defined by means of proprietary languages such as Aurora Stream Query Algebra (SQuAl)  [17] and CQL  [21]."
"Despite standardization efforts  [22], a variety of languages are still in use today. Cugola and Margara  [19] classify existing languages into three groups"
"Pattern-based: languages are used to define patterns of events using logical operators, causality relationships, and time constraints. The Rapide   [10] language is an example of this category."
"The recent emergence of cloud computing has been strongly shaping the Big Data landscape. Many authors have recognized the symbiotic relationship between these areas  [23–25], as cloud computing environments can be used to store and process Big Data and also to enable new models for data services. For instance, Chang and Wills  [26] used a cloud platform to store big biomedical data, whereas Grolinger et al.  [27] proposed a platform for knowledge generation and access using cloud technologies."
"Current CEP research has been strongly influenced by cloud computing too. For instance, TimeStream   [5], StreamCloud   [6], and StreamHub   [7] are CEP systems that use cloud infrastructures as their runtime environments."
The prevalence and success of MapReduce has motivated many researchers to work on systems that leverage its advantages while at the same time try to overcome its limitations when used for low-latency processing. StreamMapReduce   [29] and M3   [30] are examples of MapReduce-inspired systems intended for stream processing
"Simulators are a popular tool that has been used in Grid Computing research  [11,33] for many years."
"Because of its limitations, CloudSim has originated many extensions in the literature  [9,34,35]."
"Guérout et al.  [34], on the other hand, focused on implementing the DVFS model on CloudSim. Finally, Grozev and Buyya  [35] presented a model for three-tier Web applications and incorporated it into CloudSim."
"Finally, the iCanCloud simulator  [14] is similar to CloudSim, but it can also parallelize simulations and has a GUI to interact with the simulator. Its application model, however, is based on low-level primitives and needs to be significantly customized to represent CEP applications"
"For instance, most CEP systems based on imperative languages also use DAGs to represent user queries. This is the case with Aurora   [17], StreamCloud   [6], Storm   [31], S4   [32], FUGU   [37], and many others."
"Systems using declarative languages, on the other hand, create execution plans from queries that can often be mapped into DAGs  [5,18]. Even for pattern-based query languages, previous studies  [38] have shown that is possible to transform them into DAGs."
"A stateless operator, or simply an operator, can process incoming events in isolation with no dependency on any state computed from previous events. For example, an Aurora filter is an operator that routes events to alternative outputs based on attribute values  [17]."
"A scheduling strategy can fundamentally determine the performance of a CEP system by optimizing for different aspects of the system, such as overall QoS  [17] or memory consumption  [40]. Because of this significance, CEPSim also allows different scheduling strategies to be plugged in and used during a simulation."
"The queries used in the experiments in this section have been extracted from Powersmiths’ WOW system  [41], a sustainability management platform that draws on live measurements of buildings to support energy management. Powersmiths’ WOW uses Apache Storm  [31] to process in near real-time sensor readings coming from buildings managed by the platform"
"This validation approach is similar to the ones adopted by other simulators, such as NetworkCloudSim  [9], iCanCloud  [14], and Grozev and Buyya  [35]."
"This experiment analysed CEPSim’s behaviour when simulating multiple queries running concurrently. To do so, first a Storm cluster was created at the Amazon EC2 service  [45]"
"DSPLs produce software capable of adapting to changes, by means of binding the variation points at runtimeÂ  [1]. This means that we have to model the elements that could be adapted dynamically as dynamic variation points and generate, at runtime, the different variants of the DSPL."
"Variability is modelled at different abstraction levels, mostly using feature models (FM)  [3] at the requirements level and UML profiles or Architecture Description Languages (ADLs)  [4-6] at the architectural level."
"Both services are designed to be integrated in a middleware for adaptive applications developmentÂ  [21], although in this paper we focus on presenting the details of how the DRS accomplishes the runtime reconfiguration of mobile applications. "
" In contrast, in DSPLs the variability model describes the potential range of variations that can be produced at runtime for a single product, i.e.Â  the dynamic variation points, which must refer to the system architectural components. Therefore, in DSPLs the system architecture supports all possible adaptations defined by the set of dynamic variation pointsÂ  [1].Then, as part of a DSPL definition the engineer must define:"
"In this context, most DSPL approaches share some common properties with the Autonomic Computing paradigmÂ  [2] (AC) such as the monitoring of the environment and the generation of successive configurations."
"An overview of our approach and a case study are also presented.3.1ChallengesFrom the literature it is possible to identify the differences between existing DSPL approaches. The main ones areÂ  [24]: (1)Â how they model the dynamic variation points; (2)Â whether the successive configurations generated by the approach are optimal regarding some criteria or not; (3)Â if the reconfiguration plan is generated at design or runtime; (4)Â the decision making process used to trigger a reconfiguration, and (5)Â if they can be used to develop applications for resource constraint devices, or not."
" So, at run-time we generate the successive software architecture configurations by binding the architectural variation points specified in a variability languageÂ  [25]. Regarding the variability language used, we propose to use CVL as it eases the generation of the architectural variation points compared with, for instance, feature modelsÂ  [25]. In CVL the VSpecs tree is linked with the base model (i.e.Â the software architecture model) so, the architectural variation points are obtained almost directly, as we will show later."
" Note, that an exact algorithm cannot be used because the problem to be solved has been proven to be NP-hard (non-deterministic polynomial-time hard)Â  [26]."
"Generating the reconfiguration plan at runtime. Most DSPL approaches generate, at design time, the configurations that will be deployed at runtime  [13,27-29]. However, the potential number of configurations normally grows exponentially with the number of dynamic variation points. In order to cope with this serious problem, some approaches consider only a subset of the valid configurations at runtime (e.g.Â the most probable ones), which are pre-loaded in the system."
"Concretely, as shown inÂ  [23], exact techniques can only be applied to small cases at the cost of a very high execution time. Nevertheless, artificial intelligence algorithms can find nearly-optimal solutions in an efficient and scalable way. "
"Synthesis, also known as Church's solvability problem [5], is the problem of defining a circuit that continually reacts on an infinite input stream by producing one output letter after receiving one input letter"
"We assume that the reader is familiar with temporal logics [11,6] and the distributed synthesis question [12,10,9,7]. For architectures [12,9,10,7], it is enough to know that they are directed graphs, whose vertices are processes that operate synchronously on a joint system clock. "
e follow [7] by restricting the variables such that each variable can only occur on the outgoing edges of one process (but may be read by many processes) and by allowing for processes that have a fixed finite implementation
"The extension to decidable architectures is a straightforward adaptation from the extension described by Finkbeiner and Schewe [7]. Their argument is that, in the case where there is an information fork, a situation where there are two processes p0 and p1 who can receive information from the environment â directly or forwarded through a communication chain of arbitrary length â such that this pathway cannot be intercepted by the other process, then synthesis is undecidable."
"In this section we establish a close relation between our notion of strong regularity and the one usually studied in max min algebra [2 4,12,14]"
"As K is closed, by the usual Minkowski theorem [25, Corollary 18.5.1] it can be represented as the set of positive linear combinations of its extremal rays (recall that w K is called extremal if u+v=w and u,v K imply that u and v are proportional with w), which generate the whole Lin(C y)"
"For a monograph in conventional convexity see, eg, Rockafellar [25]."
"The utility function introduced there relies on the notion of possibilistic mixture, where the possibilistic mixture (which under some natural conditions is also a possibilistic measure [10]) of the possibilistic measures 1, 2 with possibilities , ,max ( , )=1, is defined as max (min ( , 1),min ( , 2)), that is, as a point on the max min segment [ 1, 2]"
"For example, in [8] Dubois and Prade developed an axiomatic approach to quantitative utility theory"
"The ability to integrate multisensory information is a fundamental feature of the brain that provides a robust perceptual experience for an efficient interaction with the environment ( Ernst & Bulthoff, 2004; Stein & Meredith, 1993; Stein, Stanford & Rowland,2009)"
"Similarly, computational models for multimodal integration are a paramount ingredient of autonomous robots to forming robust and meaningful representations of perceived events (see Ursino, Cuppini, & Magosso (2014 for a recent review)"
"Multimodal representations have been shown to improve robustness in the context of action recognition and action-driven perception, human-robot interaction, and sensory-driven motor behavior ( Bauer, Magg, & Wermter, 2015; Kachouie, Sedighadeli, Khosla, & Chu, 2014; Noda, Arie, Suga & Ogata 2014)"
"Since real-world events unfold at multiple spatial and temporal scales, artificial neurocognitive architectures should account for the efficient processing and integration of spatiotemporal stimuli with different levels of complexity ( Fonlupt, 2003; Hasson, Yang, Vallines, Heeger, & Rubin, 2008; Lerner, Honey, Silbert, & Hasson, 2011; Taylor, Hobbs, Burroni, & Siegelmann, 2015)"
"The human cortex comprises a hierarchy of spatiotemporal receptive fields for features with increasing complexity of representation ( Hasson et al., 2008; Lerner et al., 2011; Taylor et al., 2015), ie higher-level areas process information accumulated over larger spatiotemporal receptive windows"
"During their development, children have a wide range of perceptual, social, and linguistic cues at their disposal that they can use to attach a novel label to a novel referent ( Hirsch-Pasek, Golinkoff, & Hollich, 2000, chapter 6)"
"Recent experiments have shown that human infants are able to learn action word mappings using cross-situational statistics, thus in the presence of sometimes unavailable ground-truth action words ( Smith & Yu, 2008)"
"This hypothesis is supported by neurophysiological studies evidencing strong links between the cortical areas governing visual and language processing, and suggesting high levels of functional interaction of these areas for the formation of multimodal representations of audiovisual stimuli ( Belin, Zatorre, Lafaille, Ahad, & Pike, 2000; Foxe et al., 2000; Belin, Zatorre, & Ahad, 2002; Pulverm ller, 2005; Raij, Uutela, & Hari, 2000)."
"From a neurobiological perspective, neurons selective to actions in terms of time-varying patterns of body pose and motion features have been found in a wide number of brain structures, such as the superior temporal sulcus (STS), the parietal, the premotor and the motor cortex ( Giese & Rizzolatti, 2015)"
"In particular, it has been argued that the STS in the mammalian brain may be the basis of an action-encoding network with neurons driven by the perception of dynamic human bodies ( Vangeneugden, Pollick, & Vogels, 2009), and that for this purpose it receives converging inputs from earlier visual areas from both the ventral and dorsal pathways ( Beauchamp, 2005; Garcia & Grossman, 2008; Thirkettle, Benton, & Scott-Samuel, 2009)"
"Furthermore, neuroimaging studies have shown that the posterior STS shows a greater response for audiovisual stimuli than to unimodal visual or auditory stimuli ( Beauchamp,Lee, Argall, & Martin, 2004; Calvert, 2001; Senkowski, Saint-Amour, Hfle, & Foxe, 2011; Wright, Pelphrey, Allison, Mckeown, & Mccarthy, 2003)"
"Thus, the STS area is thought to be an associative learning device for linking different unimodal representations and accounting for the mapping of naturally occurring, highly correlated features such as body pose and motion, the characteristic sound of an action ( Barraclough, Xiao, Baker, Oram, & Perrett, 2005; Beauchamp et al., 2004) and linguistic stimuli ( Belin et al., 2002; Stevenson & James, 2009; Wright et al. 2003)"
"These findings together suggest that multimodal representations of actions in the brain play an important role for a robust perception of complex action patterns, with the STS representing a multisensory area in the brain network for social cognition ( Adolphs, 2003; Allison, Puce, & McCarthy, 2000; Beauchamp, 2005; Beauchamp, Yasar, Frye, & Ro, 2008)."
"Each network layer comprises a self-organizing neural network that employs neurobiologically-motivated Hebbian-like plasticity and habituation for stable incremental learning ( Marsland, Shapiro, & Nehmzow, 2002)"
"Network layers 1 and 2 comprise a two-stream hierarchy for the processing and subsequent integration of body pose and motion features, resembling the ventral and the dorsal pathway respectively for the processing of complex motion patterns ( Giese & Poggio, 2003)"
"The integration of pose and motion cues is carried out in network layer 3 (or G STS) to provide movement dynamics in the joint feature space ( Parisi et al.,2015)"
"Hierarchical learning from contiguous Growing When Required (GWR) networks ( Marsland et al., 2002) shapes a functional hierarchy that processes spatiotemporal visual patterns with an increasing level of complexity by using neural activation trajectories from lower-level layers for training higher-level layers"
"The modeling of neurobiologically observed principles underlying audiovisual integration in the STS for speech and non-speech stimuli, such as superadditivity ( Calvert, Campbell, & Brammer, 2000), spatial and temporal congruence ( Bushara, Grafman, & Hallett, 2001; Macaluso,George, Dolan, Spence, & Driver, 2004), and inverse effectiveness ( Stevenson & James, 2009), was out of the scope of this paper and will be subject of future research."
"For instance, several neurophysiological studies have evidenced strong interaction between the visual and motor representations, more specifically including the STS, parietal cortex, and premotor cortex (see Giese & Rizzolatti (2015) for a recent survey), with higher activation of neurons in the motor system for biomechanically-plausible, perceived motion sequences ( Miller & Saygin, 2013)"
"Motivated by the process of input-driven self-organization exhibited by topographic maps in the cortex ( Miikkulainen, Bednar, Choe, & Sirosh, 2005; Nelson, 2000; Willshaw & von der Malsburg, 1976 ), we proposed a learning model encompassing a hierarchy of Growing When Required (GWR) networks ( Marsland et al., 2002)"
"This kind of hierarchical aggregation is a fundamental organizational principle of cortical networks for dealing with perceptual and cognitive processes that unfold over time ( Fonlupt, 2003)."
"It has been shown that lexical features can be learned using recursive self-organizing architectures ( Strickert & Hammer, 2005), obtaining action word representations from a phonemic representation of recognized audio"
"Such a processing scheme would be in line with neurophysiological evidence supporting the hierarchical processing of aural features in the auditory cortex with increasing temporal receptive windows ( Lerner et al., 2011 )"
"Mental imagery concerns cognitive processes for the creationandmanipulationofmentalimages,andfordecisionmakingtasksonvisualobjectmatching(Kosslyn,1996;Lamm,Windischberger,Moser,&Bauer,2007).Inatypicalmentalrotationexperimentofcognitivepsychology,aparticipanthastomentallyrotateanobjectperceivedinapicturetodecideifitisthesameasatargetobjectordifferentfromit(i.e.aflippedversionofit),andthenindicatetheanswerbypressingoneoftwobuttons(Shepard&Metzler,1971;Wexler,Kosslyn,&Berthoz,1998)."
"Mental rotation has been widely investigated not only incognitive psychology, but also in cognitive neuroscience andcomputational modelling (Kosslyn, 1996; Zacks, 2008). Initially,it was proposed that the brain mechanisms underlying mentalrotation mainly involve visual and spatial perception systems(Corballis & McLaren, 1982; Shepard & Metzler, 1971). "
"According to this view, off-linecognition, such as mental rotation and imaging, is body based:‘‘even when decoupled from the environment, the activity of themind is grounded in mechanisms that evolved for interactionwith the environment—that is, mechanisms of sensory process-ing and motor control’’ (Wilson, 2002). "
"Brain-imaging evidence on the brain areasmost involved in mental rotation supports the idea that mentalrotation indeed depends on a strong integration of sensorimotor processes and covert mental simulation of motor movements (see Zacks, 2008, for a review). In particular, based on refined experimental paradigms, mental rotation has been proposed to Involve the following processes (Lammetal.,2007):"
"Indeed, endowing robots with mental rotation capabilities could increase their planning abilities in relation to the manipulation of objects, in particular, as planning in robots has mainly focused on navigation (Baldassarre, 2001, 2003; Dissanayake, Newman, Clark, Durrant-Whyte, & Csorba, 2001; Meyer & Filliat, 2003) and reaching tasks (Khatib,1986 ; Masehian & Sedighizadeh, 2007) rather than on object manipulation. "
"Based on this evidence, these ar-eas are thought to play a key role in implementing the propri-oceptive and visual information integration and transformationsupportingthecoreprocessesofthedynamicmentalrotationpro-cesses(Zacks,2008)"
"Givenitshigh-levelwithinthemotorhierarchy,thisareamightorchestratemental rotation at a high-level, as suggested by its role in motorimaging(Grafton,Arbib,Fadiga,&Rizzolatti,1996)"
"Neural maps are suitable to model cortical areas as they capture their important2D topological organisation and also facilitate the analysis andvisualisation of the processes occurring within them (Caligiore,Parisi, & Baldassarre, 2014). Population codes (Pouget, Dayan, &Zemel, 2003) are based on the idea that information (on stimuli and actions) is encoded in the brain on the basis of the activation of populations of units organised in neural maps having a broad response field"
"To implement the decision making process involved in themental rotation task, the model uses amutual inhibition model (Bogacz et al., 2006; Usher & McClelland, 2001)"
"This model (together with otheranalogous models, e.g. Bogacz et al., 2006) is very important, as it allows there production of there action times of ten recorded in psychological experiments (Caligioreetal., 2010,2008; Erlhagen & Schöner, 2002). It is one of the most accredited models of decision making processes taking place in the human brain (Bogacz, 2007)."
"For example, the interpolation methods used in some commercial mocap systems such as EVaRT and Vicon are only suitable to deal with the short-time (<0.5s) data missing problem [1]. "
"Aristidou et al. [1] used Kalman Filters to estimate missing markers in real-time without the support of any human model. However, the filtered human motion exhibits visible short latency. Moreover, Kalman Filters may fail when markers are missing or the motion data is corrupted by noise for an extended time period [12]."
"As shown in the last sub-image of Fig11(c), one head marker predicted by Dynammo [30] drifts a little away due to the reason that this marker is missing for a long period of time at the end of the motion data"
"To demystify this inconsistency, we find that SVT [30] convergences too early in handling a large-scale imperfect human motion matrix in these two cases where the total frame numbers are 4840 and 4905"
"As shown in [1–4], optimal preview control for linear systems has been well studied. The problem of preview control has been extended via further research."
"In [5–9], H∞ and H2 criteria were introduced into preview control, and robust optimal preview control for linear and nonlinear systems has been discussed."
The authors in [10–12] investigated optimal preview control for systems with multiple sampling rates by using the discrete-time lifting technique.
"In [13–15], optimal preview control for linear time-variant systems was considered."
"In recent years, based on descriptor system theory, researchers have considered some descriptor causal systems and designed an optimal controller with preview compensator [16–18]."
"The authors in [31] designed a piecewise linear time-invariant switching control law, which leads to a guarantee of Lyapunov stability with an exponential rate of convergence for the state."
"Based on the method, a kind of switching controller for discrete time system was proposed [32]."
"In [33], a localization-based method was introduced, which is manifested as the rapid convergence of the switching controller. Ref. [34] promoted the results and discussed the problem of optimal localization. Other several methods of adaptive control based on multiple models were examined in [35]."
" By defining monitoring function and switching principle similarly to [31], the problem is formally converted into a normal optimal control problem that includes preview information. The preview control controller for systems with large uncertainties can be finally obtained in this way, and adaptive preview control theory using multiple models can be established."
"Nuclear power is often regarded to be amongst the safest forms of electricity generation, taking into account the complete world-wide electricity production chains, with some arguing that this result holds even after the possibility of large nuclear accidents is included in the analysis (Kearns, Thomas, Taylor, & Boyle, 2012). "
"A number of methodologies and software packages, often referred to as Decision Support Systems, have been developed to aid this (Bartzis et al., 2000; French, 1996; Geldermann et al., 2009; Hämäläinen, Lindstedt, & Sinkko, 2000; Hoe & Müller, 2003; Landman, Päsler-Sauer, & Raskob, 2014; OECD/NEA, 2000; Papamichail & French, 2005; Wex, Schryen, Feuerriegel, & Neumann, 2014). "
"The literature on dynamic decision making and economic optimisation in the response and recovery phases is, however, considerably less mature (Altay & Green III, 2006)."
"Immediate response to a nuclear accident involves procedures for evacuation, sheltering, iodine tablets distribution, whereas recovery measures include long-term relocation and remediation, as well as potential repopulation of the affected areas (Gering, Gerich, Wirth, & Kirchner, 2013; Higgins et al., 2008; Munro, 2013). "
"The key difference between the response and recovery stages is, therefore, the timescale on which the relevant measures are implemented: while emergency response can take place on the timescale of minutes, hours and days in the immediate aftermath of a nuclear disaster, recovery measures often span over weeks, months and years (DECC, 2013)."
"Short-term response, therefore, is expected to have typical features of emergency planning, when precautionary actions may not necessarily be justified in economic terms (Dana, 2002; Klinke & Renn, 2001)."
"Long-term post-accident response and recovery planning, on the other hand, is characterised by significantly lower levels of radiological uncertainty, and requires multiple economic as well as non-economic factors to be taken into account (French, 1996; Geldermann et al., 2009; Hämäläinen et al., 2000)."
"It is possible that governments are going to prioritise the economic factors by seeking to minimise the total cost of preventative and recovery measures (Munro, 2011; 2012)."
"Of the vast literature on Chernobyl, the two key studies on the long-term measures are (Lochard & Schneider, 1992) (contamination and population distributions, resettlement and health costs) and (Jacob et al., 2009) (remediation costs in rural areas); see also (IAEA, 2006; Karaoglou, 1996). These studies are based on the actual data from the affected populations and territories, and recommend a variety of cost-efficient strategies."
"The existing studies on Fukushima have focused on providing contamination maps and summarizing early-stage radiological impacts on the environment (IRSN, 2011; 2012), analysing health effects for the affected populations (González et al., 2013; Harada et al., 2014; WHO, 2013), assessing economics of decontamination (Munro, 2013) and people’s intention to return to their evacuated family homes (Munro & Managi, 2014). A comprehensive up to date review of the multiple consequences of the Fukushima disaster (Ahn et al., 2015) advocates that “scientific and academic communities should start efforts for establishing the scientific bases, both natural and social, for better societal resilience.”"
"The detailed input data for these packages can be obtained from economic and population databases such as GIS for many locations throughout the world (De Silva & Eglese, 2000), including those near the existing nuclear installations."
"The DSS that is most relevant to the present study is COCO-2 (Higgins et al., 2008)."
"It is possible that emergency evacuations and long-term relocations inside these relatively unaffected areas might have caused psychological and economic harm comparable to (or even exceeding) the potential radiological harm averted by these actions (Ahn et al., 2015), as arguably was also the case in for a number of evacuations in the aftermath of Chernobyl (IAEA, 2006; Karaoglou, 1996; Walinder, 1995)."
"A dartboard-like structure (Gering et al., 2013) might be a good starting point, although a more detailed mosaic-like pattern tailored around the urban areas within the circular zones could provide a greater level of control "
"Ultimately, the feasibility of having different treatments within specified boundaries will depend on how densely populated the entire prototype exclusion zone is, which is part of a wider issue of siting for nuclear power installations (Grimston et al., 2014)."
"To address the need for the spatial and temporal flexibilities, we develop a decision-making model for a single economic location (say, a town, a village or an area of agricultural land) based on Bellman’s Optimal Control Theory (Bellman, 1956), which is at the basis of the Operations Research (OR) methodology. "
"In reality, any long-term decision making takes into account a number of non-economic factors such as acceptance of a decision by various groups including the public, decision-makers, stakeholders and experts (Geldermann et al., 2009) alongside the standard radiological and economic evaluation methods."
"Even though some short-term temporal and spatial flexibilities in sheltering times, iodine prophylaxis and evacuation are also possible (Dillon, 2014; Gering et al., 2013), the present study only focuses on the largely deterministic mid- and long-term problem setting."
"Much of the mid-term and long-term radiation exposure, both in urban and in rural areas, comes from ground shine, defined as “external dose direct from radioactive materials deposited on the ground” WHO (2013), which is a result of the initial radioactive deposition (Harada et al., 2014; Lochard & Schneider, 1992)."
"We restrict our analysis to ground shine and ingestion of contaminated food produce originated in rural areas, the two types of exposure most relevant on the longer timescales (Jacob et al., 2009)."
"The deposition period could last several hours, days or weeks (Ahn et al., 2015; Katata et al., 2015), and the radioactive material will usually be carried in a plume of smoke or ash depending on the type of accident that has occurred. "
"Chernobyl, the biggest nuclear disaster in history, provided extensive information on the economics of a severe nuclear accident (Jacob et al., 2009; Lochard & Schneider, 1992). "
"The economics of nuclear decontamination and assessment of policy options for the management of land around Fukushima is described in Munro (2013); see also (Munro, 2011; 2012) and the relevant WNN reports."
"According to (Walinder, 1995), “it is impossible to predict, by means of a mathematical expression, the specific outcome of a low radiation dose”."
"The concept of VSLY was developed in public policy making to put monetary value on the reduction of risk of death for an average ‘statistical’ individual (Higgins et al., 2008)."
"In addition, a number of studies since Chernobyl have shown that the health effects due to stress may be commensurate with the health effects associated directly with the radiological exposure (IAEA, 2006; Karaoglou, 1996)."
"It is assumed that the collaborating organisations follow OWLS ontology for services, as OWLS is the most widely used standard specification for adding semantics to web services [10]. OWLS provides a standard set of ontologies to the collaborating organ-isations for describing and composing web services. "
"The existing execution mechanisms from litera-ture enact automatically generated workflows for single organisa-tions only, however they can handle adhoc processes that are outside the boundaries of the organisation in the workflows [1]."
"A Business process can be defined as a set of one or more linked procedures or activities which collectively realise a business objective or policy goal, normally within the context of an organisational structure defining functional roles and relationships [32]"
"1 is the automation of a business process, in whole or part, during which documents, information or tasks are passed from one participant to another for action, according to a set of procedural rules [32]"
A workflow has two main stages [32]: Build-time stage this refers to the stage where workflow descriptions of the business process are defined or changed
"For any two organisations to proceed in business, they need to have compatible workflows and compatible means that there should be an agreed sequence of activities exchanging collaborative messages and information [6]"
The set of all interface activities in a workflow is called interface process [6]
"When two of more organisations do business together, the need for workflow collaboration across multiple organisations arises [5]"
"Considerable amount of effort is needed to ensure that workflows are compatible [7,23] and proceed into the execution stage."
"Recent research on workflow collaboration focuses on reconciling existing incompatible workflows [13,3]"
"In an alternative top-down approach, organisations meet, discuss and design collaborative processes and then implement them [5]"
"Another paradigm in the literature is automatic workflow generation, which is based on AI planning where a workflow is considered as a plan [2,10]"
"If every activity in a workflow is treated as a web service, a workflow represents a plan of web services to achieve the desired goal state from a given initial state [22]"
"Furthermore, it is a highly efficient planning system and has a Web Ontology Language for Services (OWLS) type mechanism for representing atomic tasks and decomposing composite tasks into atomic tasks [27]"
"The similar mechanism of OWLS and SHOP2 to hierarchically decompose complex tasks into sub tasks makes it straightforward to map OWLS definitions directly into SHOP2 domain [27,33] and create workflows based on the translated domain."
"In the context of semantic web services, PDDL is neither too restrictive nor too expressive and is considered as a viable compromise between expressivity and efficiency [20]"
"It is a workflow modelling language [16] and lacks the semantic precision required for automatic business process generation and execution [17], and so we cannot use it as a notation for the proposed framework"
"An aggregation significantly can reduce computational costs for obtaining inference results in DPGMs, but requires a careful consideration of indirect influences as Motzek and Mller(2015a) has shown"
"Then, in fact, the presented DMIA model represents a so-called activator dynamic Bayesian network (ADBN) Motzek and Mller (2015a,2015b) and one obtains well-defined semantics"
"One has to differentiate between induced observations and true observations; a differentiation related to Pearl's ( Pearl,2002) introduction on the do-calculus and is best explained at an example: Considering an infectious-disease monitoring system, one has to differentiate between a person being healthy, because he has been healed and between a person being tested to be healthy"
"(Boutilier et al.1996) of Xt on all its possible dependencies Z , given +set, Xt is decoupled from all other potential sources of (non)impact, and the observation xt is completely accredited to SEt"
"1 1Stephen Elop, the former Executive Vice President of Microsoft's Devices and Services and (at the time of the comment) the CEO of Nokia Corporation, speech at D9, June 1, 2011 Hence, the sheer number of applications in the marketplace has become increasingly important in marketing new mobile devices (see eg, Chen, 2010;Reuters, 2012; Lee, 2015; Smith, 2015)"
"The logic behind establishing the ecosystems is grounded on the theory of network externalities ( Katz and Shapiro,1985)"
"Due to network externalities, a large number of application developers within the ecosystem is expected to lead to a large number of applications that, in turn, will attract customers and drive device sales, leading to a virtuous circle ( Holzer and Ondrus, 2011)."
"In this study, the concept of mobile application ecosystem refers to an interconnected system comprising an ecosystem orchestrator, mobile application developers, and mobile device owners, all of whom are connected through a marketplace platform ( Hyrynsalmi, Sepp nen and Suominen,2014)"
"Hence, a mobile application ecosystem is a derivate of the more general concept of a software ecosystem ( Jansen, Finkelstein, and Brinkkemper, 2009; Bosch, 2009; Manikas and Hanssen, 2013 )."
"The increased complexity calls for a better understanding of the boundaries and structures of the ecosystems (eg, Jansen et al. 2009; Gueguen and Isckia, 2011; Hanssen, 2012)"
"Prior research has investigated the success factors of the iPhone ( Laugesen and Yuan, 2010; West and Mace,2010), the distribution and capture of value in the mobile phone supply chains ( Dedrick, Kraemer, andLinden, 2011), developers perspectives on the mobile application markets ( Lee, Lee, Shim, and_Choi, 2010; Holzer and Ondrus, 2011; Schultz, Zarnekow, Wulf, and Nguyen, 2011 ), the dynamics of the application marketplaces ( J rvi and Kortelainen, 2011; Hyrynsalmi, Suominen, and Sepp nen, 2013; Jansen and Bloemendal, 2013), standard wars and platform battles ( Heinrich, 2014; Gallagher, 2012; van de Kaa and de Vries, 2015; van de Kaa, van den Ende, de Vries and van Heck,. 2011 ) and cooperation within ecosystems ( Gueguen and Isckia, 2011)"
"Network effects can accrue from direct externalities, whereby utility increases as the number of users consuming increases; and indirect externalities, whereby the demand for a product depends on the existence of another product ( Katz and Shapiro, 1985)"
"Sellers engage in multi-homing to gain access to larger potential markets ( Rochet and Tirole, 2006), to offer their products to the same customers across different platforms, and to reduce dependency on a single market and orchestrator ( Idu, van de Zande, and Jansen, 2011)"
"Prior research has focused on software vendors multi-homing in console games marketplaces ( Landsman and Stremersch,2011), Software as a Service (SaaS) marketplaces ( Burkard, Draisbach, Widjaja, and Buxmann, 2011; Burkard, Widjaja, and Buxmann, 2012), and also within Apple's ecosystem ( Idu et al.,2011)"
"In their study on the gaming console market, Landsman and Stremersch (2011) found that the multi-homing of games has a negative effect on sales at the marketplace level, although the negative effect decreases when a platform matures or gains market share"
"Idu et al. (2011) investigated the iPhone, iPad, and Mac software marketplaces, and found that, out of the top 1,800 applications, 17.2% were multi-homed in two marketplaces and 2.1% in all three marketplaces."
"Furthermore, as pointed out by Hyrynsalmi et al. (2012) , only a small share of all applications published in the marketplace are actually downloaded, and even fewer are actually used by customers"
"For instance, in the fields of Natural Language Processing (NLP) and IR, ontology-based semantic similarity measures have been used in Word Sense Disambiguation (WSD) methods [92] , text similarity measures [86], spelling error detection [20], sentence similarity models [44,66,91], paraphrase detection [36], unified sense disambiguation methods for different types of structured sources of knowledge [73], document clustering [31], ontology alignment [30], document [74] and query anonymization [11], clustering of nominal information [9,10], chemical entity identification [40], interoperability among agent-based systems [34], and ontology-based Information Retrieval (IR) models [55,62] to solve the lack of an intrinsic semantic distance in vector ontology-based IR models [23] "
"For this reason [57,1.1], ontology-based semantic similarity measures exclusively based on is-a relationships are currently the best and most reliable strategy to estimate the degree of similarity between words and concepts [58], whilst the corpus-based similarity measures are the best strategy for estimating their degree of relatedness [8]."
"Thus, the proposal for concept similarity models to estimate the degree of similarity between word and concept pairs has been a very active line of research in the fields of cognitive sciences [106,124], artificial intelligence and Information Retrieval (IR) [107]"
"In the field of bioengineering, ontology-based similarity measures have been proposed for synonym recognition [24] and biomedical text mining [14,98,112]"
"Mendling et al [80] study the current practices in the activity labeling of business processes, whilst Dijkman et al [32] propose a similarity metric between business process models based on an ad-hoc semantic similarity metric between words in the node labels and attributes, as well as the structural similarity encoded by the concept map topology"
"HESML V1R2 [60] is distributed as a Java class library ( HESML-V1R2.jar) plus a test driver application ( HESMLclient.jar), which have been developed using NetBeans 8.0.2 for Windows, although it has been also compiled and evaluated on Linux-based platforms using the corresponding NetBeans versions"
"Likewise, Leopold et al [68] propose an automatic refactoring method of activity labels in business process modeling based on the automatic recognition of labeling styles, and Leopold et al [67] propose the inference of suitable names for business process models automatically"
"In addition to the aforementioned IC models [46], Seddiqui and Aono [120] and Pirr and Euzenat [104] propose two further intrinsic IC models not implemented by HESML which are based on the integration of all types of taxonomical relationships, and thus especially designed for semantic relatedness measures"
"In addition, we plan to provide ongoing support for further ontologies such as Wikidata [126] and the Gene Ontology (GO) [5] among others, as well as further similarity and relatedness measures"
"PosetHERep is based on our adaptation of the well-known half-edge representation in the field of computational geometry [19], also known as a double-connected edge list [17,2.2], in order to efficiently represent and interrogate large taxonomies."
"The functionality and software architecture of HESML allow the efficient and practical evaluation of large word similarity benchmarks such as SimLex [50] and ontology-based similarity measures based on the length of the shortest path, whose implementation in other software libraries requires a high computational cost that prevents their evaluation in large experimental surveys [58] and datasets"
"All the experiments compute the Pearson and Spearman correlation metrics for a set of ontology-based similarity measures on each word similarity benchmark shown in table 22, as detailed in [56]"
"These scripts take the raw output files generated by the experiments in table 11 and produce the final assembled tables as shown in [56 58], as well asfigures 2 and 3 showing the interval significance analysis in [56]"
The ReproZip program was used for recording and packaging the running of the HESMLclient program with all the reproducible experiments shown in table 11 in the HESMLv1r1 reproducible exps.rpz file available at [64]
"Because of the lack of space, WNSimRep v1 is detailed in a complementary paper, which together with the dataset files, is publicly available at [63]"
"All the corpus-based IC models are derived from the family of *add1.dat WordNet-based frequency files included in the [95] dataset, which is a dataset of corpus-based files created for a series of papers on similarity measures in WordNet, such as [93] and [96]"
"The goals of the experiments described in this section are as follows: (1) the experimental evaluation of the PosetHERep representation model and HESML, as well as their comparison with the state-of-the-art semantic measures libraries called SML [48] and WNetSS [15]; (2) a study of the impact of the size of the taxonomy on the performance and scalability of the state-of-the-art semantic measures libraries; and finally, (3) the confirmation or refutation of our main hypothesis and research questions; Q1 and Q2 introduced in section 1.1."
"On the other hand, in order to evaluate and compare the performance of WNetSS with HESML and SML, we compare the running-time of the three libraries in the evaluation of the Jiang-Conrath similarity measure [52] with the Seco et al IC model [119] in the SimLex665 dataset [50]."
"We have introduced a new and linearly scalable representation model for large taxonomies, called PosetHERep, and the HESML V1R2 [60] semantic measures library based on the former"
"ISO/IEC/(IEEE) (2007) defines an architecture as composed of: (a) the fundamental organization of a system embodied in its components; (b) their relationships to each other, and to the environment; and (c) the principles guiding its design and evolution. "
"On that basis, Panunzio and Vardanega (2013) regards the concept of software reference architecture as proceeding from:"
"In his 1972 ACM Turing lecture (Dijkstra, 1972), E.W. Dijkstra advocated a constructive approach to program correctness where program construction should follow – instead of precede – the construction of a solid proof of correctness."
"In accord with Sifakis (2005), we maintain that composability is achieved when the designated properties of individual components, captured in terms of needs and obligations, are preserved on component composition, deployment on target and execution."
"The WCET is a local property of the program (that is, the service attached to the interface in question): composability in the time dimension (Puschner et al., 2009) is achieved if the interfering effect caused by the presence of other components in the system does not prevent a safe and tight WCET bound to be determined for every single interface service"
"They are used in approaches (like the one presented herein) which present an endogenous treatment of non-functional properties (i.e., outside of the component) (Crnkovic et al., 2011). "
Chaudron and Crnkovic (2008) defines a software component as “a software building block that conforms to a component model”.
The requirement of “independent deployment of components” entailed by the definition by Szyperski (2002) is currently not a core requirement for us (and neither is in many other component-oriented approaches). 
"The semantics of that declarative language emanates from the chosen computational model: the Ravenscar Computational Model (RCM) (Burns et al., 2014) in our case. "
"In terms of the rich classification proposed in Crnkovic et al. (2011), our component model (i) addresses the modeling and implementation phases of the development life cycle, (ii) is independent from the programming language, (iii) provides constructs for interface specification, (iv) allows expressing a limited set of interaction patterns, (v) supports specification, composition and analysis of non-functional properties, and (vi) is special-purpose as intended to high-integrity embedded real-time systems."
Gößler and Sifakis (2002) focuses on integration of components with heterogeneous interactions and execution paradigms. 
"The framework aims at correct-by-construction design by achieving component composability and compositionality. That work later evolved in the conception of the BIP framework (Behaviour, Interaction, Priority) (Basu et al., 2006)."
"SaveCCM (Hansson et al., 2004) targets heavy vehicular systems. That component model supports both time-triggered and event-triggered activation events and its components are hierarchical."
"The ROBOCOP component model (Muskens et al., 2005), targets the consumer electronic domain. "
"Giotto (Henzinger et al., 2001) is a progenitor in a family of time-triggered languages and tools, which specialize for control processing where deterministic time of execution is inbred to the domain culture (Henzinger et al., 2003). "
"The Ptolemy project (Lee, 2001) studies modeling, simulation, and design of concurrent, real-time, embedded systems realized as an assembly of concurrent components."
"The ISO standard 42010 (ISO/IEC/(IEEE), 2007) stipulates that “architectural description of the system is organized into one or more constituents called views”, and a view is a partial representation of a system from a particular viewpoint, which is the expression of some stakeholders’ concerns."
"The value for the WCET can later be refined with bounds for a given target platform obtained by timing analysis (Wilhelm et al., 2008)."
"we defined the whole set of allowable containers and connectors in a library of code archetypes (Panunzio and Vardanega, 2012), which vastly simplifies automatic code generation."
"The possibility that synchronization increases with frequency is commensurate with in-vitro cell recordings ( Rocha,Doiron,Shea-Brown, Josic,&Reyes,2007) and computer simulation of both integrate and fire and Hodgkin Huxley type models ( Chawla,Lumer,&Friston,1999)."
"The first and second order derivatives of the MFCCs (with respect to time) are sometimes used as additional features ( Gutig&Sompolinsky,2009) but did not improve recognition performance on our database."
"Whilst the work in this paper provides a useful starting point, it does not make use of the network view of brain function; Price, Thierry, and Griffiths (2005), for example, propose that the human brain has not developed macro-anatomical structures dedicated to speech processing, but rather that speech-specific processing emerges at the level of functional connectivity among distributed regions"
"As the magnitude spectrum of speech is known to be stationary over a period of approximately twin=20 100ms ( Rabiner & Juang,1993) we broke up each speech time series into frames of length twin=50ms"
"The importance of a hierarchy of temporal scales is emphasized in recent work by Ghitza (2011) who provides evidence that current models of speech perception, which are driven by acoustic features alone, provide an incomplete description of speech recognition phenomena"
"The notion that regions higher up in the auditory cortical hierarchy process information at longer time scales has recently been made use of in a model of auditory sequence recognition based on stable heteroclinic channels ( Kiebel, Kriegstein, Daunizeau, & Friston, 2009)"
"The Liquid State Machine (LSM) ( Maass, Natschlager & Markram, 2002), for example, uses OT features and the temporal embedding idea proposed in the HB model, but then applies standard methods for recognizing the resulting static patterns"
"This results in good pattern discrimination abilities ( Verstraetenet al.,2005), though not as accurate as a recent approach based on OT features ( Gutig & Sompolinsky,2009)"
"As the way in which one cell or circuit couples with another can be summarized using phase interaction functions ( Penny et al.,2009) we envisage that it should be possible to identify families of neurons or neural circuits that have the appropriate synchronization properties."
"We thought it might be possible that the OT system performed better at low signal levels because it had fewer parameters than the MFCC system, and so might generalize better ( Bishop,1995)"
"We also compare augmented and minimal models using the model evidence, as computed using the Posterior Harmonic Mean (PHM) ( Gelman et al.,1995)"
"The resulting Bayes factors were 0.98 and 0.99 indicating that neither of the augmented models are significantly better than the minimal model (this would require a Bayes factor of at least three ( Gelman et al.,1995))."
"Human speech is characterized by a four-fold variation in the speed at which words are spoken ( Miller, Grosjean & Lomanto,1984), and any speech recognition system whether artificial or natural, will have to deal with this range of time-warp"
"Each nonword matched one of the words (action verbs) in duration, intensity and power spectrum, but was rendered unintelligible by removing components of the modulation power spectrum using the Modulation Transfer Function (MTF) algorithm described in Elliott and Theunissen (2009)"
"The corresponding spectrograms G(ywi,f,t) and G(yni,f,t) were then computed using a windowed multitaper method with window size N=256 samples (0.128 s), a window offset of 32 samples (0.016 s), and time-bandwidth parameter set to NW=3 ( Mitra & Pesaran, 1999 )"
"More physiologically realistic filters can be implemented by linear spacing the filter bands on a mel-frequency scale ( Ghitza, 1986), and this was implemented for the pattern recognition results described in Section 3.1."
"Such frequency tuned onset and offset detectors have been observed in the inferior colliculus of the auditory midbrain ( Casseday et al.,2002)"
"Optical imaging reveals larger time windows of temporal integration as one moves from primary to secondary auditory areas ( Harrison, Harel, Panesar,Mori,& Mount, 2000)"
" To motivate this enterprise and to understand the importance of high thread counts on highly-threaded, many-core machines, let us consider a simple application that performs Bloom filter set membership tests on an input stream of biosequence data on GPUs  [3]."
" Using a different approach, Hong etÂ al.Â  [17] propose another analytical model to capture the cost of memory operations by counting the number of parallel memory requests in terms of memory-warp parallelism (MWP) and computation-warp parallelism (CWP)."
Accessing the off-chip global memory usually takes 20 to 40 times more clock cycles than accessing the on-chip shared memory/L1 cacheÂ  [51]. 
" Based on the description from Alverson etÂ al.Â  [52] about the nature of the computations this processor was designed to run, it is a purpose-built appliance for real-time graph analytics featuring graph-optimized hardware that provides up to 512 terabytes of global shared memory, massively-multi-threaded graph processors (named Threadstorm) supporting 128 threads/processor, and highly scalable I/O. "
Dijkstra algorithm is a greedy algorithm for calculating single source shortest paths. The pseudo-code for Dijkstra algorithm is given in Algorithm [55]. 
"Such over-approximations have been used, among other things, in the analysis of cryptographic protocols [8], for termination analysis [9,13] and for establishing non-confluence of term rewrite systems [19]"
"This research was born of involvement in the development of three tools for term rewriting, CeTA [18], a certifier for termination and confluence proofs generated by provers, CSI [19], an automated confluence prover, and Image 1 [11], an automated termination prover"
"The distinguishing feature of a certifier is that it is highly trustworthy; in the case of CeTA, this is achieved by proving its code correct in the proof assistant Isabelle [17]"
"Both CSI and Image 1 use quasi-deterministic automata [13] to produce overapproximations of reachable terms, and CeTA could not certify the resulting proofs"
"Note that in both cases, we did not define new recursive functions, but just proved equalities which are treated as function definitions by Isabelle's code generator [10]."
"Consider the famous Lorenz system [60] (1)x = (y-x),y = x-y-xz,z =- z+xy,where , , are positive parameters."
"Consider the Chen system [12] (2)x =a(y-x),y =(c-a)x+cy-xz,z =-bz+xy and the Lu system [61] (3)x =a(y-x),y =cy-xz,z =-bz+xy,where a,b,c are real parameters"
"Leonov suggested to consider the following substitutions [35] (4)x hx,y hy,z hz,t h-1twith h=a"
"Remark that the transformation (4) with h=a does not change the direction of time for the positive chaotic parameters considered in the works [12,61]."
"Note that here in contrast to the previous transformation: 1) the transformation (4) with h=-c change the direction of time for c>0 considered in the works [12,61], 2) for c=0 the Chen and the Lu systems do not become linear and their dynamics may be of interest"
"Various rigorous approaches to the justification of its existence are based, for example, on the investigation of instability (hyperbolicity) of trajectories with the help of computing Lyapunov exponents, or the computation of attractor dimensions (see, eg, [27,19] and many others)"
"On the other hand, we would like to recall the classical 16th Hilbert problem (second part, [23]) on the number and mutual disposition of limit cycles in two-dimensional polynomial systems, where one of the tasks is to find the simplest system, from a certain class, with the maximum possible number of limit cycles"
"Many chaotic polynomial systems have been discovered (eg, such particular cases of three-dimensional quadratic systems as the Lorenz, the Rossler, the Sprott, the Chen, the Lu and other systems) and studied over the years, but it is not known whether the algebraically simplest chaotic flow has been identified [73,72]"
"While Lyapunov dimension of the Lorenz attractor can be obtained analytically [47], for the Chen and Lu attractors it is still an open problem"
"The Chen system is a special case of the Lorenz system and The Lu system is a particular case of the Lorenz system of [2,3] may lead to the conclusion that the study of the Chen and the Lu systems is useless, the Chen and the Lu systems stimulate the development of new methods for the analysis of chaotic systems."
The estimate from above of the Lyapunov dimension by Lyapunov functions [36] and its comparison with the local Lyapunov dimension in zero stationary point permit one to obtain the exact formula of dimension for a generalized Lorenz system (9) with a certain parameter d
" In the mechanical and mathematical literature concerning the models for piezoviscous hydrodynamic thin film lubrication problems, different classical devices have been considered, such as journal-bearings, rolling-bearings or rolling-ball-bearings (see [1], for example). "
"In these more complex settings, this way of including piezoviscous regimes has given rise to several mathematical analysis results that state the existence and the uniqueness of solution, as well as to the design of suitable numerical methods to approximate the corresponding solutions, for which there are no analytical expressions (see, for example, [4-8])"
"For the spatial discretization we consider a classical piecewise linear Lagrange finite element space, so that the discretized problem consists of finding p hn+1 Kh, such that (38) - /2 /2G(t,p hn)(p hn+1) ( h-p hn+1) dt -6 - /2 /2h ( h-p hn+1)dt, h Kh,where Vh denotes the finite element space (see [22], for example): Vh={ h C0( )/ h|E P1, E h},"
"In order to apply the duality method in [20], we introduce the indicatrix function of the convex Kh denoted by IKh, so that the variational inequality of first kind (38) can be transformed in the equivalent variational inequality of second kind that consists of finding p n+1 V0h, such that - /2 /2G(t,p n)(p n+1) ( -p n+1) dt+IKh( )-IKh(p n+1) -6 - /2 /2h ( -p n+1)dt, V0h.Moreover, we use the notation (Anp n+1, )= - /2 /2G(t,p n)(p n+1) ( -p n+1) dt,(fn, )=-6 - /2 /2h ( -p n+1)dt.Then, using the tools of subdifferential calculus, we obtain that (39) n+1=-(Anp n+1-fn) IK(p n+1),where IKh denotes the subdifferential of the indicatrix function"
"In order to apply the results in [20], for a given parameter >0, we introduce the new variable (42) n+1 IKh(p n+1)- p n+1.In terms of this new variable n+1, the equivalent formulation to (40) (41) can be written in the form:"
"The idea is to make a comparative analysis of the different models, formulations and numerical schemes presented in the previous sections, mainly taking as a reference the data set of the numerical examples in [12]"
It seems that the results obtained with the numerical schemes here implemented for the Rajagopal and Szeri model are very similar to the solution presented in [12]
"Predicting the effort required to create software has been based on numerous software size models such as the Constructive Cost Model (Anandhi and Chezian, 2014; Clark, 1996; Manalif et al., 2014) and all its alternatives (Attarzadeh and Ow, 2011; Kazemifard et al., 2011; Tadayon, 2004; Yang et al., 2006) as well as on function points (Borandag et al., 2016) and analogy based models (Idri et al., 2015). The main goal of all these approaches is to minimize prediction error. Prediction is needed during the initial phase of software project developments."
"As reported in Silhavy et al., (2015a,b) UCP has some important drawbacks. Several approaches help identify the drawbacks of the UCP method and offer solutions, many of which are based on an analogy approach."
"Analogy based size estimation is commonly used for prediction in all the methods mentioned above (Idri et al., 2015; Shepperd and MacDonell, 2012). "
"The UCP method is based on use case models, which are commonly used as functional descriptions of proposed systems or software. The method involves assigning weights to groups of actors and use cases. Karner's original UCP method (Karner, 1993) identifies three groups: simple, average and complex."
"A number of use case scenario steps are typically involved in the initial estimation process. There have also been several modifications of the original UCP principles including use case size points (Braz and Vergilio, 2006), extended UCP (Wang et al., 2009), modified UCP (Diev, 2006), adapted UCP (Mohagheghi et al., 2005), and transaction or path analysis (Robiolo et al., 2009)."
"This approach is based on analysing a scenario, not step by step, but using steps merged logically into so-called transactions in which each transaction should include more than one step. Robiolo et al., (2009) improved transactions by calculating paths by which the complexity of each transaction is based on the number of binary or multiple conditions used in the scenarios"
"Their approach is based on Robiolo and Orosco (2008), where number of transactions is equal to the number of stimuli. A stimulus is a system entry point, which generates response (transaction) of an actor action in a use case. "
"Ochodek et al., (2011a) discusses a reliability of transaction identification process and Jurkiewicz et al. (2015) discusses event identification in use cases, which should be useful for path identification"
"Diev (2006) noted that when the actors and use cases are precisely defined, unadjusted UCP (the sum of the UAW and the UUCW) can be multiplied by the technical factors. The product of the technical complexity factors (see Table 3) and unadjusted UCP is considered as the coefficient of the base system complexity in Diev (2006)."
"According to Nageswaran (2001), added effort must be taken to consider support activities such as configuration management or testing."
"Yet another modification to the UCP is called adapted UCP (Mohagheghi et al., 2005). In this method, the UCP method is adapted to provide incremental development estimations for large-scale projects. Initially, all actors are classified as average (based on the UCP native classifications) and all use cases are classified as complex. "
"Ochodek et al., (2011b) also proposed omitting UAW and the decomposition of use cases into smaller ones, which are then classified into the typical three use case categories."
"Analogy based estimation methods are discussed in Azzeh et al., (2015b), which evaluated 40 variants of the single adjustment method using four performance measures and eight test datasets."
Amasaki and Lokan (2015) addressed the problem of selecting projects using a linear regression model by testing the window principle. The window principle involves first selecting a subset of the data.
"Instead, the study by Urbanek et al., 2015b is based on artificial intelligence and is an application of the approach proposed by Senkerik et al., (2014) but with theoretical aspects of Oplatkova et al., (2013). Urbanek et al., 2015b used a symbolic regression tool, analytic programming, together with differential evolution."
"Several works have attempted to apply various prediction models to UCP. Nassif et al., (2013) presented a linear regression model with a logarithmic transformation that they created to estimate software effort from use case diagrams."
" In Nassif et al., (2011), a multiple linear regression model was developed to predict the values of the productivity factor. To adjust the values of the productivity factor, they first employed a fuzzy logic approach (Nassif et al., 2011). Then, they created an artificial neural network (multi-layer perceptron) model (Azzeh and Nassif, 2016; Nassif et al., 2015; Nassif et al., 2012, 2013)."
"Actors play roles in the UAW variables (Azzeh et al., 2015a; Silhavy et al., 2015a). A simple actor typically represents an application programming interface and a complex actor represents a human using a graphical user interface."
"The least squares method is the most common method used to fit a regression line. The case when a linear regression has only one independent variable is called simple linear regression (Bardsiri et al., 2014; Jorgensen, 2004; Montgomery et al., 2012; Shepperd and MacDonell, 2012), whereas multiple linear regression (Bardsiri et al., 2014; Jorgensen, 2004; Montgomery et al., 2012; Shepperd and MacDonell, 2012) involves more than one independent variable."
"Another type of linear regression is polynomial regression (Bardsiri et al., 2014; Jorgensen, 2004; Shepperd and MacDonell, 2012) in which the relationship between the dependent variable and the independent variables is modelled as an mth degree polynomial"
"In the case of multiple independent variables it is appropriate to use stepwise regression (Bardsiri et al., 2014; Jorgensen, 2004; Shepperd and MacDonell, 2012). The aim of the stepwise regression technique is to maximize the estimation power using the minimum number of independent variables. "
"The experiment described above was evaluated using two datasets. Dataset 1 was obtained from Silhavy et al., (2015a), in which the dataset was based on Ochodek et al., (2011b) and Subriadi and Ningrum (2014). The UAW, UUCW, TCF, and ECF are known from the UCP method."
"We can conclude that the UAW also has an effect on size estimation, which is different than the findings published previously in Ochodek et al., (2011b). Here, the UAW values were valuable because the number of actors helps determine the number of interfaces required in the project, which, in turn, impacts software product construction."
"Human motion estimation is one of the most important areas of computer vision study [1–11]. It refers to the automated prediction and estimation of human motion posture based on rigid body motion, joint angles and body segment location."
"Previous studies have shown that the performance of motion estimation is highly dependent on the quality of motion data as well as the algorithm that is developed for modeling and estimation of the model [1,4,8,9,11]. The quality of motion relies on the method of captured data, either by marker-based or marker-less motion capture. "
"The approaches to motion estimation are biomechanical based [5,7], silhouette based [2,3,6,11,12] and image based [1,4,8–10]. A biomechanical-based approach involves tissue analysis and bone and joint location, which requires expensive devices and equipment."
"The approaches to motion estimation are biomechanical based [5,7], silhouette based [2,3,6,11,12] and image based [1,4,8–10]. A biomechanical-based approach involves tissue analysis and bone and joint location, which requires expensive devices and equipment. Silhouette-based estimation is analyzed by the silhouette extract from a human image file. It is always challenging when the estimation involves more than one subject. Image-based estimation involves the extraction of still figures from video motion data."
"The methods used for estimation include cross-entropy regularization [1], Artificial Neural Network (ANN) [8], hierarchical fitting [9] and human pose recovery [10]."
" The model is applied to short temporal daily activities include walking, running, and jumping obtained from publicly available video [25–27]and experimental captures of Yoga motion activity."
"Typically, motion postures in the whole time duration are framed in image snapshots similar to those presented in Wang and Baciu [1], Tong et al. [4], Zhang et al. [8], Shen et al. [9] and Hofmann and Gavrila [10]. The images are transformed into numeric data in a 2D coordinate system of the main body joints"
"Instead of using the point-cluster technique and the Kalman filter approach as in the case of Wolf and Senesh [7], we use a polynomial fitting approach. Polynomial fitting has been used in the estimation of air quality [28] and for operator prediction in image processing [29]. "
"In addition, classification performance is compared between the actual and estimated model data aided by the Waikato Environment for Knowledge Analysis (WEKA) software [30]. The classification processes are tested on 34 built-in algorithms for the seven categories of classifiers: Bayes, Function, Lazy, Meta, Misc, Rules and Trees. "
Human motion is often represented by the original motion frame or by representing the original motion frames with a parametric or probabilistic model [31]
" By creating parameterized motions, human action can be altered based on momentary moods that describe emotions such as happiness and sadness [32]. At the same time, the motion style can also be modified by adjusting the stylistic parameter as in the work by Brand and Hertzmann [33], in which a statistical model was introduced to generate new motion sequences based on the number of stylistic degrees of freedom."
"On the other hand, motion estimation is the process of estimating the configuration of the underlying kinematic or skeletal articulation structure of a person [34]. It often begins by video motion capture or analysis of the available motion dataset. In the past, different models have been proposed to simplify captured motion matching, including sticks to indicate the human skeleton as well as ellipsoids and cylinders to represent solid human models [35]. "
"Gait motion analysis often studies walking movements, focusing on the lower body segments: the thigh, lower leg and foot. For instance, Veeraraghavan et al. [36] created a shape-based recognition system for gait motion while Zhang et al. [37] proposed a visual gait generative model (VGGM) and a kinematic gait generative model (KGGM) to represent part or whole gait modelling."
Shen et al. [12] used a Gaussian process to study the low-dimensional manifold of visual input data to reconstruct the corrupted silhouette for motion estimation. 
 Luo et al. [11] proved that multi-view video is efficient in solving high-dimensional space problems and estimating a 3D surface in a temporal sequence. 
"Biomechanical-based approaches involve the analysis of soft tissue, bone and joint locations in the human body. For this purpose, Xiao et al. [5] performed an optical motion data capture via a marker-based approach. Their method uses biomechanical information based on 28 infrared markers placed on the human body. Estimation was possible using human skeleton mapping."
" Wolf and Senesh [7], on the other hand, proposed a numerical model with no consideration for mechanical properties. They focused on the soft tissue deformation and bone position of the human body using a statistical solid dynamic method. "
"Of all of the approaches, the image-based approach is the most common method for motion estimation; an example is the monocular image sequence, a model-based approach whereby the object shape is employed in motion estimation, and an independent method is used for a priori shaped models [1]. "
"An example of object shape employment in model estimation was presented by Tong et al. [4], who estimated the joint and global location parameters of a human pose based on a monocular image using the deterministic nonlinear constraint optimization method. "
" In addition, an optical flow method to estimate the motion of gestures was proposed by Zhang et al. [8]. The optical flow method was applied on the flat surface of images to segment and recognize the gestures. ANN has also been used to estimate gesture patterns. "
Another simple and straightforward approach by Daubney et al. [41] was the sparse set of features for pose estimation in low-level motions. Low-level motion suffers from perturbations such as noise and occlusion. The strength of their method is that it can be used in low-level motions without the need for pose initialization. 
"Whereas the studies on low-level motion focus on a single type of motion activity, Bruderlin and Williams [51] proposed a time-warping method as a non-linear method to combine different movements and control the speed of motion."
"Nanni et al. [17] and Saripalle et al. [18] proposed methods using support vector machine for three orthogonal planes of motion data and posturography data, respectively. "
"To achieve a perfect fit on every segment model, the BB segment has a 2nd order polynomial fitting, while the UB and LB have a 6th order of polynomial fitting in our motion model. Another approach that is similar to the polynomial fitting adopted here is the piecewise polynomial that is commonly used in the temperature data of heat transfer [58]."
"The missing data contributes approximately 12.8% of the overall study data. Therefore, the data elimination cum regression imputation approach as reported in Chan et al. [62] is applied to eliminate and impute the missing data."
"Despite assertions some thirty-five years ago that “thefuture of operational research is past” (Ackoff, 1979), the tech-niques and methodologies are still taught in universities across theglobe and regularly used in business decision-making, in both thepublic and private sectors. "
"Many organisations have changed the name oftheir departments to include analytics; such as IBM’sBusiness Analyt-ics and Mathematical Sciences(Sutor, 2013) and Proctor and Gamble’sGlobal Analytics(Ericson, 2006) teams"
"This conclusion confirms that periodization is not only the product of theory, but it is also a producer of theory (Green, 1995). "
"Whilst examples of data mining and machine learning algorithms applied within distributed systems are numerous (e.g. Zaki & Ho, 2000), no academic literature on the application of OR/MS methods within these new architectures was found."
"The bioinformatics analysis reveals that many human diseases, such as cancer, cardiovascular disease, amyloidoses, neurodegenerative diseases, and diabetes, are correlated with proteins diagnosed to be disordered [1,2]."
"The nuclear Poly(A)-Binding Protein 1 (PABPN1) induces the formation of muscle Intranuclear Inclusions (INIs) that are the pathological hallmarks of OPMD. There is currently no cure for OPMD [2,4]."
"In image analysis, the HSV (Hue, Saturation, Value) transformation is useful for developing image processing algorithms based on descriptions of colors that are natural to humans [16]."
There are existing nonlinear functions of features known to be effective (which can be interpreted as mathematical expression models) [17-21]. 
" The Expectation Maximization (EM) is a widely used approach to learning in the presence of unobserved variables, such as in the applications of fitting high-dimensional Gaussian mixture models [24] and reducing the difference in feature distributions [25]."
"Mathematical morphology provides an approach to extracting image components, such as size, shape, boundary and connectivity, and to eliminating irrelevancies for detecting various blood cells [11,12] and cancer cells [13]."
" Yet, the graphical model method, with a prior knowledge of object shapes, is able to provide a probabilistic model to represent the relationship among the image pixels, region labels and underlying object contour [7]."
" Since each iteration of the algorithm consists of an Expectation step (E-step) followed by a Maximization step (M-step), we call it the EM algorithm [26,27]."
"The advantage of the cross-validation is that each training/test subset is independent of the others [33,34]"
"According to a recent survey of 600 software developers, managers, and executives in the United States and the United Kingdom, only 3% of the respondents said they had no plans to adopt CD ( Perforce Software Inc 2015)."
"However, implementing CD can be quite challenging ( Chen, 2015, Leppanen et al., 2015, Claps et al., 2015 "
"Although CD as a goal (a target state) is no longer a new idea and has been well documented ( Humble and Farley, 2010), the adoption journey for CD is not yet a smooth path"
"Many times, these releases would be followed by P1 (priority 1) incidents ( Rob, 2007), meaning that release activity was always full of uncertainty, failures, and stress"
"According to this distinction, Continuous Delivery is compatible with a wide range of scenarios, but Continuous Deployment is suitable only under special conditions ( O'Dell and Skelton, 2016)"
"For example, in our organization, we needed to create a change ticket, place the change request on the agenda of the next Change Advisory Board (CAB) meeting ( Rob, 2007), present the change at the meeting, receive CAB approval, confirm the deployment window, and so forth."
"However, little, if any, work has been reported that specifically seeks to understand and address this important type of software ( Rodr guez et al.,2016)."
"Recent years have witnessed tremendous growth in video traffic on the Internet as a result of higher broadband data rates, proliferation in smart handheld devices [24,95]and affordable unlimited data plans offered by Internet Service Providers [51]."
An estimated one-third of all online activities on the Internet is spent watching video according to the recent report [100]. 
" Netflix alone is reportedly streaming over 1 billion h of video each month which is equivalent to almost 7,200,000 Terabytes of video traffic [37], and this figure is rising constantly. "
"The skyrocketing demand for serving video traffic have questioned the effectiveness of the traditional solution of employing special purpose Content Delivery Networks (CDNs), to serve such content. Invented at the turn of the century [96], CDNs now constitute the backbone for serving content [25,80]. "
"Although replacing selfish self-organising swarming with centrally managed P2P content exchange can improve completion rates considerably [81], for CDN-grade reliability, peers would still need to stick around to allow other users in the swarm to complete, or to maintain a distributed copy of the entire content item across the set of active users in the swarm."
"In short, PA-CDNs work as follows: Whenever possible, i.e., whenever there is sufficient capacity to deliver content in the swarm, peers distribute chunks of content amongst each other (typically using centrally managed swarming techniques [81,93,124])."
"Unlike the previous study [66], we analyze a significantly wider range of obstacle factors, including not only the traditionally discussed technical challenges such as reliability and QoS, but also various other factors, including heterogeneity and scale, and inhomogeneous distribution of resources among users."
"Our work, with its focus on deployable peer-assisted content delivery, is complementary to several survey articles which have focused on traditional P2P-based content delivery [7], or on specific P2P issues such as P2P overlay construction [68], or chunk scheduling [62]. "
"The closest work is that of Lu et al. [66], which surveys the design space of PA-CDN architectures, and highlights a fundamental choice between a tightly coupled or loosely coupled model of co-operation between the server-assisted and peer-assisted components of the PA-CDN"
The request-routing system in CDNs typically includes two basic modules for routing user requests to the most suitable edge server; i) request-routing algorithm and ii) request-routing mechanism [80]. 
"A special purpose DNS server is programmed to redirect users’ requests to the IP address of an appropriate edge server by considering some important parameters such as load on edge server and its distance from the client, network proximity, and user perceived latency [19]."
"Content Delivery Network is a complex content distribution system and several issues and decisions are involved in managing and administrating the entire CDN infrastructure, including, where to place edge servers [18], what content to replicate [32,48,97], and on which cluster of servers to copy each piece of content [8]. "
To provide a cheap pay-as-you-go service to a broad variety of customers some CDNs have adopted cloud technologies which became known in the literature as cloud CDNs [4]. 
"Last but not least, multiple telecom operators (AT&T, BT, Orange, Telefonica, KPN and Verizon among others) to gain a better control over the data services served to their users have deployed their private telco-CDNs [16,30]."
"Overall, Cisco has estimated that content delivery network traffic will carry nearly two-thirds of all Internet video traffic by 2020 [24]. Yet, several recent studies have reported that CDNs are being stressed by the demands placed during peak hours [60,109]. In a search"
"when users agree to share their resources in return of access to the system - the content swarms are said to be self-scalable [28]- as an increase in demand for a content item yields an equal increase in the number of the content suppliers [73,83]. "
"Some peer-to-peer applications may rely on dedicated nodes to control, coordinate and manage content swarm. This structure is referred to as partially centralized P2P system  [88]"
"Similarly, PPLive - the largest P2P live streaming service - rely on dedicated Trackernodes to store the information about streaming channels, available video chunks and peers [40]. Some versions of Bit-Torrent - a popular file sharing P2P system - also rely on Tracker nodes to distribute bulk software updates or multimedia files [88]."
"For instance, the authors of [7] present a critical analysis on different design features and infrastructural properties of P2P systems and their influence on non-functional aspects such as scalability, resource management, security, fairness and self-organization"
"A comprehensive survey of various techniques proposed for structured and un-structured P2P networks has been presented in [68]. Similarly, the authors of  [62] provided an overview of different approaches to address chunk scheduling techniques and peering mechanisms."
"Not surprisingly, the majority of research efforts in peer-assisted content delivery literature have focused on developing strategies for improving quality of service in terms of reducing startup delay and playback delay [39,41,53,58,65,98,112]."
"Indeed, it has been reported in  [31] that the presence of middle boxes is a challenging issue. A peer inside the private network can initiate a connection with Peers of public network, but a reverse connection is often complicated by administrative policies [29]."
" In [11] the authors have analyzed the user trace collected from the Conviva media platform and reported a very low completion ratio among users, when they abandon sessions after watching first few chunks."
"Similarly, a measurement study of PowerInfo [119] – a video-on-demand system deployed by the world’s largest mobile phone operator (China Mobile) - has reported a 70% abandon rate among users as measured by the fraction of sessions which were abandoned after first 20 mins."
"The results suggested that mobile users abandon sessions with a higher rate, i.e., only around 30% of mobile sessions last for longer than a half of a content’s duration in comparison to around 50% for the fixed-line sessions [51]. "
"This phenomenon can be explained by the typical diurnal patterns in user accesses, when most of the users come online in the evening peak hours [11], but also by the content availability policies specific for some video on-demand websites"
" for example, in catch-up TV systems, such as BBC iPlayer [51], the content items are typically released for a limited amount of time, e.g., 7 to 30 days, and feature a burst of accesses in the first few hours after the release [50]. "
"The length of the popular video content matters, too. It has been reported in [42] that, for small size MSN videos, users generally opt to view the entire or most of the video clip, and only 20% of users watch 60% of video content with the length greater than 30 min. Moreover, a large fraction of users (i.e., 60%), watch videos without interactions (e.g., stop, forward, rewind etc.), whereas this fraction increases to 80% for videos shorter than 30 min. "
"A user session might be interrupted due to a network failure, overloaded CPU or a software crash [55]. "
"The authors in  [38] devised a mathematical model to evaluate the impact of peer-churn on a PA-CDN, when the users of set-top-boxes are not willing to share their resources. "
"Similarly, to deal with peer churn the authors in  [72] proposed Home Box-assisted approach which relies on exploiting set-top-boxes as proxies between users and CDNs."
"To improve the quality of service without putting too much of a burden on the peers, the authors of  [42] also suggest two different peer selection policies: Water Levelling (WL) and Greedy Policy (GP). "
"A biased selection of peers without considering underlying physical topology might lead to severe performance degradation in terms of access delays and bandwidth wastage if, for instance, peers located within the same building use two different ISPs and so, although physically placed close to each other, are very distant in network terms [5]. "
"The authors of  [111] proposed to limit the P2P traffic within sub-networks, or behind common gateways. The authors exploited the modified version of Kademlia distributed hash tables (DHT) to conduct searches for the closest peers."
"To achieve a lower startup delay, the authors in [112] proposed a three phase streaming hybrid CDN-P2P architecture that allows peers to download initial chunks of a content item from the geographically closed CDN nodes and remaining chunks from a P2P swarm. "
"The authors of  [39,65] proposed a strategy for improving startup delay via an effective buffer management on a peer’s side."
"Particularly, Ha et al. in [39]suggested that, for minimizing startup latency the buffer’s part at the start of the playback must be filled in with a high priority. "
"Similarly, Lu et al. in  [65] suggested organizing the playback buffer into three different regions, where a startup region and a common region are equivalent to the ones proposed in  [39]. "
"The authors of  [58,98] exploit the social ties between users and the locality of interests to assist peer-assisted sharing of content in Facebook"
An SP exploits a distributed hash tables algorithm (DHT) called a content addressable network (CAN) [84]. The algorithm is based on the binning technique proposed in  [85] and operates as follows.
"To address this concern the authors of  [115]proposed a peers authorization mechanism and a network coding scheme in which each packet is encoded and decoded at the node level using efficient linear codes, thus, allowing for copyrights protection."
The authors of  [49] proposed a control schema over copyrights at a Tracker server in which only legally authorized content items are distributed to the peers.
"A limited contribution policy presented in  [112] obliges every user to contribute some fraction of its upload bandwidth resources to a limited number of sessions, for a limited period of time or both. "
In  [34] the authors present an economic model for PA-CDNs in which user participation in peer-assistance is incentivized via free high quality video offers. 
The authors of  [76] have proposed a peer-assisted model with economic incentives for all participating parties including both peers and ISPs.
Cho and Yi  [23] presented a cooperative game theory approach to validate a profit sharing mechanism with multiple content providers and peers. 
Historically peer-to-peer systems have been ISP-oblivious and could generate significant amounts of the cross-ISP traffic - a fact which reportedly polarized ISPs attitude towards peer-to-peer systems  [88].
Xunlei is the 10th largest Internet company in China and Kankan is its peer-assisted on-demand streaming service with 31.4 million unique daily users as of the end of 2012 [122]. 
"In 2007 ChinaCache deployed about 500 cache servers in 8 districts of China, out of which 50 are the core service nodes responsible for live streaming and over 400 edge caches deployed in a close proximity to users [117]."
"NetSession [124] is a global peer-assisted content distribution network, originally developed by Red Swoosh, a P2P content delivery company founded by Travis Kalanick and Michael Todd in 2001 and acquired by Akamai Technology in 2007 [86]. "
"Spotify is a popular on-demand music streaming service which, according to [35]exploits peer-assistance to serve its 10 million-large user base around the world. "
"NetSession and LiveSky [61,63] use standard DNS request routing techniques and redirect users based on their location and the current load on the edge servers: if the nearest edge server is overloaded, the request is redirected to the next nearest and less loaded one."
"Yet, it is reported in [124] that only around 30% of NetSession users agree to participate in peer-assistance."
"However,  [59] has raised an important concern with respect to ISP-friendly peer-assisted design, suggesting that localizing ISP traffic may negatively impact the quality of service. Therefore, a detailed investigation in this regard is required in future works.

"
"Now MNOs have started to deploy their own CDN infrastructure for better control and management on the resources and bulk of video traffic [1,2,118]. "
"(iii) For input that is not in general position even more complex interactions such as vertex-events or multi-split-events are possible [2,3]. The straight skeleton is the union of the traces of wavefront vertices over the entire time of the wavefront propagation, see Fig. 1."
"Several algorithms are known for constructing unweighted straight skeletons, such as those by Aichholzer et al. [1], Eppstein and Erickson [2], Cheng and Vigneron [6], Huber and Held [3], or Vigneron and Yan [7]."
"These may be (Diggle et al., 2013; Molenberghs and Verbeke, 2005) grouped into three broad classes: (i) marginal models, that seek to relate the marginal distribution of the response variable at each time point to explanatory variables; (ii) subject-specific or random effect models, which account for heterogeneity between individuals by adopting regression-type models with random subject effects; and (iii) conditional or transition models, that focus on the conditional distribution of the response at each time-point given prior responses and possibly explanatory variables."
"The  prior responses and other covariates are treated on an equal footing as explanatory variables in a convenient parametric model, for example, a generalized linear model (Diggle et al., 2013, Chapter 10"
"Other ways to construct parsimonious higher-order Markov chain models have been proposed (Raftery and Tavare, 1994)."
"There is no requirement that the determinants of  be immediately prior to  in the ordering. An example concerning side-effect profiles in a clinical trial of neuroleptica is given in Edwards (2000, Section 7.1.3)"
"Some recent work has extended Bayesian network methodology to support context-specific modelling (Boutilier et al., 1996; Myers and Troyanskaya, 2007)"
"In this paper we study a class of models due to Ron et al. (1998) called acyclic probabilistic finite automata. Note that we use the same acronym, APFA, for both singular and plural forms (automaton and automata)"
"So an APFA can be regarded as a time-heterogeneous context-specific graphical model for discrete longitudinal data. See Edwards and Ankinakatte (in press, Section 10) for a more precise comparison of APFA with Bayesian and Markov networks"
"The structure of the paper is as follows. Section  2 introduces APFA from a statistical perspective. Section  3 describes the model selection algorithm of Ron et al. (1998) which (in a modified form) forms the core of the haplotype clustering algorithm implemented in the Beagle program (Browning and Browning, 2007a,b) that is widely used for phasing and imputation of DNA chip data."
"This section gives a brief introduction to APFA from a statistical perspective: see Edwards and Ankinakatte (in press) for a more detailed exposition. We first describe sample trees, that are closely related to APFA."
"An example is shown in Fig. 1. There are 36 observations with  and 34 with . From each node at stage one, edges branch out to stage two, based on the distinct values of  given . The process continues up to stage , and results in the construct called a sample tree. Sample trees are also known as prefix tree acceptors in machine learning (Carrasco and Oncina, 1994), and event trees in Bayesian decision theory (Smith and Anderson, 2008)"
"Automata are essentially finite state machines that output (or input) strings of symbols. Probabilistic finite automata (PFA) are automata in which strings are generated in a probabilistic manner (Vidal et al., 2005a,b), and APFA are the subclass of PFA that generate symbol strings of constant length, and so can be regarded as probability models for ordered sequences of discrete random variables."
"Thus an APFA expresses a set of context-specific conditional independence constraints on the distribution of , and in this respect it resembles the dependence graph of a traditional graphical model (Lauritzen, 1996; Edwards, 2000)."
The state merging operation and corresponding LRTs are studied in detail in Edwards and Ankinakatte (in press). It is shown that the tests are closely related to standard LRTs of independence () in two-way contingency tables
"The algorithm proposed by Ron et al. (1998) to select an APFA given a data sample proceeds as follows. The sample APFA is constructed and then simplified in a series of state merging operations. As mentioned above, the idea is to merge two nodes  and  at stage  whenever the distributions of the future , given that the process has passed through  or , are similar. To assess this (Ron et al., 1998) proposed a dissimilarity score between nodes  and , written , and a fixed threshold, . When  is small the conditional distributions of , given that the process has passed through  and  respectively, are similar. More precisely,  and  are called similar if : otherwise they are called dissimilar. Dissimilar nodes are not merged."
"The dissimilarity score proposed by Ron et al. (1998) was
"
"For various values of , we take  independent samples from a given APFA , using the data generating process described in Section  2. The simulated data sets of varying sizes are then used to build the APFA, , using the model selection methods under comparison. Then we compute the dissimilarity of the selected model  to the true model , using two measures: the Kullback–Leibler divergence (KLD), and the Kullback–Leibler increment (KLI) (see Appendix A). This is replicated ten times, and the average KL-divergence  and KL-increment  are reported.

This process is performed three times: once for each of the three data sets described in Section  5. The “true” model is constructed by applying the minimal AIC selection procedure described above to the data set. But note that since computation of the KL-divergence is computationally demanding (see Appendix A), only the first 10 variables were used to construct  for the Biofam and Duroc data sets for the KL-divergence computations. For the KL-increment comparisons, all the variables were used."
"4.2. Goodness-of-fit using 10-fold cross validation
Suppose we are given an APFA  with known edge probabilities , and a commensurate data set  of the form  for . As a measure of how well the model  fits the data set, , Thollard et al. (2000) and others suggest using a quantity called the per symbol log likelihood (psLL) for this purpose. It is defined as
(9)
where  is the number of observations in  whose root-to-sink path in  passes through , and  are the known edge probabilities. Note that since each observation in passes through  edges in , psLL is the average value of  obtained when  generates . Thus psLL is a measure of how well  fits ."
"The mildew data stem from a cross between two isolates of the barley powdery mildew fungus (Christiansen and Giese, 1990). For each of  offspring,  binary markers, each corresponding to a single locus, were recorded. One objective of the analysis is to determine the order of the loci along the chromosome. The data were obtained from the experimenters, are analysed in Edwards (1992) and are available from the Comprehensive R Archive Network (CRAN), being supplied along with the package gRapfa.

The Duroc SNP data come from a study in which 4239 pigs of the Duroc breed were genotyped using the Illumina Porcine SNP60 BeadChip. In all approximately 60 000 single nucleotide polymorphisms (SNPs) were recorded for each pig. The data and its preprocessing are further described in Edwards (2013). The data analysed here consist of  observations of  SNPs (the first 100 SNPs on chromosome 1). From a statistical point of view, SNPs are trichotomous variables (two homozygotes and a heterozygote)"
"To illustrate application of the methodology to data outside of genetics, we consider the analysis of a social science data set. The Biofam data set is derived from data obtained in a retrospective biographical survey carried out by the Swiss Household Panel in 2002. The data are freely available to the scientific community, and can be downloaded from CRAN as part of the package TraMineR (Gabadinho et al., 2011). They contain sequences of family life states recorded once a year from age 15 to 30 for  individuals born between 1909 and 1972, including only individuals who were at least 30 years old at the time of the survey. Family life state is classified into 8 categories: (i) living with parents, (ii) left home, (iii) married, (iv) left home and married, (v) have children, (vi) left home and have children, (vii) left home, married and have children, and (viii) divorced. In addition, a large number of covariates were recorded. Here for the sake of simplicity we only include sex and religion, the latter coded as ‘catholic’, ‘protestant’ or ‘other’. We combine these into one factor with six levels, i.e. the six combinations of sex and religion, and we organize the data so that this variable is placed prior to the family life state variables"
"Note also that all six covariate nodes at stage one in the sample tree are merged into one node at stage one in Fig. 4(b), implying that sex and religion do not affect the future life courses. However, as we discuss below, the tests for merging at the initial stages may suffer from low power: it would be interesting to examine this with other methods (Edwards and Ankinakatte, in press, Section 8), but we do not attempt this here."
"The computations were run under Redhat Fedora 10 Linux on a Intel i7 quad-core 2.93 GHz machine with 48 GB RAM. Beagle version 3.3.2 was used to perform the model selection algorithm of Browning and Browning (2007a,b). The remaining computations were performed in R. An R package (named gRapfa) implementing the methods described in this paper has been prepared by the authors and is available on CRAN."
"A further advantage of the likelihood-based approach is that is easily extended: for example, selection algorithms may consider steps in which more than two nodes are merged. As described in Edwards and Ankinakatte (in press, Section 10), first order Markov models correspond to APFA in which, for each stage, all incoming edges with the same symbol are merged. Thus a variant of the algorithm could be devised, based on as far as possible ‘same colour’ merging, that favourizes conceptually simpler models: this deserves further study."
"In the last years there has been a great deal of research on applying machine learning techniques and tools to processing of the functional Magnetic Resonance Imaging (fMRI) outputs [15,22,27]"
"The research has been mostly organized around the following three key areas [19]: (1) application of classification methods to fMRI data (e.g. [1,8–11,23–26]), including combinations of classifiers also known as ensemble models (e.g. [17,28,29]), (2) dimensionality reduction techniques (e.g. [2,3,10,21,23,30]) and (3) spatio-temporal filtering (e.g. [18]). "
"The alignment issue is however often ignored and all snapshots taken while a certain stimulus is active are routinely averaged [27] (in some cases trimming 1–2 initial snapshots), or labelled with this particular stimulus [17]."
" Following [17], we have first selected a subset of voxels by cross-training6 10 Support Vector Machines (SVMs) with linear kernels and then extracting top 200 contribution weights of voxels in terms of their absolute value, from each model."
 In our experiments a Support Vector Classifier (svc) from the PRTools Pattern Recognition Toolbox version 4.2.1 for MATLAB [12] has been used.
" Thus, any unordered pair (x, y) with xâBSlice(y)â§yâBSlice(x) creates an edge (x, y) in an undirected graph in which a complete subgraph is equivalent to a backward-slice MDS and a backward-slice cluster is equivalent to a maximal clique. Therefore, the clustering problem is the NP-Hard maximal cliques problem (Bomze et al., 1999) making Definition 2.2 prohibitively expensive to implement."
"Recall that the definition of a coherent dependence cluster is based on an underlying depends-on relation, which is approximated using program slicing. Pointer analysis plays a key role in the precision of slicing and the interplay between pointer analysis and downstream dependence analysis precision is complex (Shapiro and Horwitz, 1997)."
"In testing, dependence analysis has been shown to be effective at reducing the computational effort required to automate the test-data generation process (Ali et al., 2010). In software maintenance, dependence analysis is used to protect a software maintainer against the potentially unforeseen side effects of a maintenance change. "
"They are used by management researchers and practitioners (as well as other social scientists) in the context of interventions to stimulate deliberative dialogue and the development of change proposals (Beierle and Cayford, 2002; Rowe and Frewer, 2004)."
"A substantial number of these have been developed by operational researchers over the past 50 years, although the term ‘problem structuring’ itself was only introduced into the operational research (OR) lexicon a couple of decades ago (Rosenhead, 1989, 2006; Rosenhead and Mingers, 2001, 2004)."
"A distinguishing feature of PSMs, compared with many other participative methods developed by social scientists, is the use of models as ‘transitional objects’ to structure stakeholder engagement (Eden and Sims, 1979; Eden and Ackermann, 2006) and provide a focus for dialogue (Franco, 2006). "
" These evaluations are often based on explicit criteria reflecting the researcher’s experience, a given theory, a literature review and/or stakeholder expectations generated through a consultative exercise (Beierle and Konisky, 2000; Rowe and Frewer, 2004). "
"In some cases, formal evaluation instruments have been developed and applied (e.g., Duram and Brown, 1999; Rowe et al., 2004; Berry et al., 2006; Rouwette, 2011)."
"What is clear from the literature, however, is that only a very small minority of studies (e.g., Valacich and Schwenk, 1995a; Halvorsen, 2001; Rouwette et al., 2011) seek to compare between methods or across case studies undertaken by different researchers."
"A particularly significant study was undertaken by Beierle and Cayford (2002), who quantitatively compared broad classes of methods using a standard set of variables applied to 239 case studies of public participation. "
"Rowe and Frewer (2004), reflecting on social science approaches to evaluating participative methods, classify them into three types."
"White (2006) argues that very similar distinctions have been made in the OR and group decision support literatures, and preferences for universality (to a greater or lesser extent) or specificity reflect the positivist and interpretivist paradigms respectively."
"Our epistemological argument is that knowledge (or understanding) is always linked to the purposes and values of those producing or using it, and is dependent on the boundary judgements that they make (Churchman, 1970; Ulrich, 1994; Alrøe, 2000; Midgley, 2000)."
"Some features of the context-purposes-methods-outcomes relationship may be apparent early on in an intervention, while others may only emerge as the inquiry unfolds. Hence the utility of an emergent approach for the evaluation of methods, which remains open to new understandings as inquiry deepens (e.g., Kelly and Van Vlaenderen, 1995; Jenkins and Bennett, 1999; Gopal and Prasad, 2000; Allsop and Taket, 2003)."
Relevant aspects of context identified by Jackson and Keys (1984) are the complexity of the issue being addressed using a systemic method and the relationships between the participants. 
"In contrast, Margerum (2002) identifies potential contextual inhibitors of effective participation:"
" Ong (2000) discusses the facilitative effects of strong social capital, and Alberts (2007) documents the negative effects of participant inexperience and ignorance of technical issues."
Branch and Bradbury (2006) claim that a key aspect of context is managerial attitude
" McCartt and Rohrbaugh (1995) argue that a key aspect of managerial attitude is openness to change, and participative methods are often ineffective without it."
"Kelly and Van Vlaenderen (1995) and Brocklesby (2009) concentrate on stakeholder interactions, looking at how patterns of mistrust and miscommunication can become established and affect the use of participative methods."
"Champion and Wilson (2010) provide a particularly useful set of contextual variables to be considered, based on a literature review and feedback from practitioners: organisational structure; influence of the external environment; length of history of the problem in focus; politics and personalities; perceived implementation difficulty; and the level of experience of stakeholders."
"Underpinning different boundary judgements may be quite different perspectives on the nature of the context (Churchman, 1970). Therefore, exploring diverse perspectives (e.g., as advocated by Checkland (1981)) may lead to the identification of alternative possible ways of bounding a contextual analysis (Ulrich, 1994)."
"Purposes are closely linked with values and motivations (McAllister, 1999), and they are important to an evaluation because particular methods are likely to appear more or less useful depending on the purposes being pursued. "
"Different methods are generally good for different things (Flood and Jackson, 1991), and it is the perceived ‘fit’ between purpose and method that is important to evaluate: a disjunction may be responsible for an attribution of failure."
"It is important to consider possible hidden agendas as well as explicitly articulated purposes. These may significantly affect the trajectory of an intervention (for instance through sabotage), and thereby the evaluation of the method used (Ho, 1997)"
"The process of application of a method is important as well, not just the method as formally constructed (Keys, 1994). For instance, the same basic method may be enacted in quite different ways depending on the preferences and skills of the researcher/facilitator and the demands of the situation at hand."
"Compare, for example, two significantly different accounts of soft systems methodology (SSM): Checkland and Scholes (1990) discuss how the methods from SSM should be utilised in a flexible and iterative manner, while Li and Zheng (1995) insert some of the same methods into a ‘general systems methodology’. "
" Mingers (1997) describes these as the “intellectual resources” that the researcher brings into an intervention, and it is important to be able to distinguish whether problems encountered in the use of a method derive from the limitations of the method itself or from the inadequate resources of the researcher. "
"Our questionnaire was first developed in the context of a research programme aiming to generate and evaluate new systemic problem structuring methods for use in promoting sustainable resource use (Winstanley et al., 2005; Hepi et al., 2008)."
" Because of the latter, the questions had to be reasonably generic. Other authors suggest a number of different ways of producing generic evaluation criteria, and these have been summarised by Beierle and Konisky (2000) and Rowe and Frewer (2004)."
"facilitating consultation with land owners and community interest groups as part of a feasibility study for the construction of a new water storage dam (Winstanley et al., 2005)."
"working with an Australian NGO and its stakeholders in exploring policy options to address the public injecting of illicit drugs (Midgley et al., 2005);"
"facilitating workshops with the police and other stakeholders in the New Zealand criminal justice system to look at ethical issues associated with anticipated future developments of forensic DNA technologies (Baker et al., 2006);"
"reviewing the process used by the New Zealand Ministry of Research, Science and Technology to develop ‘roadmaps’ for long-term investments in environment, energy, biotechnology and nanotechnology research (Baker and Midgley, 2007)"
"developing a new collaborative evaluation approach in partnership with regional council staff responsible for facilitating community engagement in sustainability initiatives (Hepi et al., 2008)."
"At its most flexible, a pluralist practice may involve the integration of several previously distinct methods into a new whole, perhaps also incorporating the design of novel elements (Midgley, 2000)."
"The field of numerical algebraic geometry [3,24] includes a wide array of algorithms for finding and manipulating the solution sets V(f) of polynomial systems, including both isolated solutions (points) and positive-dimensional solution sets (curves, surfaces, etc.)."
"All isolated solutions have associated to them a positive integer, the multiplicity of the solution, which is greater than 1 for singular solutions [3]."
"Section 5, we describe the connection of this perturbation approach to the deflation approach of [9], the method of regenerative cascade [10], and a very early technique in the field known as the cheaterâs homotopy [17] in which the authors made use of a perturbation of fË(z) for somewhat different reasons. It is important to note that our perturbation is virtually the same as the cheaterâs homotopy, in the case where there are no parameters."
"It is observed in [17,24] that a perturbation can cause positive-dimensional irreducible components to âbreakâ into a (possibly very large) number of isolated solutions."
" The major software packages for carrying out such computations include Bertini [2], PHCpack [25], and HOM4PS-2.0 [13]."
"One very special type of homotopy is the parameter homotopy [17,19]"
This and other optimizations of regeneration are described in [9]
"Much of the theory underlying the ideas of this article was known by Morgan and Sommese in the 1980s [18] and has since been repeated in various forms, for example in [24,8]"
"Let rk(f) denote the rank of the polynomial system f(z), i.e., the dimension of the closure of the image of f(z), f(CN)â¾âCN. The rank of f(z) is an upper bound on the codimension of the irreducible components of V(f) [24]"
It should be noted that this theorem is in fact a corollary to the main result in [17].
 Regeneration can compute all of these nonsingular solutions [9]
"First, we may trivially compute the multiplicity, Î¼(zi), of each isolated solution zi of f(z)=0, as defined in [24]"
"This is based on the fact, proved as Theorem A.14.1(3) in [24], that each isolated solution zi will be the endpoint of Î¼(zi) paths beginning at points in V(f-pË)"
In this section we consider several examples where perturbed regeneration provides some benefit. All runs made use of Bertini 1.4 [2]. All reported timings except those of the last example come from runs on a 3.2GHz core of a Dell Precision Workstation with 12GB of memory.
"Next, we consider the system cpdm5, from the repository of systems [25] but originally considered in [6]"
"In the article [20], Morrison and Swinarski study a polynomial system with 13 equations, having 51 isolated solutions."
"A more specialized sort of homotopy, the 2-homogeneous homotopy [24], performs even better in this case."
"Computing the numerical irreducible decomposition [3,24], the solution set consists of 10 irreducible components of various dimensions"
"As a final example, we consider the nine-point four-bar design problem, exactly as formulated in Chapter 5 of [3]"
The regenerative cascade of [10] provides an equation-by-equation approach to computing the numerical irreducible decomposition of the solution set of a polynomial system.
"Parameter homotopies are the right tool for this job, as described briefly in Section 2.1. This idea has been implemented in Bertini [2] and Paramotopy [4]. Some background may be found in [19,17]"
"The trick to such methods is choosing an intermediate system f(v,pË) which satisfies some necessary properties, including that the solutions are smooth. The cheaters homotopy in [17] addresses this issue by including the same perturbation parameter as in Lemma 3.2."
"The numerical irreducible decomposition is the data type used in numerical algebraic geometry to store positive-dimensional solution sets. The technical definition is not necessary here but may be found in [3,24]. "
" If desired, monodromy and the trace test [3,23] could be used to find dZ points on each component Z"
"However, the introduced class of multivariate FayâHerriot models does not contain the Rao and Yu (1994) model or the GonzÃ¡lez-Manteiga etÂ al. (2008b) model as special cases."
"As Chalmers (1995) has noted: “The really hard problem of consciousness is the problem of  experience. When we think and perceive, there is a whir of information-processing, but there is also a subjective aspect."
"As Nagel (1974) has put it, there is  something it is like  to be a conscious organism. This subjective aspect is experience."
"Indeed, early mathematical results about the brain’s functional units of short-term memory (STM) and long-term memory (LTM) proved that the functional units of both STM and LTM are distributed patterns across networks of feature-selective cells (Grossberg, 1968a, 1968b, 1973). "
"ART predicts that all brain representations that solve the stability–plasticity dilemma use variations of CLEARS mechanisms (Grossberg, 1978a, 1980, 2007, 2013a)."
"Since ART was introduced in Grossberg (1976a, 1976b), it has undergone continual development to explain and predict increasingly large behavioral and neurobiological databases, ranging from normal and abnormal aspects of human and animal perception and cognition, to the spiking and oscillatory dynamics of hierarchically-organized laminar thalamocortical and corticocortical networks in multiple modalities. "
"The first paradigm is called Complementary Computing (Grossberg, 2000a). "
"Likewise, because excitatory matching is needed to generate resonances that support conscious internal representations, spatial and motor processes (“procedural memories”; Cohen & Squire, 1980; Mishkin, 1982; Scoville & Milner, 1957; Squire & Cohen, 1984) that use inhibitory matching cannot generate conscious internal representations."
"Perhaps the most basic fact about 3D vision and figure-ground perception is that its functional units are 3D boundaries and surfaces, where these words need to be properly understood. These processes were first modeled in Grossberg (1984a) and have provided a foundation for subsequent explanations and predictions about many data, including how looking at 2D pictures can generate conscious 3D percepts of occluding and occluded objects "
"Neon color spreading was reported in Varin (1971), who studied a “chromatic spreading” effect that was induced when viewing an image similar to the one "
"In summary, end gaps and end cuts are formed as a result of two successive stages of spatial and orientational competition between contrast-sensitive and orientationally tuned boundary cells (Grossberg, 1984a; Grossberg & Mingolla, 1985)."
"These orientationally-tuned simple cells  (Hubel & Wiesel, 1968) can respond to an oriented distribution of contrasts in response to scenic lines, edges, textures, and shading, not just edges alone (Fig. 4(a)). If the brain did use specialized detectors like edge detectors, then it would require many different types of specialized detectors, followed by complicated subsequent processing to try to fuse together all their information. Such an endeavor would fail if only because, in many natural scenes, lines, edges, textures, and shading are all overlaid."
"They cannot discriminate between dark-light and light-dark contrasts, or red–green and green–red contrasts, or blue–yellow and yellow–blue contrasts, because they pool together inputs from simple cells that are sensitive to all of these differences (Thorell, De Valois, & Albrecht, 1984) to form the best possible boundaries"
"These boundary completion cells are often called bipole cells (Cohen & Grossberg, 1984; Grossberg, 1984a; Grossberg & Mingolla, 1985) because they complete boundaries inwardly in an oriented manner between pairs (bipoles!) of boundary inducers. "
"More recently, Brincat and Miller (2015) have reported neurophysiological data that support the distinction between category learning within the attentional system that includes prefrontal cortex, and the orienting system that includes the hippocampus. "
"Among others,Banquet and Grossberg (1987) provide ERP markers during memory search; Brincat and Miller (2015) provide oscillatory neurophysiological markers of the interplay between prefrontal cortex and hippocampus during learning and mismatch; Otto and Eichenbaum (1992) provide neurophysiological data during hippocampal mismatch processing; and Spitzer, Desimone, and Moran (1988) provide neurophysiological data from cortical area V4 during the learning of easy vs. difficult discriminations, a process that is regulated within ART by a vigilance parameter"
"Spitzer et al. (1988) report “in the difficult condition, the animals adopted a stricter internal criterion for discriminating matching from non-matching stimuli…"
"More difficult discriminations, at least under proper circumstances, should lead to higher vigilance, more mismatch events, and thus more of the hippocampal novelty responses found by Brincat and Miller (2015) and Otto and Eichenbaum (1992). "
"Grossberg and Versace (2008), Palma, Grossberg, and Versace (2012), and Palma, Versace, Grossbergand (2012) have furthermore proposed how mismatch-activated acetylcholine release may regulate vigilance in laminar neocortical circuits that are described by spiking neurons during category learning."
"A related set of experiments concerns measuring more carefully what happens during both attentive recognition vs. mismatch reset in response to sequences of familiar vs. unfamiliar cues. Here, the following surprising discovery in Grossberg and Versace (2008) may provide a useful marker"
"In addition, the Lundqvist et al. (2016) article describes modeling ideas in which there is no temporal order represented in working memory, although temporal order information is essential for proper functioning of a working memory."
 The term attentional shroud for such a form-fitting distribution of spatial attention was introduced by Tyler and Kontsevich (1995).
"Kelly and Grossberg (2000) explain stratification percepts, including simulations of their conscious 3D surface percept properties. "
"Grossberg and Yazdanbakhsh (2005)provide model explanations and simulations of these transparency percepts, among others, including simulations of the consciously seen surface percepts, much as the model has simulated 3D surface percepts in response to many other stimuli, including stereogram images "
"Crick and Koch (1995) also proposed that visual awareness may be related to planning of voluntary movements, but without any analysis of how 3D vision occurs. "
"The classical article of Driver and Mattingley (1998) reviews visual neglect properties in individuals who have experienced IPL lesions, particularly in the right hemisphere. The text below takes as explanatory targets properties emphasized in that article"
"A neglect patient who appeared to be blind in the left visual field when fixating straight ahead, or to her left, could detect events in her left visual field when she fixated to the right (Kooistra & Heilman, 1989)."
"The implicit knowledge of parietal patients includes object attributes of neglected stimuli such as their color, shape, identity, and meaning (Mattingley, Bradshaw, & Bradshaw, 1995; McGlinchey-Berroth, Milberg, Verfaellie, Alexander, & Kilduff, 1993)."
"Lesions of the right IPL that cause visual neglect also impair the ability to maintain visual attention over sustained temporal intervals (Rueckert & Grafman, 1998), whether for visual or auditory attention (Robertson et al., 1997). This impairment can be explained by the fact that a surface-shroud resonance maintains spatial attention on an object surface"
"ARTSCAN’s explanation of visual crowding (Foley et al., 2012), and with it an explanation of the Koch and Tsuchiya (2007) discussion of how “subjects can attend to a location for many seconds and yet fail to see one or more attributes of an object at that location” "
"Foley et al. (2012) have supported this qualitative explanation of visual crowding by using the distributed ARTSCAN, or dARTSCAN, model to simulate how objects that have their own shrouds when viewed by the fovea may be enveloped by a single shroud when they are moved to the retinal periphery. "
"That is the main point of the article by Grossberg, Mingolla, and Ross (1994) whose title “A neural theory of attentive visual search: Interactions of boundary, surface, spatial, and object representations” emphasized the role of these four kinds of processes."
"Further experimental and modeling studies of crowding and visual search, and their interactions, from this perspective are much to be desired and, as illustrated by the modeling simulations of Fazl et al. (2009) and Foley et al. (2012), need to include surface-shroud resonances as one part of a unifying theory."
" In one striking classical example, alternating displays of an original and a modified scene are separated in time by brief blank fields (Rensink et al., 1997)."
"This property is called contrast normalization (Grossberg, 1973, 1980; Heeger, 1992). Due to contrast normalization, when attention focuses upon one position, activity decreases at other positions."
Foley et al. (2012) also model why the remainder of a scene does not go totally dark when such a surface-shroud resonance of focused spatial attention forms.
Mitroff and Scholl (2005) showed in this case “that object representations can be formed and updated without awareness” by making changes in displays when they were out of awareness 
Chiu and Yantis (2009) used rapid event-related MRI in humans to provide strong evidence for the ARTSCAN prediction of how a surface-shroud resonance in the Where/How stream protects an emerging view-invariant category from being prematurely reset in the What stream
"In particular, Cao et al. (2011) developed the positional ARTSCAN, or pARTSCAN, extension of the ARTSCAN model to explain how these additional object category invariances can be learned. They have used this extended model to simulate neurophysiological data of Li and DiCarlo (2008; see also Li & DiCarlo, 2010) which show that features from different objects can be merged through learning within a single invariant IT category when monkeys are presented with an object that is swapped with another object during an eye movement to foveate the original object."
"These target positions hereby control shifts of spatial attention across an attended object, and have properties that Cavanagh, Hunt, Afraz, and Rolfs (2010) have called attention pointers"
"This is proposed to occur in cortical area V3A (Fig. 21). As noted by Caplovitz and Tse (2007, p. 1179): “neurons within V3A…process continuously moving contour curvature as a trackable feature…not to solve the ‘ventral problem’ of determining object shape but in order to solve the ‘dorsal problem’ of what is going where”."
"ART proposes that this happens because both resonances interact with shared visual cortical areas, such as V4, and can thus synchronize with each other, often with gamma oscillations (Fries, 2009; Grossberg & Versace, 2008)."
" An outflow representation of the current hand/arm position, called the present position vector, or P, is subtracted from the target position to compute a difference vector, or D (Georgopoulos, Kalaska, Caminiti, & Massey, 1982; Georgopoulos, Schwartz, & Kettner, 1986) that codes the direction and distance that the hand/arm needs to move to reach the target. "
"The Vector Integration to Endpoint, or VITE, model (Fig. 27, left panel; Bullock & Grossberg, 1988) modeled these processes to clarify how the Three S’s of reaching are carried out: the flexible choice of motor Synergies, and their Synchronous performance at variable Speeds."
"A refinement that sheds the most light on auditory–visual homologs of reaching and speaking circuits is called the DIRECT model (Bullock, Grossberg, & Guenther, 1993), which also learns through a circular reaction."
"During the development of the DIRECT model by Bullock et al. (1993), an evolutionary rationale was noted for why both the hand/arm and speech articulator systems may use similar, indeed homologous, neural circuits; namely, eating preceded speech during human evolution (MacNeilage, 1998), and skillful eating requires movements that coordinate hand/arm and mouth/throat articulators, including motor-equivalent solutions for reaching and chewing. "
"The auditory continuity illusion (Bregman, 1990) illustrates ART properties during auditory streaming."
"Grossberg (1978a, 1978b) introduced a neural model of working memory upon which the more recent models listed above consistently built."
"A more recent name for this class of models is competitive queuing (Houghton, 1990). When an Item-and-Order working memory can store repeated items in a sequence, it is called an Item–Order–Rank working memory (Bradski, Carpenter, & Grossberg, 1994; Grossberg & Pearson, 2008; Silver et al., 2011)."
"Phonemic restoration (Warren & Sherman, 1974) illustrates the operation of ART mechanisms during speech perception in much the same way as the auditory continuity illusion represents them during auditory streaming"
"Jones, Farrand, Stuart, and Morris (1995) reported similar performance characteristics to those of verbal working memory for a spatial serial recall task in which visual locations were remembered. "
"Agam, Bullock, and Sekuler (2005) reported psychophysical evidence of Item-and-Order working memory properties in humans as they performed sequential copying movements, and Averbeck et al. (2002); Averbeck, Crowe, Chafee, and Georgopoulos (2003a, 2003b) reported neurophysiological evidence for such a working memory in monkeys during performance of sequential copying movements."
"The motor-to-auditory selection process mechanistically explicates the “motor theory of speech perception” (Galantucci, Fowley, & Turvey, 2006; Liberman & Mattingly, 1985)."
"The Neural Normalization Network, or NormNet, model (Ames & Grossberg, 2008) proposes that speaker normalization specializes the same kinds of neural mechanisms that are used to form auditory streams."
"This proposal clarifies how speaker normalization can transform auditory signals, right after they are separated into separate streams, for purposes of speech and language classification and meaning extraction, yet how the frequency content of the streams can be preserved for purposes of speaker identification in a separate processing stream, as illustrated in the ARTSPEECH architecture (Fig. 33, right panel) of Ames and Grossberg (2008). "
Boardman et al. (1999) developed the PHONET model to quantify how T and S working memories can use asymmetric T-to-S gain control to create rate-invariant representations of individual speech syllables or words. 
"A classical example of this phenomenon was reported by Repp (1980). Repp constructed VC-CV syllables from the syllables [ib], [ga], and [ba] to form [ib]-[ga] and [ib]-[ba]."
The comparison between resonant fusion and resonant reset that plays an important role in explaining the Repp (1980) data on category boundary shifts also helps to explain data about the way in which masking stimuli can influence error rates and reaction times during lexical decision tasks.
"Grossberg and Stone (1986b) explained the paradoxical pattern of experimental results in terms of how the ART Matching Rule works in different priming situations, including the inability of top-down expectations to act before the mask interferes with the persistence of word and non-word target representations in working memory"
"The TELOS model (Brown et al., 2004) predicted how agreement between prefrontal and parietal representations of a target position causes a parietal–prefrontal resonance that selects this target position, and opens the correct basal ganglia gate, while also enabling basal ganglia-mediated release, in a different part of the brain, of a contextually-appropriate movement command to that position."
"Subsequent neurophysiological data of Buschman and Miller (2007)supported this prediction by describing such a parietal–prefrontal resonance during movement control, and Pasupathy and Miller (2004) additionally described different time courses of activation in the prefrontal cortex and basal ganglia that are consistent with how basal ganglia-mediated gating of prefrontal cortex occurs in TELOS."
"The lisTELOS model (Silver et al., 2011) extended the TELOS model to explain and predict how sequences, or lists, of eye movements can be carried out, while continuing to simulate everything that TELOS could. "
"On the other hand, it does propose how parallel neural mechanisms for rate-invariant and speaker-normalized representations of speech, and for pitch-dependent and rhythm-dependent speech intonation (Ladefoged & Disner, 2012), may interact to achieve “online sequencing of syllables into fast, smooth, and rhythmically organized larger utterances” (Ackerman, 2008, p. 265), including how these several kinds of information are learned, stored, and combined during fluent performance and conscious awareness thereof. "
"Learning also goes on throughout life of a parallel circular reaction that links learned spectral-pitch-and-timbre categories for the recognition of heard sounds (Table 2), which are not speaker-normalized, to the motor synergies that control the pitches generated by the vocal folds (Sundberg, 1977)."
"Neural models of cognitive–emotional resonances began with the articles of Grossberg (1971, 1972a, 1972b) and Grossberg (1975) at a time when there was a major split between studies of cognition, as exemplified by the work of Chomsky (1957) in linguistics, and of emotion, as exemplified by the work of Skinner (1938) on instrumental conditioning."
"For simplicity, consider only the simplest kind of reinforcement learning, called Pavlovian or classical conditioning (Kamin, 1968, 1969; Pavlov, 1927), during which a conditioned stimulus, or CS, that initially may have no emotional significance, is paired with an unconditioned stimulus, or US, that can from the start generate a strong emotional response. "
"Damasio (1999) has derived from clinical data what can be viewed as a heuristic version of the CogEM model, and has used it to describe what can be interpreted as cognitive–emotional resonances that support “the feeling of what happens”. "
"Damasio (1999, p. 171), notes: “Attention is driven to focus on an object and the result is saliency of the images of that object in mind”, leading to what Damasio calls core consciousness. Damasio (1999) also went on to write “I do not know how the fusing, blending, and smoothing are achieved…” (p. 180). "
"These include the “dual competition” model of Pessoa (2009, p. 160): “The proposed framework is referred to as the ‘dual competition’ model to reflect the suggestion that affective significance influences competition at both the perceptual and executive levels—and because the impact is caused by both emotion and motivation”."
"The ARTSCENE model (Grossberg & Huang, 2009) and ARTSCENE Search model (Huang & Grossberg, 2010) illustrate how humans accomplish these goals."
"Tamietto and de Gelder (2010) have reviewed several different kinds of experimental evidence that led them to a similar viewpoint, but without mechanisms of adaptive resonance to derive mechanistic conclusions. "
"Clark and Squire (1998, p. 79) postulated that normal humans acquire trace conditioning because they have intact declarative or episodic memory and, therefore, can demonstrate conscious knowledge of a temporal relationship between CS and US: “trace conditioning requires the acquisition and retention of conscious knowledge [and] would require the hippocampus and related structures to work conjointly with the neocortex”."
"In addition to explaining data about normal delay and trace conditioning, the nSTART model explains and simulates many subtle data about how learning and memory consolidation are influenced by different brain lesions (Franklin & Grossberg, 2016)."
" Crick and Koch (1990)described two forms of consciousness “a very fast form, linked to iconic memory…; and a slower one [wherein] an attentional mechanism transiently binds together all those neurons whose activity relates to the relevant features of a single visual object”. "
"For example, the neural global workspace of Dehaene (2014), which builds upon the global workspace of Baars (2005), claims that “consciousness is global information broadcasting within the cortex [to achieve] massive sharing of pertinent information throughout the brain” (p. 13)."
"Continuing in the spirit of Edelman and Tononi (2000), Tononi (2004) defined a scalar function Φ, “the quantity of consciousness available to a system… as the value of a complex of elements."
"Unlike traditional team-based work, however, members of the crowd are distributed and in many cases without those obligations as found in companies (long-term contracts or roles) [1]. The crowd presents a pool of experts, who are connected amongst themselves forming a social network."
"Most approaches model the problem as finding the best match of experts to required skills taking into account multiple dimensions from technical skills, cognitive properties, and personal motivation [2–4]. Such research focuses only on properties of individual experts that are independent of the resulting team configuration."
"Sozio and Gionis describe the community formation problem [6]. Given a set of fixed members, the approach expands the team up to a maximum upper size boundary such that the communication cost within the community remains small."
"Anagnostopoulos et al. [7]address fair task distribution within a team. They apply skill matching to determine a team's ability to fulfill the overall set of tasks. While their approach takes into account team members' load and skill dependencies, the underlying social network structure has no impact on the team's fitness. "
Yang et al. [8] apply integer programming to determine the best set of group members available at a certain point in time. Their temporal scheduling technique considers the social distance between group members to avoid lacking too many direct links. 
Craig et al. [9] propose an algorithm for reasonably optimal distribution of students into groups according to group and student attributes.
"Xie et al. [10] aggregate a set of recommender results to optimally compose a package of items given some relation between the individual items and an overall package property (e.g., holiday package)."
"To the best of our knowledge, Theodoros et al. [13] discuss the only team composition approach that specifically focuses on the expert network for determining the most suitable team. Our approach differs in three significant aspects. "
"First, we model a trade-off between skill coverage and team connectivity whereas [13] treats every expert above a certain skill threshold as equally suitable and ignores every expert below that threshold."
Also Singh [14] shows that a densely connected team is vital for successful open source developer cooperation. Most importantly we apply recommendations instead of direct interaction links when the underlying network becomes too sparsely connected.
"Analysis of various network topologies [15,16] has demonstrated the impact of the network structure on efficient team composition."
"Investigations into the structure of various real-world networks provides vital understanding of the underlying network characteristics relevant to the team composition problem [18,19]."
"Complementary approaches regarding extraction of expert networks and their skill profile include mining of email data sets [22,23] or open source software repositories [24]. Additional sources include (scientific) publications and financial data [25], social network page visits [26], telecommunication data [27], and online forum posts [28]."
"Related research efforts based on non-functional aspects (i.e., non-skill related aspects) can also be found in the domain of service composition [29]. Here, services with the required capabilities need to be combined to provide a desirable, overall functionality. "
"The model of recommendation-based link establishment is closely related to link prediction in social networks. Such models are used to introduce connections between single members of a community by evaluating various properties. For instance, work by [40,41] discusses link prediction based on similarity, focusing on structural graph properties such as number of neighbors and number of in/out links."
"In social trust networks [42] recommendations reflect transitive relations among members. In that case, unconnected nodes in a trust network are connected through an intermediate node that mediates second hand knowledge among its neighbors. In the future, direct trust between humans will play an ever more important role as privacy remains a largely unsolved challenge [43]. Hence we believe that establishing explicit trust in social networks (e.g., [44,45]), respectively becoming aware of distrust, will become a significant factor in team formation."
"In today's highly dynamic large-scale networks, people are no longer able to keep track of the dynamics, such as registration of new actors in expert networks and emerging skills and expertise of collaboration partners. Since interactions and collaborations on the Web are observable, systems can analyze tasks performed in the past in order to determine network structures and member profiles automatically [46,42]."
Simulated Annealing [47] (SA) and Genetic Algorithms [48] (GA) are two common heuristics suitable for the underlying problem type.
"Previous work suggests dynamic adaptation for crossover and mutation probabilities [49], whereas [50] applies clustering techniques to determine suitable values. The correlation of population size and cross over is investigated in [51]. These three exemplary works, however, address very different problem domains. In the case of simulated annealing, work on optimizing parameters is similarly problem specific: [52] addressing a graph partitioning problem, [53] focusing on the longest common subsequence problem, and [54] dealing with distributing workload across multiple processors."
 Parameterless multi-objective algorithms such as NSGA-II [55] provide multiple pareto-optimal solutions to the team formation problem without setting α to any particular value. 
Finding a single shortest path is in O(|ECand| + |Cand|log|Cand|) [56]. 
"The bullwhip effect also has a close link with the philosophy of lean production (Ohno, 1988). Mura—the waste of unevenness—is the failure to smooth demand and is recognised as the root cause of both Muda (the seven lean wastes) and Muri (the waste of overburden). Indeed Ohno (1988) discusses the benefits of bullwhip avoidance"
"Geary, Disney, and Towill (2006) classified five routes to increase our knowledge of bullwhip effect and 10 principles to reduce it."
" Miragliotta (2006) reviewed bullwhip research in three categories; empirical assessment, causes, and remedies, and then proposed a new taxonomy to model this problem. "
" Giard and Sali (2013) categorised 53 bullwhip papers within 13 coordinates, including modelling approaches, demand models, measures, and causes. "
"Other reviews are more conceptually oriented, attempting to offer a new perspective on bullwhip (Towill, Zhou, & Disney, 2007)."
"Some reviews are not solely confined to the bullwhip effect, but also cover other supply chain modelling issues (Beamon, 1998; Min & Zhou, 2002; Sarimveis, Patrinos, Tarantilis, & Kiranoudis, 2008). "
"Interestingly, a similar phenomenon between P&G and its wholesalers has been documented during 1910s (Schisgall, 1981). "
Forrester (1961) first formalised the variance amplification effect using the ‘industrial dynamics’ approach. He later established a simulation experiment mimicking the decision making behaviour in supply chains—the famous ‘Beer Game’
" Sterman (1989) published 20 years of data from the game attributing the amplification to the tendency that players overlook the inventory-on-order (the orders placed but not yet received), a cause of amplification known as ‘irrational behaviour’."
"The production smoothing hypothesis (Holt, Modigliani, Muth, & Simon, 1960) assumes that production fluctuations increase the operational cost to the manufacturer by inducing excess machine setup, idle time and workforce hiring/firing. "
"There is also a trade-off between inventory cost and production cost, due to the stabilizing effect of inventory (Baganha & Cohen, 1998; Disney, Towill, & van de Velde, 2004). Chen and Samroengraja (2004) showed that when the cost function is concave, the replenishment policy that minimises order fluctuations is not necessarily the one that minimises total cost."
"Under non-stationary demand it is necessary to perform difference operations on the time series. That is, to measure bullwhip by the variance of order changes instead of the variance of orders itself (West, 1986). "
"Alternatively one may compare the difference between order variances and demand variances which has been proved to be finite (Gaalman & Disney, 2012). If the inventory system is to be modelled linearly, then the variance ratio is convenient because it coincides with an engineering concept called the noise bandwidth, a concept with an established theoretical basis (Åström, 1970)."
"In 1960 Holt et al. (1960) proposed the production smoothing model assuming that rational decisions regarding production quantities would lower costs by levelling production, with inventory being used as a buffer. Efforts have been made to optimise this model under various assumptions (Gaalman, 1978; Schneeweiss, 1974; Zangwill, 1966). "
" Quite contrarily, many empirical studies have found amplification between retail sales and production orders, as well as positive correlation between demand and inventory (Blanchard, 1983; Blinder, 1986; Blinder & Maccini, 1991; West, 1986). These can be viewed as early examples of the bullwhip effect in the production echelon, an effect that was then termed ‘excess volatility’"
 Ghali (2003) showed that production smoothing can be found only in a small number of industries where seasonality is stable and inventory holding cost is low.
" In 75 industries, Cachon et al. (2007) observed that 61 exhibited bullwhip when seasonality was removed, but only 39 when not. Similar findings have been reported by Bray and Mendelson (2012), on the basis of firm-level, rather than industry-level, data. "
"Baganha and Cohen (1998) observed that bullwhip effect appears in the wholesaler's echelon, and argued that the wholesaler's inventory acts as a stabiliser in the chain."
" Mollick (2004) described evidence of production smoothing in the Japanese automotive industry, where the production smoothing is more common due to the prevalence of Heijunka (levelling) and Just-In-Time manufacturing strategies. "
" Cantor and Katok (2012) introduced a cost for production and order changes, and found that production is smoothed when demand is seasonal, and that the smoothing behaviour is more eminent when the production change cost is high."
"The simplest demand model is an independently and identically distributed (i.i.d.) Gaussian white noise process (Deziel & Eilon, 1967). This model has some mathematical advantages, but may be an over-simplification as it overlooks temporal correlation in the demand signal."
"More complex ARIMA models for demand have also been studied: AR(2), AR(p) (Luong & Pien, 2007); ARMA(1,1) (Alwan, Liu, & Yao, 2003); ARMA(2,2) (Gaalman & Disney, 2009); and ARMA(p,q) (Gaalman, 2006)."
"A wide range of forecasting methods have been investigated in the bullwhip literature. Chen et al. (2000a) and Duc et al. (2008a) studied the moving average (MA) forecasting method, while Chen, Ryan, and Simchi-Levi (2000b) and Dejonckheere, Disney, Lambrecht, and Towill (2003) investigated the simple exponential smoothing (SES) method. These are both user-friendly forecasting techniques that have been widely adopted in industry."
"The impact of more sophisticated forecasting methods such as Holt's, Brown's and Damped Trend forecasting was discussed by Wright and Yuan (2008) and Li, Disney, and Gaalman (2014). These forecasting techniques are designed for seasonal and trended demand."
"Another interesting topic is the relationship between forecast accuracy and total cost. Zhang (2004a) suggested that MMSE forecasting minimises inventory-related cost. This was supported by Hussain et al. (2012) in a simulation study. However, according to some empirical (Flores, Olson, & Pearce, 1993) and analytical research (Hosoda & Disney, 2009), the most accurate forecasting does not always result in an optimal supply chain when local bullwhip or global inventory costs are taken into account (Disney, Lambrecht, Towill, & Van de Velde, 2008; Gaalman, 2006; Gaalman & Disney, 2006; Gaalman & Disney, 2009)."
"Forrester (1961) highlighted that the delays in information and material flow, a.k.a. the lead-times, is a driving factor of demand amplification."
" Lee et al. (1997) and Chen et al. (2000a) argued that bullwhip increases in lead-time, as did Steckel, Gupta, and Banerji (2004) and Agrawal, Sengupta, and Shanker (2009)."
"Modelling lead-time as a random variable mimics the volatility of real-life logistics. Chatfield, Kim, and Harrison (2004), Kim, Chatfield, Harrison, and Hayya (2006) and Duc, Luong, and Kim (2008b) showed that order variability increases with lead-time variability, a result that is also supported by the behavioural experiment conducted by Ancarani, Di Mauro, and D'Urso (2013)."
"State-dependent lead-times have been examined by So and Zheng (2003) and Boute, Disney, Lambrecht, and Van Houdt (2007), and both studies found that bullwhip is underestimated if the endogeneity of lead-time is neglected."
"The automatic pipeline, inventory and order-based production control system (APIOBPCS) proposed by John, Naim, and Towill (1994) is mathematically equivalent to Sterman's (1989)‘anchoring and adjustment heuristic’."
Deziel and Eilon (1967) proposed the first linear proportional production control policy where the same feedback parameter is assigned to both the inventory and pipeline levels.
"General guidance on tuning the feedback parameters is given by Balakrishnan, Geunes, and Pangburn (2004), Papanagnou and Halikias (2008). Graves, Kletter, and Hetzel (1998) and Boute and Van Miegham (2015) describe other proportional ordering policies."
"The problem of product/location aggregation arises when a supplier faces multiple retailers, or distribution centres in different locations, or by manufacturing different products on the same line. This problem has been investigated under (s,S) (Caplin, 1985; Kelle & Milne, 1999), (Q,T) (Cachon, 1999; Lee et al., 1997) and base stock (Sucky, 2009) policies."
"Lee and Whang (2000) summarised the common schemes for sharing information on inventory levels, sales data, sales forecast, order status and production/delivery schedules"
"From experimental and analytical evidence, some authors have found that information sharing alone cannot eliminate the bullwhip effect (Chen et al., 2000a; Croson & Donohue, 2006; Ouyang, 2007; Sodhi & Tang, 2011). "
"It has been discovered that if the order quantity is constrained to non-negativity (as opposed to the ‘costless return’ assumption, Lee et al., 1997), then highly complex and sophisticated dynamical behaviours can be found in supply chains (Mosekilde & Larsen, 1988). Moreover, this dynamical complexity is also amplified along the chain, in an effect known as chaos amplification (Hwarng & Xie, 2008). "
"The structure of real supply chains is further complicated by sourcing, distribution and transhipment activities. Ouyang and Li (2010) proposed a general supply network model that allows for transhipment, information sharing, and collaboration; they identified conditions for bullwhip."
 Akkermans and Vos (2003) measured workload in a major US telecom company. They detected the amplification of workload and identified a potential cause of the amplification: negative feedback between workload and service quality. 
 Özelkan and Çakanyildirim (2009) studied financial flows in a game theoretical two-echelon supply chain model. 
"Among these are Zhang and Burke (2011), who showed that introducing price fluctuations can either exacerbate or mitigate the bullwhip effect, based on the auto- and mutual-correlation between price and demand. "
"Recently Sodhi, Sodhi, and Tang (2014) incorporated a discretely distributed stochastic price into the economic order quantity model. They showed that the bullwhip effect persists and is positively related to the variance of price."
 Molodtsov [1] proposed an uncertainty-soft set theory that is completely new approach for modeling vagueness.
"Soft set theory is getting popularity among the researchers in these domains. Soft set has been extensively and successfully applied in decision making [2–19], data analysis [20–22], forecasting [23], simulation [24], evaluation of sound quality [25], rule mining [26], and so on."
"Combining soft sets with others mathematical theories, such as fuzzy sets [15,27–31], rough set [28,31–33], vague sets [34], interval-valued fuzzy sets [10,12,35], interval-valued intuitionistic fuzzy soft set [36], intuitionistic fuzzy soft set [35,37–39], and so on, has come forth rapidly to meet various demands in practical situations."
Parameter reduction in soft set is discussed in published papers. Maji et al. considered the initial level reduction soft set with the help of rough set approach [2]
"However, Chen et al. pointed out that the errors of soft set reduction and presented a new notion of parameterization reduction in soft set [40]"
"Kong et al. analyzed two cases, suboptimal choice and added parameter set, and introduced the definition of normal parameter reduction and its algorithm in soft set [42"
Ma et al. simplified the normal parameter reduction algorithm [43
Gong et al. proposed two parameters reduction algorithms based on bijective fuzzy soft set system [14].
"Kong et al. [42] introduced the definition of normal parameter reduction in soft set and its algorithm. In the following, the definitions of normal parameter reduction and indiscernibility relation are given"
"Managers spend up to one fifth of their working time with conflict resolution and negotiation [15,63]. They increasingly negotiate via electronic media such as e-mail, e-meeting and e-negotiation systems [73]. Electronic negotiations are not mere translations of traditional negotiations onto electronic media, but rather they provide additional value by supporting the decision making and/or communication process [62,74]. Electronic negotiation support (eNS) is realized through information and communication technology and can range from a simple message exchange to a complex support system. A negotiation support system (NSS) comprises one or more of the following functionalities: facilitation of communication, decision/negotiation analysis support, process organization and structuring, and access to information, negotiation knowledge, experts, mediators or facilitators [26]. In this context, the representation of information (textual, graphical, and auditory) is important for human–computer interactions. Due to technical advances in the last decades, users can often rapidly and effectively choose from various formats of computer generated reports. We know from empirical evidence that the way information is presented strongly influences human perceptions, preferences and decision making (e.g. [5,76]). Thus, the presentation of information is of essential importance for decision makers [70,77]"
"Although information representation is relevant, it has received little attention in negotiation research. Typically, information in e-negotiation systems is presented in text or tabular format. Except for the suggested utilization of the “negotiation dance graph” [56], to date only a “history graph” has been proposed and implemented [27,63]. A history graph exhibits the history of offers and counteroffers over time of both negotiators based on preferences of the supported user only. In contrast, the negotiation dance graph represents all offers and counteroffers in the utility of both negotiators, while time is only implicitly considered, and it provides users with information about the actual preferences of their counterparts"
"The present study aims to analyze how information presentation in these alternative formats (table, history graph and dance graph) influences the negotiators' behavior and negotiation outcomes. The paper reports on a 2006 controlled laboratory experiment. Students from three universities in Europe and the Middle East negotiated a contract in a scenario with multiple issues in the tourism industry. Using the NSS Negoisst [62,63], subjects were divided into three treatment groups using the three different representation aids on the negotiation process: a table, a negotiation history graph or a negotiation dance graph"
"The paradigm of cognitive fit suggests that effective and efficient problem solving is obtained when all tools or aids used in the problem solving process correspond to the requirements of the task [78–80]. Problem solving is seen as an outcome of the relationship between problem presentation and the problem solving task. Cognitive processes act on the information presentation and the problem-solving task to provide a mental representation of the situation. The latter is the way the problem is represented in human working memory. When the types of information in the problem presentation match those in the task, the problem solver formulates a mental representation that is based on the same type of information. In contrast, a mismatch between the problem presentation and the task leads to a mental representation based only on the problem representation. The decision maker must then mentally transform the task into a suitable form, exerting additional cognitive efforts in order to solve a particular type of problem. Similarly, if a mental representation is formulated according to the task alone, the decision maker has to transform the data of the problem presentation into an appropriate form for the task solution. In both cases, additional cognitive capacities are required for auxiliary mental steps, which typically lead to poor results for the decision maker. The cognitive fit theory encourages the use of problem representations consistent with task requirements in order to improve the decision making process for those using decision aids"
"While text-based systems constitute a minimum requirement, all other representation forms are more sophisticated. One idea to support decision makers is to quantify all available data and to implement it into numerical systems, which have already been shown to provide better support than simple textual messages. Numerical systems require well-structured inputs in a predefined format [19], show impacts of variables on results [7] and provide assessment scores [36]. However, numerical systems do not support decision makers in handling dynamic processes [7]. In negotiations, the history of exchanged offers, the concessions of the negotiation parties over time, their possible change of preferences and similar dynamic processes contain essential information for negotiators [62,81]. A more stylized information representation is essential."
"As graphs can be displayed in various formats, they often differ considerably in terms of their abstraction or arbitrariness. No unique terminology has been used for characterization of graphs. They are described as being “imaginastic,” which means that they convey continuous information, while tables are seen as “verbal” in nature, i.e. they convey discrete information [78,79]. Graphs have visuospatial properties meaning they stress information on data relationship rather than on linguistic intelligence [4,5]. Graphs facilitate the acquisition of information by focusing on single units of information and their characteristics. They also allow for the grouping of information [35] and the establishment of associations among the values of each information package (or variable) across time periods without addressing the elements separately or analytically (e.g. [4,78,79]). Graphical display formats have a sequential structure reflecting an overview of the presented information. Many perceptual inferences, including perceiving and drawing inferences, are automatically supported at low cognitive costs [8,34]. Graphs facilitate the comprehension of large amounts of quantitative information [44,67]. Empirical research has reported that subjects provided with graphical formats are more effective in trend, pattern and time sequence data detection, (e.g. [12,68,77]) and in task execution in terms of processing time (e.g. [31,32,44])."
"Concerning the level of complexity, tables outperform graphs regarding time and decision accuracy in simple decision making settings [45,58]. At a low level of complexity, graphs are perceived to be more difficult to read than tabular displays [12]. An increase in task complexity is better mediated by spatial rather than linear information displays [68]. Studies suggest that graphical decision aids are more efficient and lead to better performance when subjects face a higher cognitive load [45,58,70]. Graphs have been found to be more appropriate for the presentation of large amounts of information [12], because users have to invest less effort in order to “get the message” shown in graphical displays [5,6,40]. Users sometimes prefer graphs to tables due to their appealing format; they enjoy exercises and experience a higher level of satisfaction [40,43,77]. Still, subjects do not always prefer the most appropriate presentation format for the relevant task [20,32]"
"The most common and straightforward way to provide users of NSS with information about multi-issue offers is to present the utility values [27]. This involves analyses of the current offer and all prior offers made in the negotiation. Offers are evaluated and compared to the negotiator's aspirations, reservation level or to the BATNA (Best Alternative To Negotiated Agreement) over several periods of time, while all social interactions are processed simultaneously [1,66]."
"One way to visualize the negotiation process graphically is the history graph (see Fig. 2), which has already been implemented in NSS [63,64,82]. In the history graph, the factor “time” is represented on the horizontal axis and negotiators' “utility” is on the vertical axis. All offers are labeled on the ordinate according to the score associated with an offer. Even though offers of both parties are displayed, the calculation of the utility values is based only on the preferences of the focal user. Therefore, the history graph shows the distance between the offers submitted and received based on the focal users' value function. The history graph is designed to enable users to assess how far they are from reaching an agreement. For example, company A and company B negotiate over a contract including several issues and refer to the history graph. When company A formulates an offer, the utility rating of the offer and consequently its graphical presentation is based on the preferences of company A. When company B analyzes the offer received from company A, company B is provided with a rating and a graphical display of the offer according to the ratings of company B. This implies that each transmitted offer is rated according to the focal user only, while the preferences of the counterpart are not taken into account in the rating of offers or in the graphical displays"
"To answer the research questions, an electronic negotiation support system is required that supports business negotiations, rich communication support and various forms of decision support. Negoisst is a web-based NSS offering sophisticated support and formal document management [62,63]. Therefore, the experiments were conducted using Negoisst (see Fig. 4 for a screenshot of the system). Users negotiate via an electronic message exchange. The content of the messages is written in natural language (shown to the left of Fig. 4). In order to avoid misunderstandings and to prevent re-negotiations due to contractual ambiguities, Negoisst offers semantic and pragmatic enrichment. Semantic enrichment links free text to the negotiation agenda (shown to the right of Fig. 4). Pragmatic enrichment supports explicit intentions, because message types are indicated by the author (see Fig. 4). Negoisst also provides decision support. Negotiators specify their preferences on attributes to be negotiated and the system then computes a utility function. Each offer is rated, and both negotiators can see in a glance how well they have already achieved their goals. If a negotiator writes a message offering a certain package, then the system will calculate the utility immediately. The negotiator can check the utility value before sending the message. Negoisst automatically deduces a contract version from each message sent, as well as a message thread representing the reasons for the decisions taken. Users are able to check the contract versions as well as all exchanged messages at any time during the negotiations."
"Whether an agreement is reached or not is an indicator of the effectiveness of negotiations but not of the quality of negotiation outcomes. In the negotiation theory, three further indicators are often used to measure the quality of negotiation outcomes: joint outcome (as an indicator for efficiency), contract balance (as an indicator for fairness), and negotiator satisfaction with agreement (as a holistic assessment) [16,33,57]. Empirical evidence proves that negotiators pursuing an integrative negotiation strategy produce higher joint outcomes (e.g. [10,83,85]). Furthermore, there exists a trade-off between time/effort and decision quality or accuracy [22,24]. The development of value-creating offers, e.g. through logrolling, requires significantly more cognitive effort. This can be more easily achieved when negotiators are supported with the history graph. Therefore, we assume:"
"We expect that this additional information will change negotiation behavior in several ways. By providing utility information about both negotiators, dyads should be better able to assess whether their negotiation partner behaves fairly. Negotiators provided with this type of graph can easily see if real concessions are being made. Decision makers aware of this fact should consequently ask their opponent for fair treatment and stress the importance of fairness more often [47]. Therefore, we expect:
H 4(a)"
"Although we do not expect differences in the number of agreements between the two groups, we expect the quality of agreements to differ significantly. The visualization of changes in utilities due to modifications in single issues in the negotiation dance graph helps negotiators to identify Pareto movements and efficient alternatives [56]. "
"We assume that the visibility of differences in utilities during the negotiation process makes it more difficult to demand “the bigger share of the cake” [60]. There is an expectation of more balanced agreements when negotiators have information about utilities of both negotiation partners, and we hypothesize:
H 6(b)"
"Consequently, we expect negotiators who reach higher joint outcomes and more balanced agreements will be more content (e.g. [17,37,77]), and we hypothesize:
H 6(c)

Negotiators provided with the negotiation dance graph will be more satisfied with the agreement compared to negotiators provided with the history graph."
"Several factors that could affect negotiation process/outcomes were not investigated in this paper. First, several studies show that the level of conflict in simulation cases influences results significantly [11,53]. Conflict could be induced by varying the discussion issues and creating more integrative/conflicting bargaining settings. Users' performance could be observed by changing only external factors (in this case the bargaining situation in which negotiations are embedded). Variance in the number of issues involved in a case could also affect the end result. Another avenue of future research is the effect of additional information provided to users. The present study shows that the amount of information provided to negotiators leads to either more cooperative or more competitive behavior. Future studies should investigate the impact of different types of information implemented in different information displays. Considering the process of information gathering, future investigations also need to examine the effect of dynamic decision aids at different stages of decision making. A particular focus should be placed on the stages in which information is acquired and in which the information is evaluated. The issue of time duration of the experiment must be taken into account [51]. The effects of additional support provided by graphical aids are often seen as a trade-off between the benefits of minimizing errors and the cognitive effort or time needed in a particular task environment [22]. In the present study, there was an imposed time deadline for all users, thus the variable time was kept constant and all impacts could be considered only with regard to proxies for the quality of decisions. Raiffa [56] argues that a negotiation resembles a dance of negotiation partners. We have demonstrated with this study that there is no straight answer to the question “Shall we dance?” Rather the results suggest that the answer depends on the partners' aims (efficiency vs. fairness) quantitative vs. qualitative outcomes (utility vs. satisfaction), to dance or to skip the dance."
"Thus, every Venn diagram is an Euler diagram, but not vice versa. Euler diagrams support the interpretation of grouping information since elements in a common set are located in the same region [31].There have been a number of empirical studies conducted to ascertain the effect of different Euler diagram layout choices on user comprehension which provide a starting point for effectively laying out Euler diagrams with graphs."
"This was carried out by analyzing the layout and structure of socially constructed texts of “organizational communication” (Yates & Orlikowski, 1992) amongst people in a particular workplace or in a “community of practice” (CoP) as described by Wenger (1999), where, genre, in a textual sense, is sometimes defined as a group of texts or documents that share a communicative purpose, as determined by the discourse community which produces and/or reads them (Swales, 1990). "
"Collins, Mulholland, and Watt (2001)explained that what the community sees as important will be reflected in the implicit structures found in the objects they create and share and as Watt (2009, chap. 8) has observed: “convergence on a set of standardized document structures is both natural and helpfu"
"Layout in organizational communities causes people to focus perceptually on key parts of the text (Schmid & Baccino, 2002) and our empirical research has previously demonstrated that people use layout and other related cues to focus on key parts of the text (Clark, 2008; Clark, Ruthven, & Holt, 2008; Clark, Ruthven, & Holt, 2010; Clark, Ruthven, Holt, & Song, 2012). "
"The reader is able to perceive the meaning through interaction with the cues which exist on the outside and inside of the “frame” (Frow, 2006, chap. 5) – a term that Frow uses synonymously with genre."
"To examine genre and ways of perceiving, we used specific eye movement behavior metrics or ‘ocular metrics’ (Rayner, 1998) which have been in fairly common use in contemporary eye tracking experiments i.e. Scanpath Duration and Scanpath Length, c.f. Goldberg and Kotval (1999, p. 638). "
Aristotle (1954) considered that whatever was perceivable by the individual was reality. 
"Outside objects imposed upon the senses, and due to the power of reason, the mind was able to extricate the form, which determined the nature of the perceived object (Breure, 2001). "
" We contend that the specific contexts of researchers guide the way they delineate genre: as Kwaśnik & Crowston (2005)argue, the researcher chooses the definition applicable to the current context of the study."
"Readers viewing text(s) are always involved or relate to the complete arrays of textual meaning. This is quite closely related to Semiotic ‘intertextuality’ a term that is said to have been coined by the post-structuralist semiotician, Kristeva (1980). "
"In other words, an author or artist refers to an earlier work and subsequently converts a previous creation with it then being referred to in the new text. As Chandler puts it: “The concept of intertextuality reminds us that each text exists in relation to others. In fact, texts owe more to other texts than to their own makers” (Chandler, 2011)."
"Hirsch Jr. (1967, p. 76), explains that genre is an interpretative process called into being by the fact that “all understanding of verbal meaning is necessarily genre-bound.”"
"Hirsch’s explanation could be appropriately linked to the work pertaining to “perceptual hypotheses” by Gregory (1980) or, indeed, as we like to refer to it, ‘perpetual’ perceptual hypotheses, where we are continuously trying to ascertain what an object or text is. "
" Lorch (1989) identifies textual signals, such as headings, previews, summaries, titles, numeric signals and so on. All texts are accompanied by these types of cues or signals which ‘present’ the texts to the reader or ensure the presence of the texts in the world."
"For the purpose of this study, genre was defined by its purpose (sometimes known as substance) but mainly by form (see Fig. 2 for categories of form) as described in Dewdney, VanEss-Dykema, & MacMillan (2001) and Yates & Orlikowski (2002, p. 15). "
"The constructivists assert that the final goal in the perception process is recognition which would require intense cognitive processing, for example, Gregory (1980) and his theory of perceptual hypothesis. "
"Many consider skimming and scanning to be techniques related to searching as opposed to strategies for reading, for example, Just & Carpenter (1987). In fact, they are both correct but reading and searching are two different contexts. "
"The mental spotlight is quite a helpful analogy to describe a task, such as searching for a keyword, etc. Masson (1983) describes skimming “for most of us, rapid reading involves some form of skimming in which we try to focus on information relevant to our goal and skip over irrelevant information”. "
 Holmqvist et al. (2011)suggests that a “sequence of long saccades is likely to reflect skimming over the text”. 
"As Rayner (2009, pp. 1484) points out, equivalents between “visual search and scene perception are greater than with reading, in that “visual saliency” plays a greater role in directing fixations”."
"In each case, different ocular behavior would be expected Rayner (2009, pp. 1484). Many methodologies and algorithms have been devised for the detection of reading, firstly for a baseline, then secondly, comparing those results to other data to detect skimming, scanning or both, for example Campbell & Maglio (2001), Buscher, Dengel, van Elst, & Mittag (2008), and Buscher, Dengel, & van Elst (2008). "
" In Watt (2009, p. 171, chap. 8) he opines, genres behave as “affordances” and in essence can be filtered and categorized by form."
"Gibson’s affordances are intended to describe how meaning and perception are inter-related: he argues in Gibson (1986, p. 127) that instead of perceiving objects (for example, texts) and then adding meaning later, there are visual combinations of invariant and distinctive characteristics of objects which provide cues on how to act and behave in relation to these objects (in this case textual e-mails)."
"Alternatively, Toms & Campbell (1999b), in their study, leaned towards the Constructivist (perception for recognition) process, since they aimed to contrast the content (function) and form in order to discover whether readers can perceive and process form on its own or need semantic content to identify it. "
"Although the research carried out by Toms & Campbell (1999a, 1999b), Toms, Campbell, & Blades (1999), Toms (2001), and then Watt (2009, chap. 8), seems to indicate a leaning towards one process or another (Watt Ecological and Toms Constructivist) the latter does explore Ecological in her thesis (Toms, 1997), it may emerge that they are both correct (or indeed wrong), but for different information searching tasks and in different contexts."
"We conducted an analysis of the eyetracking data studying such basic metrics based on fixations, saccades and number of genres identified correctly along with length of time to identify, c.f. Clark et al. (2010)."
 Goldberg & Kotval (1999)conducted computer interface evaluations with twelve participants testing the interfaces whilst analyzing their scanpath behavior. 
"Lorigo et al. (2006), in an extension of the work in Pan et al. (2004), used scanpath fixations pattern-finding to compare the differences in gender and task type during a web search. They found differences in scanpaths according to gender, and the task comparison results although mixed, did not reveal any effects related to task type on scanpaths. "
"Joachims, Granka, Pan, Hembrooke, & Gay (2005) used scanpath measurements to examine the reliability of implicit feedback generated from click through data in Web searches"
 Brandt & Stark (1997) showed their participants visual imagery of irregularly-chequered diagrams.
The experimental setup of the evaluation was based on commonly used standards c.f. Joachims et al. (2007) and Kelly (2009). 
"The experimental procedures, such as questionnaire design, were based on methods and protocol used by previous interactive experiments (Dupont et al., 2010; Harper & Kelly, 2006; Huang et al., 2006; Kelly et al., 2007; Kelly, Harper, & Landau, 2008; White, Ruthven, & Jose, 2002; White et al., 2006). "
The experimental eyetracking data was input into the SPSS software along with the data used in Clark et al. (2010) and then statistically evaluated.
"The use of skimming and scanning techniques was detected by referring to the 20 possible permutations found in Campbell & Maglio (2001, p. 3)and Buscher, Dengel, van Elst, and Mittag (2008) scoring was based on the short, medium or long movements, which were given a particular score whenever they occurred on the X or Y axes gaze point."
" Just like Watt (2009, chap. 8) – in his timed response design – we balanced for length and still found a very strong effect (an interaction – between layout representations) which indicated that genre speed was a factor independent of length, as in Clark, Ruthven, & Holt (2009b) and Clark et al. (2010)."
"The scanpath duration measure is used to see how much time participants spend on processing information and “complexity” Goldberg & Kotval (1999, p. 638); a longer scanpath duration indicates participants are spending more time processing information and hence classifying information is far more ‘intensive’. "
"We intend to continue our research by looking at other genres on other web communities of practice, notably Wikipedia, to expand on previous work in Clark et al. (2009a) and Clark et al. (2012) and using, in addition, web data collected from two university intranets. "
"When I was looking for a Ph.D. dissertation topic, I accidentally came
across a paper by W. Larimore on Statistical Inference on Random Fields
in the Proceedings of the IEEE [6]."
"A deterministic alternative known as the iterated
condition mode was presented in the paper by Besag [3] on statistical
analysis of dirty pictures that appeared in the Journal of Royal
Statistical Society."
"The vision of Cloud Computing is to provide computing power as a utility, like gas, electricity or water [1]"
"This work can be integrated into the Foundations of Self-governing ICT Infrastructure (FoSII) projectÂ [2], but is on its own completely self-sufficient."
" Besides the already implemented LoM2HiS frameworkÂ [3] that takes care of monitoring the state of the Cloud infrastructure and its applications, the knowledge management (KM) system presented in this article can be viewed as another building block of the FoSII infrastructure."
"[4] proposes an approach to manage Cloud infrastructures by means of Autonomic Computing, which in a control loop monitors (M) Cloud parameters, analyzes (A) them, plans (P) actions and executes (E) them; the full cycle is known as MAPEÂ [5]"
According toÂ [6] a MAPE-K loop stores knowledge (K) required for decision-making in a knowledge base (KB) that is accessed by the individual phases. 
"On the other hand, we gathered real world data from monitoring scientific workflow applications in the field of bioinformaticsÂ [9]"
"These workflows need a huge, yet unpredictable and varying amount of resources, and are thusâdue to the needed flexibility and scalabilityâa perfect match for a Cloud computing applicationÂ [10]."
" [17,18] focus on VM migration andÂ [19] on turning on and off physical machines, whereas our paper focuses on VM re-configuration."
"Stillwell etÂ al.Â [22] in a similar setting define the resource allocation problem for static workloads, present the optimal solution for small instances and evaluate heuristics by simulations."
"Nathani etÂ al.Â [23], e.g.,Â also deal with VM placement on PMs using scheduling techniques. "
"[24] react to changing workload demands by starting new VM instances; taking into account VM startup time, they use prediction models to have VMs available already before the peak occurs."
"Other works such asÂ [25] have already considered the last escalation level (see SectionÂ 4), i.e.,Â outsourcing of applications to other Clouds. "
Paschke and BichlerÂ [26] look into a rule based approach in combination with the logical formalism ContractLog.
"Bahati and BauerÂ [28] also use policies, i.e.,Â rules, to achieve autonomic management. "
"Fourthly, compared to other SLA management projects like SLA@SOIÂ [32], the FoSII project in general is more specific on Cloud Computing aspects like deployment, monitoring of resources and their translation into high level SLAs instead of just working on high-level SLAs in general service-oriented architectures."
The idea of bandwidth sharing is a common idea in network systems as described inÂ [36]. 
" The problem stemming from escalation level 3 alone can be formulated into a binary integer problem (BIP), which is known to be NP-completeÂ [37]."
"The proof is out of scope for this paper, but a similar approach can be seen inÂ [12]. "
" Finally, the last escalation level 5 tries to outsource the application to another Cloud provider as explained, e.g.,Â in the Reservoir projectÂ [38]. "
Case Based ReasoningCase Based Reasoning is the process of solving problems based on past experienceÂ [39].
Following the principle of semantic similarity fromÂ [40] for the summation part this leads to the following equation (3)
The rules have been implemented using the Java rule engine DroolsÂ [41]. 
" Then, an up- or down-trend is randomly drawn, as well as a duration of this trend between a pre-defined number of iterations (for our evaluation this interval of iterations equals [2,6])"
"Applying and evaluating a bioinformatics workflow to the rule-based approachAs detailed inÂ [43,44], bioinformatics workflows have gained a great need for large-scale data analysis."
"Thus, Cloud computing infrastructures offer a promising way to host these sorts of applicationsÂ [10]."
The monitoring data presented in this Section was gathered with the help of the Cloud monitoring framework Lom2HisÂ [3].
"Using Lom2His we measured utilized resources of TopHatÂ [45], a typical bioinformatics workflow application analyzing RNA-Seq dataÂ [46], for a duration of about three hoursÂ [9]."
"The aligner presented here, TopHatÂ [45], consists of many sub-tasks, some of them have to be executed sequentially, whereas others can run in parallel (Fig.Â 10)."
The first sub-task aligns input reads to the given genome using the Bowtie programÂ [47].
"Normally, when setting up such a testbed as described in [9], an initial guess of possible resource consumption is done based on early monitoring data. "
"For example, in the studies on the automated segmentation from magnetic resonance images [19 21], the number of training examples is very huge (up to millions), the classes are strongly imbalanced, and generating accurate statistical solution is not trivial"
"In addition, data imbalance in huge data sets is also reported in other applicative domains, such as marketing data [22], oil spill detection or land cover changes from remote sensing images [16,27], text classification [18]and scene classification [35]"
Many classification algorithms present great limitations on large data sets and show a performance degradation due to class imbalance [14]
"Moreover, SVM classification performance can be hindered by class imbalance [1,30]"
"In fact, it is prone to generate classifier that has a strong estimation bias toward the majority class: since the number of majority class patterns exceeds that of the minority class, the class boundary becomes vulnerable to be distorted [15]"
"Nevertheless, these limitations are common to many other classification schemes such as Multi-Layer Perceptron (MLP) [7] and Logistic Regression (LR) [23]."
"Several methods to select examples in a classification problem are presented in literature, using two different approaches: the example-selection method can be embedded within the learning algorithm or the examples can be filtered before passing them to the classification scheme [2,26]"
"It is worth noting that the first type of selection methods generally work by preserving the original ratio between classes [6,11]: if there is a great skew in the data, it continues to be"
"A first approach consists of modifying SVM algorithm in order to make faster the training on large data sets; for example, Sequential Minimal Optimization (SMO) breaks the large QP problem into a series of smallest possible QP problems [25], allowing SMO to handle large training sets [25]"
"This can be considered as a good performance in terms of simple accuracy, but this is of no use since the classifier does not catch any important information on the patterns of the minority class [12]"
"Intuitively, precision is a measure of exactness (ie, of the examples predicted as positive, how many are actually labelled correctly), whereas recall is a measure of completeness (ie, how many examples of the positive class were labelled correctly) [12]"
The precision of a test is very useful to clinicians since it answers the question: How likely is it that this patient has the disease given that the test result is positive? [17]
"These metrics are simple and useful summary measures of overlapping between actual and predicted labels, which are interestingly applied to studies of reproducibility and accuracy in medical image segmentation [36]"
 Fewer values requiring interpolation also means it becomes feasible to inspect the smoothness of these values with respect to parameter variation and to apply adaptive interpolation schemes [37].
"Today, modern services require combinations of NFs, known as service chains, to satisfy their QoS requirements ( Quinn and Nadeau, 2015)"
"For instance, Amazon offers services that allow tenants to build their own virtual infrastructure by combining functions such as filtering, routing, slicing, and load balancing ( Amazon,2016)"
"In such an environment, even state of the art frameworks such as ClickOS ( Martins et al., 2014) and NetVM ( Hwang et al., 2014) cannot achieve high-performance, as there is a substantial throughput degradation when interconnecting multiple NFs"
"Recent efforts, such as E2 ( Palkar et al.,2015) and OpenNetVM ( Zhang et al.,2016), overcome this problem by eliminating hypervisor and paravirtualization overheads via lightweight NFs (eg, placed in containers) interconnected with fast, custom software switches."
"Although techniques such as single root I/O virtualization (SR-IOV) can bypass the hypervisor and pass packets from the NICs to the virtual machines (VMs) ( Amazon,2016), cloud applications still use costly system calls to interact with the NICs"
"In the first column of Table 1, we state the comparisons we made throughout this paper among ( i) standalone NFs that use different network drivers in user or kernel-space and ( ii) chained user-space NFs, interconnected either with a kernel-based Open vSwitch (OVSK) ( Open vSwitch, 2016) software switch or back-to-back (B2B)"
"Earlier efforts have successfully applied similar techniques ( Rizzo, 2012; Kim et al.,2012;DPDK,2016) to amortize the system calls overhead."
"As a future work, we aim to further improve the I/O performance of SCC by integrating the asynchronous, zero-copy I/O proposed by Drepper ( Drepper,2006) into FastClick"
