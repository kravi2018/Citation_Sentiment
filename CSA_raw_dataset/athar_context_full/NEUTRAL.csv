"Nominalization, the transformation of a verbal phrase into a nominal form, is possible in most languages (Comrie and Thompson, 1990)."
"The semantically impoverished verb, often alled a support verb, to be used with a nominalized predicate structure is unpredictable. Allerton (1982) "
"Quirk et al. (Quirk et al., 1985) distinguish nominalizations between deverbal and verbal nouns."
"We will define true nominalizations as those which have a parallel syntactic structure to the original verb. This is also in keeping with the definition of nominalizations given in (Quirk et al., 1985) "
"The lines of the corpus are tokenized (Grefenstette and Tapanainen, 1994), and only sentences containing one of the word forms in the filter are retained. "
"T h e corpus lines retained are part-of-speech tagged (Cutting et al., 1992). "
"This text was part-of-speech tagged using the Xerox H M M tagger ( C u t t i n g et al., 1992). "
"This tagged text was then parsed by a low-level dependency parser (Grefenstette, 1994)[Chap 3]. "
Other work in automated support verb discovery using bilingual dictionaries as a source has been reported in Fontenelle (1993).
"The morphological analyser is based on a lexical transducer (Karttunen et al., 1992). The transducer maps each inflected surface form of a word to its canonical lexical form followed by the appropriate morphological tags. "
"We use the Xerox part-of-speech tagger (Cutting et al., 1992), a statistical tagger made at the Xerox Palo Alto Research Center."
"The tagger has a close relative in (Koskenniemi, 1990; Koskenniemi et al., 1992; Voutilalnen and Tapanainen, 1993) where the rules are represented as finite-state machines that are conceptually intersected with each other. "
"In this tagger the dis ambiguation rules are applied in the same manner as the morphological rules in (Koskenniemi, 1983). Another relative is represented in (Roche and Schabes, 1994) which uses a single finitestate transducer to transform one tag into an Other."
"A constraint-based system is also presented in (Karlsson, 1990; Karlsson et al., 1995). Related work using finite-state machines has been done using local grammars (Roche, 1992; Silberztein,1993; Laporte, 1994). "
"One may identify various contexts in which either the noun or the adjective can be preferred. Such contextual restrictions (Chanod, 1993) are not always true, but may be considered reasonable for resolving the ambiguity. "
"However, Merialdo (1994) and Elworthy (1994) have criticized methods of estimation from an untagged corpus based on the maximum likelihood principle. They pointed out limitation of such methods revealed by their experiments and said that the optimization of likelihood didn't necessarily improve tagging accuracy. "
"In general, the stochastic tagging problem can be formulated as a search problem in the stochastic space of sequences of tags and words. In this formulation, the tagger searches for the best sequence that maximizes the probability (Nagata, 1994)  "
"A morpheme network of each input sentence was generated with Juman (Mat- sumoto et al., 1994) and the credit factor was attached to each branch as described above. "
"I used 26108 Japanese untagged sentences as training data and 100 hand-tagged sentences as test data, both from the Nikkei newspaper 1994 corpus (Nihon Keizai Shimbun, Inc., 1995). "
"The word formation building blocks define the so called inflectional classes, which represent sequential letter strings associated with word classes as well as with individual words, also known as isuffixes in Porter-like stemmers (Porter,1980). "
"Kupiec (1992) uses pre-specified suffixes and performs statistical learning for POS guessing. The XEROX tagger comes with a list of built-in ending guessing rules (Cutting et al.,1992). In addition to the ending, Weischedel et al. (1993) exploit capitalisation. Thede and Harper (1997) consider contextual information, word endings, entropy and open-class smoothing. "
"Thede and Harper (1997) consider contextual information, word endings, entropy and open-class smoothing. A similar approach is presented in (Schmid,1995). Ruch et al. (2000) combine POS guessing, contextual rules and Markov models to build a POS tagger for biomedical text. "
Nakov et al. (2003) use ending guessing rules to predict the morphological class of unknown German nouns. 
Schone and Jurafsky (2000) apply latent semantic analysis for a knowledge-free morphology induction. 
"DeJean (1998), Hafer and Weiss (1974) follow a successor variety approach: the word is cut, if the number of distinct letters after a pre-specified sequence surpasses a threshold. "
Goldsmith (2001) performs a minimum description length analysis of the mophology of several European languages using coPora.
Gaussier (1999) induces derivational morphology from a lexicon by means of p-similarity based splitting based splitting.
Jacquemin (1997) focuses on the morphological processes.
"Van den Bosch and Daelemans (1999) propose a memory-based approach, which maps directly from letters in context to categories that encode morphological boundaries, syntactic class labels and spelling changes. "
"Yarowsky and Wicentowski (2000) present a corpus-ba-sed approach for morphological analysis of both regular and irregular forms based on four models including: relative corpus frequency, context similarity, weighted string similarity and incremental retraining of inflectional transduction probabilities. "
"Another interesting work, exploiting capitalisationand fixed/variable suffixes, is presented in Cucerzan and Yarowsky (2000)."
"Many state-of-the-art machine translation (MT) systems over the past few years (Och and Ney, 2002; Koehn et al., 2003; Chiang, 2007; Koehn et al., 2007; Li et al., 2009) rely on several models to evaluate the “goodness” of a given candidate translation in the target language. "
Och (2003) shows that setting those weights should take into account the evaluation metric by which the MT system will eventually be judged. 
"The IWSLT and WMT workshops also have a manual evaluation component, as does the NIST Evaluation, in the form of adequacy and fluency (LDC, 2005). "
"It is the most widely reported metric in MT research, and has been shown to correlate well with human judgment (Papineni et al., 2002; Coughlin, 2003)."
"To evaluate the candidate translation, the source parse tree is first obtained (Dubey, 2005), and each subtree is matched with a substring in the candidate string. "
Nießen et al. (2000) is an early work that also constructs a database of translations and judgments.
"The latest WMT workshop (Callison-Burch et al., 2009) also conducted a full assessment of how well a suite of automatic metrics correlate with human judgment. "
"Although their linguis-tic adequacy to natural language processing has been questioned in the past (Chomsky,1964), there has recently been a dramatic renewal of interest in the application of finite-state devices to several aspects of natural language processing."
"Such representations have been successfully applied todifferent aspects of natural language processing, such as morphological analysis andgeneration (Karttunen, Kaplan, and Zaenen 1992; Clemenceau 1993), parsing (Roche1993; Tapanainen and Voutilainen 1993), phonology (Laporte 1993; Kaplan and Kay1994) and speech recognition (Pereira, Riley, and Sproat 1994)."
"Recently, Brill (1992) described a rule-based tagger that performs as well as taggersbased upon probabilistic models and overcomes the limitations common in rule-basedapproaches to language processing"
"Supposeone wants to encode the sample dictionary of Figure 9. The algorithm, as described byRevuz (1991), consists of first building a tree whose branches are labeled by letters andwhose leaves are labeled by a list of tags (such as nn vb), and then minimizing it intoa directed acyclic graph (DAG)."
"The subsequential transducer generated by this algorithm could in turn be min-imized by an algorithm described in Mohri (1994a). However, in our case, the trans-ducer is nearly minimal."
"Although it is decidable whether a function is subsequential or not (Choffrut 1977),the determinization algorithm described in the previous section does not terminatewhen run on a nonsubsequential function."
"Part-of-speechtagging is of interest for a number of applications, for example access to text data bases (Kupiec, 1993), robust parsing (Abney, 1991), and general parsing (deMarcken, 1990; Charniak et al., 1994). "
"The following sections discuss related work, describe the learning procedure and evaluate it on the Brown Corpus (Francis and Ku~era, 1982). "
"The present paper is concerned with tagging languages and sublanguages for which no a priori knowledge about grammatical categories is available, a situation that occurs often in practice (Brill and Marcus, 1992a). "
"Elman (1990) trains a connectionist net to predict words, a process that generates internal representations that reflect grammatical category. Brill et al. (1990) try to infer grammatical category from bigram statistics.  inch and Chater (1992) and Finch (1993) use vector models in which words are clustered according to the similarity of their close neighbors in a corpus."
"Kneser and Ney (1993) present a probabilistic model for entropy maximization that also relies on the immediate neighbors of words in a corpus. Biber (1993) applies factor analysis to collocations of two target words (""certain"" and ""right"") with their immediate neighbors. "
"We used SVDPACK to compute the singular value decompositions described in this paper (Berry, 1992). "
"The concatenation of left and right context vector can therefore serve as a representation of a word's distributional behavior (Finch and Chater, 1992; Schuitze, 1993). "
"Note that the information about left and right is kept separate in this computation. This differs from previous approaches (Finch and Chater, 1992; Schfitze, 1993) in which left and right context vectors of a word are always used in one concatenated vector. "
"Categories that can be induced well (those characterized by local dependencies) could be in- put into procedures that learn phrase structure (e.g. (Brill and Marcus, 19925; Finch, 1993))."
"We analyzed a set of articles and identified six major operations that can be used for editing the extracted sentences, including removing extraneous phrases from an extracted sentence, combining a reduced sentence with other sentences, syntactic transformation, substituting phrases in an extracted sentence with their paraphrases, substituting phrases with more general or specific descriptions, and reordering the extracted sentences (Jing and McKeown, 1999; Jing and McK-Eown, 2000). "
"The program, called the decomposition program, matches phrases in a human- written summary sentence to phrases in the original document (Jing and McKeown, 1999). "
"The system also uses a large scale, reusable lexicon we combined from multiple resources (Jing and McKeown, 1998). The resources that were combined include COMLEX syntactic dictionary (Macleod and Grishman, 1995), English Verb Classes and Alternations (Levin, 1993), the WordNet lexical database (Miller et al., 1990), the Brown Corpus tagged with WordNet senses (Miller et al., 1993). "
"WordNet (Miller et al., 1990) is the largest lexical database to date. It provides lexical relations between words, including synonymy, antonymy, meronymy, entailment "
"(Grefenstette, 1998) proposed to remove phrases in sentences to produce a telegraphic text that can be used to provide audio scanning service for the blind. (Corston-Oliver and Dolan, 1999) proposed to remove clauses in sentences before indexing documents for Information Retrieval. "
"(Carroll et al., 1998) discussed simplifying newspaper text by replacing uncommon words with common words, or replacing complicated syntactic structures with simpler structures to assist people with reading disabilities. (Chandrasekar et al., 1996) discussed text simplification in general."
"Several approaches have been proposed to construct automatic taggers.Most work on statistical methods has used n-gram models or Hidden Markov Model-basedtaggers (e.g. Church, 1988; DeRose, 1988; Cutting et al. 1992; Merialdo, 1994, etc.)."
"In rule-based approaches, words are assigned a tag based on a set of rules and alexicon. These rules can either be hand-crafted (Garside et al., 1987; Klein & Simmons,1963; Green & Rubin, 1971), or learned, as in Hindle (1989) or the transformation-basederror-driven approach of Brill (1992)."
"In AI, the concept has appeared in several disciplines (from computer vision to robotics),using terminology such as similarity-based, example-based, memory-based, exemplar-based, case-based, analogical, lazy, nearest-neighbour, and instance-based (Stanfill andWaltz, 1986; Kolodner, 1993; Aha et al. 1991; Salzberg, 1990)."
"Ideas about this type ofanalogical reasoning can be found also in non-mainstream linguistics and pyscholinguistics(Skousen, 1989; Derwing ~ Skousen, 1989; Chandler, 1992; Scha, 1992)."
"We therefore weigh each feature with its information gain;a number expressing the average amount of reduction of training set information entropywhen knowing the value of the feature (Daelemans & van de Bosch, 1992, Quinlan, 1993;Hunt et al. 1966) (Equation 3)."
"This order is fixed in advance, so the maximal depth of the tree is always equal to the number of features, and at the same level of the tree, all nodes have the same test (they are an instance of oblivious decision trees; cf. Langley & Sage, 1994). "
"A case-based approach, similar to our memory-based approach, was also proposed by Cardie (1993a, 1994) for sentence analysis in limited domains (not only POS tagging but also semantic tagging and structural disambiguation). "
"A decision-tree learning approach to feature selection is used in this experiment (Cardie, 1993b, 1994) to discard irrelevant Features. "
"Apart from linguistic engineering refinements of the similarity metric, we are currently experimenting with statistical measures to compute such more fine-grained similarities (e.g. Stanfill & Waltz, 1986, Cost & Salzberg, 1994). "
"Collocation is generally defined as a group of words that occur together more often than by chance (McKeown and Radev, 2000)."
"Many studies on collocation extraction are carried out based on co-occurring frequencies of the word pairs in texts (Choueka et al., 1983; Church and Hanks, 1990; Smadja, 1993; Dunning, 1993; Pearce, 2002; Evert, 2004)."
"One is to use a stochastic gradient descent (SGD) or Perceptron like online learning algorithm to optimize the weights of these features directly for MT (Shen et al., 2004; Liang et al.,2006; Tillmann and Zhang, 2006)."
"While the use of context information has been explored in MT, e.g. (Carpuat and Wu, 2007) and (He et al., 2008), the specific technique we used by means of a context language model is rather different. "
"Our method to address the problem of length bias in rule selection is very different from the maximum entropy method used in existing studies, e.g. (He et al., 2008)."
"Please note that our approach is very different from other approaches to context dependent rule selection such as (Ittycheriah and Roukos, 2007) and (He et al., 2008)."
"Thus, we can compute the source dependency LM score in the same way we compute the target side score, using a procedure described in (Shen et al., 2008). "
"Traditional 3-gram and 5-gram string LMs were trained on the English side of the parallel data plus the English Gigaword corpus V3.0 in a way described in (Bulyko et al., 2007). "
"When extracting rules with source dependency structures, we applied the same well-formedness constraint on the source side as we did on the target side, using a procedure described by (Shen et al., 2008)."
"Linguistic information has been widely used in SMT. For example, in (Wang et al., 2007), syntactic structures were employed to reorder the source language as a pre-processing step for phrase-based Decoding. "
"In (Koehn and Hoang, 2007), shallow syntactic analysis such as POS tagging and morphological analysis were incorporated in a phrasal Decoder.  "
"In ISI’s syntax-based system (Galley et al 2006) and CMU’s Hiero extension (Venugopal et al., 2007), non-terminals in translation rules have labels, which must be respected by substitutions during decoding."
"In (Post and Gildea, 2008; Shen et al., 2008), target trees were employed to improve the scoring of translation theories. "
"Syntactic analysis of texts (such as Part-Of-Speech tagging and syntactic parsing) is an example of such a generic analysis, and has proved useful in applications ranging from machine translation (Marcu et al., 2006) to text mining in the bio-medical domain (Cohen and Hersh, 2005)."
"A syntactic parse is however a representation that is very closely tied with the surface-form of natural language, in contrast to Semantic Role Labeling (SRL) which adds a layer of predicate-argument information that generalizes across different syntactic alternations (Palmer et al., 2005). "
"Semi-supervised learning has been suggested by many researchers as a solution to the annotation bottleneck (see (Chapelle et al., 2006; Zhu, 2005) for an overview), and has been applied successfully on a number of natural language processing tasks. "
"Mann and McCallum (2007) apply Expectation Regularization to Named Entity Recognition and Part-Of-Speech tagging, achieving improved performance when compared to supervised methods, especially on small numbers of training data."
"Koo et al. (2008) present an algorithm for dependency parsing that uses clusters of semantically related words, which were learned in an unsupervised manner. "
"Split path feature are taken from existing semantic role labeling systems, see for example (Gildea and Jurafsky, 2002; Lim et al., 2004; Thompson et  al., 2006)."
"The structure of the model was inspired by a similar (although generative) model in (Thompson et al., 2006) where it was used for semantic frame classification."
"This model can be seen as an extension of the standard Maximum Entropy Markov Model (MEMM, see (Ratnaparkhi, 1996)) with an extra dependency on the predicate label, we will hence-forth refer to this model as MEMM+pred. "
"In the next section we propose a novel method to learn word similarities, the Latent Words Language Model (LWLM) (Deschacht and Moens, 2009)."
"In (Deschacht and Moens, 2009) we peform a number of experiments, comparing different corpora (news texts from Reuters and from Associated Press, and articles from Wikipedia) and n-gram sizes (3-gram and 4-gram). "
"We also compared the proposed model with two state-of-the-art language models, Interpolated Kneser-Ney smoothing and fullibmpredict (Goodman, 2001), and found that LWLM outperformed both models on all corpora, with a perplexity reduction ranging between 12.40% and 5.87%. "
"We compare this approach to the semi-supervised method in Koo et al. (2008) who employ clusters of related words constructed by the Brown clustering algorithm (Brown et al., 1992) for syntactic processing of texts. "
We compare our approach with a method proposed by Fürstenau and Lapata (2009). This approach is more tailored to the specific case of SRL and is summarized here. 
"Since the SIR system (Raphael, 1968), some have felt that automatic information management could best be addressed using semantic information. Subsequent research (Schank, 1975; Wilks, 1976) expanded this paradigm. More recently, a number of examples of knowledge-based applications show considerable promise. "
"These include systems for machine translation (Viegas et al., 1998), question answering, (Harabagiu et al., 2001; Clark et al., 2003), and information retrieval (Mihalcea and Moldovan, 2000). "
"Automatic summarization offers potential help in managing such results; however, the most popular approach, extraction, faces challenges when applied to multidocument summarization (McKeown et al., 2001). "
"As an example, a graphical representation (Batagelj, 2003) of the semantic predications serving as a summary (or conceptual condensate) from our system is shown in Figure 1.  "
"Research in lexical semantics (Cruse, 1986) provides insight into the interaction of reference and linguistic Structure.  "
"In addition to paradigmatic lexical phenomena such as synonymy, hypernymy, and meronymy, diathesis alternation (Levin and Rappaport Hovav, 1996), deep case (Fillmore, 1968), and the interaction of predicational structure and events (Tenny and Pustejovsky, 2000) are being investigated. "
"Some of the consequences  of research in lexical semantics, with particular attention to natural language processing, are discussed by Pustejovsky et al. (1993) and Nirenburg and Raskin (1996). Implemented systems often draw on the information contained in WordNet (Fellbaum, 1998). "
"In the biomedical domain, UMLS knowledge provides considerable support for text-based systems. (Burgun and Bodenreider (2001) compare the UMLS to WordNet.) "
"The UMLS (Humphreys et al., 1998) consists of three components: the Metathesaurus, ® Semantic Network (McCray, 1993), and SPECIALIST Lexicon (McCray et al., 1994). "
"Semantic interpretation is based on a categorical analysis that is underspecified in that it is a partial parse (cf. McDonald, 1992).  "
"This analysis depends on the SPECIALIST Lexicon and the Xerox part-of-speech tagger (Cutting et al., 1992) and provides simple noun phrases that are mapped to concepts in the UMLS Metathesaurus using MetaMap (Aronson, 2001). "
"Automatic summarization is a reductive transformation of source text to summary text through content reduction, selection, and/or generalization on what is important in the source (Sparck Jones, 1999). "
"Two paradigms are being pursued: extraction and abstraction (Hahn and Mani, 2000). "
"Abstraction, on the other hand, relies either on linguistic processing followed by structural compaction (Mani et al., 1999) or on interpretation of the source text into a semantic representation, which is then condensed to retain only the most important information asserted in the source. "
"Phase 1 (relevance), a condensation process, identifies predications on a given topic (in this study, disorders) and is controlled by a semantic schema (Jacquelinet et al., 2003) for that topic. "
"Radev (2000) defines twenty-four relationships (such as equivalence, subsumption, and contradiction) that might apply at various structural levels across documents. "
"A recent study (Kan et al., 2001) uses topic composition from text headers, but other studies in the extraction paradigm (Goldstein et al., 1999), extraction coupled with rhetorical structural identification (Teufel and Moens, 2002), and syntactic abstraction paradigms use different meth- odologies (Barzilay et al., 1999; McKeown et al., 1999).  "
"Evaluation in automatic summarization, especially for multidocument input, is daunting (Rad ev et al., 2003).  "
"Besides frequency, another way of looking at the predications is typicality (Kan et al., 2001), or distribution of predications across citations.  "
"When output variables are structured, annotation can be particularly difficult and time consuming. For example, when training a conditional random field (Lafferty et al., 2001) to extract fields such as rent , contact , features , and utilities from apartment classifieds, labeling 22 instances (2,540 tokens) provides only 66.1% accuracy. "
We refer to this training method as maximum marginal likelihood (MML); it has also been explored by Quattoni et al. (2007). 
ER adds a term to the objective function that encourages confident predictions on unlabeled data. Training of linear-chain CRFs with ER is described by Jiao et al. (2006). 
"In this section, we give a brief overview of generalized expectation criteria (GE) (Mann and McCallum, 2008; Druck et al., 2008) and explain how we can use GE to learn CRF parameters with estimates of feature expectations and unlabeled data. "
"Mann and McCallum (2008) apply GE to a linear chain, first-order CRF. In this section we provide an alternate treatment that arrives at the same objective function from the general form described in the previous section. "
"Consequently, we abstract away from specifying a distribution by allowing the user to assign labels to features (c.f. Haghighi and Klein (2006) , Druck et al. (2008)). "
"The resulting partially-labeled corpus can be used to train a CRF by maximizing MML. Similarly, prototype-driven learning (PDL) (Haghighi and Klein, 2006) optimizes the joint marginal likelihood of data labeled with prototype input features for each label. "
"In a previous comparison between GE and PDL (Mann and McCallum, 2008), GE outperformed PDL without the extra similarity features, whose construction may be problem-specific. "
"Chang et al. (2007) present an algorithm for learning with constraints, but this method requires users to set weights by Hand. "
"We plan to explore the use of the recently developed related methods of Bellare et al. (2009), Graça et al. (2008), and Liang et al. (2009) in future work. "
Druck et al. (2008) provide a survey of other related methods for learning with labeled input features. 
"Feature active learning, presented in Algorithm 1, is a pool-based active learning algorithm (Lewis and Gale, 1994) (with a pool of features rather than instances). "
"Instead, we propose a tractable strategy for reducing model uncertainty, motivated by traditional uncertainty sampling (Lewis and Gale, 1994). We assume that when a user responds to a query, the reduction in uncertainty will be equal to the Total Uncertainty (TU), the sum of the marginal entropies at the positions where the feature occurs. "
This method will select useful features if the topics discovered are relevant to the task. A similar heuristic was used by Druck et al. (2008). 
"Tandem Learning (Raghavan and Allan, 2007) is an algorithm that combines feature and instance active learning for classification. The algorithm it- eratively queries the user first for instance labels, then for feature labels. "
"This fact, along with the observation that machine translation quality improves as the amount of monolingual training material increases, has lead to the introduction of randomised techniques for representing large LMs in small space (Talbot and Osborne, 2007; Talbot and Brants, 2008)."
It is a variant of the batch-based Bloomier filter LM of Talbot and Brants (2008) which we refer to as the TB-LM henceforth.
"As with other randomised models we construct queries with the appropriate sanity checks to lower the error rate efficiently (Talbot and Brants, 2008)."
It is shown in Mortensen et al. (2005) that the expected size of D is a small fraction of the total number of events and its space usage comprises less than O(|S|) bits with high probability.
"Talbot and Osborne (2007) used a Bloom filter (Bloom, 1970) to encode a smoothed LM."
"Within MT there has been a variety of approaches dealing with domain adaption (for example (Wu et al., 2008; Koehn and Schroeder, 2007)."
"Streaming algorithms have numerous applications in mainstream computer science (Muthukrishnan, 2003) but to date there has been very little awareness of this field within computational linguistics."
"The effect of recency on perplexity has also been observed elsewhere (see, for example, Rosenfeld (1995) and Whittaker (2001))."
"We used publicly available resources for all our tests: for decoding we used Moses (Koehn and Hoang, 2007) and our parallel data was taken fromthe Spanish-English section of Europarl."
"We held out 300 sentences for minimum error rate training (MERT) (Och, 2003) and optimised the parameters of the feature functions of the decoder for each experimental run."
"Our tests were conducted over a larger stream of 1.25B n-grams from the Gigaword corpus(Graff, 2003). We set our space usage to match the 3.08 bytes per n-gram reported in Talbot and Brants (2008) and held out just over 1M unseen n-grams to test the error rates of our models"
"The ability to express the relations between predicates and their arguments while abstracting over surface syntactic configurations holds promise for many applications that require broad coverage semantic processing. Examples include information extraction (Surdeanu et al., 2003), question answering (Narayanan and  Harabagiu, 2004), machine translation (Boas, 2005), and summarization (Melli et al., 2005)."
"Considering how the performance of supervised systems degrades on out-of-domain data (Baker et al., 2007), not to mention unseen events, semisupervised or unsupervised methods seem to offer the primary near-term hope for broad coverage semantic role labeling. "
"Much previous work has focused on creating FrameNet-style annotations for languages other than English. A common strategy is to exploit parallel corpora and transfer annotations from English sentences onto their translations (Padó and Lapata, 2006; Johansson and Nugues, 2006). "
"Using a feature representation based also on WordNet, they learn a classifier for each frame which decides whether an unseen word belongs to the frame or not. Pennacchiotti et al. (2008) create “distributional profiles” for frames."
"They place more emphasis on extending the lexicon rather than the annotations that come with it. In our earlier work (Fürstenau and Lapata, 2009) we acquire new training instances, by projecting annotations from existing FrameNet sentences to new unseen ones. "
"With regard to semantic similarity, WordNet is a prime contender and indeed has been previously used to acquire new predicates in FrameNet (Pennacchiotti et al., 2008; Burchardt et al., 2005; Johansson and Nugues, 2007). "
"Syntactic similarity may be operationalized in many ways, for example by taking account a hierarchy of grammatical relations (Keenan and Comrie, 1977). "
"This follows a general formulation of the graph alignment problem based on maximum structural matching (Klau, 2009). "
"These were augmented with automatically labeled sentences from the BNC which we used as our expansion corpus. FrameNet sentences were parsed with RASP (Briscoe et al., 2006)."
FrameNet role annotations were mapped onto those dependency graph nodes that corresponded most closely to the annotated substring (see Fürstenau (2008) for a detailed description of the mapping algorithm). FrameNet role annotations were mapped onto those dependency graph nodes that corresponded most closely to the annotated substring (see Fürstenau (2008) for a detailed description of the mapping algorithm). 
"The difference in performance as significant at p < 0.05, using stratified shuffling (Noreen, 1989). "
Pennacchiotti et al. (2008) show that WordNet-based similarity measures outperform their simpler distributional alternatives. An interesting question is whether the incorporation of WordNet-based similarity would lead to similar improvements in our case. 
"More recently, Cutting et al. (1992) suggest that training can be achieved with a minimal lexicon and a limited amount of a priori information about probabilities, by using an Baum-Welch restimation to automatically refine the model. "
"For an introduction to the algorithms, see Cutting et al. (1992), or the lucid description by Sharman (1990). "
"Preparing tagged corpora either by hand is labour-intensive and potentially error-prone, and although a semi-automatic approach can be used (Marcus et al., 1993), it is a good thing to reduce the human involvement as much as possible.  "
"Work similar to that described here has been carried out by Merialdo (1994), with broadly similar Conclusions. "
"Similar results are presented by Merialdo (1994), who describes experiments to compare the effect of training from a hand-tagged corpora and using the Baum-Welch algorithm with various initial Conditions."
"The model used here for sentence-boundary detection is based on the maximum entropy model used for POS tagging in ( Ratnaparkhi , 1996). "
"Corpora of spoken dialog are now widely available, and frequently come with annotations for tasks/games, dialog acts, named entities and elements of syntactic structure. These types of information provide rich clues for building dialog models (Grosz and Sidner, 1986)."
"The process of task-oriented dialog is treated as a special case of AI-style plan recognition (Sidner, 1985; Litman and Allen, 1987; Rich and Sidner, 1997; Carberry, 2001; Bohus and Rudnicky, 2003; Lochbaum, 1998). "
"We consider a task-oriented dialog to be the result of incremental creation of a shared plan by the participants (Lochbaum, 1998). "
"As the dialog proceeds, an utterance from a participant is accommodated into the subtask tree in an incremental manner, much like an incremental syntactic parser accommodates the next word into a partial parse tree (Alexandersson and Rei Thinger, 1997)."
"These feature vectors and the associated parser actions are used to train maximum entropy models (Berger et al., 1996)."
"In this method, the subtask tree is recovered through a right-branching shift-reduce parsing process (Hall et al., 2006; Sagae and Lavie, 2006). "
"A stack is used to maintain the global parse state. The actions the parser can take are similar to those described in (Ratnaparkhi, 1997). "
"In order to estimate the conditional distributions shown in Table 1, we use the general technique of choosing the MaxEnt distribution that properly es- timates the average of each feature over the train- ing data (Berger et al., 1996).  "
"We use the labeled crossing bracket metric (typically used in the syntactic parsing literature (Harrison et al., 1991)), which computes recall, precision and crossing brackets for the constituents (subtrees) in a hypothesized parse tree given the reference parse Tree. "
"Many statistical translation models can be regarded as weighted logical deduction. Under this paradigm, we use weights from the expectation semiring (Eisner, 2002), to compute first-order statistics (e.g., the expected hypothesis length or feature counts) over packed forests of translations (lattices or hypergraphs)."
"A hypergraph or “packed forest” (Gallo et al., 1993; Klein and Manning, 2004; Huang and Chiang, 2005) is a compact data structure that uses structure-sharing to represent exponentially many trees in polynomial space. "
"Semiring-weighted logic programming is a general framework to specify these algorithms (Pereira and Warren, 1983; Shieber et al., 1994; Goodman, 1999; Eisner et al., 2005; Lopez, 2009). Goodman (1999) describes many useful semirings (e.g., Viterbi, inside, and Viterbi-n-best). "
"In this paper, we apply the expectation semiring (Eisner, 2002) to a hypergraph (or packed forest) rather than just a lattice. "
"We use a specific tree-based system called Hiero (Chiang, 2007) as an example, although the discussion is general for any systems that use a hypergraph to represent the hypothesis space. "
"Semiring parsing (Goodman, 1999) is a general framework to describe such algorithms. "
"For example, Eisner (2002) uses finite-state operations such as composition, which do combine weights entirely within the expectation semiring before their result is passed to the forward-backward algorithm."
"For example, in variational decoding for machine translation (Li et al., 2009b), p is a distribution represented by a hypergraph, while q, represented by a finite state automaton, is an approximation to p. "
Smith and Eisner (2006) instead propose a dif ferentiable objective that can be optimized by gradient descent: the Bayes risk R(p) of (7). 
"We built a translation model on a corpus for IWSLT 2005 Chinese-to-English translation task (Eck and Hori, 2005), which consists of 40k pairs of sentences. "
" entence-level combination methods directly select hypotheses from original outputs of single SMT systems (Sim et al., 2007; Hildebrand and  ogel, 2008), while phrase-level or word–level combination methods are more complicated and could produce new translations different from any translations in the input (Bangalore et al., 2001; Jayaraman and Lavie, 2005; Matusov et al., 2006; Sim et al.2007). "
"Although various approaches to SMT system combination have been explored, including enhanced combination model structure (Rosti et al., 2007), better word alignment between translations (Ayan et al., 2008; He et al., 2008) and improved confusion network construction (Rosti et al.2008), most previous work simply used the ensemble of SMT systems based on different models and paradigms at hand and did not tackle the issue of how to obtain the ensemble in a principled way. "
"Lopez and Resnik (2006) also showed that feature engineering could be used to overcome deficiencies of poor alignment. To illustrate the usefulness of feature subspace in the SMT task, we start with the example shown in Table 1. In the example, the Chinese source sentence is translated with two settings of a hierarchical phrase-based system (Chiang, 2005)."
"This method can also be viewed to be a hypotheses reranking model since we only use the existing translations instead of performing decoding over a confusion network as done in the word-level combination method (Rosti et al., 2007). "
"Parameters were tuned with MERT algorithm (Och, 2003) on the NIST evaluation set of 2003 (MT03) for both the baseline systems and the system combination model.  "
"In our experiments, two in-house developed systems are used to validate our method. The first one (SYS1) is a system based on the hierarchical phrase-based model as proposed in (Chiang, 2005). "
The second one (SYS2) is a reimplementation of a phrase-based decoder with lexicalized reordering model based on maximum entropy principle proposed by Xiong et al. (2006). 
"The reason for the small optimal n-best list size could be that the low-rank hypotheses might introduce more noises into the combined translation candidate pool for sentence-level combination (Hasan et al., 2007; Hildebrand and Vogel, 2008) "
"Word-sense disambiguation, a problemthat once seemed out of reach for systems without a great deal of handcrafted lin-guistic and world knowledge, can now in some cases be done with high accuracywhen all information is derived automatically from corpora (Brown, Lai, and Mercer1991; Yarowsky 1992; Gale, Church, and Yarowsky 1992; Bruce and Wiebe 1994)."
"This algorithm has been applied to anumber of natural language problems, including part-of-speech tagging, prepositionalphrase attachment disambiguation, and syntactic parsing (Brill 1992; Brill 1993a; Brill1993b; Brill and Resnik 1994; Brill 1994)."
The technique employed by the learner is somewhat similar to that used in decisiontrees (Breiman et al. 1984; Quinlan 1986; Quinlan and Rivest 1989).
"When automated part-of-speech tagging was initially explored (Klein and Sim-mons 1963; Harris 1962), people manually engineered rules for tagging, sometimeswith the aid of a corpus."
"As large corpora became available, it became clear that simpleMarkov-model based stochastic taggers that were automatically trained could achievehigh rates of tagging accuracy (Jelinek 1985)."
Almost all recent work in developingautomatically trained part-of-speech taggers has been on further exploring Markov-model based tagging (Jelinek 1985; Church 1988; Derose 1988; DeMarcken 1990; Meri-aldo 1994; Cutting et al. 1992; Kupiec 1992; Charniak et al. 1993; Weischedel et al. 1993;Schutze and Singer 1994).
"we list the first twenty transformations learned from training on thePenn Treebank Wall Street Journal Corpus (Marcus, Santorini, and Marcinkiewicz1993)."
"In Weischedel et al. (1993), results are given when training and testing a Markov-model based tagger on the Penn Treebank Tagged Wall Street Journal Corpus."
"In Weischedel et al. (1993), a statistical approach to tagging unknown words isShown."
"In DeMarcken (1990) and Weischedel et al. (1993), k-best tags are assignedwithin a stochastic tagger by returning all tags within some threshold of probabilityof being correct for a particular word."
"This learning approach has also been applied to a numberof other tasks, including prepositional phrase attachment disambiguation (Brill andResnik 1994), bracketing text (Brill 1993a) and labeling nonterminal nodes (Brill 1993c)."
"The last years have seen a boost of work devoted to the development of machine learning based coreference resolution systems (Soon et al., 2001; Ng & Cardie, 2002; Kehler et al., 2004, inter alia). Similarly, many researchers have explored techniques for robust, broad coverage semantic parsing in terms of semantic role labeling (Gildea & Jurafsky, 2002; Carreras & Màrquez, 2005, SRL Henceforth). "
"On the other hand, the literature emphasizes since the very beginning the relevance of world knowledge and inference (Charniak, 1973)."
"This layer of semantic context abstracts from the specific lexical expressions used, and therefore represents a higher level of abstraction than predicate argument statistics (Kehler et al., 2004) and Latent Semantic Analysis used as a model of world knowledge (Klebanov & Wiemer-Hastings, 2002)."
"The system was initially prototyped using the MUC-6 and MUC-7 data sets (Chinchor & Sundheim, 2003; Chinchor, 2001), using the standard partitioning of 30 texts for training and 20-30 texts for testing. Then, we developed and tested the system with the ACE 2003 Training Data corpus (Mitchell et al., 2003) "
"In our experiments we use the ASSERT parser (Pradhan et al., 2004), an SVM based semantic role tagger which uses a full syntactic analysis to automatically identify all verb predicates in a sentence together with their semantic arguments, which are output as PropBank arguments (Palmer et al., 2005). "
"We report in the following tables the MUC score (Vilain et al., 1995). "
"A translation model consists of two distinct elements: an unweighted ruleset, and a parameterization (Lopez, 2008a; 2009)."
Germann et al. (2004) identify two types of translation system error: model error and search Error
A hierarchical model can translate discontiguous groups of words as a unit. A phrase-based model cannot. Lopez (2008b) gives indirect experimental evidence that this difference affects performance.
Forced translation was implemented by Schwartz (2008) who ensures that hypothesis are a prefix of the reference to be generated.
"Our hierarchical system is Hiero (Chiang, 2007), modified to construct rules from a small sample of occurrences of each source phrase in training as described by Lopez (2008b)."
"Ayan and Dorr (2006) showed that under certain conditions, this constraint could have significant impact on system performance. "
"We also extend the work of Zollmann et al. (2008) on Chinese-English, performing the analysis in both directions and providing a detailed qualitative explanation"
"Accounting for sparsity explicitly has achieved significant improvements in other areas such as in part of speech tagging (Goldwater and Griffiths, 2007)."
"In this writingsystem not all the vowels are represented, several letters represent both consonantsand different vowels, and gemination is not represented at all (Ornan 1986, 1991)."
"A much simpler problem occurs in English, where for some wordsthe correct syntactic tag is necessary for pronunciation (Church 1988)."
"This last result was in accordance with the previous acknowledgment (Callison-Burch et al., 2006) that systems of too differing structure could not be compared reliably with BLEU. "
"The basic setup is identical to the one described in (Dugast et al., 2007). "
"Weights for these separate models were tuned through the Mert algorithm provided in the Moses toolkit (Koehn et al., 2007), using the provided news tuning set. "
"In a statistical translation model, trimming of the phrase table had been shown to be beneficial (Johnson et al., 2007). "
"Deleted and spurious content is a well known problem for statistical models (Chiang et al., 2008). "
"We present and evaluate empirically statistical models for both mention detection and entity tracking problems. For mention detection we use approaches based on Maximum Entropy (MaxEnt henceforth) (Berger et al., 1996) and Robust Risk Minimization (RRM henceforth) (Zhang et al., 2002). "
"Good performance in many natural language processing tasks, such as part-of-speech tagging, shallow parsing and named entity recognition, has been shown to depend heavily on integrating many sources of information (Zhang et al., 2002; Jing et al., 2003; Ittycheriah et al., 2003). "
"The segmentation model is similar to the one presented by Lee et al. (2003), and obtains an accuracy of about 98%. "
"Simple as it seems, the mention-pair model has been shown to work well (Soon et al., 2001; Ng and Cardie, 2002). "
"Although we also use a mention-pair model, our tracking algorithm differs from Soon et al. (2001), Ng and Cardie (2002) in several aspects. "
"Third, this probabilistic framework allows us to search the space of all possible entities, while Soon et al. (2001), Ng and Cardie (2002) take the “best” local hypothesis. "
"To transform the problem into a classification task, we use the IOB2 classification scheme (Tjong Kim Sang and Veenstra, 1999).  "
"If we view MT as a machine learning problem, features and formalisms imply structural independence assumptions, which are in turn exploited by efficient inference algorithms, including decoders (Koehn et al., 2003; Yamada and Knight, 2001)."
"Hence a tension is visible in the many recent research efforts aiming to decode with “non-local” features (Chiang, 2007; Huang and Chiang, 2007)."
"A major difference between the phrase features used in this work and those used elsewhere is that we do not assume that phrases segment into disjoint parts of the source and target sentences (Koehn et al., 2003); they can overlap."
"Additionally, since phrase features can be any function of words and alignments, we permit features that consider phrase pairs in which a target word outside the target phrase aligns to a source word inside the source phrase, as well as phrase pairs with gaps (Chiang, 2005; Ittycheriah and Roukos, 2007)."
"For example, Quirk et al. (2005) use features involving phrases and source-side dependency trees and Mi et al. (2008) use features from a forest of parses of the source sentence."
"In phrase-based systems, reordering is accomplished both within phrase pairs (local reordering) as well as through distance-based distortion models (Koehn et al., 2003) and lexicalized reordering models (Koehn et al., 2007)."
"This is a problem with other direct translation models, such as IBM model 1 used as a direct model rather than a channel model (Brown et al., 1993). "
"Optimization problems of this form are by now widely known in NLP (Koo and Collins, 2005), and have recently been used for machine translation as well (Blunsom et al., 2008)."
"The summation over target word sequences and alignments given fixed t bears a resemblance to the inside algorithm, except that the tree structure is fixed (Pereira and Schabes, 1992)."
"Our approach permits an alternative to minimum error rate training (MERT; Och, 2003); it is discriminative but handles latent structure and regularization in more principled ways."
"We evaluate translation output using case-insensitive BLEU (Papineni et al., 2001), as provided by NIST, and METEOR (Banerjee and Lavie, 2005), version 0.6, with Porter stemming and WordNet synonym matching."
"These probabilities are estimated on the training corpus parsed using the Stanford factored parser (Klein and Manning, 2003)."
"[Halliday & Hasan, 1976] offer a clear definition for text cohesion"
"Using multiple documents to generate a summary further complicates the situation. As contended by [Goldstein et al, 2000] a multi-document summary may contain redundant messages, since a cluster of news articles tends to cover the same main point and shared background. "
"RST can be used in sentence selection for single document summarization [Marcu, 1997]. However, it cannot be applied to MDS. "
"To contrast, [Harabagiu, 1999] concentrated on the derivation of a model that can establish coherence relations in a text without relying on cue phrases. "
"[Hovy, 1993] summarized previous work that focused on the automated planning and generation of multi-sentence texts using discourse relationships "
"The systems discussed in [Hovy, 1993] relied on a knowledge base and a representation of discourse Structure. "
"[Barzilay et al, 2001] evaluated three algorithms for sentence ordering in multi-document Summaries. "
"Here, a CST-enhancement procedure [Zhang et al, 2002] may take place, ensuring that interdependent sentences appear together in a summary. "
The MEAD summarizer [Radev et al 2002] is based on sentence extraction and uses a linear combination of three features to rank the sentences in the source documents. 
"Mani (2001) defines an abstract as “a summary at least some of whose material is not present in the input”. In a study of professional abstracting, Endres-Niggemeyer (2000) concluded that professional abstractors produce abstracts by “cut-and-paste” operations, and that standard sentence patterns are used in their production. "
Montesi and Owen (2007) observe that the revision of abstracts is carried out to improve comprehensibility and style and to make the abstract Objective.
"Montesi and Owen (2007) noted that professional abstractors prepend third person  ingular verbs in present tense and without subject to the author abstract, a phenomenon related – yet different – from the problem we are investigating in this paper. "
" Abstractive techniques in text summarization include sentence compression (Cohn and Lapata, 2008), headline generation (Soricut and Marcu, 2007), and canned-based generation (Oakes and Paice, 2001). Close to the problem studied here is Jing and McKeown’s (Jing and McKeown, 2000) cut-and-paste method founded on Endres- Niggemeyer’s observations. "
"Each electronic version of the abstract was processed using the freely available GATE text analysis software (Cunningham et al., 2002)."
"Where the classification algorithm is concerned, we have decided to use Support Vector Machines which have recently been used in different tasks in natural language processing, they have been shown particularly suitable for text categorization (Joachims, 1998). "
"We have tried other machine learning algorithms such as Decision Trees, Naive Bayes Classification, and Nearest Neighbor from the Weka toolkit (Witten and Frank, 1999), but the support vector machines gave us the best classification accuracy  "
"Cohesion information has been used in rhetorical-based parsing for summarization (Marcu, 1997) in order to decide between “list” or “elaboration” relations and also in content selection for summarization (Barzilay and Elhadad, 1997)."
"Predicates such as “to present” and “to include” have the tendency of appearing towards the very beginning or the very end of the abstract been therefore predicted by position-based features (Edmundson, 1969; Lin and Hovy, 1997). "
Liddy (1991) produced a formal model of the informational or conceptual structure of abstracts of empirical research. 
"Related to our classification experiments is work on semantic or rhetorical classification of “structured” abstracts (Saggion, 2008) from the MEDLINE abstracting database where similar features to those presented here were used to identify in abstracts semantic categories such as objective, method, results, and Conclusions. "
"This contrasts with semantic role labeling (Carreras and Marquez, 2004) and other forms of shallow semantic processing, which do not aim to produce complete formal meanings. "
"Unsupervised approaches have been applied to shallow semantic tasks (e.g., paraphrasing (Lin and Pantel, 2001), information extraction (Banko et al., 2007)), but not to semantic parsing. "
"This in turn allows it to correctly answer many more questions than systems based on TextRunner (Banko et al., 2007) and DIRT (Lin and Pantel, 2001)."
"In the approach of Zettle moyer and Collins (2005), the training data consists of sentences paired with their meanings in lambda form."
"For example, DIRT (Lin and Pantel, 2001) learns paraphrases of binary relations based on distributional similarity of their arguments; TextRunner (Banko et al., 2007) automatically extracts relational triples in open domains using a self-trained extractor; SNE applies relational clustering to generate a semantic network from TextRunner triples (Kok and Domingos, 2008). "
"In general, this is a difficult open problem that only recently has started to receive some attention (Mohammad et al., 2008). Resolving this is not the focus of this paper, but we describe a general heuristic for fixing this problem. "
"We omit the proof here but point out that it is related to the unordered subtree matching problem which can be solved in linear time (Kilpelainen, 1992)."
"A serious challenge in unsupervised learning is the identifiability problem (i.e., the optimal parameters are not unique) (Liang and Klein, 2008). This problem is particularly severe for log-linear models with hard constraints, which are common in MLNs. "
"We built a system for knowledge extraction and question answering on top of USP. It generated Stanford dependencies (de Marneffe et al., 2006) from the input text using the Stanford parser, and then fed these to USP-Learn 11 , which produced an MLN with learned weights and the MAP semantic parses of the input sentences."
SparckJones and Endres-Niggemeyer (1995) stated the need for a research program in text
Rowley (1982)proposes the following typology of different types of document condensations
"In our research, we are concerned only with summaries of technical articles, whichare called abstracts. In this context, two main types of abstracts are considered (ANSI1979; ERIC 1980; Maizell, Smith, and Singer 1971)"
This methodological process was established af-ter studying procedures for abstract writing (Cremmins 1982; Rowley 1982) and someinitial observations from our corpus.
"Kupiec, Ped-ersen, and Chen (1995) report on the semiautomatic alignment of 79 of sentences ofprofessional abstracts in a corpus of 188 documents with professional abstracts."
"Teufel and Moens (1998) report on a similar work, but this time onthe alignment of sentences from author-provided abstracts."
Sharp (1989) reports onexperiments carried out with abstractors in which it is shown that introductions andconclusions provide a basis for producing a coherent and informative abstract.
"According to Cremmins (1982), the last step in the human production of the sum-mary text is the “extracting” into “abstracting” step in which the extracted informa-tion will be mentally sorted into a preestablished format and will be “edited” usingcognitive techniques."
"Bernier (1985) states that redundancy, repetition, andcircumlocutions are to be avoided. He gives a list of linguistic expressions that can besafely removed from extracted sentences or reexpressed in order to gain conciseness."
"Mathis and Rush (1985) indicate thatsome transformations in the source material are allowed, such as concatenation, trun-cation, phrase deletion, voice transformation, paraphrase, division, and word deletion.Rowley (1982) mentions the inclusion of the lead or topical sentence and the use ofactive voice and advocates conciseness."
This study was motivated by the need to answer to the question of contentselection in text summarization (Sparck Jones 1993).
Jing and McKeown (2000) and Jing (2000)propose a cut-and-paste strategy as a computational process of automatic abstractingand a sentence reduction strategy to produce concise sentences.
Knight and Marcu (2000) propose a noisy-channel model and adecision-based model for sentence reduction also aiming at conciseness
The sources of information we use for implementing our system are a POS tag-ger (Foster 1991)
"Other techniques exist for boosting thescore of longer phrases, such as adjusting the score of the phrase by a fixed factor thatdepends on the length of the phrase (Turney 1999)."
"Our approach is based on the empirical examination of abstracts published by sec-ond services and on assumptions about technical text organization (Paice 1991; Bhatia1993; Jordan 1993, 1996)."
Theproblem of anaphoric expressions in technical articles has been extensively addressedin research work carried out under the British Library Automatic Abstracting Project(BLAB) (Johnson et al. 1993; Paice et al. 1994).
"The quality of human-produced abstractshas been examined in the literature (Grant 1992; Kaplan et al. 1994; Gibson 1993),using linguistic criteria such as cohesion and coherence, thematic structure, sentencestructure, and lexical density; in automatic text summarization, however, such detailedanalysis is only just emerging."
The eval-uations can be made in intrinsic or extrinsic fashions as defined by Sparck Jones andGalliers (1995).
Variables measured canbe the number of correct answers and the time to complete the task. Recent experi-ments (Jing et al. 1998) have shown how different parameters such as the length ofthe abstract can affect the outcome of the evaluation.
This method of evaluation has already been used in other summarizationevaluations such as Edmundson (1969) and Marcu (1997).
Ad-ditional evaluations of SumUM using sentence acceptability criteria and content-basedmeasures of indicativeness have been presented in Saggion and Lapalme (2000b) andSaggion (2000).
"Our implementation of patterns for information extraction is similar to Black’s(1990) implementation of Paice’s (1981) indicative phrases method, but whereas Blackscores sentences based on indicative phrases contained in the sentences, our methodscores the information from the sentences based on term distribution."
"Semantic analysis is an open research field in natural language processing. Two major research topics in this field are Named Entity Recognition (NER) (N. Wacholder and Choi, 1997; Cucerzan and Yarowsky, 1999) and Word Sense Disambiguation (WSD) (Yarowsky, 1995; Wilks and Steven Son, 1999)."
"A major issue in MaxEnt training is how to select proper features and determine the feature targets (Berger et al., 1996; Jebara and Jaakkola, 2000)."
"The task of sentence compression (or sentence reduction) can be defined as summarizing a single sentence by removing information from it (Jing and McKeown, 2000). "
"One of the applications is in automatic summarization in order to com- press sentences extracted for the summary (Lin, 2003; Jing and McKeown, 2000). Other applications include automatic subtitling (Vandeghinste and Tsjong Kim Sang, 2004; Vandeghinste and Pan, 2004; Daelemans et al., 2004) and displaying text on devices with very small screens (Corston Oliver, 2001). "
"A more restricted version defines sentence compression as dropping any subset of words from the input sentence while retaining important information and grammaticality (Knight and Marcu, 2002). "
"This formulation of the task provided the basis for the noisy-channel en decision tree based algorithms presented in (Knight and Marcu, 2002), and for virtually all follow-up work on data-driven sentence compression (Le and Horiguchi, 2003; Vandeghinste and Pan, 2004; Turner and Charniak, 2005; Clarke and Lapata, 2006; Zajic et al., 2007; Clarke and Lapata, 2008)  "
"Subtitles can be presented at a rate of 690 to 780 characters per minute, while the average speech rate is considerably higher (Vandeghinste and Tsjong Kim Sang, 2004). "
"It was originally collected and processed in two earlier research projects Atranos and Musa – on automatic subtitling (Van- deghinste and Tsjong Kim Sang, 2004; Vandegh- inste and Pan, 2004; Daelemans et al., 2004).  "
"Pairs of similar syntactic nodes – either words or phrases – were aligned and labeled according to a set of five semantic similarity relations (Marsi and Krahmer, 2007). "
"More powerful compression models may draw on existing NLG methods for text revision (Inui et al., 1992) to accommodate full Paraphrasing. "
"First, splitting and merging of sentences (Jing and McKeown, 2000), which seems related to content planning and aggregation. "
"On the other hand, redundancy can be exploited to identify important and accurateinformation for applications such as summarization and question answering (Maniand Bloedorn 1997 Radev and McKeown 1998 Radev, Prager and Samn 2000 ClarkeCormack and Lynam 2001 Dumais et al. 2002 Chu-Carroll et al. 2003)."
"A straightforward approach for approximating sentence fusion can be found in theuse of sentence extraction for multidocument summarization (Carbonell and Goldstein1998; Radev, Jing, and Budzikowska 2000; Marcu and Gerber 2001; Lin and Hovy2002)."
"Evalua-tion involving human judges revealed that Simfinder identifies similar sentences with49.3 precision at 52.9 recall (Hatzivassiloglou, Klavans, and Eskin 1999)."
"To identify themes, Simfinder extracts linguistically motivated features for eachsentence, including WordNet synsets (Miller et al. 1990) and syntactic dependencies,such as subject–verb and verb–object relations."
"The first two of these scores are produced by Simfinder, and the salience score iscomputed using lexical chains (Morris and Hirst 1991; Barzilay and Elhadad 1997) asdescribed below."
Lexical chains—sequences of semantically related words—are tightly connected tothe lexical cohesive structure of the text and have been shown to be useful for determin-ing which sentences are important for single-document summarization (Barzilay andElhadad 1997; Silber and McCoy 2002).
"To increase the coherence of the output text, we identify blocks of topicallyrelated themes and then apply chronological ordering on blocks of themes using themetime stamps (Barzilay, Elhadad, and McKeown 2002)."
A representation which fits these requirements is adependency-based representation (Melcuk 1988).
"In fact, we have developed a rule-basedcomponent that transforms the phrase structure output of Collins’s (2003) parser intoa representation in which a node has a direct link to its dependents."
"Our manual analysis of paraphrased sen-tences (Barzilay 2003) revealed that such alignments most frequently occur in pairs ofnoun phrases (e.g., faculty member and professor) and pairs including verbs with parti-cles (e.g., stand up, rise)."
Weautomatically constructed the paraphrasing dictionary from a large comparable newscorpus using the co-training method described in Barzilay and McKeown (2001).
"As previously observedin the literature (Mani, Gates, and Bloedorn 1999; Jing and McKeown 2000), such com-ponents include a clause in the clause conjunction, relative clauses, and some ele-ments within a clause (such as adverbs and prepositions)."
"While the ordering of many sentenceconstituents is determined by their syntactic roles, some constituents, such as time,location and manner circumstantials, are free to move (Elhadad et al. 2001)."
We trained a trigram model with Good–Turing smoothingover 60 megabytes of news articles collected by Newsblaster using the second versionCMU–Cambridge Statistical Language Modeling toolkit (Clarkson and Rosenfeld 1997).
"In the previous version of thesystem (Barzilay, McKeown, and Elhadad 1999), we performed linearization of afusion dependency structure using the language generator FUF/SURGE (Elhadadand Robin 1996)."
"In our previous work, we evaluated the overall summarization strategy of MultiGenin multiple experiments, including comparisons with human-written summaries inthe Document Understanding Conference (DUC) 11 evaluation (McKeown et al. 2001;McKeown et al. 2002) and quality assessment in the context of a particular informa-tion access task in the Newsblaster framework (McKeown et al. 2002)"
"From theoverlap data, we computed weighted recall and precision based on fractional count(Hatzivassiloglou and McKeown 1993)."
"In addition to sentence fusion, compressionalgorithms (Chandrasekar, Doran, and Bangalore 1996; Grefenstette 1998; Mani, Gates,and Bloedorn 1999; Knight and Marcu 2002; Jing and McKeown 2000; Reizler et al. 2003)and methods for expansion of a multiparallel corpus (Pang, Knight, and Marcu 2003)are other instances of such methods."
"While earlier approaches for text compression werebased on symbolic reduction rules (Grefenstette 1998; Mani, Gates, and Bloedorn 1999),more recent approaches use an aligned corpus of documents and their human writtensummaries to determine which constituents can be reduced (Knight and Marcu 2002;Jing and McKeown 2000; Reizler et al. 2003)."
Knight and Marcu (2000) treat reduction as a translation process using a noisy-channel model (Brown et al. 1993).
"The alignment method described in Section 3 falls into a class of tree comparisonalgorithms extensively studied in theoretical computer science (Sankoff 1975; Findenand Gordon 1985; Amir and Keselman 1994; Farach, Przytycka, and Thorup 1995)and widely applied in many areas of computer science, primarily computational bi-ology (Gusfield 1997)."
"In the NLP context, this class of algorithms has been used previously in example-based machine translation, in which the goal is to find an optimal alignment betweenthe source and the target sentences (Meyers, Yangarber, and Grishman 1996)."
"Forexample, all summary sentences may contain the full description of a named entity(e.g., President of Columbia University Lee Bollinger), while the use of shorter descriptionssuch as Bollinger or anaphoric expressions in some summary sentences would in-crease the summary’s readability (Schiffman, Nenkova, and McKeown 2002; Nenkovaand McKeown 2003)."
"The application of the word segmenter is described elsewhere (Nagata, 1996)."
"We then describe the word segmentation algorithm and the new word extraction method, with their derivation as an approximation of a generalization of the Forward-Backward algorithm (Baum, 1972).  "
"The underlying idea of the replacement is the same as Turing's estimates in back-off smoothing (Katz, 1987). "
"For Chinese, (Sproat et al., 1994) used the word unigram model in their word segmenter based on weighted finite-state transducer.  "
"Our translation system makes use of a hierarchical phrase-based translation model (Chiang, 2007), which we argue is a strong baseline for these language pairs."
"First, such a system makes useof lexical information when modeling reordering (Lopez, 2008), which has previously been shown to be useful in German-to- nglish translation (Koehn et al., 2008)."
The rule feature values were computed online during decoding using the suffix array method described by Lopez (2007).
"Since the official evaluation criterion for WMT09 is human sentence ranking, we chose to minimize a linear combination of two common evaluation metrics, BLEU and TER (Papineni et al., 2002; Snover et al., 2006), during system development and tuning "
"Doing so enables us to use possibly inaccurate approaches to guess the segmentation of compound words, allowing the decoder to decide which to use during translation. This is a further development of our general source-lattice approach to decoding (Dyer et al., 2008)."
"To tune the model parameters, we selected a set of compound words from a subset of the German development set, manually created a linguistically plausible segmentation of these words, and used this to select the parameters of the log-linear model using a lattice minimum error training algorithm to minimize WER (Macherey et al., 2008)."
"For the test data, we created a lattice of every possible segmentation of any word 6 characters or longer and used forward-backward pruning to prune out low probability segmentation paths (Sixtus and Ortmanns, 1999)."
"Although during minimum error training we assume a decoder that uses the maximum derivation decision rule, we find benefits to translating using a minimum risk decision rule on a test set (Kumar and Byrne, 2004)."
"The emerging technology of information extraction (Appelt and Israel 1997, Hearst 1999) provides a means of gaining access to this information."
"Considerable interest in information extraction has concentrated on identifying named entities in text pertaining to current events (for example, Wacholder et al. 1997, Voorhees and Harman 1998, and MUC-7); however, several recent efforts have been directed at biomolecular data (Blaschke et al. 1999, Craven and Kumlien 1999, and Rindflesch et al. 2000)"
"The SPECIALIST minimal commitment parser relies on the SPECIALIST Lexicon as well as the Xerox stochastic tagger (Cutting et al. 1992). The output produced is in the tradition of partial parsing (Hindle 1983, McDonald 1992, Weischedel et al. 1993) and concentrates on the simple noun phrase, what Weischedel et al. (1993)"
"Several approaches provide similar output based on statistics (Church 1988, Zhai 1997, for example), a finite-state machine (AitMokhtar and Chanod 1997), or a hybrid approach combining statistics and linguistic rules (Voutilainen and Padro 1997)."
"The SPECIALIST parser is based on the notion of barrier words (Tersmette et al. 1988), which indicate boundaries between phrases. "
"As the first step in the process, an existing program, MetaMap, (Aronson et al. 1994) attempts to map each simple noun phrase to a concept in the UMLS Metathesaurus."
In this context such a word is often an acronym not defined locally and indicates the presence of a binding term (Fukuda et al. 1998).
"Rindflesch et al. (1999) use the term ""macro-noun phrase"" to refer to structures that include reduced relative clauses (commonly introduced by prepositions or participles) as well as appositives."
"For English, we obtained results comparable with the results presented in (Merialdo, 1992) as well as in (Church, 1992). "
"Given the comparability of the accuracy of the rule-based part-of-speech (POS) tagger (Brill, 1992) with the accuracy of the stochastic tagger and given the fact that a rule-based POS tagger has never been used for a Slavic language we have tried to apply rule-based methods even for Czech. "
"Note especially, that Czech nouns are divided into four classes according to gender (Sgall, 1967) and into seven classes according to ease. "
"We have used the basic source channel model (de-scribed e.g. in (Merialdo, 1992))."
"(Schiller, 1996) describes the general architecture of the tool for noun phrase mark-up based on finitestate techniques and statistical part-of-peech disambiguation for seven European languages. "
The first step is the extraction of important concepts from the source text by building an intermediate representation of some sort. The second step uses this intermediate representation to generate a summary (Sparck Jones 1993). 
"In the research presented here, we concentrate on the first step of the summarization process and follow Barzilay and Elhadad (1997) in employing lexical chains to extract important concepts from a document. "
"The concept of lexical chains was first introduced by Morris and Hirst. Basically, lexical chains exploit the cohesion among an arbitrary number of related words (Morris and Hirst 1991). "
"Considering the size of most documents, the linear nature of this algorithm makes it usable for generalized summarization of large documents (Silber and McCoy 2000). "
"Our method affords a considerable speedup for these smaller documents. For instance, a document that takes 300 seconds using Barzilay and Elhadad’s method takes only 4 seconds using ours (Silber and McCoy 2000). "
"The first, scientific documents with abstracts, represents a readily available class of summaries often discussed in the literature (Marcu 1999).  "
"The ITC-irst system (Chen et al., 2005) is based on a log-linear model which extends the original IBM Model 4 (Brown et al., 1993) to phrases (Koehn et al., 2003; Federico and Bertoldi, 2005). "
"While feature functions exploit statistics extracted from monolingual or word-aligned texts from the training data, the scaling factors λ of the log-linear model arestimated on the development data by applying a minimum error training procedure (Och, 2004). "
"Hence, either the best translation hypothesis is directly extracted from the word graph and output, or an N-best list of translations is computed (Tran et al., 1996). "
"Starting from the parallel training corpus, provided with direct and inverted alignments, the so called union alignment (Och and Ney, 2003) is Computed. "
"Target language models (LMs) used by the decoder and rescoring modules are, respectively, estimated from 3-gram and 4-gram statistics by applying the modified Kneser-Ney smoothing method (Goodman and Chen, 1998). "
"The Arabic-to-English system has been trained with the data provided by the International Work- shop on Spoken Language Translation 2005 The context is that of the Basic Traveling Expression Corpus (BTEC) task (Takezawa et al., 2002). "
"Phrase-based method (Koehn et al., 2003; Och and Ney, 2004; Koehn et al., 2007) and syntax- based method (Wu, 1997; Yamada and Knight, 2001; Eisner, 2003; Chiang, 2005; Cowan et al., 2006; Marcu et al., 2006; Liu et al., 2007; Zhang et al., 2007c, 2008a, 2008b; Shen et al., 2008; Mi and Huang, 2008) represent the state-of-the-art technologies in statistical machine translation (SMT). "
"In this paper, we bring forward the first idea by studying the issue of how to utilize structured syntactic features for phrase reordering in a phrase-based SMT system with BTG (Bracketing Transduction Grammar) constraints (Wu, 1997).  "
"In syntax-based method, word reordering is implicitly addressed by translation rules, thus the performance is subject to parsing errors to a large extent (zhang et al 2007a) and the impact of syntax on reordering is difficult to single out (Li et al., 2007)."
"This makes the phrase based method particularly weak in handling global phrase reordering. From machine learning viewpoint (Vapnik, 1995), it is computationally infeasible to explicitly generate features involving structured information in many NLP applications. "
"Indeed, using tree kernel methods to mine structured knowledge has shown success in some NLP applications like parsing (Collins and Duffy,2001), semantic role labeling (Moschitti, 2004 Zhang et al., 2007b), relation extraction (Zhang et al., 2006), pronoun resolution (Yang et al.,2006) and question classification (Zhang and Lee, 2003). "
"In this paper, phrase reordering is recast as a classification issue as done in previous work (Xiong et al., 2006 & 2008; Zhang et al., 2007a). "
"In total, we arrive at 13 features, including 8 boundary word features, 4 (kinds of) internal word features and 1 LM feature. The first 12 features have been proven useful (Xiong et al., 2006; Zhang et al., 2007a) to phrase reordering. "
"In convolution tree kernel (Collins and Duffy, 2001), a parse tree T is implicitly represented by a vector of integer counts of each sub-tree type (regardless of its Ancestors)"
"The composite kernel Kcis a linear combination of the two individual kernels, where the coefficient α is set to its default value 0.3 as that in Moschitti (2004)’s implementation. "
"The Stanford parser (Klein and Manning, 2003) is used to parse Chinese sentences on the training, dev and test sets."
"Besides the the case-sensitive BLEU-4 (Papineni et al., 2002) used in the two experiments, we design another evaluation metrics Reordering Accuracy (RAcc) for forced decoding evaluation. "
"However, while researchers have shown that it is sometimes possible to annotate corpora that capture features of interpretation, to provide empirical support for theories, as in (Eugenio et al., 2000), or to build classifiers that assist in dialogue reasoning, as in (Jordan and Walker, 2005), it is rarely feasible to fully annotate the interpretations themselves. "
"Our specific approach is based on contribution tracking (DeVault, 2008), a framework which casts linguistic inference in situated, task-oriented dialogue in probabilistic terms. "
"As in the experiments of Clark and Wilkes- Gibbs (1986) and Brennan and Clark (1996), one of the players, who plays the role of director, instructs the other player, who plays the role of matcher, which object is to be added next to the Scene. "
"COREF treats interpretation broadly as a problem of abductive intention recognition (Hobbs et al., 1993). "
Interpretations are constructed abductively in that the initial actions in the sequence need not be directly tied to observable events; they may be tacit in the terminology of Thomason et al. (2006). 
"When a dialogue act is preceded by tacit actions in an interpretation, the speaker of the utterance implicates that the earlier tacit actions have taken place (DeVault, 2008)."
"We chose to train maximum entropy models (Berger et al., 1996). Our learning framework is described in Section 4.1; the results in Section 4.2  "
"We used the MALLET maximum entropy classifier (McCallum, 2002) as an off-the-shelf, trainable maximum entropy model. Each run involved two steps."
"Our work adds to a body of research learning deep models of language from evidence implicit in an agent’s interactions with its environment. It shares much of its motivation with co-training (Blum and Mitchell, 1998) in improving initial models by leveraging additional data that is easy to obtain. "
"Closer in spirit is AI research on learning vocabulary items by connecting user vocabulary to the agent’s perceptual representations at the time  of utterance (Oates et al., 2000; Roy and Pentland, 2002; Cohen et al., 2002; Yu and Ballard, 2004; Steels and Belpaeme, 2005)."
"In general, dialogue coherence is an important source of evidence for all aspects of language, for both human language learning (Saxton et al., 2005) as well as machine models. For example, Bohus et al. (2008) use users’ confirmations of their spoken requests in a multi-modal interface to tune the system’s ASR rankings for recognizing subsequent utterances. "
"Thus, even when spoken language interfaces use probabilistic inference for dialogue management (Williams and Young, 2007), new techniques may be needed to mine their experience for correct interpretations."
"Word alignment is a critical component in training statistical machine translation systems and has received a significant amount of research, for example, (Brown et al., 1993; Ittycheriah and Roukos, 2005; Fraser and Marcu, 2007), including work leveraging syntactic parse trees, e.g., (Cherry and Lin, 2006; DeNero and Klein, 2007; Fossum et al., 2008)."
"Word alignment is also a required first step in other algorithms such as for learning sub-sentential phrase pairs (Lavie et al., 2008) or the generation of parallel treebanks (Zhechev and Way, 2002)."
"We finally also include as alignment candidates those word pairs that are transliterations of each other to cover rare proper names (Hermjakob et al., 2008), which is important for language pairs that don’t share the same alphabet such as Arabic and English"
"The test set includes only sentences for which our English parser (Soricut and Marcu, 2003) could produce a parse tree, which effectively excluded a few very long Sentences."
"In the first set of experiments, we compare two settings of our UALIGN system with other aligners, GIZA++ (Union) (Och and Ney, 2003) and LEAF (with 2 iterations) (Fraser and Marcu, 2007). The GIZA++ aligner is based on IBM Model 4 (Brown et al., 1993)."
"These corpus-based models can be represented e.g. as collocational matrices (Garside et al. (eds.) 1987: Church 1988), Hidden Markov models (cf. Cutting et al. 1992), local rules (e.g. Hindle 1989) and neural networks (e.g. Schmid 1994).  "
"One doubt concerns the notion 'correct analysis"". For example Church (1992) argues that linguists who manually perform the tagging task using the double blind method disagree about the correct analysis in at least 3% of all words even after they have negotiated about the initial disagreements. "
"However, ""Vbutilainen (1995) has shown that EngCG combined with a syntactic parser produces morphologically unambiguous output with an accuracy of 99.3%, a figure clearly better than that of the statistical tagger in the experiments below (however. the test data was not the same). "
"The stochastic tagger was trained on a sample of 357,000 words from the Brown University Corpus of Present-Day English (Francis and Kucera 1982) that was annotated using the EngCG tags. "
"The tag N-gram probabilities, and both the scheme and its application to these two tasks are described in detail in (Samuelsson 1996), where it was also shown to compare favourably to (deleted) interpolation, see (Jelinek and Mercer 1980), even when the back-off weights of the latter were optimal. "
"Generally speaking, a joint system is slower than a pipeline system in training. (Xue and Palmer, 2004) found out that different features suited for different sub-tasks of SRL, i.e. argument identification and classification. "
"Though aiming at Chinese SRL, (Xue, 2006) reported that their experiments show that simply adding the verb data to the training set of NomBank and extracting the same features from the verb and noun instances will hurt the overall performance. "
"From the results of CoNLL-2008 shared task, the top system by (Johansson and Nugues, 2008) also used two different subsystems to handle verbal and nominal predicates, respectively."
"A word-pair classification is used to formulate semantic dependency parsing as in (Zhao and Kit, 2008). "
"Note that this pruning algorithm is slightly different from that of (Xue and Palmer, 2004), the predicate itself is also included in the argument candidate list as the nominal predicate sometimes takes itself as its argument. "
"The concept of support verb was broadly used (Toutanova et al., 2005; Xue, 2006; Jiang and Ng, 2006) 4 , we here extend it to nouns and prepositions. "
"As an optimal feature template subset cannot be expected to be extracted from so large a set by hand, a greedy feature selection similar to that in (Jiang and Ng, 2006; Ding and Chang, 2008) is applied."
"Though the time complexity of the algorithm given by (Jiang and Ng, 2006) is also linear, it should assume all feature templates in the initial selected set ‘good’ enough and handles other feature template candidates in a strict incremental way. "
Chen et al. (2008) used features derived from short dependency pairs based on large-scale auto-parsed data to enhance dependency parsing.
Koo et al. (2008) presentednew features based on word clusters obtained from large-scale unlabeled data and achieved large improvement for English and Czech. 
Nivre and McDonald (2008) presented an integrating method to provide additional information for graph-based and transition-based parsers. 
"They were chosen among the 20 participating systems either because they held better results (the first four participants) or because they used some joint learning techniques (Henderson et al., 2008). "
"The results of (Titov et al., 2009) that use the similar joint learning technique as (Henderson et al., 2008) are also Included . "
"Among the statistical approaches, the Maximum Entropy framework has a very strong position. Nevertheless, a recent independent comparison of 7 taggets (Zavrel and Daelemans, 1999) has shown that another approach even works better "
"Additionally, we present results of the tagger on the NEGRA corpus (Brants et al., 1999) and the Penn Treebank (Marcus et al., 1993). The Penn Treebank results reported here for the Markov model approach are at least equivalent to those reported for the Maximum Entropy approach in (Ratnaparkhi, 1996). For a comparison to other taggers, the reader is referred to (Zavrel and Daelemans, 1999) "
"A theoretical motivated argumentation uses the standard deviation of the m a x i m u m likelihood probabilities for the weights 0i (Samuelsson, 1993) This leaves room for interpretation."
"The processing time of the Viterbi algorithm (Rabiner, 1989) can be reduced by introducing a beam Search. "
"The German NEGRA corpus consists of 20,000 sentences (355,000 tokens) of newspaper texts (Frank-furter Rundschau) that are annotated with parts-of-speech and predicate-argument structures (Skut et al., 1997)."
"According to current tagger comparisons (van Halteren et al., 1998; Zavrel and Daelemans, 1999), and according to a comparsion of the results pre- sented here with those in (Ratnaparkhi, 1996), the Maximum Entropy framework seems to be the only other approach yielding comparable results to the one presented here. "
"The literature on word space models (Sahlgren, 2006) has focused on taxonomic similarity (synonyms, antonyms, co-hyponyms. . . ) and general association (e.g., finding topically related words), exploiting the idea that taxonomically or associated words will tend to occur in similar contexts, and thus share a vector of cooccurring words. "
"The literature on relational similarity, on the other hand, has focused on pairs of words, devising various methods to compare how similar the contexts in which target pairs appear are to the contexts of other pairs that instantiate a relation of interest (Turney, 2006; Pantel and Pennacchiotti, 2006). "
"Beyond these domains, purely corpus-based methods play an increasingly important role in modeling constraints on composition of words, in particular verbal selectional preferences – finding out that, say, children are more likely to eat than apples, whereas the latter are more likely to be eaten (Erk, 2007; Padó et al., 2007). "
"In all of our experiments, the same normalization method and classification algorithm is used with the default parameters: First, a TF-IDF feature weighting is applied to the cooccurrence matrix (Salton and Buckley, 1988). "
"For other pattern recognition related coding (e.g., cross validation, scaling, etc.) we made use of the Matlab PRTools (Duin, 2001).  "
"This task, introduced by Landauer and Dumais (1997), consists of 80 multiple choice questions in which a word is given as the stem and the correct choice is the word which has the closest meaning to that of the stem, among 4 candidates. "
"Linguists have long been interested in the semantic constraints that verbs impose on their arguments, a broad area that has also attracted computational modeling, with increasing interest in purely corpus-based methods (Erk, 2007; Padó et al., 2007). "
"As has been stressed at least since Chomsky’s early work (Chomsky, 1957), no matter how large a corpus is, if a phenomenon is productive there will always be new well-formed instances that are not in the corpus. "
"An end result of the project is ConceptNet 3, a large scale semantic network consisting of relations between concept pairs (Havasi et al., 2007).  "
"Intuitively, AUC is the probability that a randomly picked positive instance’s estimated posterior probability is higher than a randomly picked negative instance’s estimated posterior probability (Fawcett, 2006). "
"It is well-known that such distributions can represent meaning reasonably well, at least for meaning-comparison purposes (Landauer and Dumais, 1997)."
"Distributional information has uses beyond part of speech induction. For example, it is possible to augment a fixed syntactic or semantic taxonomy with such information to good effect (Hearst and Schütze, 1993). "
"System combination for machine translation (MT) has emerged as a powerful method of combining the strengths of multiple MT systems and achieving results which surpass those of each individual system (e.g. Bangalore, et. al., 2001, Matusov, et. al., 2006, Rosti, et. al.,2007a)."
"Confusion networks allow word-level system combination, which was shown to outperform sentence re-ranking methods and phrase-level combination (Rosti, et. al. 2007a)."
"The choice is performed to maximize a scoring function using a set of features and a log-linear model (Matusov, et. al 2006, Rosti, et al. 2007a)."
"Incremental alignment methods have been proposed to relax the independence assumption of pair-wise alignment (Rosti et al. 2008, Li et al. 2009). Such methods align hypotheses to a partially constructed CN in some order."
"Each backbone produces a separate CN and the decision of which CN to choose is taken at a later decoding stage, but this still restricts the possible orders and alignments greatly (Rosti et al. 2008, Matusov et al. 2008)."
"In the scenario that N-best lists are available from individual systems for combination, the weight of each hypothesis can be computed based on its rank in the N-best list (Rosti et. al. 2007a). "
The word posterior feature is the same as the one proposed by Rosti et. al. (2007a). i.e
The second feature we used is a bi-gram voting feature proposed by Zhao and He (2009)
"To compute scores for word pairs, we perform pair-wise hypothesis alignment using the indirect HMM (He et al. 2008) for every pair of input hypotheses."
Decoding is based on a beam search algorithm similar to that of the phrase-based MT decoder (Koehn 2004b).
"In order to prevent the garbage collection problem where many words align to a rare word at the other side (Moore, 2004), we further impose the limit that if one word is aligned to more than T words, these links are sorted by their alignment score and only the top T links are kept. "
"Results are reported in case insensitive BLEU score in percentages (Papineni et. al., 2002)."
"Rule ordering issue has been discussedby Voutilainen(1994), but he has recently indicated 1that insensitivity to rule ordering is not a propertyof their system (although Voutilainen(1995a) statesthat it is a very desirable property) but rather isachieved by extensively testing and tuning the rules."
"Onthe other hand, in languages like Turkish or Finnishwith very productive agglutinative morphology, itis possible to produce thousands of forms (or evenmillions (Hankamer, 1989)) from a given root wordand the kinds of ambiguities one observes are quitedifferent than what is observed in languages like En-Glish"
"Furthermore, Turkish allows very productive derivational processes and the information about the derivational structure of a word form is usually crucial for disambiguation (Oflazer and Tiir, 1996). "
"The preprocessor module also performs a number of additional functions such as grouping of lexicalizcd and non-lexicalized collocations, compound verbs, etc., (Ofiazer and Kurubz, 1994; Oflazer and Tiir, 1996). "
"This is very similar to Brill's use of contexts to induce transformation rules for his tagger (Brill, 1992; Brill, 1995), but instead of generating transformation rules from a training text, we gather statistics and apply them to parses in the text being disambiguated. "
"The proposed approach is also amenable to an efficient implementation by finite state transducers (Kaplan and Kay, 1994). "
"The larger P(cldi) a document di has, the more probably it will be categorized into category c. This is called the Probabilistic Ranking Principle (PRP) (Robertson, 1977). Several strategies can be used to assign categories to a document based on PRP (Lewis, 1992). "
"There are several ways to calculate P(c[d). Three representatives are (Robertson and Sparck Jones, 1976), (Kwok, 1990), and (Fuhr, 1989). "
"Term weighting for target documents would also be necessary for sophisticated information retrieval (Fuhr, 1989; Kwok, 1990). "
"Furthermore, Fuhr (1989) pointed out that transformation, as in Eq. (6), is not monotonic of P(cld ). It follows then, that C T does not satisfy the probabilistic ranking principle ( P R P ) any More. "
"In the following experiments we used this binary estimation method, but non binary estimates could be used as in (Fuhr, 1989). "
"In particular, because of problem 3, P(cld) would become an illegitimate value. In our experiments, as well as in Lewis' experiments (1992), P(cld ) ranges from 0 to more than 101°. "
"Lewis proposed the proportional assignment strategy based on the probabilistic ranking principle (Lewis, 1992). "
"The best known measures for evaluating text categorization models are recall and precision, calculated by the following equations (Lewis, 1992): "
The superiority of proportional assignment over the other strategies has already been reported by Lewis (1992). 
"Thus, most of recent works in this research area are based on extraction (Goldstein et al., 1999). "
"Subsequent works have demonstrated the success of Luhn’s approach (Buyukkokten et al., 2001; Lam-Adesina and Jones, 2001; Jaruskulchai et al., 2003). Edmunson (1969) proposed the use of other features such as title words, sentence locations, and bonus words to improve sentence extraction. Goldstein et al. (1999) presented an extraction technique that assigns weighted scores for both statistical and linguistic features in the sentence."
"Recently, Salton et al. (1999) have developed a model for representing a document by using undirected graphs.  "
"Chuang and Yang (2000) studied several algorithms for extracting sentence segments, such as decision tree, naive Bayes classifier, and neural network."
"The graph G is called the text relationship map of D (Salton et al., 1999). "
"The typical approach for testing a summarization system is to create an “ideal” summary, either by professional abstractors or merging summaries provided by multiple human subjects using methods such as majority opinion, union, or intersection (Jing et al., 1998). "
"In this paper, we attempt to bring some clarity to the situation by taking a closer look at one of these existing methods. Specifically, we cast the popular technique of cube pruning (Chiang, 2007) in the well-understood terms of heuristic search (Pearl, 1984). "
"We show how this insight enables us to easily develop faster and exact variants of cube pruning for tree-to-string transducer-based MT (Galley et al., 2004; Galley et al., 2006; DeNero et al., 2009). "
"This should be regarded neither as a surprise nor a criticism, considering cube pruning’s origins in hierarchical phrase-based MT models (Chiang, 2007), which have only a small number of distinct Nonterminals."
"But the situation is much different in treeto-string transducer-based MT (Galley et al., 2004; Galley et al., 2006; DeNero et al., 2009). "
"Binarizing the grammars (Zhang et al., 2006) further increases the size of these sets, due to the introduction of virtual nonterminals. "
"A* has nice guarantees (Dechter and Pearl, 1985), but it is space-consumptive and it is not anytime. For a use case where we would like a finer- rained speed/quality tradeoff, it might be useful to consider an anytime search algorithm, like depth-first branch-and-bound (Zhang and Korf, 1995). "
"Annotations should not be influenced by theory-specific considerations. Nevertheless, different theory-specific representations shMl be recoverable from the annotation, cf. (Marcus et al., 1994). "
"The scheme must provide representational means for all phenomena occurring in texts. Disambiguation is based on human processing skills (cf. (Marcus et at., 1994), (Sampson, 1995), (Black et al. , 1996)). "
"Due to the substantial differences between existing models of constituent structure, tile question arises of how the theory , requirement can be satisfied. At this point the importance of the underlying argument : is emphasised (cf. (Lehmaim et al., 1996), (Marcus et al., 1994), (Sampson, 1995)). "
"This requirement speaks against the traditional sort of dependency trees, in which heads are represented as non-terminal nodes, cf. (Hudson, 1984). "
"As keyboard input is more efficient than mouse input (cf. (Lehmalm et al., 1995)) mnost effort has been put in developing an efficient keyboard interLace. "
"Grammatical functions are assigned using standard statistical part-of-speech tagging methods (cf. e.g. (Cutting et al., 1992) and (Feldweg, 1995)). "
"Actually, our dependency structure alignment is almost the same as that of Filippova and Strube (2008), and our lead sentence plays the role of a basis tree in the Barzilay and McKeown approach (2005). "
"This revision strategy was employed by the human reviser mentioned in section 2, and we consider this to be effective because our target document has a so-called inverse pyramid structure (Robin and McKeown 1996), in which the first sentence is elaborated by the following sentences. "
"Notice that the term coreferential is used in an extended way as it is usually used to describe the phenomena in noun group pairs (Mitkov, 2002).   "
"The articles were morphologically analyzed by Mecab (Kudo et al., 2003) and syntactically parsed by Cabocha (Kudo and Matsumoto, 2002). "
"We planned a linguistic evaluation like DUC2005 (Hoa Trang, 2005). "
"An attempt was made in the CoNLL 2007 shared task to apply SCL to non-projective dependency parsing (Shimizu and Nakagawa, 2007), however, without any clear conclusions. "
"Therefore, whenever we have ac cess to a large amount of labeled data from some “source” (out-of-domain), but we would like a model that performs well on some new “target” domain (Gildea, 2001; Daumé III, 2007), we face the problem of domain adaptation. "
"For example, the performance of a statistical parsing system drops in an appalling way when a model trained on the Wall Street Journal is applied to the more varied Brown corpus (Gildea, 2001) "
"The problem itself has started to get attention only recently (Roark and Bacchiani, 2003; Hara et al., 2005; Daumé III and Marcu, 2006; Daumé III, 2007; Blitzer et al., 2006; McClosky et al., 2006; Dredze et al., 2007).  "
"We distinguish two main approaches to domain adaptation that have been addressed in the literature (Daumé III, 2007): supervised and semi-supervised. "
"In supervised domain adaptation (Gildea, 2001; Roark and Bacchiani, 2003; Hara et al., 2005; Daumé III, 2007), besides the labeled source data, we have access to a comparably small, but labeled amount of target data. "
"In contrast, semi-supervised domain adaptation (Blitzer et al., 2006; McClosky et al., 2006; Dredze et al., 2007) is the scenario in which, in addition to the labeled source data, we only have unlabeled and no labeled target domain Data. "
"Thus, one conclusion from that line of work is that as soon as there is a reasonable (often even small) amount of labeled target data, it is often more fruitful to either just use that, or to apply simple adaptation techniques (Daumé III, 2007; Plank and van Noord, 2008). "
"In contrast, Dredze et al. (2007) report on “frustrating” results on the CoNLL 2007 semi-supervised adaptation task for dependency parsing, i.e. ”no team was able to improve target domain performance substantially over a state of the art baseline”. "
"So far, most previous work on domain adaptation for parsing has focused on data-driven systems (Gildea, 2001; Roark and Bacchiani, 2003; McClosky et al., 2006; Shimizu and Nakagawa, 2007), i.e. systems employing (constituent or de endency based) treebank grammars (Charniak, 1996)."
"The system just ended up at rank 7 out of 8 teams. However, based on annotation differences in the datasets (Dredze et al., 2007) and a bug in their system (Shimizu and Nakagawa, 2007), their results are inconclusive. 1 Thus, the effectiveness of SCL is rather unexplored for parsing. "
"The output of the parser is dependency structure based on the guidelines of CGN (Oost Dijk, 2000). "
"The Maximum Entropy model (Berger et al., 1996; Ratnaparkhi, 1997; Abney, 1997) is a conditional model that assigns a probability to every possible parse ω for a given sentence s."
"The parameters (weights) θj can be estimated efficiently by maximizing the regularized conditional likelihood of a training corpus (Johnson et al., 1999; van Noord and Malouf, 2005)"
"SCL (Structural Correspondence Learning) (Blitzer et al., 2006; Blitzer et al., 2007; Blitzer, 2008) is a recently proposed domain adaptation technique which uses unlabeled data from both source and target domain to learn correspondences between features from different Domains."
"Intuitively, if we are able to find good correspondences among features, then the augmented labeled source domain data should transfer better to a target domain (where no labeled data is available) (Blitzer et al., 2006)."
"As pointed out by Blitzer et al. (2006), each instance will actually contain features which are totally predictive of the pivot features (i.e. the pivot itself)."
"While SCL has been successfully applied to PoS tagging and Sentiment Analysis (Blitzer et al., 2006; Blitzer et al., 2007), its effectiveness for parsing was rather unexplored. "
"In the part-of-speech hterature, whether taggers are based on a rule-based approach (Klein and Simmons, 1963), (Brill, 1992), (Voutilainen, 1993), or on a statistical one (Bahl and Mercer, 1976), (Leech et al., 1983), (Merialdo, 1994), (DeRose, 1988), (Church, 1989), (Cutting et al., 1992), there is a debate as to whether more attention should be paid to lexical probabilities rather than contextual ones. "
"Consider, for example, a word like cover as discussed by Voutilainen (Karlsson et al., 1995): in the Brown and the LOB Corpus (Johansson, 1980), the word ""cover"" is a noun 40% of the occurrences and a verb 60% of the other, but in the context of a car maintenance manual, it is a noun 100~0 of the time. "
"We also argue against Church's position, supporting the claim that more attention needs to be paid to contextual information for part-of-speech disambiguation (Tzoukermann et ai., 1995). "
"this technique generally consists of using a small tagged corpus to train a system and having the system tag another subset of the corpus that gets disambiguated later. (Derouault and Merialdo, 1986) have used these techniques but the necessary human effort is still considerable. "
"In some sense, this approach is similar to the notion of ""ambiguity classes"" explained in (Kupiec, 1992) and (Cutting et al., 1992) where words that belong to the same part-of-speech figure together. "
"(Chanod and Tapanainen, 1995) compare two tagging frameworks for tagging French, one that is statistical, built upon the Xerox tagger (Cutting et al., 1992), and another based on linguistic constraints only. "
"We have used a toolkit developed at AT&T Bell Laboratories (Pereira et al., 1994) which manipulates weighted and unweighted finite-state machines (acceptors or transducers). "
"The morphological FST is generated automatically from a large dictionary of French of about 90,000 entries and on-line corpora, such as Le Monde Newspapers (ECI, 1989 and 1990). "
"The highest ranking material can then be extracted and displayed verbatim as extracts (Luhn 1958; Edmundson 1969; Paice 1990; Kupiec, Pedersen, and Chen 1995). Extracts are often useful in an information retrieval environment since they give users an idea as to what the source document is about (Tombros and Sanderson 1998; Mani et al. 1999), but they are texts of relatively low quality.  "
"Because of this, it is generally accepted that some kind of postprocessing should be performed to improve the final result, by shortening, fusing, or otherwise revising the material (Grefenstette 1998; Mani, Gates, and Bloedorn 1999; Jing and McKeown 2000; Barzilay et al. 2000; Knight and Marcu 2000). "
An extrinsic evaluation (Teufel 2001) shows that the output of our system is al-ready a useful document surrogate in its own right.
"Research is often described as a problem-solving activity (Jordan 1984; Trawinski 1989; Zappen 1983). Three information types can be expected to occur in any research article: problems (research goals), solutions (methods), and results. In many disciplines, particularly the experimental sciences, this problem-solution structure has been crystallized in a fixed presentation of the scientific material as introduction, method, result and discussion (van Dijk 1980). "
"In contrast to the view of science as a disinterested fact Factory, researchers like Swales (1990) have long claimed that there is a strong social aspect to science, because the success of a researcher is correlated with her ability to convince the field of the quality of her work and the validity of her arguments. Authors construct an argument that Myers (1992) calls the “rhetorical act of the paper” "
"They are useful indicators of overall importance (Pollock and Zamora 1975); they can also be relatively easily recognized with information extraction techniques (e.g., regular expressions).  "
"Citation indexes are constructs that contain pointers between cited texts and citing texts (Garfield 1979), traditionally in printed form. When done on-line (as in CiteSeer [Lawrence, Giles, and Bollacker 1999], or as in Nanba and Okumura’s [1999] work), citations are presented in context for users to browse. "
"Citations may vary in many dimensions; for example, they can be central or perfunctory, positive or negative (i.e., critical); apart from scientific reasons, there is also a host of social reasons for citing (“politeness, tradition, piety” [Ziman 1969]).  "
"As our immediate goal is to select important content from a text, we also need a second set of gold standards that are defined by relevance (as opposed to rhetorical status). Relevance is a difficult issue because it is situational to a unique occasion (Saracevic 1975 Sparck Jones 1990 Mizzaro 1997) "
"Humans perceive relevance differently from each other and differently in different situations. Paice and Jones (1993) report that they abandoned an informal sentence selection experiment in which they used agriculture articles and experts in the field as participants, as the participants were too strongly influenced by their personal research interest.  "
"Rath, Resnick, and Savage (1961) report that six participants agreed on only 8 of 20 sentences they were asked to select out of short Scientific American texts and that five agreed on 32 of the Sentences. "
Edmundson and Wyllys (1961) find similarly low human agreement for research articles. More recent experimentsreporting more positive results all used news text (Jing et al. 1998; Zechner 1995).
"Recently, researchers have been looking for more objective definitions of relevance. Kupiec, Pedersen, and Chen (1995) define relevance by abstract similarity "
"In our case, neither assumption holds. First, the experiments in Teufel and Moens (1997) showed that in our corpus only 45of the abstract sentences appear elsewhere in the body of the document (either as a close variant or in identical form), whereas Kupiec, Pedersen, and Chen report a figure of 79.  "
"Author summaries tend to be less systematic (Rowley 1982) and more “deep generated,” whereas sum- maries by professional abstractors follow an internalized building plan (Liddy 1991) and are often created through sentence extraction (Lancaster 1998).  "
"The annotation experiment described here (and in Teufel, Carletta, and Moens [1999] in more detail) tests the rhetorical annotation scheme presented in section "
"We use the kappa coefficient K (Siegel and Castellan 1988) to measure stability and reproducibility, following Carletta (1996).  "
We now describe an automatic system that can perform extraction and classification of rhetorical status on unseen text (cf. also a prior version of the system reported in Teufel and Moens [2000] and Teufel [1999]). 
"To test if such a simple approach would be enough, we performed a text categorization experiment, using the Rainbow mplementation of a naı̈ve Bayes term requency times inverse document frequency (TF*IDF) method (McCallum 1997) and onsidering each sentence as a document. "
"This is not surprising, given that the definition of our task has little to do with the distribution of “content-bearing” words and phrases, much less so than the related task of topic segmentation (Morris and Hirst 1991; Hearst 1997; Choi 2000), or Saggion and Lapalme’s (2000) approach to the summarization of scientific articles, which relies on scientific concepts and their relations."
"We use a naı̈ve Bayesian model as in Kupiec, Pedersen, and Chen’s (1995) experiment (cf. Figure 9)."
"Rhetorical zones appear in typical positions in the article, as scientific argumentation follows certain patterns (Swales 1990)."
"Kupiec, Pedersen, and Chen (1995) report sentence length as a useful feature for text extraction. "
Sentences containing many “content-bearing” words have been hypothesized to be good candidates for text extraction. Baxendale (1958) extracted all words except those on the stop list from the title and the headlines and determined for each sentence whether or not it contained these words.  
How content-bearing a word is can also be measured with frequency counts (Salton and McGill 1983). 
"Linguistic features like tense and voice often correlate with rhetorical zones; Biber (1995) and Riley (1991) show correlation of tense and voice with prototyp- ical section structure (“method,” “introduction”). "
"In order to avoid a full Viterbi search of all possibilities, we perform a beam search with width of three among the candidates of the previous sentence, following Barzilay et al. (2000). "
We measured associations using the log-likelihood measure (Dunning 1993) for each combination of target category and semantic class by converting each cell of the contingency into a 2×2 contingency table. 
"Table 10 shows the results in terms of three overall measures: kappa, percentage accuracy, and macro-F (following Lewis [1991]).  "
"When looking at the numerical values, however, one should keep in mind that macroaveraging results are in general numerically lower (Yang and Liu 1999). "
An extrinsic evaluation additionally showed that the end result provides considerable added value when compared to sentence extracts (Teufel 2001)
"Further work can be done on the semantic verb clusters described in section 4.2. Klavans and Kan (1998), who use verb clusters for document classification according to genre, observe that verb information is rarely used in current practical natural language Applications. "
"There are good reasons for using such a hand-crafted, genre-specific verb lexicon instead of a general resource such as WordNet or Levin’s (1993) classes "
"In our experiments, we used the Hidden Markov Model (HMM) tagging method described in [Cutting et al, 1992]."
"The HMM training and tagging programs in our experiment [Wilkens and Kupiec, 1995] are based on bigrams, i.e. only the immediate context of a word is taken into account."
"For the part-of-speech tagging problem, it is known that assigning the most common part of speech for each lexical item gives a baseline of 90% accuracy [Brill, 1992]. "
"Corpus-based information can be represented e.g. as neural networks (Eineborg and Gamback 1994; Schmid 1994), local rules (Brill 1992), or collocational matrices (Garside 1987). "
"Syntactic analysis is carried out in another reductionistic parsing framework known as Finite State Intersection Grammar (Koskenniemi 1990; Koskenniemi, Tapanainen and Voutilainen 1992; Tapanainen 1992; Voutilainen and Tapanainen 1993; Voutilainen 1994). "
"Several taggers based on rules, stochastic models, neural networks, and hybridsystems have already been presented for Part-of-speech (POS) tagging. Rule-basedtaggers (Brill 1992; Elenius 1990; Jacobs and Zernik 1988; Karlsson 1990; Karlsson etal. 1991; Voutilainen, Heikkila, and Antitila 1992; Voutilainen and Tapanainen 1993)use POS-dependent constraints defined by experienced linguists."
"In the case where additionally raw untagged text isavailable, the Maximum Likelihood training can be used to reestimate the parametersof H M M taggers (Merialdo 1994)."
Connectionist models have been used successfully for lexical acquisition (Eineborgand Gamback 1993; Elenius 1990; Elenius and Carlson 1989; Nakamura et al. 1990).
"In taggers thatare based on hidden Markov models (HMM), parameters of the unknown words areestimated by taking into account morphological information from the last part of theword (Dermatas and Kokkinakis 1994; Maltese and Mancini 1991)."
"Similar results have beenreported by Maltese and Mancini (1991) for the Italian language. Weischedel et al.(1993) have used four categories of word morphology, such as inflectional endings,derivational endings, hyphenation, and capitalization."
"When the training text is adequate to estimate the tagger parameters, moreefficient stochastic taggers (Dermatas and Kokkinakis 1994; Maltese and Mancini 1991;Weischedel et al. 1993) and training methods can be implemented (Merialdo 1994)."
"The first-(Rabiner 1989) and second- (He 1988) order Viterbi algorithms have been presentedelsewhere. Recently, Tao (1992) described the Viterbi algorithm for generalized HMMs."
The optimum tag tio is estimated using the probabilities of the forward-backwardalgorithm (Rabiner 1989)
The probabilities in equation 4 are estimated recursively for the first- (Rabiner1989) and second-order HMM (Watson and Chung 1992).
The scaling processintroduced in this case multiplies the forward and backward probabilities by a scalingfactor at selective word events in order to keep the computations within the floating-point dynamic range of the computer (Rabiner 1989).
"Currently, most of the POS tagger accuracy reports are based on the experiments involving Penn Treebank data (Marcus, 1993). "
"For example, while the TnT tagger performs at 97% accuracy on known words in the Treebank, the accuracy drops to 89% on unknown words (Brandts, 2000). "
"The LT POS tagger is reported to perform at 93.6-94.3% accuracy on known words and at 87.7-88.7% on unknown words using a cascading unknown word “guesser” (Mikheev, 1997). "
"The only way to avoid it, is to anonymize the notes prior to POS tagging which in itself is a difficult and expensive process (Ruch et al. 2000).  "
"In order to test some of our assumptions regarding how the differences between general English language and the language of clinical notes may affect POS tagging, we have trained the HMM-based TnT tagger (Brandts, 2000) with default parameters at the tri-gram level both on Penn Treebank and the clinical notes data. "
"Since keyboard input is most efficient for assigning categories to words and phrases, cf. (Lehmann et al.,- 1996; Marcus et al., 1994), and structural manipulations are executed most efficiently using the mouse, both an elaborate keyboard and optical interface is Provided. "
"Since broad-coverage parsers for German, especially robust parsers that assign predicate-argument structure and allow crossing branches, are not available, or require an annotated traing corpus (cf. (Collins, 1996), (Eisner, 1996))."
"The task can be performed by a chunk parser that is equipped with an appropriate finite state grammar (Abney, 1996). "
"The contexts are smoothed by linear interpolation of unigrams, bigrams, and trigrams. Their weights are calculated by deleted interpolation (Brown et al., 1992). "
"For example, (Briscoe and Carroll, 1993) train an LR parser based on a general grammar to be able to distinguish between likely and unlikely sequences of parsing actions; (Andry et al., 1994) automatically infer sortal constraints, that can be used to rule out otherwise grammatical constituents; and (Grishman et al., 1984) describes methods that reduce the size of a general grammar to include only rules actually useful for parsing the training corpus. "
"Constituent pruning is a bottom-up approach, and is complemented by a second, top-down, method based on Explanation-Based Learning (EBL; (Mitchell et al., 1986; van Harmelen and Bundy, 1988)). "
"The scheme is fully implemented within a version of the Spoken Language Translator system (Rayner et al., 1993; Agniis et al., 1994), and is normally applied to input in the form of small lattices of hypotheses produced by a speech recognizer. "
"That is, an assumption of full statistical dependence (Yarowsky, 1994), rather than the more common full independence, is made  "
"The basic corpus used was a set of 16,000 utterances from the Air Travel Planning (ATIS; (Hemphill et al., 1990)) domain. "
"Care was taken to ensure not just that the utterances themselves, but also the speakers of the utterances were disjoint between test and training data; as pointed out in (Rayner et al., 1994a), failure to observe these precautions can result in substantial spurious improvements in test data results. "
"The 16,000 sentence corpus was analysed by the SRI Core Language Engine (Alshawi (ed), 1992), using a lexicon extended to cover the ATIS domain (Rayner, 1994). "
"The four sets of outputs from the parser were then translated into Swedish by the SLT transfer and generation mechanism (Agn et al., 1994). "
"Making such an assumption is reasonable since POS taggers that can achieve accuracy of 96% are readily available to assign POS to unrestricted English sentences (Brill, 1992; Cutting et al., 1992)."
"To evaluate the performance of LEXAS, we conducted two tests, one on a common data set used in (Bruce and Wiebe, 1994), and another on a larger data set that we separately collected. "
"One exception is the sense-tagged data set used in (Bruce and Wiebe, 1994), which has been made available in the public domain by Bruce and Wiebe. "
"Note that the sense definitions used in this data set are those from Longman Dictionary of Contemporary English (LDOCE) (Procter, 1978). "
"Bruce and Wiebe also performed a separate test by using a subset of the ""interest"" data set with only 4 senses (sense 1, 4, 5, and 6), so as to compare their results with previous work on WSD (Black, 1988; Zernik, 1990; Yarowsky, 1992), which were tested on 4 senses of the noun ""interest"". However, the work of (Black, 1988; Zernik, 1990; Yarowsky, 1992) were not based on the present set of sentences, so the comparison is only suggestive. "
"Previous work on using the unordered set of surrounding words have used a much larger window, such as the 100-word window of (Yarowsky, 1992), and the 2-sentence context of (Leacock et al., 1993). "
"Our experimental finding, t h a t local collocations are the most predictive, agrees with past observa- tion that humans need a narrow window of only a few words to perform WSD (Choueka and Lusignan, 1985). "
"To get an idea of how the sense assignments of our d a t a set compare with those provided by WoRDNET linguists in SEMCOR, the sense-tagged subset of Brown corpus prepared by Miller et al. (Miller et al., 1994), we compare a subset of the occurrences that overlap. "
"This should not be too surprising, as it is widely believed that sense tagging using the full set of refined senses found in a large dictionary like WORDNET involve making subtle human judgments (Wilks et al., 1990; Bruce and Wiebe, 1994), such that there are many genuine cases where two humans will not agree fully on the best sense assignments."
"This default strategy has been advocated as the baseline performance level for comparison with WSD programs (Gale et al., 1992)."
"The work of (Cardie, 1993) used a case-based approach that simultaneously learns part of speech, word sense, and concept activation knowledge, although the method is only tested on domain-specific texts with domain-specific word senses. "
"We also implemented a version of Hobbs’s (1978) well-known pronoun interpretation algorithm as a baseline, in which no machine learning is involved. "
"first, the linguistic ap-proach, in which the model is written by a linguist,generally in the form of rules or constraints (Vouti-lainen and Jgrvinen, 1995). Second, the automaticapproach, in which the model is automatically ob-tained from corpora (either raw or annotated) 1 , andconsists of n-grams (Garside et al., 1987; Cuttinget ah, 1992), rules (Hindle, 1989) or neural nets(Schmid, 1994)."
"Thehigh level data trend acquires more sophisticated in-formation, such as context rules, constraints, or de-cision trees (Daelemans et al., 1996; M/~rquez andRodriguez, 1995; Samuelsson et al., 1996)."
"The ac-quisition methods range from supervised-inductive-learning-from-example algorithms (Quinlan, 1986; A h a et al., 1991) to genetic algorithm strategies(Losee, 1994), through the transformation-basederror-driven algorithm used in (Brill, 1995), Stillanother possibility are the hybrid models, which tryto join the advantages of both approaches (Vouti-lainen and Padr6, 1997)."
"The constraint language is able to express thesame kind of patterns than the Constraint Gram-mar formalism (Karlsson et al., 1995), although in adifferent formalism."
"Decision trees,recently used in NLP basic tasks such as taggingand parsing (McCarthy and Lehnert, 1995: Daele-mans et al., 1996; Magerman, 1996), are suitable forperforming this task."
"Itconstructs the trees in a top - down way, guided bythe distributional information of the examples, butnot on the examples order (Quinlan, 1986)."
"There exist two main families of attribute-selecting functions: information-based (Quinlan, 1986: Ldpez, 1991) and statistically--based (Breiman et al., 1984; Mingers, 1989). "
"Some s.vs terns perform a previous recasting of the attributes in order to have only binary-valued attributes and to deal with binary trees- (Magerman, 1996)."
"Experimental tests (M&rquez and Rodriguez, 1995) have shown that the pruning process reduces tree sizes at about 50% and improves their accuracy in a 2-5%. "
"The compatibility value for each constraint is the mutual information between the tag and the context (Cover and Thomas, 1991). "
Usual tagging algorithms are either n - g r a m oriented -such as Viterbi algorithm (Viterbi. 1967)- or a d - hoc for every case when they must deal with more complex information.
"The algorithm has been applied to part-of-speech tagging (Padr6, 1996), and to shallow parsing (Voutilainen and Padro. 1997).  "
"In addition, M L indicates a baseline model conraining no constraints (this will result in a most- likely tagger) and H M M stands for a hidden Markov model bigram tagger (Elworthy, 1992). "
"The two systems we use are E N G C G (Karlsson et al., 1994) and the Xerox Tagger (Cutting et al., 1992). We discuss problems caused by the fact that these taggers use different tag sets, and present the results obtained by applying the combined taggers to a previously unseen sample of text. "
"The English Constraint Grammar Parser, ENGCG  Voutilainen et al., 1992; Karlsson el al., 1994), is based on Constraint Grammar, a parsing framework proposed by Fred Karlsson (1990). "
"The ENGTWOL lexicon is based on the two-level  model (Koskenniemi, 1983). "
"The Xerox Tagger 1, XT, (Cutting et al., 1992) is a statistical tagger made by Doug Cutting, Julian Kupiec, Jan Pedersen and Penelope Sibun in Xerox PARC. It was trained on the untagged Brown Corpus (Francis and Kubera, 1982). "
"The tagger itself is based on the Hidden Markov Model (Baum, 1972) and word equivalence classes (Kupiec, 1989). "
"The system was tested against 26,711 words of newspaper text from The Wall Street Journal, The Economist and Today, all taken from the 200-million word Bank of English corpus by the COBUILD team at the University of Birmingham, England (see also (J/irvinen, 1994)). "
"The differences were jointly examined by the judges to see whether they were caused by inattention or by a genuine difference of opinion that could not be resolved by consulting the documentation that outlines the principles adopted for this grammatical representation (for the most part documented in (Karlsson et al., 1994)).  "
"We could use partly disambiguated text (e.g. the output of parsers D1, D2 or D3~) and disambiguate the result using a knowledge-based syntactic parser (see experiments in (Vou- tilainen and Tapanainen, 1993)).  "
"Corpus-derived distributional semantic spaces have proved valuable in tackling a variety of tasks, ranging from concept categorization to relation extraction to many others (Sahlgren, 2006; Turney, 2006; Padó and Lapata, 2007). "
"From a cognitive angle, corpus-based models hold promise as simulations of how humans acquire and use conceptual and linguistic information from their environment (Landauer and DuMais, 1997). "
"a database of interconnected concepts and properties (Rogers and McClelland, 2004), adapting the information stored there to the task at hand "
Such tasks will require an extension of the current framework of Turney (2008) beyond evidence from the direct cooccurrence of target word pairs. 
"Concept similarity is often measured by vectors of co-occurrence with context words that are typed with dependency information (Lin, 1998; Curran and Moens, 2002). "
"Detecting whether a pair expresses a target relation by looking at shared connector patterns with model pairs is a common strategy in relation extraction (Pantel and Pennacchiotti, 2008). "
"We computed the weight (strength of association) for all the tuples extracted in this way using the local MI measure (Evert, 2005), that is theoretically justified, easy to compute for triples and robust against overestimation of rare events. "
"The myPlain model implements a classic “flat” co-occurrence approach (Sahlgren, 2006) in which we keep track of verb-to-noun co-occurrence within a window that can include, maximally, one intervening noun, and noun-to-noun co-occurrence with no more than 2 intervening nouns. "
"The myHAL model uses the same co-occurrence window, but, like HAL (Lund and Burgess, 1996), treats left and right co-occurrences as distinct features. "
"Like in the DV model of Padó and Lapata (2007), only pairs connected by target links are preserved, but the links themselves are not part of the model. "
"Much work in computational linguistics and related fields relies on measuring similarity among words/concepts in terms of their patterns of co-occurrence with other words/concepts (Sahlgren, 2006). "
"In the example, the patterns of co-occurrence suggest that objects of killing are rather similar to subjects of dying, hinting at the classic cause(subj,die(obj)) analysis of killing by Dowty (1977) and many others. "
We took 232 causative/inchoative verbs and 170 non- alternating transitive verbs from Levin (1993).  
In  it was observed that a significant percent of the queries made by a user in a search engine are associated to a repeated search Output sequence optimization Rather than basing classifications only on model parameters estimated from cooccurrences between input and output symbols employed for maximizing the likelihood of pointwise singlelabel predictions at the output level  classifier output may be augmented by an optimization over the output sequence as a whole using optimization techniques such as beam searching in the space of a conditional markov models output  or hidden markov models  
Dredze et al yielded the second highest score1 in the domain adaptation track  The IBM models  search a version of permutation space with a onetomany constraint  propose the use of language models for sentiment analysis task and subjectivity extraction 
In training process  we use GIZA   4 toolkit for word alignment in both translation directions  and apply growdiagfinal method to refine it  The models in the comparative study by  did not include such features  and so  again for consistency of comparison  we experimentally verified that our maximum entropy model  a  consistently yielded higher scores than when the features were not used  and  b  consistently yielded higher scores than nave Bayes using the same features  in agreement with  
 and  et al We used the WordNet   Similarity package  to compute baseline scores for several existing measures  noting that one word pair was not processed in WS353 because one of the words was missing from WordNet 
We use MER  to tune the decoders parameters using a development data set The training set is extracted from TreeBank  section 1518  the development set  used in tuning parameters of the system  from section 20  and the test set from section 21 For nonlocal features  we adapt cube pruning from forest rescoring   since the situation here is analogous to machine translation decoding with integrated language models  we can view the scores of unit nonlocal features as the language model cost  computed onthefly when combining subconstituents 
31 Agreement for Emotion Classes The kappa coefficient of agreement is a statistic adopted by the Computational Linguistics community as a standard measure for this purpose  ITGs translate into simple  22   BRCGs in the following way  see  for a definition of ITGs 
This may be because their system was not tuned using minimum error rate training  However  most of the existing models have been developed for English and trained on the Penn Treebank   which raises the question whether these models generalize to other languages  and to annotation schemes that differ from the Penn Treebank markup 
Following   we used sections 018 of the Wall Street Journal  WSJ  corpus for training  sections 1921 for development  and sections 2224 for final evaluation 
In   the authors provide some sample subtrees resulting from such a 1000word clustering We took part the Multilingual Track of all ten languages provided by the CoNLL2007 shared task organizer  To set the weights  m  we carried out minimum error rate training  using BLEU  as the objective function 
Use of global features for structured prediction problem has been explored by several NLP applications such as sequential labeling  and dependency parsing  with a great deal of success It is important to realize that the output of all mentioned processing steps is noisy and contains plenty of mistakes  since the data has huge variability in terms of quality  style  genres  domains etc  and domain adaptation for the NLP tasks involved is still an open problem  
They are also used for inducing alignments  In recent work   proposed a general framework for including morphological features in a phrasebased SMT system by factoring the representation of words into a vector of morphological features and allowing a phrasebased MT system to work on any of the factored representations  which is implemented in the Moses system 
2 Architecture of the system The goal of statistical machine translation  SMT  is to produce a target sentence e from a source sentence f It is today common practice to use phrases as translation units  and a log linear framework in order to introduce several models explaining the translation process  e    argmaxp  e f   argmaxe LCB exp  summationdisplay i ihi  e  f   RCB  1  The feature functions hi are the system models and the i weights are typically optimized to maximize a scoring function on a development set  
1 Introduction Sentiment analysis have been widely conducted in several domains such as movie reviews  product reviews  news and blog reviews  Their approaches include the use of a vectorbased information retrieval technique   binbash  line 1  a  command not found Our do  mains are more varied  which may results in more recognition errors 
The corpus was aligned with GIZA    and symmetrized with the growdiagfinaland heuristic  BLEU  was devised to provide automatic evaluation of MT output Statistics in linguistics  Oxford  Basil Blackwell rawString citation citation validtrue authors author N Chinchor author authors title Evaluating message understanding systems  an analysis of the third Message Understanding Conference  MUC3 title date 1993 date journal Computational Linguistics journal volume 19 volume pages 409  449 pages marker Chinchor  1993 marker rawString Chinchor  N  et al  1993 
Note that it is straightforward to calculate these expected counts using a variant of the insideoutside algorithm  applied to the  dependencyparsing data structures  for projective dependency structures  or the matrixtree theorem  for nonprojective dependency structures 
Following   we consider an anaphoric reference  NPi  correctly resolved if NPi and its closest antecedent are in the same coreference chain in the resulting partition This algorithm adjusts the loglinear weights so that BLEU  is maximized over a given development set 
We discriminatively trained our parser in an online fashion using a variant of the voted perceptron  In fact  we found that it doesnt do so badly at all  the bitag HMM estimated by EM achieves a mean 1to1 tagging accuracy of 40   which is approximately the same as the 413  reported by  for their sophisticated MRF model  Motivation and Prior Work While several authors have looked at the supervised adaptation case  there are less  and especially less successful  studies on semisupervised domain adaptation  
There are other types of variations for phrases  for example  insertion  deletion or substitution of words  and permutation of words such as view point and point of view are such variations    search engines   uses the Altavista web browser  while we consider and combine the frequency information acquired from three web search engines 
Many methods have been proposed to measure the cooccurrence relation between two words such as 2   mutual information   ttest   and loglikelihood  It has been argued that the reliability of a coding schema can be assessed only on the basis of judgments made by naive coders  
to the pairwise TER alignment described in  We obtain aligned parallel sentences and the phrase table after the training of Moses  which includes running GIZA     growdiagonalfinal symmetrization and phrase extraction  
From the above discussion  we can see that traditional tree sequencebased method uses single tree as translation input while the forestbased model uses single subtree as the basic translation unit that can only learn treetostring  rules 
Our baseline method for ambiguity resolution is the Collins parser as implemented by Bikel  We then built separate EnglishtoSpanish and SpanishtoEnglish directed word alignments using IBM model 4   combined them using the intersect  grow heuristic   and extracted phraselevel translation pairs of maximum length 7 using the alignment template approach  
For instance  both Pang and Lee  and   consider the thumbs upthumbs down decision  is a film review positive or negative  Binarizing the syntax trees for syntaxbased machine translation is similar in spirit to generalizing parsing models via markovization  4 Options from the Translation Table Phrasebased statistical machine translation methods acquire their translation knowledge in form of large phrase translation tables automatically from large amounts of translated texts  
For example   collected reviews from a movie database and rated them as positive  negative  or neutral based on the rating  eg  number of stars  given by the reviewer The earliest work in this direction are those of          and   established that it is important to tune  the tradeoff between Precision and Recall  to maximize performance 
Moreover  under this view  SMT becomes quite similar to sequential natural language annotation problems such as partofspeech tagging and shallow parsing  and the novel training algorithm presented in this paper is actually most similar to work on training algorithms presented for these task  eg the online training algorithm presented in  and the perceptron training algorithm presented in  Feature weights vector are trained discriminatively in concert with the language model weight to maximize the BLEU  automatic evaluation metric via Minimum Error Rate Training  MERT   
 and Wiebe  2000  focused on learning adjectives and adjectival phrases and Wiebe et al A number of alignment techniques have been proposed  varying from statistical methods  to lexical methods  
Even though there are some studies that compare the results from statistically computed association measures with word association norms from psycholinguistic experiments  there has not been any research on the usage of a digital  networkbased dictionary reflecting the organization of the mental lexicon to our knowledge 
Finally  we use as a feature the mappings produced in  of WordNet senses to Oxford English Dictionary senses 3 Semantic Representation 31 The Need for Dependencies Perhaps the most common representation of text for assessing content is BagOfWords or BagofNGrams  First  we adopt an ONTOLOGICALLY PROMISCUOUS representation  that includes a wide variety of types of entities 
For each training direction  we run GIZA     specifying 5 iterations of Model 1  4 iterations of the HMM model   and 4 iterations of Model 4 To set the weight vector w  we train twenty averaged perceptrons  on different shuffles of data drawn from sections 0221 of the Penn Treebank 
In the refined model 2  alignment probabilities a  ilj  l  m  are included to model the effect that the position of a word influences the position of its translation 6 Conclusions and Future Directions In previous work  statistical NLP computation over large corpora has been a slow  of ine process  as in KNOWITALL  and also in PMIIR applications such as sentiment classi cation  
The first solution might also introduce errors elsewhere As  already noted  ` While this automatic derivation process introduced a small percentage of errors on its own  it was the only practical way both to provide the amount of training data required and to allow for fullyautomatic testing  1 To train their system  R&M used a 200kword chunk of the Penn Treebank Parsed Wall Street Journal  tagged using a transformationbased tagger  and extracted base noun phrases from its parses by selecting noun phrases that contained no nested noun phrases and further processing the data with some heuristics  like treating the possessive marker as the first word of a new base noun phrase  to flatten the recursive structure of the parse 
This can either be semisupervised parsing  using both annotated and unannotated data  or unsupervised parsing  training entirely on unannotated text In   as well as other similar works   only lefttoright search was employed 
This iterative optimiser  derived from a word disambiguation technique   finds the nearest local maximum in the lexical cooccurrence network from each concept seed This model is related to the averaged perceptron algorithm of  
a22 a14 is the sufficient statistic of a16 a14 Then  we can rewrite a2a24a3 a10a27 a42a7 a25 as  a5a7a6a9a8a11a10 a23 a3 a10 a7 a15 a27 a25a18a17a26a25 a12a28a27 a5a7a6a29a8a30a10 a23 a3 a10 a7 a15 a27 a25a18a17 3 Loss Functions for Label Sequences Given the theoretical advantages of discriminative models over generative models and the empirical support by   and that CRFs are the stateoftheart among discriminative models for label sequences  we chose CRFs as our model  and trained by optimizing various objective functions a31 a3 a10a36 a25 with respect to the corpus a36 The application of these models to the label sequence problems vary widely 
Pustejovsky confronted with the problem of automatic acquisition more extensively in  PropBank encodes propositional information by adding a layer of argument structure annotation to the syntactic structures of the Penn Treebank  
Jiao et al propose semisupervised conditional random fields  that try to maximize the conditional loglikelihood on the training data and simultaneously minimize the conditional entropy of the class labels on the unlabeled data  report extracting database records by learning record field compatibility Unfortunately  a counterexample illustrated in  shows that the max function does not produce valid kernels in general 
2 Detecting DiscourseNew Definite Descriptions 21 Vieira and Poesio Poesio and Vieira  carried out corpus studies indicating that in corpora like the Wall Street Journal portion of the Penn Treebank   around 52  of DDs are discoursenew   and another 15  or so are bridging references  for a total of about 6667  firstmention The distinction between lexical and relational similarity for word pair comparison is recognized by   hecallstheformer attributional similarity   though the methods he presents focus on relational similarity 
The POS disambiguation has usually been performed by statistical approaches mainly using hidden markov model  HMM    et al  1992  Kupiec As a baseline model we used a maximum entropy tagger  very similar to the one described in  
We assign tags of partofspeech  POS  to the words with MXPOST that adopts the Penn Treebank tag set  Such coarsegrained inventories can be produced manually from scratch  or by automatically relating  or clustering  existing word senses  a contextual word cw that occurs in the paragraphs of bc  a loglikelihood ratio  G2  test is employed   which checks if the distribution of cw in bc is similar to the distribution of cw in rc  p  cw bc   p  cw rc   null hypothesis  
In this paper we use the socalled Model 4 from  We would expect the opposite effect with handaligned data  Extensions to Hiero Several authors describe extensions to Hiero  to incorporate additional syntactic information   or to combine it with discriminative latent models  The other form of hybridization   a statistical MT model that is based on a deeper analysis of the syntactic 33 structure of a sentence   has also long been identified as a desirable objective in principle  consider   
272 Similaritybased estimation was first used for language modeling in the cooccurrence smoothing method of Essen and Steinbiss   derived from work on acoustic model smoothing by Sugawara et al Following the setup in   we initialize the transition and emission distributions to be uniform with a small amount of noise  and run EM and VB for 1000 iterations 
Our method uses assumptions similar to  et al 1996 but is naturally suitable for distributed parallel computations The agreement on identifying the boundaries of units  using the statistic discussed in   was  9  for two annotators and 500 units   the agreement on features  2 annotators and at least 200 units  was as follows  UTYPE   76  VERBED   9  FINITE   81 
In comparison we introduce 28 several metrics coefficients reported in Albrecht and Hwa  including smoothed BLEU   METEOR   HWCM   and the metric proposed in Albrecht and Hwa  using the full feature set This is one manifestation of what is commonly referred to as the data sparseness problem  and was discussed by  as a sideeffect of specificity 
Techniques for weakening the independence assumptions made by the IBM models 1 and 2 have been proposed in recent work  C3BTC5 and CCCDCA were used in  and   respectively 
In this work we will use structured linear classifiers  This is the best automatically learned partofspeech tagging result known to us  representing an error reduction of 44  on the model presented in   using the same data splits  and a larger error reduction of 121  from the more similar best previous loglinear model in Toutanova and Manning  
The template we use here is similar to   but we have added extra context words before the X and after the Y Our morphological processing also differs from  
The algorithm employs the OpenNLP MaxEnt implementation of the maximum entropy classification algorithm  to develop word sense recognition signatures for each lemma which predicts the most likely sense for the lemma according to the context in which the lemma occurs 
Alternatively  order is modelled in terms of movement of automatically induced hierarchical structure of sentences  Parameters used to calculate P  D  are trained using MER training  on development data 
Chiang  distinguishes statistical MT approaches that are syntactic in a formal sense  going beyond the nitestate underpinnings of phrasebased models  from approaches that are syntactic in a linguistic sense  ie taking advantage of a priori language knowledge in the form of annotations derived from human linguistic analysis or treebanking1 The two forms of syntactic modeling are doubly dissociable  current research frameworks include systems that are nite state but informed by linguistic annotation prior to training  eg     and also include systems employing contextfree models trained on parallel text without bene t of any prior linguistic analysis  eg 
This can be done in a supervised   a semisupervised  or a fully unsupervised way  Equation  3  reads If the target noun appears  then it is distinguished by the majority The loglikelihood ratio  decides in which order rules are applied to the target noun in novel context 
Statisticbased algorithms based on Belief Network  such as HiddenMarkovModel  HMM     Lexicalized HMM  and MaximalEntropy model  use the statistical information of a manually tagged corpus as background knowledge to tag new sentences One way of resolving query ambiguities is to use the statistics  such as mutual information   to measure associations of query terms  on the basis of existing corpora  
The wn   similarity package  to compute the Jiang & Conrath  J&C  distance  as in   propose using a statistical word alignment algorithm as a more robust way of aligning  monolingual  outputs into a confusion network for system com2  construct lattices over paraphrases using an iterative pairwise multiple sequence alignment  MSA  algorithm 
One of the first large scale hand tagging efforts is reported in   where a subset of the Brown corpus was tagged with WordNet July 2002  pp 42 Experiments To build all alignment systems  we start with 5 iterations of Model 1 followed by 4 iterations of HMM   as implemented in GIZA    
The usual recall and precision metrics  eg  how many of the interesting bits of information were detected  and how many of the found bits were actually correct  require either a test corpus previously annotated with the required information  or manual evaluation  
The CRF tagger was implemented in MALLET  using the original feature templates from  Given a set of terms with unknown sentiment orientation   then uses the PMIIR algorithm  to issue queries to the web and determine  for each of these terms  its pointwise mutual information  PMI  with the two seed words across a large set of documents 
This is similartothegraphconstructionmethodof  and Rao et al We distinguish two main approaches to domain adaptation that have been addressed in the literature   supervised and semisupervised 
SmadjaFrank1993The experimental results in  show a negative impact on the parsing accuracy from too long dependency relation k   P  A  P  E   3  1P  E   suggests that the units over which the kappa statistic is computed affects the outcome 
Among the chunk types  NP chunking is the first to receive the attention   than other chunk types  such as VP and PP chunking  This model shares some similarities with the stochastic inversion transduction grammars  SITG  presented by Wu in  
  makes a similar point  noting that for reviews  the whole is not necessarily the sum of the parts   Identifying transliteration pairs is an important component in many linguistic applications which require identifying outofvocabulary words  such as machine translation and multilingual information retrieval  
 and Chan et al Based on these grammars  a great number of SMT models have been recently proposed  including stringtostring model  Synchronous FSG    treetostring model  TSGstring    stringtotree model  stringCFGTSG    treetotree model  Synchronous CFGTSG  DataOriented Translation   and so on 
We use a standard maximum entropy classifier  implemented as part of MALLET  8412 only PTB  baseline  8358 1st  8342 2nd  8338 3rd  8308 third row lists the three highest scores of the domain adaptation track of the CoNLL 2007 shared task 
Johnson 1997 notes that this structure has a higher probability than the correct flat structure given counts taken from the treebank for a standard PCFGWe used a loglinear model with no Markov dependency between adjacent tags 3 and trained the parameters of the model with the perceptron algorithm  with averaging to control for overtraining  
In Turneys work  the cooccurrence is considered as the appearance in the same window  Named entities also pose another problem with the  coreference model  since it models only the heads of NPs  it will fail to resolve some references to named entities   Ford Motor Co  Ford   while erroneously merging others   Ford Motor Co  Lockheed Martin Co  
For the constituentbased models  constituent information was obtained from the output of  for English and Dubeys parser  2004  for German 
This finding has been previously reported  among others  in  In order increase the likelihood that 909 only true paraphrases were considered as phraselevel alternations for an example  extracted sentences were clustered using completelink clustering using a technique proposed in  
As reported in   parameter averaging can effectively avoid overfitting Several representations to encode region information are proposed and examined  
One important application of bitext maps is the construction of translation lexicons  and  as discussed  translation lexicons are an important information source for bitext mapping This method is described hereafter  while the subsequent steps  that use deeper  rulebased  levels of knowledge  are implemented into the ARIOSTO_LEX lexical learning system  described in  
This can be the base of a principled method for detecting structural contradictions  6 Related Work Several works attempt to extend WordNet with additional lexical semantic information  
In order to improve sentencelevel evaluation performance  several metrics have been proposed  including ROUGEW  ROUGES  and METEOR  
Besides the the casesensitive BLEU4  used in the two experiments  we design another evaluation metrics Reordering Accuracy  RAcc  for forced decoding evaluation 
2 Syntacticoriented evaluation metrics We investigated the following metrics oriented on the syntactic structure of a translation output  POSBLEU The standard BLEU score  calculated on POS tags instead of words  POSP POS ngram precision  percentage of POS ngrams in the hypothesis which have a counterpart in the reference  POSR Recall measure based on POS ngrams  percentage of POS ngrams in the reference which are also present in the hypothesis  POSF POS ngram based Fmeasure  takes into account all POS ngrams which have a counter29 part  both in the reference and in the hypothesis 
4 Building Noun Similarity Lists A lot of work has been done in the NLP community on clustering words according to their meaning in text  
In other words   4b  can be used in substitution of  4a   whereas  5b  can not  so easily 41n   a value of K between 8 and I indicates good agreement  a value between 6 and 8 indicates some agreement 
For example   have studied synchronous context free grammar 
 Peter F Brown  Vincent J Della Pietra  Petere V deSouza  Jenifer C Lai  and Robert L Mercer 
Table 1 reports values for the Kappa  K  coefficient of agreement  for Forward and Backward Functions 6 The columns in the tables read as follows  if utterance Ui has tag X  do coders agree on the subtag  
When we have a junction tree for each document  we can efficiently perform belief propagation in order to compute argmax in Equation  1   or the marginal probabilities of cliques and labels  necessary for the parameter estimation of machine learning classifiers  including perceptrons   and maximum entropy models  
   concordancing for bilingual lexicography   computerassisted language learning  corpus linguistics  Melby 
In particular  previous work  has investigated the use of Markov random fields  MRFs  or loglinear models as probabilistic models with global features for parsing and other NLP tasks 
The model consists of a set of wordpair parameters p  t  s  and position parameters p  j  i     in model 1  IBM1  the latter are fixed at 1   1  1   as each position  including the empty position 0  is considered equally likely to contain a translation for w Maximum likelihood estimates for these parameters can be obtained with the EM algorithm over a bilingual training corpus  as described in  
The candidates of unknown words can be generated by heuristic rules  or statistical word models which predict the probabilities for any strings to be unknown words  
For each differently tokenized corpus  we computed word alignments by a HMM translation model  and by a word alignment refinement heuristic of growdiagfinal  
On the other hand  purely statistical systems  extract discriminating MWUs from text corpora by means of association measure regularities 
The chunker is trained on the answer side of the Training corpus in order to learn 2 and 3word collocations  defined using the likelihood ratio of  
The f are trained using a heldout corpus using maximum BLEU training  
Standard CI Model 1 training  initialised with a uniform translation table so that t  ejf  is constant for all sourcetarget word pairs  f  e   was run on untagged data for 10 iterations in each direction  
According to the document  it is the output of Ratnaparkhis tagger  
1 Introduction Current methods for largescale information extraction take advantage of unstructured text available from either Web documents  or  more recently  logs of Web search queries  to acquire useful knowledge with minimal supervision 
6 The Experimental Results We used the Penn Treebank  to perform empirical experiments on this parsing model 
Given a source sentence f  the preferred translation output is determined by computing the lowestcost derivation  combination of hierarchical and glue rules  yielding f as its source side  where the cost of a derivation R1 Rn with respective feature vectors v1   vn Rm is given by msummationdisplay i  1 i nsummationdisplay j  1  vj  i Here  1   m are the parameters of the loglinear model  which we optimize on a heldout portion of the training set  using minimumerrorrate training  
Also related are the areas of word alignment for machine translation   induction of translation lexicons   and crosslanguage annotation projections to a second language  
In Table 6 we report our results  together with the stateoftheart from the ACL wiki5 and the scores of   PairClass  and from Amac Herdagdelens PairSpace system  that was trained on ukWaC 
The features used in this study are  the length of t  a singleparameter distortion penalty on phrase reordering in a  as described in   phrase translation model probabilities  and 4gram language model probabilities logp  t   using KneserNey smoothing as implemented in the SRILM toolkit 
We have computed the BLEU score  accumulated up to 4grams    the NIST score  accumulated up to 5grams    the General Text Matching  GTM  Fmeasure  e  12    and the METEOR measure  
As   we adopted an evaluation of mutual information as a cohesion measure of each cooccurrence 
The last two counts  CAUS and ANIM  were performed on a 29million word parsed corpus  gall Street Journal 1988  provided by Michael Collins   
The tagger used is thus one that does not need tagged and disambiguated material to be trained on  namely the XPOST originally constructed at Xerox Parc  
We perform word alignment using GIZA     symmetrize the alignments using the growdiagfinaland heuristic  and extract phrases up to length 3 
Congress of the Italian Association for Artificial Intelligence  Palermo  1991 B Boguraev  Building a Lexicon  the Contribution of Computers  IBM Report  TJ Watson Research Center  1991 M Brent  Automatic Aquisition of Subcategorization frames from Untagged Texts  in  N Calzolari  R Bindi  Acquisition of Lexical Information from Corpus  in  K W   P Hanks  Word Association Norms  Mutual Information  and Lexicography  Computational Linguistics  vol 
4 Maximum Entropy To explain our method  we l  riefly des   ribe the con   ept of maximum entrol  y Recently  many al  lnoaches l  ased on the maximum entroi  y lnodel have t  een applied to natural language processing  
Note that this early discarding is related to ideas behind cube pruning   which generates the top n most promising hypotheses  but in our method the decision not to generate hypotheses is guided by the quality of hypotheses on the result stack 
We build a subset S C   incrementally by iterating to adjoin a feature f E   which maximizes loglikelihood of the model to S This algorithm is called the Basic Feature Selection  
From this point of view  some of the measures used in the evaluation of Machine Translation systems  such as BLEU   have been imported into the summarization task 
For the multilingual dependency parsing track  which was the other track of the shared task  Nilsson et al achieved the best performance using an ensemble method  
However  by exploiting the fact that the underlying scores assigned to competing hypotheses  w  e  h  f   vary linearly wrt changes in the weight vector  w   proposed a strategy for finding the global minimum along any given search direction 
11 However  modeling word order under translation is notoriously difficult   and it is unclear how much improvement in accuracy a good model of word order would provide 
 present a system called BABAR that uses contextual role knowledge to do coreference resolution 
The system described in  also makes use of syntactic heuristics 
     Dave et al 
Our MT baseline system is based on Moses decoder  with word alignment obtained from GIZA    
1 Motivation A major component in phrasebased statistical Machine translation  PBSMT   is the table of conditional probabilities of phrase translation pairs 
     Dave et al 
12 Related Work Recently  discriminative methods for alignment have rivaled the quality of IBM Model 4 alignments  
The disambiguation algorithms also require that the semantic relatedness measures WordNet   Similarity  be installed 
An analysis of the alignments shows that smoothing the fertility probabilities significantly reduces the frequently occurring problem of rare words forming garbage collectors in that they tend to align with too many words in the other language  
Moses provides BLEU  and NIST   but Meteor  and TER  can easily be used instead 
In an evaluation on the PENN treebank   the parser outperformed other unlexicalized PCFG parsers in terms of labeled bracketing fscore 
Since the word support model and triple context matching model have been proposed in our previous work  at the SIGHAN bakeoff 2005  and 2006   the major descriptions of this paper is on the WBT model 
612 ROUGE evaluation Table 4 presents ROUGE scores  of each of humangenerated 250word surveys against each other 
Two are conditionalized phrasal models  each EM trained until performance degrades  CJPTM3 as described in  Phrasal ITG as described in Section 41 Three provide alignments for the surface heuristic  GIZA   with growdiagfinal  GDF  Viterbi Phrasal ITG with and without the noncompositional constraint We use the Pharaoh decoder  with the SMT Shared Task baseline system  
These words and phrases are usually compiled using different approaches  Hatzivassiloglou and McKeown  1997  Kaji and Kitsuregawa  2006   and Nasukawa  2006  Esuli and Sebastiani  2006  Breck et al  2007  Ding  Liu and Yu 
3 Online Learning Again following   we have used the single best MIRA   which is a variant of the voted perceptron  for structured prediction 
CPSTM  i   l This metric corresponds to the STM metric presented by  
Our experiments created translation modules for two evaluation corpora  written news stories from the Penn Treebank corpus  and spoken taskoriented dialogues from the TRAINS93 corpus  
CIT  
31 A Note on StateSplits Recent studies  suggest that categorysplits help in enhancing the performance of treebank grammars  and a previous study on MH  outlines specific POStags splits that improve MH parsing accuracy 
Given this  the mutual information ratio  is expressed by Formula 1 
In addition to sentence fusion  compression algorithms  and methods for expansion of a multiparallel corpus  are other instances of such methods 
by diagand symmetrization  
There are many research directions  eg  sentiment classification  classifying an opinion document as positive or negative    subjectivity classification  determining whether a sentence is subjective or objective  and its associated opinion    featuretopicbased sentiment analysis  assigning positive or negative sentiments to topics or product features   Hu and Liu 2004  Popescu and Etzioni  2005  Carenini et al  2005  Ku et al  2006  Kobayashi  Inui and Matsumoto  2007  Titov and  
23 Online Learning Again following   we have used the single best MIRA   which is a margin aware variant of perceptron  for structured prediction 
Reported work includes improved model variants  and applications such as web data extraction   scientific citation extraction   word alignment   and discourselevel chunking  
They are not used in LN  but they are known to be useful for WSD  
3 Building the CatVar The CatVar database was developed using a combination of resources and algorithms including the Lexical Conceptual Structure  LCS  Verb and Preposition Databases   the Brown Corpus section of the Penn Treebank   an English morphological analysis lexicon developed for PCKimmo  Englex    NOMLEX   Longman Dictionary of Contemporary English 2For a deeper discussion and classification of Porter stemmers errors  see  
POS tag the text using the tagger of  
The learning algorithm used for each stage of the classification task is a regularized variant of the structured Perceptron  
An alternative is to create an automatic system that uses a set of training questionanswer pairs to learn the appropriate questionanswer matching algorithm  
 compared two Bayesian inference algorithms  Variational Bayes and what we call here a pointwise collapsed Gibbs sampler  and found that Variational Bayes produced the best solution  and that the Gibbs sampler was extremely slow to converge and produced a worse solution than EM 
  Pereira and Tishby   and Pereira  Tishby  and Lee  propose methods that derive classes from the distributional properties of the corpus itself  while other authors use external information sources to define classes  Resnik  uses the taxonomy of WordNet    uses the categories of Roget s Thesaurus  Slator  and Liddy and Paik  use the subject codes in the LDOCE  Luk  uses conceptual sets built from the LDOCE definitions 
For the Brown corpus  we based our division on  
Much of the work in subjectivity analysis has been applied to English data  though work on other languages is growing  eg  Japanese data are used in   Chinese data are used in   and German data are used in  
Morphosyntacticinformationhas in fact been shown to significantlyimprove the extractionresults  
For the chunk part of the code  we adopt the Inside  Outside  and Between  IOB  encoding originating from  
Following   Iusevariational Bayes EM  during the Mstep for the transition distribution  l 1 j i  f  E  ni  j   i  f  E  n i   C i   3  f  v   exp   v    4  60  v   braceleftBigg g  v 1 2  ifv  7  v  1  1v ow 
Our results are similar to those for conventional phrasebased models  
While Kazama and Torisawa used a chunker  we parsed the definition sentence using Minipar  
Perhaps the most wellknown method is maximum marginal relevance  MMR    as well as crosssentence informational subsumption   mixture models   subtopic diversity   diversity penalty   and others 
Before training the classifiers  we perform feature ablation by imposing a count cutoff of 10  and by limiting the number of features to the top 75K features in terms of log likelihood ratio  
32 The parsers The parsers that we chose to evaluate are the C&C CCG parser   the Enju HPSG parser   the RASP parser   the Stanford parser   and the DCU postprocessor of PTB parsers   based on LFG and applied to the output of the Charniak and Johnson reranking parser 
Training via the voted perceptron algorithm  or using a maxmargin criterion also correspond to the first option  eg McCallum and Wellner   Finley and Joachims   
Furthermore  early work on classbased language models was inconclusive  
Measures of crosslanguage relatedness are useful for a large number of applications  including crosslanguage information retrieval   crosslanguage text classification   lexical choice in machine translation   induction of translation lexicons   crosslanguage annotation and resource projections to a second language  
Related Work The recent availability of large amounts of bilingual data has attracted interest in several areas  including sentence alignment   word alignment   alignment of groups of words   and statistical translation  
51 The baseline System used for comparison was Pharaoh   which uses a beam search algorithm for decoding 
1 Introduction Base noun phrases  baseNPs   broadly the initial portions of nonrecursive noun phrases up to the head   are valuable pieces of linguistic structure which minimally extend beyond the scope of named entities 
As modern systems move toward integrating many features   resources such as this will become increasingly important in improving translation quality 
Metrics in the Rouge family allow for skip ngrams   Kauchak and Barzilay  take paraphrasing into account  metrics such as METEOR  and GTM  calculate both recall and precision  METEOR is also similar to SIA  in that word class information is used 
We then piped the text through a maximum entropy sentence boundary detector  and performed text normalization using NSW tools  
For English  we have used sections 0306 of the WSJ portion of the Penn Treebank  distributed by the Linguistic Data Consortium  LDC   which have frequently been used to evaluate sentence boundary detection systems before  compare Section 7 
In the concept extension part of our algorithm we adapt our concept acquisition framework  to suit diverse languages  including ones without explicit word segmentation 
  or  ST    where no labeled target domain data is available  eg 
Moreover  rather than predicting an intrinsic metric such as the PARSEVAL Fscore  the metric that the predictor learns to predict can be chosen to better fit the final metric on which an endtoend system is measured  in the style of  
A possible solution to his problem might be the use of more general morphological rules like those used in partofspeech tagging models  eg  1 2 3 4 530 40 50 60 70 80 90 100 level error RAND BASE Boost_S NNtfidf NB Boost_M Figure 6  Comparison of all models for a129 a48a51a95a66a97a98a97a180a222    where all suffixes up to a certain length are included 
4 Experiments The experiments described here were conducted using the Wall Street Journal Penn Treebank corpus  
Such studies follow the empiricist approach to word meaning summarized best in the famous dictum of the British 3 linguist JR Firth You shall know a word by the company it keeps Firth 1957 p 11 Context similarity has been used as a means of extracting collocations from corpora eg by Church & Hanks 1990 and by Dunning 1993 of identifying word senses eg by Yarowski 1995 and by Schutze 1998 of clustering verb classes eg by Schulte im Walde 2003 and of inducing selectional restrictions of verbs eg by Resnik 1993 by Abe & Li 1996 by Rooth et al
For both experiments  we used dependency trees extracted from the Penn Treebank  using the head rules and dependency extractor from Yamada and Matsumoto  2003  
411 Lexical cooccurrences Lexical cooccurrences have previously been shown to be useful for discourse level learning tasks  
Our evaluation metric is BLEU  
Similar to  eg    we use a Naive Bayes algorithm trained on word features cooccurring with the subjective and the objective classifications 
44 Experiment 2   s Words We also conducted translation on seven of the twelve English words studied in   
Researchers extracted opinions from words  sentences  and documents  and both rulebased and statistical models are investigated  
Since there is no wellagreed to definition of what an utterance is  we instead focus on intonational phrases   which end with an acoustically signaled boundary lone 
Running words 1864 14437 Vocabulary size 569 1081 Table 2  ChineseEnglish corpus statistics  using Phramer   a 3gram language model with KneserNey smoothing trained with SRILM  on the English side of the training data and Pharaoh  with default settings to decode 
These tags are drawn from a tagset which is constructed by 363 extending each argument label by three additional symbols a80a44a81a83a82a84a81a86a85  following  
The first SMT systems were developed in the early nineties  
Document level sentiment classification is mostly applied to reviews  where systems assign a positive or negative sentiment for a whole review document  
A description of the flat featurized dependencystyle syntactic representation we use is available in   which describes how the entire Penn Treebank  was converted to this representation 
According to this model  when translating a stringf in the source language into the target language  a string e is chosen out of all target language strings e if it has the maximal probability given f   e  arg maxe LCB Pr  e f  RCB  arg maxe LCB Pr  f e  Pr  e  RCB where Pr  f e  is the translation model and Pr  e  is the target language model 
 evaluates both estimation techniques on the Bayesian bitag model  Goldwater and Griffiths  emphasize the advantage in the MCMC approach of integrating out the HMM parameters in a tritag model  yielding a tagging supported by many different parameter settings 
Conjunctions are a major source of errors for English chunking as well  9  and we plan to address them in future work 
21 The Evaluator The evaluator is a function ptt s which assigns to each targettext unit t an estimate of its probability given a source text s and the tokens t which precede t in the current translation of s 1 Our approach to modeling this distribution is based to a large extent on that of the IBM group Brown et al  1993 but it differs in one significant aspect whereas the IBM model involves a noisy channel decomposition we use a linear combination of separate predictions from a language model ptlt  and a translation model ptls 
 shows that setting those weights should take into account the evaluation metric by which the MT system will eventually be judged 
1 word w 2 word bigram w1w2 3 singlecharacter word w 4 a word of length l with starting character c 5 a word of length l with ending character c 6 spaceseparated characters c1 and c2 7 character bigram c1c2 in any word 8 the first  last characters c1  c2 of any word 9 word w immediately before character c 10 character c immediately before word w 11 the starting characters c1 and c2 of two consecutive words 12 the ending characters c1 and c2 of two consecutive words 13 a word of length l with previous word w 14 a word of length l with next word w Table 1 Feature templates for the baseline segmentor 2 The Baseline System We built a twostage baseline system using the perceptron segmentation model from our previous work Zhang and Clark 2007 and the perceptron POS tagging model from Collins 2002
A number of systems for automatically learning semantic parsers have been proposed  
791 and score the alignment template models phrases  
The pervading method for estimating these probabilities is a simple heuristic based on the relative frequency of the phrase pair in the multiset of the phrase pairs extracted from the wordaligned corpus  
Metrics based on syntactic similarities such as the headword chain metric  HWCM   
4 Filtering with the CFG Rule Dictionary We use an idea that is similar to the method proposed by Ratnaparkhi  for partofspeech tagging 
 used the BaseNP tag set as presented in   I for inside a BaseNP  O for outside a BaseNP  and B for the first word in a BaseNP following another BaseNP 
After maximum BLEU tuning  on a heldout tuning set  we evaluate translation quality on a heldout test set 
SEPepsilon aA # epsilon  # aepsilon aepsilon bepsilon bB UNKepsilon cC bepsilon cBC e   E epsilon   depsilon depsilon epsilonepsilon bAB # bA # B # e   DE cepsilon dBCD e   DE Figure 1  Illustration of dictionary based segmentation finite state transducer 31 Bootstrapping In addition to the model based upon a dictionary of stems and words  we also experimented with models based upon character ngrams  similar to those used for Chinese segmentation  
The results evaluated by BLEU score  is shown in Table 2 
2 Evaluating Heterogeneous Parser Output Two commonly reported shallow parsing tasks are NounPhrase  NP  Chunking  and the CoNLL2000 Chunking task   which extends the NPChunking task to recognition of 11 phrase types1 annotated in the Penn Treebank 
There are many possible methods for combining unlabeled and labeled data   but we simply concatenate unlabeled data with labeled data to see the effectiveness of the selected reliable parses 
We ran the decoder with its default settings and then used Moses implementation of minimum error rate training  to tune the feature weights on the development set 
55 Dependency validity features Like   we extract the dependency path from the question word to the common word  existing in both question and sentence   and the path from candidate answer  such as CoNLL NE and numerical entity  to the common word for each pair of question and candidate sentence using Stanford dependency parser  
Assuming that the parameters P  etk fsk  are known  the most likely alignment is computed by a simple dynamicprogramming algorithm1 Instead of using an ExpectationMaximization algorithm to estimate these parameters  as commonly done when performing word alignment   we directly compute these parameters by relying on the information contained within the chunks 
We use binary Synchronous ContextFree Grammar  bSCFG   based on Inversion Transduction Grammar  ITG    to define the set of eligible segmentations for an aligned sentence pair 
1 Introduction Over the past decade  researchers at IBM have developed a series of increasingly sophisticated statistical models for machine translation  
Given phrase p1 and its paraphrase p2  we compute Score3  p1  p2  by relative frequency   Score3  p1  p2   p  p2 p1   count  p2  p1  P pprime count  pprime  p1   7  People may wonder why we do not use the same method on the monolingual parallel and comparable corpora 
These include cube pruning   cube growing   early pruning   closing spans   coarsetofine methods   pervasive laziness   and many more 
In recent years  sentiment classification has drawn much attention in the NLP field and it has many useful applications  such as opinion mining and summarization  
To counteract this  we introduce two brevity penalty measures  BP  inspired by BLEU  which we incorporate into the loss function  using a product  loss  1PrecBP  BP1  exp  1max  1  rc    6  BP2  exp  1max  cr  rc   where r is the reference length and c is the candidate length 
129 5 Active learning Whereas a passive supervised learning algorithm is provided with a collection of training examples that are typically drawn at random  an active learner has control over the labeled data that it obtains  
The Penn Treebank documentation  defines a commonly used set of tags 
One of the main directions is sentiment classification  which classifies the whole opinion document  eg  a product review  as positive or negative  
Finally  we are investigating several avenues for using this system output for Machine Translation  MT  including   1  aiding word alignment for other MT system   and  2  aiding the creation various MT models involving analyzed text  eg   
Hence we use a beamsearch decoder during training and testing  our idea is similar to that of  who used a beamsearch decoder as part of a perceptron parsing model 
22 Corpus occurrence In order to get a feel for the relative frequency of VPCs in the corpus targeted for extraction namely 0 5 10 15 20 25 30 35 40 0 10 20 30 40 50 60 70 VPC types  Corpus frequency Figure 1 Frequency distribution of VPCs in the WSJ Tagger correctextracted Prec Rec Ffl1 Brill 135135 1000 0177 0301 Penn 667800 0834 0565 0673 Table 1 POSbased extraction results the WSJ section of the Penn Treebank we took a random sample of 200 VPCs from the Alvey Natural Language Tools grammar Grover et al  1993 and did a manual corpus search for each
However  the pb features yields no noticeable improvement unlike in prefect lexical choice scenario  this is similar to the findings in  
 gave a systematic examination of the efficacy of unigram  bigram and trigram features drawn from different representations surface text  constituency parse tree and dependency parse tree 
Consider the lexical model pw  ry rx   defined following   with a denoting the most frequent word alignment observed for the rule in the training set 
Statistical Model In SIFTs statistical model  augmented parse trees are generated according to a process similar to that described in  
35 Regularization We apply lscript1 regularization Ng 2004 Gao et al 2007 to make learning more robust to noise and control the effective dimensionality of the feature spacebysubtractingaweightedsumofabsolutevalues of parameter weights from the loglikelihood of the training data w  argmaxw LLw summationdisplay i Ciwi 6 We optimize the objective using a variant of the orthantwise limitedmemory quasiNewton algorithm proposed by Andrew & Gao 20073 All values Ci are set to 1 in most of the experiments below although we apply stronger regularization Ci  3 to reordering features
Our method does not suppose a uniform distribution over all possible phrase segmentationsas  since each phrase tree has a probability 
After this conversion  we had 1000 positive and 1000 negative examples for each domain  the same balanced composition as the polarity dataset  
The MT community has developed not only an extensive literature on alignment   but also standard  proven alignment tools such as GIZA    
51 Evaluation of Translation Translations are evaluated on two automatic metrics  Bleu  and PER  position independent errorrate  
These methods have been used in machine translation   terminology research and translation aids   bilingual lexicography   collocation studies   wordsense disambiguation  and information retrieval in a multilingual environment  
For instance  for Maximum Entropy  I picked  for the basic theory   for an application  POS tagging in this case   and  for more advanced topics such as optimization and smoothing 
In comparison with shallow semantic analysis tasks such as wordsense disambiguation Ide and Jeaneronis 1998 and semantic role labeling Gildea and Jurafsky 2002 Carreras and M`arquez 2005 which only partially tackle this problem by identifying the meanings of target words or finding semantic roles of predicates semantic parsing Kate et al  2005 Ge and Mooney 2005 Zettlemoyer and Collins 2005 pursues a more ambitious goal  mapping natural language sentences to complete formal meaning representations MRs where the meaning of each part of a sentence is analyzed including noun phrases verb phrases negation quantifiers and so on
The kappa value  was used to evaluate the agreement among the judges and to estimate how difficult the evaluation task was 
Table lookup using an explicit translation lexicon is sufficient and preferable for many multilingual NLP applications  including ` crummy  MT on the World Wide Web   certain machineassisted translation tools  eg 
The local dependencies between sentiment labels on sentences is similar to the work of  where soft local consistency constraints were created between every sentence in adocument and inference wassolved using a mincut algorithm 
Training Set  Labeled English Reviews   There are many labeled English corpora available on the Web and we used the corpus constructed for multidomain sentiment classification  9  because the corpus was largescale and it was within similar domains as the test set 
Atthefinestlevel  thisinvolvesthealignment of words and phrases within two sentences that are known to be translations  
The parser is coupled with an online averaged perceptron  as the learning method 
Statistical parsers have been developed for TAG   LFG   and HPSG   among others 
For the give source text  S  it finds the most probable alignment set  A  and target text  T  Aa SaTpSTp       1  Brown  proposed five alignment models  called IBM Model  for an EnglishFrench alignment task based on equa68 tion  1  
Large volumes of training data of this kind are indispensable for constructing statistical translation models   acquiring bilingual lexicon   and building examplebased machine translation  EBMT  systems  
We use the default configuration of the measure in WordNet   Similarity012 package   and  with a single exception  the measure performed below Gic  see BP in table 1 
The previous studies  with the exception of   used smaller gazetteers than ours 
This approach is similar to that of seed words  eg    or hook words  eg    in previous work 
The features we use are shown in Table 2  which are based on the features used by  and Uchimoto et al 
Consequently  here we employ multiple references to evaluate MT systems like BLEU  and NIST  
31 A simple solution  suggests that in order to have an ITG take advantage of a known partial structure  one can simply stop the parser from using any spans that would violate the structure 
Similarly   propose a relative distortion model to be used with a phrase decoder An extension to WordNet was presented by  Discovering orientations of context dependent opinion comparative words is related to identifying domain opinion words  
Given a weight vector w  the score wf  x  y  ranks possible labelings of x  and we denote by Yk  w  x  the set of k top scoring labelings for x We use the standard B  I  O encoding for named entities  
To use the data from NANC  we use selftraining  
2 Related work  recently advocated the need for a uniform approach to corpusbased semantic tasks 
Similar to work in image retrieval   we cast the problem in terms of Machine Translation  given a paired corpus of words and a set of video event representations to which they refer  we make the IBM Model 1 assumption and use the expectationmaximization method to estimate the parameters      m j ajm jvideowordpl Cvideowordp 1    1     1  This paired corpus is created from a corpus of raw video by first abstracting each video into the feature streams described above 
In this paper  sentence pairs are extracted by a simple model that is based on the socalled IBM Model1  
The piecewise linearity observation made in  is no longer applicable since we can not move the log operation into the expected value 
Table 4 shows the linguistic features of the resulting model compared to the models of Carroll and Rooth     and Charniak  2000  
2 Related Work There has been a large and diverse body of research in opinion mining  with most research at the text   sentence  or word  level 
A hierarchical alignment algorithm is a type of synchronous parser where  instead of constraining inferences by the production rules of a grammar  the constraints come from word alignments and possibly other sources  
1 Introduction on measures for interrater reliability   on frameworks for evaluating spoken dialogue agents  and on the use of different corpora in the development of a particular system  The CarnegieMellon Communicator  Eskenazi et al 
In the future  we will experiment with semantic  rather than positional  clustering of premoditiers  using techniques such as those proposed in  
Our learning method is an extension of Collinss perceptronbased method for sequence labeling  
These feature vectors and the associated parser actions are used to train maximum entropy models  
prime 1 1 1 05 05 1 05 05 01 01 01 00001 00001 01 00001 00001 Further  we ran each setting of each estimator at least 10 times  from randomly jittered initial starting points  for at least 1000 iterations  as  showed that some estimators require many iterations to converge 
We use the discriminative perceptron learning algorithm  to train the values of vectorw 
7 Experiments To show the effectiveness of crosslanguage mention propagation information in improving mention detection system performance in Arabic  Chinese and Spanish  we use three SMT systems with very competitive performance in terms of BLEU11  
The tagger described in this paper is based on the standard Hidden Markov Model architecture  
We measured associations using the loglikelihood measure  for each combination of target category and semantic class by converting each cell of the contingency into a 22 contingency table 
32 Rare Word Accuracy For these experiments  we use the Wall Street Journal portion of the Penn Treebank  
 extracts rules from nonanaphoric noun phrases and noun phrases patterns  which are then applied to test data to identify existential noun phrases 
2 Previous Approaches  method of estimating phrasetranslation probabilities is very simple 
The form of the maximum entropy probability model is identical to the one used in   k f   wiwi1  wi2  at  ri  YIj  I Otj p  wilwil  wi2  attri   Z  Wil  wi2  attri  k to t j  l where wi ranges over V t3 stop 
Instead  researchers routinely use automatic metrics like Bleu  as the sole evidence of improvement to translation quality 
Rapp     but using cosine rather than cityblock distance to measure profile similarity 
32 Maximum Entropy ME models implement the intuition that the best model will be the one that is consistent with the set of constrains imposed by the evidence  but otherwise is as uniform as possible  
The first model  referred to as Maxent1 below  is a loglinear combination of a trigram language model with a maximum entropy translation component that is an analog of the IBM translation model 2  
In Yarowsky s experiment   an average of 3936 examples were used to disambiguate between two senses 
Err510
For Hw6  students compared their POS tagging results with the ones reported in  
A statistical language model a lexicalized PCFG  is derived from the analysis grammar by processing a corpus using the same grammar with no statistical model and recording frequencies of substructures built by each rule 
We use GIZA    to do mton wordalignment and adopt heuristic growdiagfinaland to do refinement 
The surface heuristic can define consistency according to any word alignment  but most often  the alignment is provided by GIZA    
154 2 Translation Models 21 Standard Phrasebased Model Most phrasebased translation models  rely on a preexisting set of wordbased alignments from which they induce their parameters 
Overall  agreement among judges for 250 propositions 601 A commonly used metric for evaluating interrater reliability in categorization of data is the kappa statistic  
A more optimistic view can be found in   they argue that a near100  interjudge agreement is possible  provided the partofspeech annotation is done carefully by experts 
From the extracted ngrams  those with a flequc ` ncy of 3 or more were kept  other approaches get rid of ngrams of such low frequencies   
This was expected  as it has been observed before that very simple smoothing techniques can perform well on large data sets  such as web data  
On the other hand   proposed an algorithm  borrowed to the field of dynamic programming and based on the output of their previous work  to find the best alignment  subject to certain constraints  between words in parallel sentences 
21 Minimum Error Rate Training The predominant approach to reconciling the mismatch between the MAP decision rule and the evaluation metric has been to train the parameters of the exponential model to correlate the MAP choice with the maximum score as indicated by the evaluation metric on a development set with known references  
In earlier work  only singletons were used as seed words  varying their number allows us to test whether multiple seed words have a positive effect in detection performance 
stituent alignments  
Many strategies have been proposed to integrate morphology information in SMT  including factored translation models   adding a translation dictionary containing inflected forms to the training data   entirely replacing surface forms by representations built on lemmas and POS tags   morphemes learned in an unsupervised manner   and using Porter stems and even 4letter prefixes for word alignment  
Manually defined heuristics are used to automatically annotate each tree in the treebank with partially specified HPSG derivation trees Headargumentmodifier distinctions are made for each node in the tree based on Magerman 1994 and Collins 1997 336 ODonovan et al LargeScale Induction and Evaluation of Lexical Resources the whole tree is then converted to a binary tree heuristics are applied to deal with phenomena such as LDDs and coordination and to correct some errors in the treebank and finally an HPSG category is assigned to each node in the tree in accordance with its CFG category
A synchronous 363 binarization method is proposed in  whose basic idea is to build a leftheavy binary synchronous tree  with a lefttoright shiftreduce algorithm 
This hypothesized relationship between distributional similarity and semantic similarity has given rise to a large body of work on automatic thesaurus generation  
The details of the algorithm can be found in the literature for statistical translation models  such as  
For comparing the sentence generator sample to the English sample  we compute loglikelihood statistics  on neighboring words that at least cooccur twice 
To support distributed computation   we further split the Ngram data into shards by hash values of the first bigram 
See  for an application of the boosting approach to named entity recognition  and Walker  Rambow  and Rogati  for the application of boosting techniques for ranking in the context of natural language generation 
The first stage parser is a bestfirst PCFG parser trained on sections 2 through 22  and 24 of the Penn WSJ treebank  
35 The Experiments We have ran LexTract on the onemillionword English Penn Treebank  and got two Treebank grammars 
The results are comparable to other results reported using the InsideOutside method   see Table 7 
Meanwhile  it is common for NP chunking tasks to represent a chunk  eg  NP  with two labels  the begin  eg  BNP  and inside  eg  INP  of a chunk  
Portage is a statistical phrasebased SMT system similar to Pharaoh  
Like   we used mutual information to measure the cohesion between two words 
Workshop Towards GenreEnabled Search Engines booktitle pages 13  20 pages editor In G Rehm and M Santini  editors editor contexts context ork on an intradocument  or page segment level because a single document can contain instances of multiple genres  eg  contact information  list of publications  CV  see  
Again  we find the clearest patterns in the graphs for precision  where Malt has very low precision near the root but improves with increasing depth  while MST shows the opposite trend  
Fortunately  using distributional characteristics of term contexts  it is feasible to induce partofspeech categories directly from a corpus of suf cient size  as several papers have made clear  
Abney  notes important problems with the soundness of the approach when a unificationbased grammar is actually determining the derivations  motivating the use of loglinear models  for parse ranking that Johnson and colleagues further developed  
6 Discourse Context  pointed out that the sense of a target word is highly consistent within any given document  one sense per discourse  
3 Model As an extension to commonly used lexical word pair probabilities p  f e  as introduced in   we define our model to operate on word triplets 
To derive the joint counts c   s   t  from which p   s  t  and p   t  s  are estimated  we use the phrase induction algorithm described in   with symmetrized word alignments generated using IBM model 2  
We utilise the automatic annotation algorithm of  to derive a version of PennII where each node in each tree is annotated with an LFG functional annotation  ie an attribute value structure equation  
Most previous work with CRFs containing nonlocal dependencies used approximate probabilistic inference techniques  including TRP  and Gibbs sampling  
We tokenized sentences using the standard treebank tokenization script  and then we performed partofspeech tagging using MXPOST tagger  
Training of the phrase translation model builds on top of a standard statistical word alignment over the training corpus of parallel text  for identifying corresponding word blocks  assuming no further linguistic analysis of the source or target language 
Section 7 considers recent efforts to induce effective procedures for automated sense labeling of discourse relations that are not lexically marked  
However  most of them fail to utilize nonsyntactic phrases well that are proven useful in the phrasebased methods  
Words are encoded through an automatic clustering algorithm  while tags  labels and extensions are normally encoded using diagonal bits 
4 Features For our experiments we use the features proposed  motivated and described in detail by  
 presented a thorough discussion on the Yarowsky algorithm 
In the field of statistical analysis of natural language data  it is common to use measures of lexical association  such as the informationtheoretic measure of mutual information  to extract useful relationships between words  eg   
We hence chose transformationbased learning to create this  shallow  segmentation grammar  converting the segmentation task into a tagging task  as is done in 85   inter alia  
In particular  previous work  has investigated the use of Markov random fields  MRFs  or loglinear models as probabilistic models with global features for parsing and other NLP tasks 
 reports results for different numbers of hidden states but it is unclear how to make this choice a priori  while Goldwater & Griffiths  leave this question as future work 
1 Introduction Word Sense Disambiguation  WSD  competitions have focused on general domain texts  as attested in the last Senseval and Semeval competitions  
Historybased models for predicting the next parser action  3 
33 Features Similar to the default features in Pharaoh   we used following features to estimate the weight of our grammar rules 
Previous research has addressed revision in singledocument summaries   and has suggested that revising summaries can make them more informative and correct errors 
2 Three New Features for MT Evaluation Since our sourcesentence constrained ngram precision and discriminative unigram precision are both derived from the normal ngram precision  it is worth describing the original ngram precision metric  BLEU  
This concept of alignment has been also used for tasks like authomatic vocabulary derivation and corpus alignment  
We use the Stanford parser  with its default Chinese grammar  the GIZA    alignment package with its default settings  and the ME tool developed by  
The idea is that the translation of a sentence x into a sentence y can be performed in the following steps1   a  If x is small enough  IBMs model 1  is employed for the translation 
1 A cept is defined as the set of target words connected to a source word  
Measures of attributional similarity have been studied extensively  due to their applications in problems such as recognizing synonyms   information retrieval   determining semantic orientation   grading student essays   measuring textual cohesion   and word sense disambiguation  
Note that the algorithm from  was designed for discriminatively training an HMMstyle tagger 
In this years shared task we evaluated a number of different automatic metrics  Bleu  Bleu remains the de facto standard in machine translation evaluation 
In computational linguistics  our pattern discovery procedure extends over previous approaches that use surface patterns as indicators of semantic relations between nouns or verbs   inter alia  
SMT has evolved from the original wordbased approach  into phrasebased approaches  and syntaxbased approaches  
Various clustering techniques have been proposed  which perform automatic word clustering optimizing a maximumlikelihood criterion with iterative clustering algorithms 
   and Lee   Wilson et al 
The approach is able to achieve 94  precision and recall for base NPs derived from the Penn Treebank Wall Street Journal  
orgpubscitations  j ournalstoms1986 12 2  p154meht a  Mutual Information Given the definition of Mutual Information   I  x  y   log 2 P  x  y  P  x  P  y   we consider the distribution of a window word according to the contingency table  a  in Table 4 
Parse selection constitutes an important part of many parsing systems  
To generate phrase pairs from a parallel corpus  we use the ` diagand  phrase induction algorithm described in   with symmetrized word alignments generated using IBM model 2  
In NLP community  it has been shown that having more data results in better performance  
Such a coding procedure covers  for example  how segmentation of a corpus is performed  if multiple tagging is allowed and if so  is it unlimited or are there just certain combinations of tags not allowed  is look ahead permitted  etc For further information on coding procedures we want to refer to  and for good examples of coding books see  for example      or  
2 Statistical Word Alignment According to the IBM models   the statistical word alignment model can be generally represented as in Equation  1  
2 Data 21 The US Congressional Speech Corpus The text used in the experiments is from the United States Congressional Speech corpus   which is an XML formatted version of the electronic United States Congressional Record from the Library of Congress1 
Among the four steps  the hypothesis alignment presents the biggest challenge to the method due to the varying word orders between outputs from different MT systems  
 has proposed a bootstrapping method for word sense disambiguation 
Language modeling   nounclustering   constructing syntactic rules for SMT   and finding analogies  are examples of some of the problems where we need to compute relative frequencies 
This has been now an active research area for a couple of decades  
Giza   is a freely available implementation of IBM Models 15  and the HMM alignment   along with various improvements and modifications motivated by experimentation by Och & Ney  
Although the BLEU  score from Finnish to English is 218  the score in the reverse direction is reported as 130 which is one of the lowest scores in 11 European languages scores  
 for English  but not identical to strictly anaphoric ones5   since a nonanaphoric NP can corefer with a previous mention 
We report case sensitive Bleu  scoreBleuCforallexperiments 
Using GIZA   model 4 alignments and Pharaoh   we achieved a BLEU score of 03035 
Making such an assumption is reasonable since POS taggers that can achieve accuracy of 96  are readily available to assign POS to unrestricted English sentences  
3 OverviewofExtractionWork 31 English As one mightexpect  the bulk of the collocation extractionwork concernsthe English language    amongmany others1 
6 Experiments We evaluated the translation quality of the system using the BLEU metric  
The loglinear model feature weights were learned using minimum error rate training  MERT   with BLEU score  as the objective function 
Sentiment classification at the sentencelevel has also been studied  
This difference was highlighted in the 3http    w3msivxusejhamaltparser  studyof   whichshowed that the difference is reflected directly in the error distributions of the parsers 
43 Relaxing Length Restrictions Increasing the maximum phrase length in standard phrasebased translation does not improve BLEU  
The importance of including single nonheadwords is now also uncontroversial   and the current paper has shown the importance of including two and more nonheadwords 
We annotated with the BIO tagging scheme used in syntactic chunkers  
We follow the approach of bootstrapping from a model with a narrower parameter space as is done in  eg Och and Ney  and  
6 Results We trained on the standard Penn Treebank WSJ corpus  
For instance  BLEU and ROUGE  are based on ngram precisions  METEOR  and STM  use wordclass or structural information  Kauchak  2006  leverages on paraphrases  and TER  uses editdistances 
Rulesize and lexicalization affect parsing complexity whether the grammar is binarized explicitly  or implicitly binarized using Earlystyle intermediate symbols  
Many methods for calculating the similarity have been proposed  
This idea of employing ngram cooccurrence statistics to score the output of a computer system against one or more desired reference outputs has its roots in the BLEU metric for machine translation  and the ROUGE  metric for summarization 
Nakagawa  and   also showed the effectiveness of global features in improving the accuracy of graphbased parsing  using the approximate Gibbs sampling method and a reranking approach  respectively 
One interesting approach to extending the current system is to introduce a statistical translation model  to filter out irrelevant translation candidates and to extract the most appropriate subpart from a long English sequence as the translation by locally aligning the Japanese and English sequences 
294 Fraser and Marcu Measuring Word Alignment Quality for Statistical Machine Translation 22 Measuring Translation Performance Changes Caused By Alignment In phrasedbased SMT  the knowledge sources which vary with the word alignment are the phrase translation lexicon  which maps source phrases to target phrases using counts from the word alignment  and some of the word level translation parameters  sometimes called lexical smoothing  
Such techniques are currently being applied in many areas  including language identification  authorship attribution   text genre classification   topic identification   and subjective sentiment classification  
or cooking  which agrees with the knowledge presented in previous work  
This ITG constraint is characterized by the two forbidden structures shown in Figure 1  
One is distortion model  which penalizes translations according to their jump distance instead of their content 
 ii  Apply some statistical tests such as the Binomial Hypothesis Test  and loglikelihood ratio score  to SCCs to filter out false SCCs on the basis of their reliability and likelihood 
Second  it can be applied to control the quality of parallel bilingual sentences mined from the Web  which are critical sources for a wide range of applications  such as statistical machine translation  and crosslingual information retrieval  
For example  the word alignment computed by GIZA   and used as a basis to extract the TTS templates in most SSMT systems has been observed to be a problem for SSMT   due to the fact that the wordbased alignment models are not aware of the syntactic structure of the sentences and could produce many syntaxviolating word alignments 
Baseline We use the Moses MT system  as a baseline and closely follow the example training procedure given for the WMT07 and WMT08 shared tasks4 In particular  we perform word alignment in each direction using GIZA     apply the growdiagfinaland heuristic for symmetrization and use a maximum phrase length of 7 
2 Related Work This method is similar to blockorientation modeling  and maximum entropy based phrase reordering model   in which local orientations  leftright  of phrase pairs  blocks  are learned via MaxEnt classifiers 
The straightforward way is to first generate the best BTG tree for each sentence pair using the way of   then annotate each BTG node with linguistic elements by projecting sourceside syntax tree to BTG tree  and finally extract rules from these annotated BTG trees 
Measurement of Beliability The Kappa Statistic Following Jean   we use the kappa statistic  to measure degree of agreement among subjects 
As  point out  WordNet does not encode antonymy across partofspeech  for example  legallyembargo  
In comparison  we deployed the GIZA   MT modeling tool kit  which is an implementation of the IBM Models 1 to 4  
This is the shared task baseline system for the 2006 NAACLHLT workshop on statistical machine translation  and consists of the Pharaoh decoder   SRILM   GIZA     mkcls   Carmel 1 and a phrase model training code 
 claimed that this approximation achieved essentially equivalent performance to that obtained when directly using the loss as the objective  O  lscript 
There also have been prior work on maintaining approximate counts for higherorder language models  LMs     operates under the model that the goal is to store a compressed representation of a diskresident table of counts and use this compressed representation to answer count queries approximately 
Therefore the probability of alignment aj for position j should have a dependence on the previous alignment position O j_l  P    j    j1  A similar approach has been chosen by  and  
54 Domain Adaptation 541 FeatureBased Approaches Onewayofadaptingalearnertoanewdomainwithout using any unlabeled data is to only include features that are expected to transfer well  
Our test set is 3718 sentences from the English Penn treebank  which were translated into German 
Given a set of evidences E over all the relevant word pairs  in   the probabilistic taxonomy learning task is defined as the problem of finding the taxonomy hatwideT that maximizes the 67 probability of having the evidences E  ie  hatwideT  arg max T P  E T  In   this maximization problem is solved with a local search 
Interannotator agreement was measured using the kappa  K  statistics  on 1502 instances  three Switchboard dialogues  marked by two annotators who followed specific written guidelines 
 noted that the unigram unpredictable might have a positive sentiment in a movie review  eg unpredictable plot   but could be negative in the review of an automobile  eg unpredictable steering  
To prune away those pairs  we used the loglikelihoodratio algorithm  to compute the degree of association between the verb and the noun in each pair 
Probabilistic generative models like IBM 15 Brown et al 1993 HMM Vogel et al 1996 ITG Wu 1997 and LEAF Fraser and Marcu 2007 define formulas for Pf  e or Pe f with okvoon ororok sprok atvoon bichat dat erok sprok izok hihok ghirok totat dat arrat vat hilat okdrubel okvoon anok plok sprok atdrubel atvoon pippat rrat dat okvoon anok drok brok jok atvoon krat pippat sat lat wiwok farok izok stok totat jjat quat cat lalok sprok izok jok stok wat dat krat quat cat lalok farok ororok lalok sprok izok enemok wat jjat bichat wat dat vat eneat lalok brok anok plok nok iat lat pippat rrat nnat wiwok nok izok kantok okyurp totat nnat quat oloat atyurp lalok mok nok yorok ghirok clok wat nnat gat mat bat hilat lalok nok crrrok hihok yorok zanzanok wat nnat arrat mat zanzanat lalok rarok nok izok hihok mok wat nnat forat arrat vat gat Figure 1 Word alignment exercise Knight 1997
Still  however  such techniques often require seeds  or prototypes  cf    which are used to prune search spaces or direct learners 
As resolving direct anaphoric descriptions  the ones where anaphor and antecedent have the same head noun  is a much simpler problem with high performance rates as shown in previous results   these heuristics should be applied first in a system that resolves definite descriptions 
et al  2004  CollinsThompson and Callan  2005   and Ramage  2007  
We benchmark our results against a model  Hiero  which was directly trained to optimise BLEUNIST using the standard MERT algorithm  and the full set of translation and lexical weight features described for the Hiero model  
Although to a lesser extent  measures of word relatedness have also been applied on other languages  including German   Chinese   Dutch  and others 
This method is very similar to some ideas in domain adaptation   but we argue that the underlying problems are quite different 
This wrong translation of content words is similar to the incorrect omission reported in   which both hurt translation adequacy 
1 Introduction In global linear models  GLMs  for structured prediction   eg     the optimal label y for an input x is y  arg max yY  x  w f  x  y   1  where Y  x  is the set of possible labels for the input x  f  x  y  Rd is a feature vector that represents the pair  x  y   and w is a parameter vector 
We use the same preprocessing steps as Turian and Melamed   during both training and testing  the parser is given text POStagged by the tagger of   with capitalization stripped and outermost punctuation removed 
2 Phrasebased SMT We use a phrasebased SMT system  Pharaoh    which is based on a loglinear formulation  
To quickly  and approximately  evaluate this phenomenon  we trained the statistical IBM wordalignment model 4  1 using the GIZA   software  for the following language pairs  ChineseEnglish  Italian English  and DutchEnglish  using the IWSLT2006 corpus  for the first two language pairs  and the Europarl corpus  for the last one 
Alignment spaces can emerge from generative stories   from syntactic notions   or they can be imposed to create competition between links  
1 Introduction Many different statistical tests have been proposed to measure the strength of word similarity or word association in natural language texts  
For example  it has been observed that texts often contain multiple opinions on different topics   which makes assignment of the overall sentiment to the whole document problematic 
In each experiment  performance IMutu   d Information provides an estimate of the magnitude of the ratio t  ctw    n the joint prol  ability P  verbnoun 1  reposition   and the joint probability a  suming indcpendcnce P  verbnoun  P  prcl  osition  s      
We use the IBM Model 1   uniform distribution  and the Hidden Markov Model  HMM  firstorder dependency    to estimate the alignment model 
The parameters  j  were trained using minimum error rate training  to maximize the BLEU score  on a 150 sentence development set 
For these first SMT systems  translationmodel probabilities at the sentence level were approximated from wordbased translation models that were trained by using bilingual corpora  
 binarize grammars into CNF normal form  while  allow only GriebachNormal form grammars 
Equation  10  is of interest because the ratio p  C v  r   p  C r  can be interpreted as a measure of association between the verb v and class C This ratio is similar to pointwise mutual information  and also forms part of Resniks association score  which will be introduced in Section 6 
Recently  it has gained renewed attention as empirical methods in parsing have emphasized the importance of relations between words  see  eg     which is what dependency grammars model explicitly  but contextfree phrasestructure grammars do not 
It is clear that Appendix B contains far fewer true noncompositional phrases than Appendix A 7 Related Work There have been numerous previous research on extracting collocations from corpus  eg   and  
model reranking has also been established  both for synchronous binarization  and for targetonly binarization  
From wordlevel alignments  such systems extract the grammar rules consistent either with the alignments and parse trees for one of languages   or with the the wordlevel alignments alone without reference to external syntactic analysis   which is the scenario we address here 
A remedy is to aggressively limit the feature space  eg to syntactic labels or a small fraction of the bilingual features available  as in   but that reduces the benefit of lexical features 
 Classification allows a word to align with a target word using the collective translation tendency of words in the same class 
51 The Prague Dependency Tree Bank  PDT in the sequel   which has been inspired by the buildup of the Penn Treebank   is aimed at a complex annotation of  a part of  the Czech National Corpus  CNC in the sequel   the creation of which is under progress at the Department of Czech National Corpus at the Faculty of Philosophy  Charles University  the corpus currently comprises about 100 million tokens of word forms  
The overall POS tag distribution learned by EM is relatively uniform  as noted by   and it tends to assign equal number of tokens to each tag label whereas the real tag distribution is highly skewed 
1 Introduction Since 1995  a few statistical parsing algorithms  demonstrated a breakthrough in parsing accuracy  as measured against the University of Pennsylvania TREEBANK as a gold standard 
The results are quite promising  our extraction method discovered 89  of the WordNet cousins  and the sense partitions in our lexicon yielded better values  than arbitrary sense groupings on the agreement data 
We use the following features for our rules  sourceand targetconditioned neglog lexical weights as described in Koehn et al  2003b  neglog relative frequencies lefthandsideconditioned targetphraseconditioned sourcephraseconditioned  Counters no rule applications no target words  Flags IsPurelyLexical ie  contains only terminals IsPurelyAbstract ie  contains only nonterminals IsXRule ie  nonsyntactical span IsGlueRule 139  Penalties rareness penalty exp1  RuleFrequency unbalancedness penalty MeanTargetSourceRatio  no source words no target words 4 Parsing Our SynCFG rules are equivalent to a probabilistic contextfree grammar and decoding is therefore an application of chart parsing
Algorithm 1 SCL  1  Select m pivot features 
 applies this approach to the socalled IBM Candide system to build context dependent models  compute automatic sentence splitting and to improve word reordering in translation 
The techniques examined are Structural Correspondence Learning  SCL   and Selftraining  
The perceptron has been used in many NLP tasks  such as POS tagging   Chinese word segmentation  and so on 
Presently  many systems        focus on online recognition of proper nouns  and have achieved inspiring results in newscorpus but will be deteriorated in special text  such as spoken corpus  novels 
For MCE learning  we selected the reference compression that maximize the BLEU score    argmax rR BLEU  r  R r   from the set of reference compressions and used it as correct data for training 
In addition to adapting the idea of Head Word Chains   we also compared the input sentences argument structures against the treebank for certain syntactic categories 
312 Kappa Kappa  is an evaluation measure which is increasingly used in NLP annotation work  
1 Introduction Texttotext generation is an emerging area of research in NLP  
Having a single  canonical tree structure for each possible alignment can help when flattening binary trees  as it indicates arbitrary binarization decisions  
We use GIZA    to train generative directed alignment models  HMM and IBM Model4  from training recordtext pairs 
 introduced a transformationbased learning method which considered chunking as a kind of tagging problem 
The Penn Treebank annotation  was chosen to be the first among equals  it is the starting point for the merger and data from other annotations are attached at tree nodes 
We can then use this newly identified set to   1  use  s method to find the orientation for the terms and employ the terms and their scores in a classifier  and  2  use  s method to find the orientation for the terms and add the new terms as additional seed terms for a second iteration As opposed to    we do not use the web as a resource to find associations  rather we apply the method directly to indomain data 
The second uses Lin dependency similarity  a syntacticdependency based distributional word similarity resource described in  9 
A monotonous segmentation copes with monotonous alignments  that is  j  k aj  ak following the notation of  
We also can not use prior graph construction methods for the document level  such as physical proximity of sentences  used in   at the word sense level 
The model scaling factors are optimized on the development corpus with respect to mWER similar to  
For these classications  we calculated a kappa statistic of 0528  
Feature weights of both systems are tuned on the same data set3 For Pharaoh  we use the standard minimum errorrate training   and for our system  since there are only two independent features  as we always fix  1   we use a simple gridbased lineoptimization along the languagemodel weight axis 
In most statistical machine translation  SMT  models   some of measure words can be generated without modification or additional processing 
The resulting corpus contains 385 documents of American English selected from the Penn Treebank   annotated in the framework of Rhetorical Structure Theory 
We solve SAT analogies with a simplified version of the method of  
Also  the aspect of generalizing features across different products is closely related to fully supervised domain adaptation   and we plan to combine our approach with the idea from Daume III  2007  to gain insights into whether the composite backoff features exhibit different behavior in domaingeneral versus domainspecific feature subspaces 
1 Introduction The task of sentence compression  or sentence reduction  can be defined as summarizing a single sentence by removing information from it  
Because of these kinds of results  the vast majority of statistical parsing work has focused on parsing as a supervised learning problem  
5 The SemCor collection  is a subset of the Brown Corpus and consists of 352 news articles distributed into three sets in which the nouns  verbs  adverbs  and adjectives have been manually tagged with their corresponding WordNet senses and partofspeech tags using Brills tagger  
There are also approaches to anaphora resolution using unsupervised methods to extract useful information  such as gender and number   or contextual roleknowledge  
 discuss the influence of bias towards highor lowfrequency items for different tasks  correlation with WordNetderived neighbor sets and pseudoword disambiguation   and it would not be surprising if the different highfrequency bias were leading to different results 
Some of these have been previously employed for various tasks by Gabrilovich and Markovitch    Overell and Ruger     and Suchanek et al 
Model 4 of  is also a firstorder alignment model  along the source positions  like the HMM  trot includes also fertilities 
 ie   ll  Lj   maz  zi  j  u   i  I where xi  j  u  E Qi and max  xi  j  u   is the highest score in the line of the matrix Qi which corresponds to the head word sense j n is the number of modifiers of the head word h at the current tree level  and k i Lj  j  l Lj where k is the number of senses of the head word h The reason why gj  I0  is calculated as a sum of the best scores  ll   rather than by using the traditional maximum likelihood estimate   Gah eta  
SIGHAN  the Special Interest Group for Chinese Language Processing of the Association for Computational Linguistics  conducted three prior word segmentation bakeoffs  in 2003  2005 and 2006   which established benchmarks for word segmentation and named entity recognition 
We show translation results in terms of the automatic BLEU evaluation metric  on the MT03 ArabicEnglish DARPA evaluation test set consisting of a212a89a212a89a87 sentences with a98a89a212a161a213a89a214a89a215 Arabic words with a95 reference translations 
Wu and Weld  and   calculate the overlap between contexts of named entities and candidate articles from Wikipedia  using overlap ratios or similarity scores in a vector space model  respectively 
In our approach  equation  1  is further normalized so that the probability for different lengths of F is comparable at the word level  m m j n i ijm eft l EFP  1 10    1  1        2  The alignment models described in  are all based on the notion that an alignment aligns each source word to exactly one target word 
3http    wwwopenofficeorg Another corpora based method due to Turney and Littman  tries to measure the semantic orientation O  t  for a term t by O  t   summationdisplay tiS  PMI  t  ti  summationdisplay tjS PMI  t  tj  where S  and S are minimal sets of polar terms that contain prototypical positive and negative terms respectively  and PMI  t  ti  is the pointwise mutual information  between the terms t and ti 
To evaluate sentence automatically generated with taking consideration word concatenation into by using references varied among humans  various metrics using ngram precision and word accuracy have been proposed  word string precision  for summarization through word extraction  ROUGE  for abstracts  and BLEU  for machine translation 
3 Evaluation We trained our model parameters on a subset of the provided dev2006 development set  optimizing for caseinsensitive IBMstyle BLEU  with several iterations of minimum error rate training on nbest lists 
Levin  assumes that the syntactic realization of a verb s arguments is directly correlated with its meaning  cf 
The fstructures are created automatically by annotating nodes in the gold standard WSJ trees with LFG functional equations and then passing these equations through a constraint solver  
First  we trained a finitestate shallow parser on base phrases extracted from the Penn Wall St Journal  WSJ  Treebank  
 Termbased versions of this premise have motivated much sentimentanalysis work for over a decade   
In particular  knowing a little about the structure of a language can help in developing annotated corpora and tools  since a little knowledge can go a long way in inducing accurate structure and annotations  
As shown in   using this representation  a linear classifier can not distinguish sentences sampled from a trigram and real sentences 
Researchers have focused on learning adjectives or adjectival phrases  and verbs   but no previous work has focused on learning nouns 
Probabilistic translation models generally seek to find the translation string e that maximizes the probability Pra5 ea6fa7  given the source string f  
joint likelihood JL productdisplay i p parenleftBig xiyi  vector parenrightBig conditional likelihood CL productdisplay i p parenleftBig yi  xivector parenrightBig classification accuracy Juang and Katagiri 1992 summationdisplay i yi yxi expected classification accuracy Klein and Manning 2002 summationdisplay i p parenleftBig yi  xivector parenrightBig negated boosting loss Collins 2000  summationdisplay i p parenleftBig yi  xivector parenrightBig1 margin Crammer and Singer 2001  st bardbl vectorbardbl  1iy negationslash yi vector  vectorfxiyi   vectorfxiy   expected local accuracy Altun et al  2003 productdisplay i productdisplay j p parenleftBig lscriptjY   lscriptjyi   xivector parenrightBig Table 1 Various supervised training criteria
Most work on discriminative training for SMT has focussed on linear models  often with margin based algorithms   or rescaling a product of submodels  
Bikel and Chiang  in fact contains two parsers  one is a lexicalized probabilistic contextfree grammar  PCFG  similar to   the other is based on statistical TAG  
Finally  it should be noted that in the current implementation  we have not applied any of the possible optimizations that appear in the literature  to speed up normalization of the probability distribution q These improvements take advantage of a models structure to simplify the evaluation of the denominator in  1  
1510 5 Related Work In recent years  many research has been done on extracting relations from free text  eg     however  almost all of them require some languagedependent parsers or taggers for English  which restrict the language of their extractions to English only  or languages that have these parsers  
To compare the performance of system  we recorded the total training time and the BLEU score  which is a standard automatic measurement of the translation qualit  
Typically  a small set of seed polar phrases are prepared  and new polar phrases are detected based on the strength of cooccurrence with the seeds  
The basic LCS has a problem that it does not differentiate LCSes of different spatial relations within their embedding sequences    
3 Parse Tree Features We tagged each candidate transcription with  1  partofspeech tags  using the tagger documented in   and  2  a full parse tree  using the parser documented in Collins  1999  
