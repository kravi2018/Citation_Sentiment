"We generate all the morphologically related forms of the word pair using a lexical transducer for English ( Karttunen et al., 1992)."
"The Xerox tagger is claimed (Cutting el al., 1992) to be adaptable and easily trained; only a lexicon and suitable amount of untagged text is required.  "
"Our constraint-based tagger is based on techniques that were originally developed for morphological analysis. The disambiguation rules are similar to phonological rewrite rules (Kaplan and Kay, 1994), and the parsing algorithm is similar to the algorithm for combining the morphological rules with the lexicon (Karttunen, 1994). "
"We also tried combining the tuggers, using first the rules and then the statistics (a similar approach was also used in (Tapanainen and Vouti- Lainen, 1994)). "
"Kupiec (1992) has proposed an estimation method for the N-gram language model using the Baum-Welch reestimation algorithm (Rabiner et al., 1994) from an untagged corpus and Cutting et al. (1992) have applied this method to an English tagging system.Takeuchi and Matsumoto (1995) also have developed an extended method for unsegmented languages (e.g., Japanese) and applied it to their Japanese tagger.  "
"Juman (Matsumoto et al., 1994) was used in our experiments to generate the morpheme network. Juman is a rule-based Japanese tagging system which uses hand-coding cost values that represent the implausibility of morpheme connections, and word- and tag-occurences. "
"The credit factor improved the upper bound of the estimation accuracy from an untagged corpus. However, at higher levels of tagging accuracy, the reestimation m e t h o d based on the Baum-Welch algorithm is limited by the noise of untagged corpora. On this point, I agree with Merialdo (1994) and Elworthy (1994). "
"In the bigram model, we can weight each probability of a pair of tags in both models estimated from tagged or untagged corpora. A smoothing method, such as deleted interpolation (Jelinek, 1985), can be used for Weighting."
"A very influential is the work of Brill (1997), who induces more linguistically motivated rules exploiting both a tagged corpus and a lexicon. He does not look at the affixes only, but also checks their POS class in a lexicon. Mikheev (1997) proposes a similar approach, but learns the rules from raw as opposed to tagged text. Daciuk (1999) speeds up the process by means of finite state transducers. "
"In particular, in our experiments we used the Large Grammatical Dictionary of Bulgarian (Paskaleva,2003), created at the Linguistic Modelling Department of the Bulgarian Academy of Sciences (CLPP-BAS) and comprising approximately 995,000 wordforms (about 65,000 lemmas), encoded in DELAF format (Silberztein,1993). "
"Our approximate rules are similar to the ones proposed by Mikheev (1997), who uses a dictionary to build POS prediction rules with four parts "
"Next, it looks promising to try to estimate the dictionary word frequencies using a search engine instead of text corpus, as proposed by Lapata and Keller (2004) "
"Finally, we would like to explore the machine learning potential offered by morphological dictionaries with application to other related tasks such as stemming (Nakov, 2003), lemmatisation and POS tagging "
"Och (2003) provides evidence that Λ should be chosen by optimizing an objective function based on the evaluation metric of interest, rather than likelihood. Since the error surface is not smooth, and a grid search is too expensive, Och suggests an alternative, efficient, line optimization approach. "
"In spite of these advantages, recent work has pointed out a number of problematic aspects of BLEU that should cause one to pause and reconsider the reliance on it. Chiang et al. (2008) investigate several weaknesses in BLEU and show there are realistic scenraios where the BLEU score should not be trusted, and in fact behaves in a counter-intuitive manner. Furthermore, Callison Burch et al. (2006) point out that it is not always appropriate to use B LEU to compare systems to each other. "
"We use Joshua (Li et al., 2009), in our experiments. Joshua is a hierarchical parsing-based MT system, and it can be instructed to produce derivation trees instead of the candidate sentence string Itself. "
"The MT system we used is Joshua (Li et al., 2009), a software package that comes complete with a grammar extraction module and a MERT module, in addition to the decoder itself. "
"On the other hand, Snow et al. (2008) illustrate how AMT can be used to collect data in a “fast and cheap” fashion, for a number of NLP tasks, such as word sense disambiguation. They go a step further and model the behavior of their annotators to reduce annotator bias. "
The formaliza-tion of this notion and an algorithm for computing the composed transducer are wellknown and are described originally by Elgot and Mezei (1965).
"We empirically compared our tagger with Eric Brill's implementation of his tagger,and with our implementation of a trigram tagger adapted from the work of Church(1988) that we previously implemented for another purpose."
"Independently, Cutting et aL (1992) quote a performance of 800 words per secondfor their part-of-speech tagger based on h i d d e n Markov models."
This compression is achievedwhile maintaining random access using a procedure for sparse data tables followingthe method given by Tarjan and Yao (1979).
"No pretagged text is necessary for Hidden Markov Models (Jelinek, 1985; Cutting et al., 1991; Kupiec, 1992). "
Brill and Marcus (1992a) have shown that the effort necessary to construct the part-of-speech lexicon can be considerably re- duced by combining learning procedures and a partial part-of-speech categorization elicited from an informant. 
"We obtained 47,025 50-dimensional reduced vectors from the SVD and clustered them into 200 classes using the fast clustering algorithm Buckshot (Cutting et al., 1992)"
"There is both synchronic (Ross, 1972) and diachronic (Tabor, 1994) evidence suggesting that words and their uses can inherit properties from several prototypical syntactic categories. "
"We use the English Slot Grammar(ESG) parser developed at IBM (McCord, 1990) to analyze the syntactic structure of an input sentence and produce a sentence parse tree. The ESG parser not only annotates the syntactic category of a phrase, it also annotates the thematic role of a phrase . "
The same asymptotic complexityis of course found for memory storage in this approach. We use IGTrees (Daelemans etal. 1996) to compress the memory. IGTree is a heuristic approximation of the IB-IGAlgorithm.
"The recursive algorithms for tree construction (except the final pruning) and retrieval are given in Figures 1 and 2. For a detailed discussion, see  aelemans et al. (1996). "
"A windowing approach (Sejnowski & Rosenberg, 1987) was used to represent the tagging task as a classification problem. "
"Again, we take advantage of the data fusion capabilities of a memory-based approach by combining these two sources of information in the case representation, and having the information gain feature relevance weighting technique figure out their relative relevance (see Schmid, 1994; Samuelsson, 1994 for similar solutions) "
"The experimental methodology was taken from Machine Learning practice (e.g. Weiss & Kulikowski, 1991): independent training and test sets were selected from the original corpus, the system was trained on the training set, and the generalization accuracy (percentage of correct category assignments) was computed on the independent test set. "
"The above problems could be partially solved by introducing more resources into collocation extraction, such as chunker (Wermter and Hahn, 2004), parser (Lin, 1998; Seretan and We hrli, 2006) and WordNet (Pearce, 2001)."
"We adapt the bilingual word alignment model, IBM Model 3 (Brown et al., 1993), to monolingual word alignment. "
"We take BBN’s HierDec, a string-to-dependency decoder as described in (Shen et al., 2008), as our baseline for the following two reasons "
"Marton and Resnik (2008) introduced features defined on constituent labels to improve the Hier-o system (Chiang, 2005). However, due to the limitation of MER training, only part of the feature space could used in the system. This problem was fixed by Chiang et al. (2008), which used an online learning method (Crammer and Singer, 2003) to handle a large set of features. "
"Most SMT systems assume that translation rules can be applied without paying attention to the sentence context. A few studies (Carpuat and Wu, 2007; Ittycheriah and Roukos, 2007; He et al., 2008; Hasan et al., 2008) addressed this defect by selecting the appropriate translation rules for an input span based on its context in the input sentence."
"The direct translation model in (Ittycheriah and Roukos, 2007) employed syntactic (POS tags) and context information (neighboring words) within a maximum entropy model to predict the correct transfer rules. A similar technique was applied by He et al. (2008) to improve the Hiero system."
"Automatic analysis of natural language is still a very hard task to perform for a computer. Although some successful applications have been developed (see for instance (Chinchor, 1998)), implementing an automatic text analysis system is still a labour and time intensive task. "
"Gildea and Jurafsky (2002) were the first to describe a statistical system trained on the data from the FrameNet project to automatically assign semantic roles. This approach was soon followed by other researchers (Surdeanu et al., 2003; Pradhan et al., 2004; Xue and Palmer, 2004), focusing on improved sets of features, improved machine learning methods or both, and SRL became a shared task at the CoNLL 2004, 2005 and 2008 Conferences"
"The best system (Johansson and Nugues, 2008) in CoNLL 2008 achieved an F1-measure of 81.65% on the workshop’s evaluation Corpus."
"To the best of our knowledge no system was able to reproduce the successful results of (Swier and Stevenson, 2004) on the PropBank Roleset. "
Our approach most closely resembles the work of Fürstenau and Lapata (2009) who automatically expand a small training set using an automatic dependency alignment of unlabeled sentences.
"Fillmore (1968) introduced semantic structures called semantic frames, describing abstract actions or common situations (frames) with common roles and themes (semantic roles). Inspired by this idea different resources were constructed, including FrameNet (Baker et al., 1998) and PropBank (Palmer et al., 2005)."
"An alternative approach to semantic role labeling is the framework developed by Halliday (1994) and implemented by Mehay et al. (2005). PropBank has thus far received the most attention of the research community, and is used in our work "
"To estimate the parameters of the MEMM+predmodel we turn to the successful Maximum Entropy (Berger et al., 1996) parameter estimation Method."
"The Distributional Hypothesis, supported by theoretical linguists such as Harris (1954), states that words that occur in the same contexts tend to have similar meanings. This suggests that one can learn the similarity between two words automatically by comparing their relative contexts in a large unlabeled corpus, which was confirmed by different researchers (e.g. (Lin, 1998; McDonald and Ramscar, 2001; Grefenstette, 1994)). "
"We employ a slightly different clustering method here, the fullibmpredict method discussed in (Goodman, 2001). This method was shown to outperform the class based model proposed in (Brown et al., 1992) and can thus be expected to discover better clusters of words. "
"Statistical systems have enjoyed considerable success for information retrieval, especially using the vector space model (Salton et al., 1975)."
"Our summarization system relies on semantic predications provided by SemRep (Rindflesch and Fiszman, 2003), a program that draws on UMLS information to provide underspecified semantic interpretation in the biomedical domain (Srinivasan and Rindflesch, 2002; Rindflesch et al., 2000). "
"However, due to the challenges in providing semantic representation, semantic abstraction has not been widely pursued, although the TOPIC system (Hahn and Reimer, 1999) is a notable exception. "
"Phase 4 (saliency) is the final transforma tion phase and its operations are adapted from TOPIC’s (Hahn and Reimer, 1999) saliency operators. "
"Sub-events (Daniel et al., 2003) and sub-topics (Saggion and Lapalme, 2002) also contribute to the framework used for comparing documents in multidocument summarization. "
"For example, both Haghighi and Klein (2006) and Mann and McCallum (2008) have demonstrated results better than 66.1% on the apartments task described above using only a list of 33 highly discriminative features and the labels they indicate. "
"In traditional active learning (Settles, 2009), the machine queries the user for only the labels of instances that would be most helpful to the machine. "
"In this paper, we advocate using generalized expectation (GE) criteria (Mann and McCallum, 2008) for learning with labeled features. We provide an alternate treatment of the GE objective function used by Mann and McCallum (2008) and a novel speedup to the gradient computation. "
"Computing the first term of the covariance in Equation 2 requires a marginal distribution over three labels, two of which will be consecutive, but the other of which could appear anywhere in the sequence. We can compute this marginal using the algorithm of Mann and McCallum (2008). "
"Motivated by the feature query selection method of Tandem Learning (Raghavan and Allan, 2007) (see Section 4.2 for further discussion), we consider a feature selection metric similarity (sim) that is the maximum similarity to a labeled feature, weighted by the log count of the feature. "
"Liang et al. (2009) simultaneously developed a method for learning with and actively selecting measurements, or target expectations with associated noise. The measurement selection method proposed by Liang et al. (2009) is based on Bayesian experimental design and is similar to the expected information gain method described above. Consequently this method is likely to be intractable for real applications. "
"Chang et al. (2007) only obtain better results than 88.2% on cora when using 300 labeled examples (two hours of estimated annotation time), 5000 additional unlabeled examples, and extra test time inference constraints. "
"Concurrent work has used approximate counting schemes based on Morris (1978) to estimate in small space frequencies over a high volume input text stream (Van Durme and Lall, 2009; Goyal et al., 2009)."
Randomised algorithms are not the only compact representation schemes. Church et al. (2007) looked at Golomb Coding and Brants et al. (2007) used tries in a distributed setting. These methods are less succinct than randomised approaches. 
"Much progress in the area of semantic role labeling is due to the creation of resources like FrameNet (Fillmore et al., 2003), which document the surface realization of semantic roles in real world corpora. "
"Specifically, we view the task of inferring annotations for new verbs as an instance of a structural matching problem and follow a graph-based formulation for pairwise global network alignment (Klau, 2009). "
"Previous work has mainly used WordNet (Fellbaum, 1998) to extend FrameNet. For example, Burchardt et al. (2005) apply a word sense disambiguation system to annotate predicates with a WordNet sense and hyponyms of these predicates are then assumed to evoke the same frame. Johansson and Nugues (2007) treat this problem as an instance of supervised classification. "
In this paper we generalize the proposals of Pennacchiotti et al. (2008) and Fürstenau and Lapata (2009) in a unified framework. We create training data for semantic role labeling of unknown predicates by projection of annotations from labeled onto unlabeled data. 
"FrameNet provides definitions for more than 500 frames, of which we entertain only a small number. This is done using a method similar to Pennacchiotti et al. (2008). "
"We solve this optimization problem with a version of the branch-and-bound algorithm (Land and Doig, 1960). In general, this graph alignment problem is NP-hard (Klau, 2009) and usually solved approximately following a procedure similar to beam search. "
Our evaluation assessed the performance of a semantic frame and role labeler with and without the annotations produced by our method. The labeler followed closely the implementation described in Johansson and Nugues (2008). 
"We extracted features from dependency parses corresponding to those routinely used in the semantic role labeling literature (see Baker et al. (2007) for an overview). SVM classifiers were trained 2 with the L IB L INEAR library (Fan et al., 2008) and learned to predict the frame name, role spans, and role labels. "
"The algorithm is again described by Cutting et al. and by Sharman, and a mathematical justification for it can be tbund in Huang et al. (1990). "
"The first major use of HMMs for part of speech tagging was in CLAWS (Garside et al., 1987) in the 1970s. "
"One of the most effective taggers based on a pure HMM is that developed at Xerox (Cutting et al., 1992). An important aspect of this tagger is that it will give good accuracy with a minimal amount of manually tagged training data. "
"For example, CLAWS (Garside ct al., 1987) normalises the lexical probabilities by the total frequency of the word rather than of the tag. Consulting the Baum- Welch re-estimation formulae suggests that the approach described is more appropriate, and this is confirmed by slightly greater tagging accuracy. "
"In (Riley, 1989), Riley describes a decision-tree based approach to the problem. His performance on /he Brown corpus is 99.8%, using a model learned t'rom a corpus of 25 million words. "
"There has now been considerable work on discourse parsing using statistical bottom-up parsing (Soricut and Marcu, 2003), hierarchical agglomerative clustering (Sporleder and Lascarides, 2004), parsing from lexicalized tree-adjoining grammars (Cristea, 2000), and rule-based approaches that use rhetorical relations and discourse cues (Forbes et al., 2003; Polanyi et al., 2004; LeThanh et al., 2004). With the exception of Cristea (2000), most of this research has been limited to non-incremental parsing of textual monologues where, in contrast to incremental dialog parsing, predicting a system action is not relevant."
"In addition to the hand-crafted models listed above, researchers have built stochastic plan recognition models for interaction, including ones based on Hidden Markov Models (Bui, 2003; Blaylock and Allen, 2006) and on probabilistic context-free grammars (Alexandersson and Reithinger, 1997; Pynadath and Wellman, 2000). "
"This encoding was previously used for incremental sentence parsing by (Costa et al., 2001). With this method, there are many more choices of decision for the parser (195 decisions for our data) compared to the shift-reduce (32) and start-complete (82) methods. "
"We use the machine learning toolkit LLAMA (Haffner, 2006), which encodes multiclass classication problems using binary MaxEnt classifiers to increase the speed of training and to scale the method to large data sets. "
"We note that these results are competitive with those reported in the literature (e.g. (Poesio and Mikheev, 1998; Serafin and Eugenio, 2004)), although the dialog corpus and the label sets are different. "
"The expectation semiring (Eisner, 2002), originally proposed for finite-state machines, is one such “training” semiring, and can be used to compute feature expectations for the Estep of the EM algorithm, or gradients of the like-lihood function for gradient descent.  "
"We implement the expectation and variance semirings in Joshua (Li et al., 2009a), and demonstrate their practical benefit by using minimum-risk training to improve Hiero (Chiang, 2007). "
"A much more efficient approach (usually) is the traditional inside-outside algorithm (Baker, 1979). "
"If entropy H(p) is large (e.g., small γ), the Bayes risk Thus, Smith and Eisner (2006) try to avoid local minima by starting with large H(p) and decreasing it gradually during optimization. This is called deterministic annealing (Rose, 1998). "
"We used a 5-gram language model with modified Kneser-Ney smoothing, trained on the bitext’s English using SRILM (Stolcke, 2002). "
"MR (with or without DA) is scalable to tune a large number of features, while MERT is not. To achieve competitive performance, we adopt a forest reranking approach (Li and Khudanpur, 2009; Huang, 2008)."
"Clearly, adding more features improves (statistically significant) the case with only five features. We plan to incorporate more informative features described by Chiang et al. (2009). "
"Our approach is theoretically elegant, like other work in this vein (Goodman, 1999; Lopez, 2009; Gimpel and Smith, 2009). We used it practically to enable a new form of minimum-risk training that improved Chinese-English MT by 1.0 BLEU point. "
"Research on Statistical Machine Translation (SMT) has shown substantial progress in recent years. Since the success of phrase-based methods (Och and Ney, 2004; Koehn, 2004), models based on formal syntax (Chiang, 2005) or linguistic syntax (Liu et al., 2006; Marcu et al., 2006) have also achieved state-of-the-art performance."
"The second aspect motivating our work comes from the subspace learning method in machine learning literature (Ho, 1998), in which an ensemble of classifiers are trained on subspaces of the full feature space, and final classification results are based on the vote of all classifiers in the Ensemble. "
Nowadays most of the state-of-the-art SMT systems are based on linear models as proposed in Och and Ney (2002). 
"Since we also adopt a linear scoring function in Equation (3), the feature weights of our combination model can also be tuned on a development a set to optimize the specified evaluation metrics using the standard Minimum Error Rate Training (MERT) algorithm (Och 2003). "
"Our method is similar to the work proposed by Hildebrand and Vogel (2008). However, except the language model and translation length, we only use intra-hypothesis n-gram agreement features as Hildebrand and Vogel did and use additional intra-hypothesis n-gram disagreement feature  as Li et al. (2009) did in their co-decoding Method. "
Statistical significance is computed using the bootstrap re-sampling method proposed by Koehn (2004).  
"A number of part-of-speech taggers are readilyavailable and widely used, all trained and retrainable on text corpora (Church 1988;Cutting et al. 1992; Brill 1992; Weischedel et al. 1993)."
"Endemic structural ambiguity,which can lead to such difficulties as trying to cope with the many thousands of possi-ble parses that a grammar can assign to a sentence, can be greatly reduced by addingempirically derived probabilities to grammar rules (Fujisaki et al. 1989; Sharman, Je-linek, and Mercer 1990; Black et al. 1993) and by computing statistical measures oflexical association (Hindle and Rooth 1993)."
Aneffort has recently been undertaken to create automated machine translation systemsin which the linguistic information needed for translation is extracted automaticallyfrom aligned corpora (Brown et al. 1990).
"There are a number of efforts worldwide to manually annotate largecorpora with linguistic information, including parts of speech, phrase structure andpredicate-argument structure (e.g., the Penn Treebank and the British National Corpus(Marcus, Santorini, and Marcinkiewicz 1993; Leech, Garside, and Bryant 1994))."
"Useful tools, such as large aligned corpora (e.g., the aligned Hansards (Galeand Church 1991)) and semantic word hierarchies (e.g., Wordnet (Miller 1990)), havealso recently become available."
"Part-of-speech tagging is an activearea of research; a great deal of work has been done in this area over the past few years(e.g., Jelinek 1985; Church 1988; Derose 1988; Hindle 1989; DeMarcken 1990; Merialdo1994; Brill 1992; Black et al. 1992; Cutting et al. 1992; Kupiec 1992; Charniak et al. 1993;Weischedel et al. 1993; Schutze and Singer 1994)."
"Also, it ispossible to cast a number of other useful problems as part-of-speech tagging problems,such as letter-to-sound translation (Huang, Son-Bell, and Baggett 1994) and buildingpronunciation networks for speech recognition. Recently, a method has been proposedfor using part-of-speech tagging techniques as a method for parsing with lexicalizedgrammars (Joshi and Srinivas 1994)."
Roche and Schabes (1995) show a method for converting a listof tagging transformations into a deterministic finite state transducer with one statetransition taken per word of input; the result is a transformation-based tagger whosetagging speed is about ten times that of the fastest Markov-model tagger.
"In this respect, the present work is closer in spirit to Ji et al. (2005), who explore the employment of the ACE 2004 relation ontology as a semanticfilter. "
"For learning coreference decisions, we used a Maximum Entropy (Berger et al., 1996) model. "
"First, a set of pre-processing components including a chunker and a named entity recognizer is applied to the text in order to identify the noun phrases, which are further taken as REs to be used for instance generation. Instances are created following Soon et al. (2001)."
"Following Ng & Cardie (2002), our baseline system reimplements the Soon et al. (2001) system. "
"Systems were optimized on the WMT08 French- nglish development data (2000 sentences) using minimum error rate training (Och, 2003) and tested on the WMT08 test data (2000 sentences). "
All other settings were left at their default values as described by Chiang (2007) and Koehn et al. (2007).
Zhang et al. (2008) and Wellington et al. (2006) answer the question: what is the minimal grammar that can be induced to completely describe a training set? 
"One such problem is sense disambiguation.In the context of machine translation, Dagan and Itai (Dagan, Itai, and Schwall 1991;Dagan and Itai 1994) used corpora in the target language to resolve ambiguities inthe source language. Yarowsky (1992) proposed a method for sense disambiguationusing wide contexts."
"Most successfulmethods have followed speech recognition systems (Jelinek, Mercer, and Roukos 1992)and used large corpora to deduce the probability of each part of speech in the currentcontext (usually the two previous words--trigrams). These methods have reportedperformance in the range of 95-99% ""correct"" by word (DeRose 1988; Cutting et al.1992; Jelinek, Mercer, and Roukos 1992; Kupiec 1992)."
One good example for this is full-text retrieval systems (Choueka 1980). Suchsystems must handle the morphological ambiguity problem.
"Ornan (1986) for instance, developed a new writing system for Hebrew, called'The Phonemic Script.' This script enables the user to write Hebrew texts that aremorphologically unambiguous, in order to use them later as an input for variouskinds of natural language applications."
Choueka andLusignan (1985) presented a system for the morphological tagging of large texts that isbased on the short context of the word but also depends heavily on human interaction.
"Methods using the short context of a word in order to resolve ambiguity (usu-ally categorical ambiguity) are very common in English and other languages (DeRose1988; Church 1988; Karlsson 1990). A system using this approach was developed byLevinger and Ornan in order to serve as a component in their project of morphologicaldisambiguation in Hebrew (Levinger 1992). The main resource, used by this systemfor disambiguation, is a set of syntactic constraints that were defined manually bythe authors and followed two theoretical works that defined short context rules forHebrew (Pines 1975; Albeck 1992)."
Such a system that usesthe morpho-lexical probabilities together with a syntactic knowledge is described inLevinger (1992).
The combined system tackles the disambiguation problem by combining two kindsof linguistic information sources: Morpho-Lexical Probabilities and Syntactic Con-straints (a full description of this system can be found in Levinger [1992]).
"Previous results had shown a rather satisfying performance for hybrid systems such as the Statistical Phrase-based Post-Editing (SPE) (Simard et al., 2007) combination in comparison with purely phrase-based statistical models, reaching similar BLEU scores and often receiving better human judgement (German to English at WMT2007) against the BLEU metric. "
"We presented a few improvements to the Statistical Post Editing setup. They are part of an effort to better integrate a linguistic, rule-based system and the statistical correcting layer also illustrated in (Ueffing et al., 2008). "
"In order to push further this rule-extraction approach and according to our previous work (Dugast et al., 2007) (Dugast et al., 2008), the most promising would probably be the use of alternative meanings and a language model to decode the best translation in such a lattice. "
"Similarly to classical NLP tasks such as base noun phrase chunking (Ramshaw and Marcus, 1994), text chunking (Ramshaw and Marcus, 1995) or named entity recognition (Tjong Kim Sang, 2002), we formulate the mention detection problem as a classification problem, by assigning to each token in the text a label, indicating whether it starts a specific mention, is inside a specific mention, or is outside any mentions.  "
"In contrast, in a rule based system, the system designer would have to consider how, for instance, a WordNet (Miller, 1995) derived information for a particular example interacts with a part-of-speech-based information and chunking information. That is not to say, ultimately, that rule-based systems are in some way inferior to statistical models – they are built using valuable insight which is hard to obtain from a statistical-model only approach."
"Instead of a word-based model, we build a character-based one, since word segmentation errors can lead to irrecoverable mention detection errors; Jing et al. (2003) also observe that character-based models are better performing than word-based ones for Chinese named entity recognition. "
"The core of the approach is a novel decoder based on lattice parsing with quasisynchronous grammar (Smith and Eisner, 2006), a syntactic formalism that does not require source and target trees to be isomorphic."
Lopez (2009) recently argued for a separation between features/formalisms (and the independence assumptions they imply) from inference algorithms in MT; this separation is widely appreciated in machine learning.
"We define a single, direct log-linear translation model (Papineni et al., 1997; Och and Ney, 2002) that encodes most popular MT features and can be used to encode any features on source and target sentences, dependency trees, and alignments."
"We exploit similar approximate inference methods in regularized pseudolikelihood estimation (Besag, 1975) with hidden variables to discriminatively and efficiently train our model."
"Phrase-based systems such as Moses (Koehn et al., 2007) explicitly search for the highest-scoring string in which all source words are translated."
Recently Chiang (2007) introduced cube pruning as an approximate decoding method that extends a DP decoder with the ability to incorporate features that break the Markovian independence assumptions DP exploits. Techniques like cube pruning can be used to include the non-local features in our decoder.
"We recently proposed cube summing an approximate technique that permits the use of non-local features for inside DP algorithms (Gimpel and Smith, 2009). Cube summing is based on a slightly less greedy variation of cube pruning (Chiang, 2007) that maintains k-best lists of derivations for each DP chart item."
"For our target-language syntactic features g syn , we  use features similar to lexicalized CFG events (collins, 1999), specifically following the dependency model of Klein and Manning (2004)."
"Previous research has addressed revision in single-document summaries [Jing & McKeown, 2000] [Mani et al, 1999] and has suggested that revising summaries can make them more informative and correct errors. We believe that a generateand-revise strategy might also be used in creating better multiple-document summaries, within the framework of current extractive summarization Systems. "
"Rhetorical Structure Theory (RST) [Mann & Thompson, 1988] has contributed a great deal to the understanding of the discourse of written Documents. "
"Inspired by RST, [Radev, 2000] endeavored to establish a Cross-document Structure Theory (CST) that is more appropriate for MDS. "
"Based on RST, [Marcu, 2000] established a Rhetorical Parser. The parser exploits cue phrases in an algorithm that discovers discourse relationships between phrases in a text. "
"[Mani et al, 1999] focused on the revision of single-document summaries in order to improve their Informativeness. They noted that such revision might also fix ‘coherence errors.’ "
"To contrast, [Jing & McKeown, 2000] concentrated on analyzing human-written summaries in order to determine how professionals construct summaries. They found that most sentences could be traced back to specific cut-and-paste operations applied to the source document. "
"[Filatova & Hovy, 2001] addressed the issue of resolving temporal references in news stories. Although events in articles are not always presented in chronological order, readers must be able to reconstruct the timeline of events in order to comprehend the story. They endeavored to develop a module that could automatically assign a time stamp to each clause in a document. "
"To the best of our knowledge, and with the exception of Saggion and Lapalme (2002) indicative generation approach which included operations to add extra linguistic material to generate an indicative abstract, the work presented here is the first to investigate this relevant operation in the field of text abstracting and to propose a robust computational method for its simulation. In this paper we are interest "
"One could rely on existing trainable sentence selection (Kupiec et al., 1995) or even phrase selection (Banko et al., 2000) strategies to pick up appropriate β i ’s from the document to be abstracted and rely on recent information ordering techniques to sort the β i fragments (Lapata, 2003). "
"The features used for the experiments reported here are inspired by previous work in text summarization on content selection (Kupiec et al., 1995), rhetorical classification (Teufel and Moens, 2002), and information ordering (Lapata, 2003). "
"Note that in this work we have decided to evaluate the predicted structure against the true structure (a hard evaluation measure), in future work we will assess the abstracts with a set of quality questions similar to those put forward by the Document Understanding Conference Evaluations (also in a way similar to (Kan and McKeown, 2002) who evaluated their abstracts in a retrieval environment)."
"The insertion in the abstract of linguistic material not present in the input document has been addressed in paraphrase generation (Barzilay and Lee, 2004) and canned-based summarization (Oakes and Paice, 2001) in limited domains.  "
Saggion and Lapalme (2002) have studied and implemented a rule-based “verb selection” operation in their SumUM system which has been applied to introduce document topics during indicative summary generation. 
"In this paper we develop the first unsupervised approach to semantic parsing, using Markov logic (Richardson and Domingos, 2006). "
"Manually encoding all these variations into the grammar is tedious and error-prone. Supervised semantic parsing addresses this issue by learning to construct the grammar automatically from sample meaning annotations (Mooney, 2007). "
"In many NLP applications, there exist rich relations among objects, and recent work in statistical relational learning (Getoor and Taskar, 2007)and structured prediction (Bakir et al., 2007) has shown that leveraging these can greatly improve Accuracy. "
"One of the most powerful representations for this is Markov logic, which is a probabilistic extension of first-order logic (Richardson and Domingos, 2006).. Markov logic makes it possible to compactly specify probability distributions over complex relational domains, and has been successfully applied to unsupervised coreference resolution (Poon and Domingos, 2008) andother tasks. "
"In particular, lexical entries are no longer limited to be adjacent words as in Zettlemoyer and Collins (2005), but can be arbitrary fragments in a dependency tree. "
"We used the GENIA dataset (Kim et al., 2003) as the source for knowledge extraction. "
"The closest available system to USP in aims and capabilities is TextRunner (Banko et al., 2007), and we compare with it. TextRunner is the state-of-the-art system for open-domain informa- tion extraction; its goal is to extract knowledge from text without using supervised labels. "
The analysis presented here and the idea of the alignments havebeen greatly influenced by the exploration of abstracting manuals (Cremmins 1982).Our conceptual model comes mainly from the empirical analysis of the corpus buthas also been influenced by work on discourse modeling (Liddy 1991) and in the phi-losophy of science (Bunge 1967).
"We measure coselection between sentences produced by each methodand the sentences selected by the assessors, computing recall, precision, and F-scoreas in Firmin and Chrzanowski (1999). In order to obtain a clear picture, we borrowedthe scoring methodology proposed by Salton et al. (1997), additionally considering thefollowing situations"
"Maximum Entropy (MaxEnt) principle has been successfully applied in many classification and tagging tasks (Ratnaparkhi, 1996; K. Nigam and A.McCallum, 1999; A. McCallum and Pereira, 2000). We use MaxEnt modeling as thelearning component."
"To discover useful features, we exploit the concept of Association Rules (AR) (R. Agrawal and Swami, 1993; Srikant and Agrawal, 1997), which is originally proposed in Data Mining research field to identify frequent itemsets in a large Database."
"Many researchers (Blum and Mitchell, 1998; K. Nigam and Mitchell, 2000; Corduneanu and Jaakkola, 2002) have attempted to improve performance with unlabeled data. In this paper, we also propose a framework to bootstrap with unlabeled data."
"There is a growing amount of work on automatic extraction of paraphrases from text corpora (Lin and Pantel, 2001; Barzilay and Lee, 2003; Ibrahim et al., 2003; Dolan et al., 2004). "
"For more abstract matching, we would need syntacti- cally parsed data (Lin and Pantel, 2001). We ex- pect that this would also positively affect the cov- Erage."
"Our generation strategy is reminiscent of Robinand McKeown’s (1996) earlier work on revision for summarization, although Robin andMcKeown used a three-tiered representation of each sentence, including its semanticsand its deep and surface syntax, all of which were used as triggers for revision."
"This approach, originally proposed by Knight andHatzivassiloglou (1995) and Langkilde and Knight (1998), is a standard method usedin statistical generation."
"While this approach exploits only syntactic and lexical information, Jing andMcKeown (2000) also rely on cohesion information, derived from word distribution ina text"
"In addition to reducing the original sentences, Jing andMcKeown (2000) use a number of manually compiled rules to aggregate reducedsentences; for example, reduced clauses might be conjoined with and."
"The only other text-to-text generation approach able to produce new utterances isthat of Pang, Knight, and Marcu (2003)."
"Our algorithmperforms local alignment, while the algorithm of Pang, Knight, and Marcu (2003)performs global alignment."
"Second, our method is an instance of a multisequence alignment, 15 in contrast to thepairwise alignment described in Meyers, Yangarber, and Grishman (1996)."
"In fact, previous applications of multisequence alignment have beenshown to increase the accuracy of the comparison in other NLP tasks (Barzilay andLee 2002; Bangalore, Murdock, and Riccardi 2002; Lacatusu, Maiorano, and Harabagiu2004); unlike our work these approaches operate on strings, not trees, and with theexception of (Lacatusu, Maiorano, and Harabagiu 2004), they apply alignment to paral-lel data, not comparable texts."
Recent research (Daume et al. 2002) has show that syntax-based languagemodels are more suitable for language generation tasks; the study of such models isa promising direction to explore.
"Second, we devised a method to obtain the expected word N-gram count in the target texts, using an N-best word segmentation algorithm (Nagata, 1994). "
"In English part of speech taggers, the maximization of Equation (1) to get the most likely tag sequence, is accomplished by the Viterbi algorithm (Church, 1988), and the maximum likelihood estimates of the parameters of Equation (2) are obtained from untagged corpus by the Forward Backward algorithm (Cutting et al., 1992). "
"N-best word segmentation hypotheses can be obtained by using the Forward-DP Backward A* algorithm (Nagata, 1994)."
"(Chang et al., 1995) proposed an automatic dictionary construction method for Chinese from a large unsegmented corpus (311591 sentences) with the help of a small segmented seed corpus (1000 Sentences). "
"This is achieved by introducing an explicit statistical model of unknown words, and by using an N best word segmentation algorithm (Nagata, 1994)as an approximation of the generalized Forward Backward algorithm."
"We trained a 5-gram language model from the provided English monolingual training data and the non-Europarl portions of the parallel training data using modified Kneser-Ney smoothing as implemented in the SRI language modeling toolkit (Kneser and Ney, 1995; Stolcke, 2002)."
"To tune the feature weights of our system, we used a variant of the minimum error training algorithm (Och, 2003) that computes the error statistics from the target sentences from the translation search space (represented by a packed forest) that are exactly those that are minimally discriminable by changing the feature weights along a single vector in the dimensions of the feature space (Macherey et al., 2008)."
"To construct the segmentation lattices, we define a log-inear model of compound word segmentation inspired by Koehn and Knight (2003), making use of features including number of morphemes hypothesized, frequency of the segments as free-standing morphemes in a training corpus, and letters in each segment. "
"To address this, we explicitly generate beginning and end sentence markers as part of the translation process, as sug- gested by Xiong et al. (2008). The results of doing this are shown in Table 2. "
"In our experiments, we treated the 128 most frequent words in the corpus as function words, similar to Setiawan et al. (2007)."
"In support of this processing, we rely on the linguistic and domain knowledge contained in the National Library of Medicine's Unified Medical Language System (UMLS ®) as well an existing tool, the SPECIALIST minimal commitment parser (Aronson et al. 1994). The UMLS (Humphreys et al. 1998) consists of several knowledge sources applicable in the biomedical domain: the Metathesaums, Semantic Network, and SPECIALIST Lexicon (McCray et al. 1994). "
In order to identify binding terminology in text we rely on the approach discussed in (Rindfiesch et al. 1999).
ARBITER pursues limited coordination identification in the spirit of Agarwal and Boggess (1992) and Rindflesch (1995). Only binding terms are considered as candidates for coordination.
"For training in the English experiments, we used WSJ (Marcus et al., 1993). We had to change the format of WSJ to prepare it for our tagging software."
"A simple rule-based part of speech (RBPOS) tagger is introduced in (Brill, 1992). The accuracy of this tagger for English is comparable to a stochastic English POS tagger. "
"Before trying some completely different approach, we would like to improve the current simple approach by some other simple measures: adding a morphological analyzer (Hajji, 1994) as a frontend to the tagger (serving as a ""supplier"" of possible tags, instead of just taking all tags occurring in the training data for a given token), simplifying the tagset, adding more data. "
"To select these, we use the idea of strong chains introduced by Barzilay and Elhadad (1997). "
"Hybrid approaches such as extracting phrases instead of sentences and recombining these phrases into salient text have been proposed (Barzilay, McKeown, and Elhadad 1999). "
"Other recent work looks at summarization as a process of revision; in this work, the source text is revised until a summary of the desired length is achieved (Mani, Gates, and Bloedorn 1999). Additionally, some research has explored cutting and pasting segments of text from the full document to generate a summary (Jing and McKeown 2000). "
"Exploiting the maximum entropy (Berger et al., 1996) framework, the conditional distribution Pr(e, a | f ) can be determined through suitable real valued functions (called features) h  and takes the parametric Form "
"This pre-rocessing step can be accomplished by applying the GIZA++ toolkit (Och and Ney, 2003) that provides Viterbi alignments based on IBM Model-4. "
"In general, phrases are extracted with maximum length in the source and target defined by the parameters J max and I max . All such phrase-pairs are efficiently computed by an 2 algorithm with complexity O(lI max J max ) (Cet- tolo et al., 2005). "
"Thanks to the nice property of kernel-basedmachine learning method that can implicitly ex-plore (structured) features in a high dimensional feature space (Vapnik, 1995), in this paper we propose using convolution tree kernel (Haussler, 1999; Collins and Duffy, 2001) to explore the structured syntactic knowledge for phrase reordering and further combine the tree kernel with other diverse linear features into a composite kernel to strengthen the model’s predictive ability."
"We use the MaxEnt-based BTG translation system (Xiong et al., 2006) as our baseline. It is a phrase-based SMT system with BTG reordering constraint."
"Thus, it is critical to understand which portion of a parse tree (i.e. structured feature space) is the most effective to represent a reordering instance. Motivated by the work of (Zhang et al., 2006), we here examine four cases that contain different sub-structures as shown in Fig. 1. "
"We generate the boundary word features from the extracted reordering instances in the same way as discussed in Xiong et al. (2006) and use Zhang’s MaxEnt Tools 2 to train a reordering model for the 2 nd baseline system. Similarly, we use the algorithm 1 in Xiong et al. (2008) to extract features and use the same MaxEnt Tools to train a reordering model for the 3 rd baseline system. "
"Nevertheless, since the seminal work of Hobbs et al. (1993), it has been possible to conceptualize pragmatic interpretation as a unified reasoning process that selects a representation of the speaker’s contribution that is most preferred according to a background model of how speakers tend to behave. "
"We continue a tradition of research that uses simple referential communication tasks to explore the organization and processing of human-c mputer and mediated human–human conversation, including recently (DeVault and Stone, 2007; Gergle et al., 2007; Healey and Mills, 2006; Schlangen and Fernández, 2007). "
Our specific task is a two player object-identification game adapted from the experiments of Clark and Wilkes-Gibbs (1986) and Brennan and Clark (1996) 
"We give a brief sketch here to highlight the content of COREF’s representations, the sources of information that COREF uses to construct them, and the demands they place on disambiguation. See DeVault (2008) for full details. "
"In the second step, the selected features were used to train the model to estimate probabilities. We used MALLET’s implementation of Limited memory BFGS (Nocedal, 1980). "
"To quantify the performance of the learned model in comparison to our baseline, we adapt the mean reciprocal rank statistic commonly used for evaluation in information retrieval (Vorhees, 1999). We expect that a system will use the probabilities calculated by a disambiguation model to decide which interpretations to pursue and how to follow them up through the most efficient interaction."
The first serious linguistic competitor to data-driven statistical taggers is the English Constraint Grammar parser. EngCG (cf. Voutilainen et al. 1992; Karlsson et al. (eds.) 1995). The tagger consists of the following sequentially applied modules
"However, Voutilainen and Jarvinen (1995) empirically show that an interjudge agreement virtually of 1()0% is possible, at least with the EngCG tag set if not with the original Brown Corpus tag set. "
"By varying the threshold, we can perform a recall-precision, or error-rate-ambiguity, tradeoff. A similar strategy is adopted in (de Mar- Cken 1990). "
"The results from CoNLL shared tasks in 2005 and 2008 (Carreras and Marquez, 2005; Koomen et al., 2005; Surdeanu et al., 2008; Johansson and Nugues, 2008), further show that SRL pipeline may be one of the standard to achieve a state-of-the-art performance in practice. "
"As for the former (hereafter it is referred to synPth), we continue to use a dependency version of the pruning algorithm of (Xue and Palmer, 2004). The pruning algorithm is readdressed as the following. "
"One is gold-standard syntactic input, and other two are based on automatically parsing results of two parsers, the state-of-the-art syntactic parser described in (Johansson and Nugues, 2008) 7 (it is referred to Johansson) and an integrated parser described as the following (referred to MSTME )"
"The parser is basically based on the MSTParser 8 using all the features presented by (McDonald et al., 2006) with projective parsing. Moreover, we exploit three types of additional features to improve the parser. "
"For the sake of efficiency, we use a fast transition based parser based on maximum entropy as in Zhao and Kit (2008). We still use the similar feature notations of that work "
"As mentioned by (Pradhan et al., 2004), argument identification plays a bottleneck role in improving the performance of a SRL system. The effectiveness of the proposed additional pruning techniques may be seen as a significant improvement over the original algorithm of (Xue and Palmer, 2004). "
"(Titov et al., 2009) reported the best result by using joint learning technique up to now. The comparison indicates that our integrated system outputs a result quite close to the state-of-the-art by the pipeline system of (Johansson and Nugues, 2008) as the same syntactic structure input is adopted. It is worth noting that our system actually competes with two independent sub-systems of (Johansson and Nugues, 2008), one for verbal predicates, the other for nominal predicates. "
"Recent comparisons of approaches that can be trained on corpora (van Halteren et al., 1998; Volk and Schneider, 1998) have shown that in most cases statistical aproaches (Cutting et al., 1992; Schmid, 1995; Ratnaparkhi, 1996) yield better results than finite state, rule-based, or memory-based taggers (Brill, 1993; Daelemans et al., 1996)."
"the method of handling unknown words that seems to work best for inflected languages is a suffix analysis as proposed in (Samuelsson, 1993). Tag probabilities are set according to the word's endIng."
" uilding on a recent proposal in this direction by Turney (2008), we propose a generic method of this sort, and we test it on a set of unrelated tasks, reporting good performance across the board with very little task-specific tweaking. "
"Turney (2008) is the first, to the best of our knowledge, to raise the issue of a unified approach. In particular, he treats synonymy and association as special cases of relational similarity "
"Mirkinet al. 2006 also integrate information from the lexical patterns in which two words co-occur and similarity of the contexts in which each word occurs on its own, to improve performance in lexical entailment Acquisition. "
The first task we evaluated our algorithm on is the SAT analogy questions task introduced by Turney et al. (2003).
We adopt a similar approach to the one used in Turney (2008) and consider each question as a separate binary classification problem with one positive training instance and 5 unknown pairs. 
"Specifically, we test selectional preferences on the dataset constructed by Padó (2007), that collects average plausibility judgments (from 20 speakers) for nouns as either subjects or objects of verbs (211 noun-verb pairs). "
"Thus, corpus-based approaches may have serious difficulties in capturing these relations (Havasi et al., 2007), but there are reasons to believe that they could still be useful: Eslick (2006) uses the assertions of ConceptNet as seeds to parse Web search results and augment ConceptNet by new candidate relations."
"Fortunately, using distributional characteristics of term contexts, it is feasible to induce part-of-speech categories directly from a corpus of sufficient size, as several papers have made clear (Brown et al., 1992; Schütze, 1993; Clark, 2000). "
"There are many applications of computational linguistics, particularly those involving “shallow” processing, such as information extraction, which can benefit from such automatically derived information, especially as research into acquisition of grammar matures (e.g., (Clark, 2001))."
"Our approach to inducing syntactic clusters is closely related to that described in Brown, et al, (1992) which is one of the earliest papers on the subject. "
"We seek to find a partition of the vocabulary that maximizes the mutual information between term categories and their contexts. We achieve this in the framework of information theoretic co-clustering (Dhillon et al., 2003), in which a space of entities, on the one hand, and their contexts, on the other, are alternately clustered in a way that maximizes mutual information between the two spaces."
"We conducted experiments with the Reuters-21578 Corpus a relatively tiny one for such experiments. Clark (2000) reports results on a corpus containing 12 million terms, Schütze (1993) on one containing 25 million terms, and Brown, et al, (1992) on one containing 365 million terms. In contrast, we count approximately 2.8 million terms in Reuters-21578."
Schütze ( 1993; 1995)proposes two distinct methods by which ambiguity may be resolved. 
"Most state-of-the-art system combination methods are based on constructing a confusion network (CN) from several input translation hypotheses, and choosing the best output from the CN based on several scoring functions (e.g. Rosti et. al., 2007a, He et. al., 2008, Matusov et al. 2008)."
"Prior work has used a number of heuristics to deal with these problems (Matusov, et. al., 2006, He et al 08). Some work has made such decisions in a more principled fashion by computing model-based scores (Matusov et al. 2008), but still special- purpose algorithms and heuristics are needed and a single alignment is fixed."
"Other than confusion-network-based algorithms, work most closely related to ours is the method of MT system combination proposed in (Jayaraman and Lavie 2005), which we will refer to as J&L."
"If one of the two words is ε, the posterior of aligning word ε to state j is computed as suggested by Liang et al. (2006)"
"Raw text is processed by a preprocessor which segments the text into sentences using various heuristics about punctuation, and then to- kenizes and runs it through a wide-coverage high performance morphological analyzer developed using two-level morphology tools by Xerox (Kart- Tunen, 1993). "
"However, the use of regular relations and finite state transducers (Kaplan and Kay, 1994) provide a very efficient implementation Method. "
"We have recently completed a prototype implementation of this approach (in C) for English (Brown Corpus) and have obtained quite similar results (Tiir, Of- lazer, and Oz-kan, 1997). "
"The convenience of adding new rules in without worrying about where exactly it goes in terms of rule ordering (something that hampered our progress in our earlier work on disambiguating Turkish morphology (Oflazer and KuruSz, 1994; Oflazer and Tiir, 1996)), has also been a key positive point. "
"This has been quite useful for our work on tagging English (Tfir, Oflazer, and 0z-kan, 1997) where such rules with negative weights were used to fine tune the behavior of the tagger in various prob- lematic cases. "
"While many text categorization models have been proposed so far, in this paper, we concentrate on the probabilistic models (Robertson and Sparck Jones, 1976; Kwok, 1990; Fuhr, 1989; Lewis, 1992; Croft, 1981; Wong and Yao, 1989; Yu et al., 1989) because these models have solid formal grounding in probability theory. "
Robertson and Sparck Jones (1976) make use of the well-known logistic (or log-odds) transformation of the probability P(c]d). 
"In general, frequently appearing terms in a document play an important role in information retrieval (Salton and McGill, 1983). Salton and Yang experimentally verified the importance of within-document term frequencies in their vector model (Salton and Yang, 1973). "
"A well-known remedy for this problem is to use (r + 0.5)/(R + 1) as the estimate of P(T = lie ) (Robertson and Sparck Jones, 1976). While various smoothing methods (Church and Gale, 1991; Jelinek, 1990) are also applicable to these situations and would be expected to work better, we used the simple ""add one"" remedy in the following experiments."
"To solve problems 1 and 2 of PRW, Kwok (1990) stresses the assumption that a document consists of terms. This theory is called the Component Theory (CT). "
Fuhr (1989) solves problem 2 by assuming that a document is probabilistically indexed by its term vectors. This model is called Retrieval with Probabilistie Indexing (RPI). 
"There are several strategies for assigning categories to a document based on the probability P(cld ). The simplest one is the k-per-doc strategy (Field, 1975) that assigns the top k categories to each document. "
"We have to compare our probabilistic model to other non probabilistic models like decision tree/rule based models, one of which has recently been reported to be promising (Apt4 et al., 1994). "
"While we used simple document representation in which a document is defined as a set of nouns, there could be considered several improvements, such as using phrasal information (Lewis, 1992), clustering terms (Sparck Jones, 1973), reducing the number of features by using local dictionary (Apt4 et al., 1994), etc. "
"If we need to generate summaries that can be used to indicative what topics are addressed in the original document, and thus can be used to alert the uses as the source content, i.e., the indicative function (Mani et al., 1999), extraction approach is capable of handling this kind of tasks. "
"A comprehensive survey of text summarization approaches can be found in (Mani, 1999). We briefly review here based on extraction approach. Luhn (1959) proposed a simple but effective approach by using term frequencies and their related positions to weight sentences that are extracted to form a summary."
The first known supervised learning algorithm was proposed by Kupiec et al. (1995). Their approach estimates the probability that a sentence should be included in a summary given its feature values based on the independent assumption of Bayes’ Rule. 
"In order to determine word boundaries, we employed the longest matching algorithm (Sornlertlamvanich, 1993). "
"We further describe an efficient approach to alleviate this problem by using an idea of phrase construction (Ohsawa et al., 1998). "
"Our approach is reminiscent of Luhn’s approach (1959) but uses the other term weighting technique instead of the term frequency. Luhn suggested that the frequency of a word occurrence in a document, as well as its relative position determines its significance in that document. "
"More recent works have also employed Luhn’s approach as a basis component for extracting relevant sentences (Buyukkokten et al.,2001; Lam- desina and Jones, 2001). This approach performs well despite of its simplicity. In our previous work (Jaruskulchai et al., 2003), we also applied this approach for summarizing and browsing Thai documents through PDAs. "
"In our work, we decide to use TLTF (Term Length Term Frequency) term weighting technique (Banko et al., 1999) for scoring words in the document instead of TFIDF. "
"The recent approach for editing extracted text spans (Jing and McKeown, 2000) may also produce improvement for our algorithm."
"This assumption underlies a growing number of recent syntactic theories which give up the context-free constituent ba.ckbone, cf. (McCawley, 1987), (Dowty, 1989), (Reape, 1993), (Kathol and Pollard, 1995). These approaches provide an adequate explanation for several issues problematic ibr phrase-structure g r a m m a r s (clause union, extraposition, diverse second-position phenomena). "
Jing and McKeown (1999; 2000) found that human summarization can be traced back to six cut-and-paste operations of a text and proposed a revision method consisting of sentence reduction and combination modules with a sentence extraction part. Mani and colleagues (1999) proposed a summarization system based on “draft and revision” together with sentence extraction. The revision part is achieved with the sentence aggregation and smoothing modules. 
"To ameliorate this, revision of the extracted sentences is also thought to be effective, and many ideas and methods have been proposed so far. For example, Otterbacher and colleagues (2002) analyzed manually revised extracts and factored out cohesion problems. Nenkova (2008) proposed a revision idea that utilizes noun coreference with linguistic quality improvements in mind."
Barzilay and McKeown (2005) proposed an idea called sentence fusion that integrates information in overlapping sentences to produce a non- overlapping summary sentence. 
Their algorithm was further modified and applied to the German biographies by Filippova and Strube (2008)
"Like the work of Jing and McKeown (2000) and Mani et al. (1999), our work was inspired by the summarization method used by human abstractors."
"Identifying our coreferential chunks is even harder than the conventional coreference resolution, and we made a simplifying assumption as in Nenkova (2008) with some additional conditions that were obtained through our preliminary experiments "
"The paper presents an application of Structural Correspondence Learning (SCL) (Blitzer et al., 2006) for domain adaptation of a stochastic attribute-value grammar (SAVG). So far, SCL has been applied successfully in NLP for Part-of-Speech tagging and Sentiment Analysis (Blitzer et al., 2006; Blitzer et al., 2007). "
"Studies on the supervised task have shown that straightforward baselines (e.g. models based on source only, target only, or the union of the data) achieve a relatively high performance level and are “surprisingly difficult to beat” (Daumé III, 2007).  "
"While several authors have looked at the supervised adaptation case, there are less (and especially less successful) studies on semi-supervised domain adaptation (McClosky et al., 2006; Blitzer et al., 2006; Dredze et al., 2007). Of these, McClosky et al. (2006) deal specifically with self training for data-driven statistical parsing. They show that together with a re-ranker, improvements are obtained. Similarly, Structural Correspondence Learning (Blitzer et al., 2006; Blitzer et al., 2007; Blitzer, 2008) has proven to be successful for the two tasks examined, PoS tagging and Sentiment Classification."
"The few studies on adapting disambiguation models (Hara et al., 2005; Plank and van Noord, 2008) have focused exclusively on the supervised scenario."
"We examine the effectiveness of Structural Correspondence Learning (SCL) (Blitzer et al., 2006) for this task, a recently proposed adaptation technique shown to be effective for PoS tagging and Sentiment Analysis. The system used in this study is Alpino, a wide-coverage Stochastic Attribute Value Grammar (SAVG) for Dutch (van Noord and Malouf, 2005; van Noord, 2006). "
"Alpino (van Noord and Malouf, 2005; van Noord, 2006) is a robust computational analyzer for Dutch that implements the conceptual two-stage parsing Approach. "
"Pivots are features occurring frequently and behaving similarly in both domains (Blitzer et al., 2006). They are inspired by auxiliary problems from Ando and Zhang (2005)."
"This allows us to get a possibly noisy, but more abstract representation of the underlying data. The set of features used in Alpino is further described in van Noord and Malouf (2005)."
"In practice, there are more free parameters and model choices (Ando and Zhang, 2005; Ando, 2006; Blitzer et al., 2006; Blitzer, 2008) besides the ones discussed above."
"Due to the positive results in Ando (2006), Blitzer et al. (2006) include this in their standard setting of SCL and report results using block SVDs only. "
"If we want to compare the performance of disambiguation models, we can employ the φ mesure (van Noord and Malouf, 2005; van Noord, 2007). Intuitively, it tells us how much of the disambiguation problem has been solved. "
"(Church, 1992) claims that part-of-speech taggers depend almost exclusively on lexical probabilities, whereas other researchers, such as Voutilainen (Karlsson et al., 1995) argue that word ambiguities vary widely in function of the specific text and genre. Indeed, part of Church's argument is relevant if a system is based on a large corpus such as the Brown corpus (Francis and Ku~era, 1982) which represents one million surface forms of morpho-syntacticaJly disambiguated words from a range of balanced texts. "
"Given the problems created by estimating probabilities on a corpus of restricted size, we present in Section 4 a solution for coping with these difficulties. We suggest a new paradigm called genotype , derived from the concept of ambiguity class (Kupiec, 1992), which gives a more efficient representation of the data in order to achieve more accuracy in part-of-speech disambiguation. "
"(Brill, 1995) presents a rule-based part-of-speech tagger for unsupervised training corpus. Some of the rules of his system and the fact that he uses a minimal training corpus suggests some similarities with our system, but the main aim of the work is to investigate methods to combine supervised and unsupervised training in order to come up with a highly performing tagger. "
"Summarization of such texts requires a different approach from, for example, that used in the summarization of news articles. For example, Barzilay, McKeown, and Elhadad (1999) introduce the concept of information fusion, which is based on the identification of re- curren  description of the same events in news articles. This approach works well because in the news domain, newsworthy events are frequently repeated over a short period of time. In scientific writing, however, similar “events” are rare  "
Metadiscourse is ubiquitous in scientific writing: Hyland (1998) found a metadiscourse phrase on average after every 15 words in running text. 
"Paice (1990) introduces grammars for pattern matching of indicator phrases, e.g., the aim/purpose of this paper/article/study and we conclude/propose. "
This heterogeneity is in stark contrast to the systematic structures Liddy (1991) found to be produced by professional abstractors.  
"A simpler machine learning approach using only word frequency information and no other features, as typically used in tasks like text classification, could have been employed (and indeed Nanba and Okumura [1999] do so for classifying citation Contexts). "
"For the task of dialogue act disambiguation, Samuel, Carberry, and Vijay-Shanker (1999) suggest a method of automatically finding cue phrases for disambiguation. It may be possible to apply this or a similar method to our data and to compare the performance of automatically gained resources with manual ones. "
"Resnik and Diab (2000) present yet other measures of verb similarity, which could be used to arrive at a more data-driven definition of verb classes. "
"We also plan to consider reasonable applications for semantic tagging. One possibility would be to use semantic tagging in the framework of an intelligent on line dictionary lookup such as LocoLex [Bauer et al, 1995]. "
"The learning corpus can consist of plain text, but the best results seem achievable with annotated corpora (Merialdo 1994; Elworthy 1994). "
"With respect to computational morphology, witness for instance the success of the Two-Level paradigm introduced by Koskenniemi (1983) "
"Before proceeding further with the main argument, consider three very recent hybrids – systems that employ linguistic rules for resolving some of the ambiguities before using automatically generated corpus-based information: collocation matrices (Leech, Garside and Bryant 1994), Hidden Markov Models (Tapanainen and Voutilai- nen 1994), or syntactic patterns (Tapanainen and Jairvinen 1994).   "
"Functional representation of phrases and clauses has been introduced to facilitate expressing syntactic generMisations. The representation is introduced in (Voutilainen and Tapanainen 1993; Voutilainen 1994); here, only the main characteristics are given "
"A small error ratehas been achieved by such systems when a restricted, application-dependent POS setis used; e.g., an error rate of 2-6 percent has been reported by Marcus, Santorini, andMarcinkiewicz (1993) using the Penn Treebank corpus."
"Recently, several solutions to the problem of tagging unknown words have beenpresented (Charniak et al. 1993; Meteer, Schwartz, and Weischedel 1991)."
"Hypothesesfor unknown words, both stochastic (Dermatas and Kokkinakis 1993, 1994; Malteseand Mancini 1991; Weischedel et al. 1993), and connectionist (Eineborg and Gamback1993; Elenius 1990) have been applied to unlimited vocabulary taggers."
Various interpolation techniques have been proposed for the estimationof the model parameters for unseen events or to smooth the modelparameters (Church and Gale 1991; Essen and Steinbiss 1992; Jardinoand Adda 1993; Katz 1987; McInnes 1992).
"For more details on the linguistic specifications of the annotation scheme see (Skut et al 1997). A similar approach has been also successfully applied in the T S N L P database, cf. (Lehmann et al 1996). "
"Experience gained from the development of the Penn Treebank (Marcus et al., 1994) has shown that automatic annotation is useful only if it is absolutely correct, while wrong analyses are often difficult to detect and their correction can be time-consuming. "
"The work reported here is a logical continuation of two specific strands of research aimed in this general direction. The first is the popular idea of statistical tagging e.g. (DeRose, 1988; Cutting et al., 1992; Church, 1988). "
"In the specific case of part-of-speech tagging, it is well-known (DeMarcken, 1990) that a large proportion of the incorrect tags can be eliminated safely i.e. with very low risk of eliminating correct tags. "
"This part of the paper is essentially an extension and generalization of the line of work described in (Rayner, 1988; Rayner and Samuelsson, 1990; Samuelsson and Rayner, 1991; Rayner and Samuels- son, 1994; Samuelsson, 1994b). "
"The result is a ""specialized"" grammar; this has a larger number of rules, but a simpler structure, allowing it in practice to be parsed very much more quickly using an LR- based method (Samuelsson, 1994a). "
"In the second phase, the resulting set of ""chunked"" rules is converted into LR table form, using the method of (Samuelsson, 1994a). "
"The experiment was carried out using both the chunking criteria from (Rayner and Samuelsson, 1994) (the ""Old"" scheme), and the chunking criteria described in Section 3 above (the ""New"" scheme). "
"Preliminary experiments we have carried out on the Swedish version of the CLE (Gambaick and Rayner 1992) have been encouraging; using exactly the same pruning methods and EBL chunking criteria as for English, we obtain comparable speed-ups. "
"We intend to do so soon, and also to repeat the experiments on the French version of the CLE (Rayner, Carter and Bouillon, 1996). "
"Our approach has been fully implemented in the program LExAs. Part of the implementation uses PEBLS (Cost and Salzberg, 1993; Rachlin and Salzberg, 1993), a public domain exemplar-based learning system. "
"This metric for measuring distance is adopted from (Cost and Salzberg, 1993), which in turn is adapted from the value difference metric of the earlier work of (Stanfill and Waltz, 1986). "
"One line of research focuses on the use of the knowledge contained in a machine-readable dictionary to perform WSD, such as (Wilks et al., 1990; Luk, 1995). In contrast, LEXAS uses supervised learning from tagged sentences, which is also the approach taken by most recent work on WSD, including (Bruce and Wiebe, 1994; Miller et al., 1994; Leacock et al., 1993; Yarowsky, 1994; Yarowsky, 1993; Yarowsky, 1992). "
"Most recently, Yarowsky used an unsupervised learning procedure to perform WSD (Yarowsky, 1995), although this is only tested on disambiguating words into binary, coarse sense distinction. The effectiveness of unsupervised learning on disambiguating words into the refined sense distinction of WoRBNET needs to be further investigated. "
"Our point of departure is the work of Lappin and Leass (1994, henceforth L&L) and Dagan et al. (1995). (See also Dagan and Itai (1990).  "
"Dagan et al. (1995) then developed a postprocessor based on predicate-argument statistics that was used to override RAP’s decision when it failed to express a clear preference between two or more antecedents, which resulted in a modest rise in performance (2.5%). "
"As previously indicated, the weight-based scheme of L&L suggests MaxEnt modeling (Berger et al., 1996) as a particularly natural choice for a machine learning approach. "
"We took two approaches to smoothing. First, because Dagan et al. used Good-Turing smoothing in their experiments, we did likewise so as to replicate their work as closely as possible. Second, we tried an approach based on the distributional clustering method of Pereira et al. (1993).  "
"We carried out an error analysis to gain further insight into this question. To address the data-sparsity issue, we employed the technique used in Keller and Lapata (2003, K&L) to get a more robust approximation of predicate-argument counts. "
"We describe a POS tag-ger based on the work described in (Padr6, 1996),that is able to use bi/trigram information, auto-matically learned context constraints and linguisti-cally motivated manually written constraints."
"The usual solutions to this problem are: l) Prune the tree. either during the construction process (Quinlan. 1993) or afterwards (Mingers, 1989); 2) Smooth the conditional probability distributions using fresh corpus a (Magerman, 1996). "
In a first step the tree is completely expanded and afterwards it is pruned following a minimal cost-complexity criterion (Breiman et al.. 1984). 
"Consider more complex context features, such as non-limited distance or barrier rules in the style of (Samuelsson et al., 1996). "
"This accuracy compares very favourably with results reported in (de Marcken, 1990; Weisehedel et al., 1993; Kempe, 1994) - for instance, to reach the recall of 99.3 %, the system by (Weischedel et al., 1993) has to leave as many as three readings per word in its output. "
"The tagger is reported (Cutting el al., 1992) to have a better than 96 % accuracy in the analysis of parts of the Brown Corpus. The accuracy is similar to other probabilistic taggers.  "
"Only in the analysis of a few words it was agreed that a multiple choice was appropriate because of different meaning-level interpretations of the utterance (these were actually headings where some of the grammatical information was omitted). Overall, these results agree with our previous experiences (Karlsson et al., 1994) "
"We could leave the text partly disambiguated, and use a syntactic parser that uses both lin-guistic knowledge and corpus-based heuristics (see (Tapanainen and J//rvinen, 1994)) "
"Turney (2008) recently advocated the need for a uniform approach to corpus-based semantic tasks. Turney recasts a number of semantic challenges in terms of relational or analogical similarity. Thus, if an algorithm is able to tackle the latter, it can also be used to address the former. Turney tests his system in a variety of tasks, obtaining good results across the board. "
Our approach to selectional preference is nearly identical to the one of Padó et al. (2007). We solve SAT analogies with a simplified version of the method of Turney (2006). 
"Finally, our method to detect verb slot similarity is analogous to the “slot overlap” of Joanis et al. (2008) and others. "
"We use the dataset of Rubenstein and Good enough (1965), consisting of 65 noun pairs rated by 51 subjects on a 0-4 similarity scale (e.g. car- automobile 3.9, cord-smile 0.0). "
"Following Padó and Lapata (2007), we use Pearson’s r to evaluate how the distances (cosines) in the CxLC space between the nouns in each pair correlate with the ratings. "
"Syntactic alterations (Levin, 1993) represent a key aspect of the complex constraints that shape the syntax-semantics interface.  "
"In particular, we need to develop a backoff strategy for unseen pairs in the relational similarity tasks, that, following Turney (2006), could be based on constructing surrogate pairs of taxonomically similar words found in the CxLC space "
"We plan to explore how contextual effects can be modeled in our framework, focusing in particular on how composition affects word meaning (Erk and Padó, 2008). "
"Alternatively, DM could be represented as a three-mode tensor in the framework of Turney (2007), enabling smoothing operations analogous to singular value decomposition. "
In Statistical Machine Translation  SMT   recent work shows that WSD helps translation quality when the WSD system directly uses translation candidates as sense inventories  Most semiautomated approaches have met with limited success  and supervised learning models have tended to outperform dictionarybased classi cation schemes  
While studies have shown that ratings of MT systems by BLEU and similar metrics correlate well with human judgments   we are not aware of any studies that have shown that corpusbased evaluation metrics of NLG systems are correlated with human judgments  correlation studies have been made of individual components   but not of systems While EM has worked quite well for a few tasks  notably machine translations  starting with the IBM models 5   it has not had success in most others  such as partofspeech tagging   namedentity recognition  and contextfreegrammar induction  numerous attempts  too many to mention  
Incremental topdown and leftcorner parsers have been shown to effectively  and efficiently  make use of nonlocal features from the leftcontext to yield very high accuracy syntactic parses   and we will use such rich models to derive our scores Erk  compared a number of techniques for creating similarword sets and found that both the Jaccard coefficient and  s informationtheoretic metric work best 
4 Extended Minimum Error Rate Training Minimum error rate training  is widely used to optimize feature weights for a linear model  When we run our classifiers on resourcetight environments such as cellphones  we can use a random feature mixing technique  or a memoryefficient trie implementation based on a succinct data structure  to reduce required memory usage Evaluation Criteria Wellestablished objective evaluation measures like the word error rate  WER   positionindependent word error rate  PER   and the BLEU score  were used to assess the translation quality 
In recent several years  the system combination methods based on confusion networks developed rapidly   which show stateoftheart performance in benchmarks Although bialignments are known to exhibit high precision   in the face of sparse annotations we use unidirectional alignments as a fallback  as has been proposed in the context of phrasebased machine translation  
It is based on Incremental Sigmoid Belief Networks  ISBNs   a class of directed graphical model for structure prediction problems recently proposed in   where they were demonstrated to achieve competitive results on the constituent parsing task In our experience  this approach is advantageous in terms of translation quality  eg by 07  in BLEU compared to a minimum Bayes risk primary  
To solve this problem  we adopt an idea one sense per collocation which was introduced in word sense disambiguation research  The results show that  as compared to BLEU  several recently proposed metrics such as Semanticrole overlap   ParaEvalrecall   and METEOR  achieve higher correlation Parsing models have been developed for different languages and stateoftheart results have been reported for  eg  English  
For English  after a relatively big jump achieved by   we have seen two significant improvements   and  pushed the results by a significant amount each time In our final comparison  we have also included the results of   because it has surpassed  as well and we have used this tagger in the data preparation phase 
However  evaluations on the widely used WSJ corpus of the Penn Treebank  show that the accuracy of these parsers still lags behind the stateoftheart 
22 Maximum Entropy Models Maximum entropy  ME  models   also known as 928 loglinear and exponential learning models  provide a general purpose machine learning technique for classification and prediction which has been successfully applied to natural language processing including part of speech tagging  named entity recognition etc Maximum entropy models can integrate features from many heterogeneous information sources for classification Introduction Many stateoftheart machine translation  MT  systems over the past few years  rely on several models to evaluate the goodness of a given candidate translation in the target language 
The use of dependencies in MT evaluation has not been extensively researched before  one exception here would be    and requires more research to improve it  but the method shows potential to become an accurate evaluation metric 
Albeit simple  the algorithm has proven to be very efficient and accurate for the task of parse selection  It is an online training algorithm and has been successfully used in many NLP tasks  such as POS tagging   parsing   Chinese word segmentation   and so on 
We wish to minimize this error function  so we select accordingly  argmin summationdisplay a E  a   a   argmax a p  a  f e     4  Maximizing performance for all of the weights at once is not computationally tractable  but  has described an efficient onedimensional search for a similar problem For French\/English translation we use a state of the art phrasebased MT system similar to  
In agreement with recent results on parsing with lexicalised probabilistic grammars   our main result is that statistics over lexical features best correspond to independently established truman intuitive preferences and experimental findings Since human evaluation is costly and difficult to do reliably  a major focus of research has been on automatic measures of MT quality  pioneered by BLEU  and NIST  
It is interesting to note that while the study of how the granularity of contextfree grammars CFG affects the performance of a parser eg in the form 86 nIP [] n2NP [SUBJ] n4NR [] GSC4ES JiangZemin n3VP [] n5VV [] ESDO interview n6NP [OBJ] n7NR [ADJUNCT] AIC Thai n8NN [] D3D2 president f             PRED ESDO SUBJ f2  PRED GSC4ESNTYPE proper NUM sg   OBJ f3       PRED D3D2 NTYPE common NUM sg ADJUNCT   f4  PRED AICNTYPE proper NUM sg                          N  F nn3n5f n2n4f2 n6n8f3 n7f4 Figure  Cand fstructures with  links for the sentence GSC4ESESDOAICD3D2 of grammar transforms Johnson 998 and lexicalisation Collins 997 has attracted substantial attention to our knowledge there has been a lot less research on this subject for surface realisation a process that is generally regarded as the reverse process of parsing
The former term P  E  is called a language model  representing the likelihood of E The latter term P  J E  is called a translation model  representing the generation probability from E into J As an implementation of P  J E   the word alignment based statistical translation  has been successfully applied to similar language pairs  such as FrenchEnglish and German English  but not to drastically dierent ones  such as JapaneseEnglish 
The best previous result is an accuracy of 56   Introduction Hierarchical approaches to machine translation have proven increasingly successful in recent years   and often outperform phrasebased systems  on targetlanguage fluency and adequacy 
Throughout  the likelihood ratio  is used as significance measure because of its stable performance in various evaluations  yet many more measures are possible 
However  the study of  provides interesting insights into what makes a good distributional similarity measure in the contexts of semantic similarity prediction and language modeling While  does not discuss distinguishing more than 2 senses of a word  there is no immediate reason to doubt that the ` one sense per collocation ' rule  would still hold for a larger number of senses 
Similaritybased smoothing  provides an intuitively appealing approach to language modeling which is the classic work on collocation extraction  uses a twostage filtering model in which  in the first step  ngram statistics determine possible collocations and  in the second step  these candidates are submitted to a syntactic valida7Of course  lexical material is always at least partially dependent on the domain in question 
One of the most effective taggers based on a pure HMM is that developed at Xerox  The success of recent highquality parsers  relies on the availability of such treebank corpora Synchronous binarization  solves this problem by simultaneously binarizing both source and targetsides of a synchronous rule  making sure of contiguous spans on both sides whenever possible 
 reported very high results  96  on the Brown corpus  for unsupervised POS tagging using Hidden Markov Models  HMMs  by exploiting handbuilt tag dictionaries and equivalence classes 
All the enumerated segment pairs are listed in the following table  Feature x  y Feature x  y AM  c  c0 AM2  c2c  c0 AM 2 c  c0c AM2 2 c2c  c0c AM 3 c  c0cc2 AM3  c3c2c  c0 We use Dunnings method  because it does not depend on the assumption of normality and it allows comparisons to be made between the signiflcance of the occurrences of both rare and common phenomenon 
3 Extending Bleu and Ter with Flexible Matching Many widely used metrics like Bleu  and Ter  are based on measuring string level similarity between the reference translation and translation hypothesis  just like Meteor Most of them  however  depend on finding exact matches between the words in two strings 
Promising features might include those over source side reordering rules  or source context features  
The BLEU metric  and the closely related NIST metric  along with WER and PER 48 have been widely used by many machine translation researchers 
A later study  found that performance increased to 872  when considering only those portions of the text deemed to be subjective 
Introduction We have seen rapid recent progress in machine translation through the use of rich features and the development of improved decoding algorithms  often based on grammatical formalisms If we view MT as a machine learning problem  features and formalisms imply structural independence assumptions  which are in turn exploited by efficient inference algorithms  including decoders  
Although stateoftheart statistical parsers  are more accurate  the simplicity and efficiency of deterministic parsers make them attractive in a number of situations requiring fast  lightweight parsing  or parsing of large amounts of data 
 has been unable to find real examples of cases where hierarchical alignment would fail under these conditions  at least in fixedwordorder languages that are lightly inflected  such as English and Chinese  p 385  
Introduction Phrasebased method  and syntaxbased method  represent the stateoftheart technologies in statistical machine translation  SMT  Introduction Phrasebased translation  and hierarchical phrasebased translation  are the state of the art in statistical machine translation  SMT  techniques 
Although the Kappa coefficient has a number of advantages over percentage agreement  eg  it takes into account the expected chance interrater agreement  see  for details   we also report percentage agreement as it allows us to compare straightforwardly the human performance and the automatic methods described below  whose performance will also be reported in terms of percentage agreement 
Recent innovations have greatly improved the efficiency of language model integration through multipass techniques  such as forest reranking   local search   and coarsetofine pruning  However  since most of statistical translation models  are symmetrical  it is relatively easy to train a translation system to translate from English to Chinese  except that weneed to train aChinese language model from the Chinese monolingual data 
In terms of applying nonparametric Bayesian approaches to NLP   evaluated the clustering properties of DPMMs by performing anaphora resolution with good results The full model yields a stateoftheart BLEU  score of 08506 on Section 23 of the CCGbank  which is to our knowledge the best score reported to date 40 using a reversible  corpusengineered grammar 
They have been successfully applied in several tasks  such as information retrieval  and harvesting thesauri  showed that the results for FrenchEnglish were competitive to stateoftheart alignment systems Another kind of popular approaches to dealing with query translation based on corpusbased techniques uses a parallel corpus containing aligned sentences whose translation pairs are corresponding to each other  The corpusbased statistical parsing community has many fast and accurate automated parsing systems  including systems produced by   Charniak  997  and Ratnaparkhi  997  
Headlexicalized stochastic grammars have recently become increasingly popular  Unsupervised algorit ~ m ~ such as  have reported good accuracy that rivals that of supervised algorithms is one of the most famous work that discussed learning polarity from corpus 
This method  initially proposed by   was successfully evaluated in the context of the SENSEVAL framework  Classifier Training We chose maximum entropy  as our primary classifier because the highest performing systems in both the SemEval2007 preposition sense disambiguation task  and the general word sense disambiguation task  used it 
2 Related Work Recently  several successful attempts have been made at using supervised machine learning for word alignment  Pr  cJ  aJ eI   p  J I   I    J Jproductdisplay j   p  cj eaj   8  32 Loglikelihood ratio The loglikelihood ratio statistic has been found to be accurate for modeling the associations between rare events  
In contrast  the idea of bootstrapping for relation and information extraction was first proposed in   and successfully applied to the construction of semantic lexicons   named entity recognition   extraction of binary relations   and acquisition of structured data for tasks such as Question Answering  Perceptronbased training To tune the parameters w of the model  we use the averaged perceptron algorithm  because of its efficiency and past success on various NLP tasks  
But in fact  the issue of editing in text summarization has usually been neglected  notable exceptions being the works by  and Mani  Gates  and Bloedorn  999  We also use Cube Pruning algorithm  to speed up the translation process 
An important contribution to interactive CAT technology was carried out around the TransType  TT  project  We conclude by noting that English language models currently used in speech recognition  and automated language translation  are much more powerful  employing  for example  7gram word models  not letter models  trained on trillions of words 
22 ITG Space Inversion Transduction Grammars  or ITGs  provide an efficient formalism to synchronously parse bitext More recently   have proposed methods for automatically extracting from a corpus heads that correlate well with discourse novelty 
A key component of the parsing system is a Maximum Entropy CCG supertagger  which assigns lexical categories to words in a sentence They provide pairs of phrases that are used to construct a large set of potential translations for each input sentence  along with feature values associated with each phrase pair that are used to select the best translation from this set The most widely used method for building phrase translation tables  selects  from a word alignment of a parallel bilingual training corpus  all pairs of phrases  up to a given length  that are consistent with the alignment 
In the SMT research community  the second step has been well studied and many methods have been proposed to speed up the decoding process  such as nodebased or spanbased beam search with different pruning strategies  and cube pruning  
A promising approach may be to use aligned bilingual corpora  especially for augmenting existing lexicons with domainspecific terminology  It was found to produce automated scores  which strongly correlate with human judgements about translation fluency  Introduction The most widely used alignment model is IBM Model 4  We view this as a particularly promising aspect of our work  given that phrasebased systems such as Pharaoh  perform better with higher recall alignments 
To facilitate comparisons with previous work   we used the training\/development\/test partition defined in the corpus and we also used the automaticallyassigned part of speech tags provided in the corpus0 Czech word clusters were derived from the raw text section of the PDT 0  which contains about 39 million words of newswire text We trained the parsers using the averaged perceptron   which represents a balance between strong performance and fast training times 
This kind of corpus has served as an extremely valuable resource for computational linguistics applications such as machine translation and question answering   and has also proved useful in theoretical linguistics research  Motivation Statistical partofspeech disambiguation can be efficiently done with ngram models  
For the IBM models defined by a pioneering paper   a decoding algorithm based on a lefttoright search was described in  Introduction The emergence of phrasebased statistical machine translation  PSMT   has been one of the major developments in statistical approaches to translation 
In the supervised setting  a recent paper by  shows that  using a very simple feature augmentation method coupled with Support Vector Machines  he is able to effectively use both labeled target and source data to provide the best results in a number of NLP tasks Constraining learning by using document boundaries has been used quite effectively in unsupervised word sense disambiguation  
His results may be improved if more sophisticated methods and larger corpora are used to establish similarity between words  Introduction Statistical parsing models have been shown to be successful in recovering labeled constituencies  and have also been shown to be adequate in recovering dependency relationships  
 Introduction Recent works in statistical machine translation  SMT  shows how phrasebased modeling  significantly outperform the historical wordbased modeling  Tools like Xtract  were based on the work of Church and others  but made a step forward by incorporating various statistical measurements like zscore and variance of distribution  as well as shallow linguistic techniques like partofspeech tagging and lemmatization of input data and partial parsing of raw output Minimum Error Rate Training A good way of training is to minimize empirical top error on training data  
2 Parsing Model The Berkeley parser  is an efficient and effective parser that introduces latent annotations  to refine syntactic categories to learn better PCFG grammars However  to be more expressive and flexible  it is often easier to start with a general SCFG or treetransducer  
This is a common technique in machine translation for which the IBM translation models are popular methods  Comparison to selftraining For completeness  we also compared our results to the selflearning algorithm  which has commonly been referred to as bootstrapping in natural language processing and originally popularized by the work of Yarowsky in word sense disambiguation  
There are only a few successful studies  such as  for chunking and  on constituency parsing 
To reduce the knowledge engineering burden on the user in constructing and porting an IE system  unsupervised learning has been utilized  eg Riloff   Yangarber et al 
Recently socalled reranking techniques  such as maximum entropy models  and gradient methods   have been applied to machine translation  MT   and have provided significant improvements 
Previous work for English  has shown that lexicalization leads to a sizable improvement in parsing performance Tighter integration of semantics into the parsing models  possibly in the form of discriminative reranking models   is a promising way forward in this regard 
The creation of the Penn English Treebank   a syntactically interpreted corpus  played a crucial role in the advances in natural language parsing technology  for English Bleu is fast and easy to run  and it can be used as a target function in parameter optimization training procedures that are commonly used in stateoftheart statistical MT systems  
For our experiments  we chose GIZA    and the RA approach  the best known alignment combination technique as our initial aligners 42 TBL Templates Our templates consider consecutive words  of size   2 or 3  in both languages It has been known for some years that good performance can be realized with partial tagging and a hidden Markov model  
2 Lexicalized parse trees The first successful work on syntactic disambiguation was based on lexicalized probabilistic contextfree grammar  LPCFG   Statistical Learning Model 32 Nave Bayes Learning Nave Bayes learning has been widely used in natural language processing with good results such as statistical syntactic parsing   hidden language understanding  
 Introduction IBM Model   is a wordalignment model that is widely used in working with parallel bilingual corpora 
Such methods have also been a key driver of progress in statistical machine translation  which depends heavily on unsupervised word alignments  
 improves the F score from 882  to 897   while Charniak and Johnson  2005  improve from 903  to 94  
In   anotherstateoftheartWSDengine  acombination of naive Bayes  maximum entropy  boosting and Kernel PCA models  is used to dynamically determine the score of a phrase pair under consideration and  thus  let the phrase selection adapt to the context of the sentence 
The creation of the Penn English Treebank   a syntactically interpreted corpus  played a crucial role in the advances in natural language parsing technology  for English 
Nowadays  most of the stateoftheart SMT systems are based on bilingual phrases  
Global information is known to be useful in other NLP tasks  especially in the named entity recognition task  and several studies successfully used global features  
All stateoftheart widecoverage parsers relax this assumption in some way  for instance by  i  changing the parser in step  3   such that the application of rules is conditioned on other steps in the derivation process   or by  ii  enriching the nonterminal labels in step    with contextinformation   along with suitable backtransforms in step  4  
Support Vector Machines  SVMs   and Maximum Entropy  ME  method  are powerful learning methods that satisfy such requirements  and are applied successfully to other NLP tasks  
Among these techniques  SCL  Structural Correspondence Learning   is regarded as a promising method to tackle transferlearning problem Several generalpurpose offtheshelf  OTS  parsers have become widely available  
The best example of such an approach is   who proposes a method that automatically identifies collocations that are indicative of the sense of a word  and uses those to iteratively label more examples Introduction There has been a great deal of progress in statistical parsing in the past decade  
An especially wellfounded framework for doing this is maximum entropy  Stateofart systems for doing word alignment use generative models like GIZA    Comparison with Previous Top Systems and Related Work In POS tagging  the previous best performance was reported by  as summarized in Table 7 work is perhaps one of the most notable examples of unsupervised polarity classification 
Far from full syntactic complexity  we suggest to go back to the simpler alignment methods first described by  The latentannotation model  is one of the most effective unlexicalized models Online votedperceptrons have been reported to work well in a number of NLP tasks  
Because it is not feasible here to have humans judge the quality of many sets of translated data  we rely on an array of well known automatic evaluation measures to estimate translation quality  BLEU  is the geometric mean of the ngram precisions in the output with respect to a set of reference translations 
22 Unsupervised Parameter Estimation We can perform maximum likelihood estimation of the parameters of this model in a similar fashion to that of Model 4   described thoroughly in  
To overcome this problem  unsupervised learning methods using huge unlabeled data to boost the performance of rules learned by small labeled data have been proposed recently     
Moreover  the deterministic dependency parser of Yamada and Matsumoto   when trained on the Penn Treebank  gives a dependency accuracy that is almost as good as that of  and Charniak  2000  
David McClosky  Eugene Charniak  and Mark Johnson Brown Laboratory for Linguistic Information Processing  BLLIP  Brown University Providence  RI 0292  dmcc ec mj   csbrownedu Abstract Selftraining has been shown capable of improving on stateoftheart parser performance  despite the conventional wisdom on the matter and several studies to the contrary  
We conclude with some challenges that still remain in applying proactive learning for MT 2 Syntax Based Machine Translation In recent years  corpus based approaches to machine translation have become predominant  with Phrase Based Statistical Machine Translation  PBSMT   being the most actively progressing area  
 demonstrated that semisupervised WSD could be successful 
Second  benefits for sentiment analysis can be realized by decomposing the problem into S\/O  or neutral versus polar  and polarity classification  
This similarity score is computed as a max over a number of component scoring functions some based on external lexical resources including  various string similarity functions of which most are applied to word lemmas  measures of synonymy hypernymy antonymy and semantic relatedness including a widelyused measure due to Jiang and Conrath 997 based on manually constructed lexical resources such as WordNet and NomBank  a function based on the wellknown distributional similarity metric of Lin 998 which automatically infers similarity of words and phrases from their distributions in a very large corpus of English text The ability to leverage external lexical resources both manually and automatically constructedis critical to the success of MANLI
The search across a dimension uses the efficient method of  In the hierarchical phrasebased model   and an inversion transduction grammar  ITG    the problem is resolved by restricting to a binarized form where at most two nonterminals are allowed in the righthand side 
In syntactic parse reranking supersenses have been used to build useful latent semantic features  In agreement with recent resuits on parsing with lexicalised probabilistic grammars   we find that statistics over lexical  as opposed to structural  features best correspond to human intuitivejudgments and to experimental findings 
ntroduction In recent years  statistical machine translation have experienced a quantum leap in quality thanks to automatic evaluation  and errorbased optimization  Introduction Automatic Metrics for machine translation  MT  evaluation have been receiving significant attention in the past two years  since IBM 's BLEU metric was proposed and made available  Introduction Treebankbased probabilistic parsing has been the subject of intensive research over the past few years  resulting in parsing models that achieve both broad coverage and high parsing accuracy  
Probably the most widely used feature weighting function is pointwise Mutual Information MI Church and Patrick 990 Hindle 990 Luk 995 Lin 998 Gauch Wang and Rachakonda 999 Dagan 2000 Baroni and Vegnaduzzo 2004 Chklovski and Pantel 2004 Pantel and Ravichandran 2004 Pantel Ravichandran and Hovy 2004 Weeds Weir and McCarthy 2004 dened by weight MI wflog 2 Pwf PwPf  We calculate the MI weights by the following statistics in the space of cooccurrence instances S weight MI wflog 2 countwf nrels countw countf 2 where countwf is the frequency of the cooccurrence pair wf  in S countwand countf are the independent frequencies of w and f in Sandnrels is the size of SHigh MI weights are assumed to correspond to strong wordfeature associations
To some extent  this can probably be explained by the strong tradition of constituent analysis in AngloAmerican linguistics  but this trend has been reinforced by the fact that the major treebank of American English  the Penn Treebank   is annotated primarily with constituent analysis 
As a side product  we find empirical evidence to suggest that the effectiveness of rule lexicalization techniques  and parent annotation techniques  is due to the fact that both lead to a reduction in perplexity in the automata induced from training corpora 
There has been considerable skepticism over whether WSD will actually improve performance of applications  but we are now starting to see improvement in performance due to WSD in crosslingual information retrieval  and machine translation  and we hope that other applications such as questionanswering  text simplication and summarisation might also benet as WSD methods improve 
2 Maximum Entropy Models Maximum entropy  ME  models   also known as loglinear and exponential learning models  provideageneralpurposemachinelearningtechnique for classification and prediction which has been successfully applied to natural language processing including part of speech tagging  named entity recognition etc Maximum entropy models can integrate features from many heterogeneous information sources for classification 
 Introduction Stateoftheart Statistical Machine Translation  SMT  systems usually adopt a twopass search strategy  as shown in Figure  
5 The statistical parser The parsing model is the one proposed in Merlo and Musillo   which extends the syntactic parser of Henderson  and  with annotations which identify semantic role labels  and has competitive performance 
Study in collocation extraction using lexical statistics has gained some insights to the issues faced in collocation extraction  
Effective training algorithm exists  once the set of features a42 a57 a6 aa33a8 a7a54a8 a7a00a85a68a5 a53 is selected 
There are several distance measures suitable for this purpose  such as the mutual information   the dice coefficient   the phi coefficient   the cosine measure  and the confidence  
 Introduction Cooccurrence statistics extracted from corpora lead to good performance on a wide range of tasks that involve the identification of the semantic relation between two words or concepts  
Maximum entropy models  are a class of exponential models which require no unwarranted independence assumptions and have proven to be very successful in general for integrating information from disparate and possibly overlapping sources 
Support Vector Machines  SVMs   and Maximum Entropy  ME  method  are powerful learning methods that satisfy such requirements  and are applied successfully to other NLP tasks  
42 Support Vector Machines We chose to adopt a tagging perspective for the Simple NP chunking task  in which each word is to be tagged as either B  I or O depending on wether it is in the Beginning  Inside  or Outside of the given chunk  an approach first taken by   and which has become the defacto standard for this task 
In order to overcome this  some unsupervised learning methods and minimallysupervised methods  eg    have been proposed 
For the current work  the Loglikelihood coefficient has been employed   as it is reported to perform well among other scoring methods  
This averaging effect has been shown to help overfitting  
Finally  to estimate the parameters i of the weighted linear model  we adopt the popular minimum error rate training procedure  which directly optimizes translation quality as measured by the BLEU metric 
Indeed  researchers have shown that gigantic language models are key to stateoftheart performance   and the ability of phrasebased decoders to handle largesize  highorder language models with no consequence on asymptotic running time during decoding presents a compelling advantage over CKYdecoders  whosetimecomplexitygrowsprohibitively large with higherorder language models 
 Introduction During the last four years  various implementations and extentions to phrasebased statistical models  have led to significant increases in machine translation accuracy  
With the indepth study of opinion mining  researchers committed their efforts for more accurate results  the research of sentiment summarization   domain transfer problem of the sentiment analysis  and finegrained opinion mining  are the main branches of the research of opinion mining 
Finally  the translation model can be formalized as the following optimization problem argmax logPr  D   st mwsummationdisplay j   Pr  wj ok     k This optimization problem can be solved by the EM algorithm  
First  we compared our system output to human reference translations using Bleu   a widelyaccepted objective metric for evaluation of machine translations 
 Introduction Phrasebased method  and syntaxbased method  represent the stateoftheart technologies in statistical machine translation  SMT  
According to our experience  the best performance is achieved when the union of the sourcetotarget and targettosource alignment sets  is used for tuple extraction  some experimental results regarding this issue are presented in Section 422  
Nonparametricmodels  may be appropriate 
6 Related Work The popular IBM models for statistical machine translation are described in  
We use five sentiment classification datasets  including the widelyused movie review dataset  MOV   as well as four datasets containing reviews of four different types of products from Amazon  books  BOO   DVDs  DVD   electronics  ELE   and kitchen appliances  KIT    
 shows that baseNP recognition  Fz  I  920  is easier than finding both NP and VP chunks  Fz    88  and that increasing the size of the training data increases the performance on the test set 
Movies Reviews  This is a popular dataset in sentiment analysis literature  
36 Parameter Estimation To estimate parameters k   k K   lm  and um  we adopt the approach of minimum error rate training  MERT  that is popular in SMT  
 Introduction In recent years  Bracketing Transduction Grammar  BTG  proposed by  has been widely used in statistical machine translation  SMT  
While significant time savings have already been reported on the basis of automatic pretagging  eg  for POS and parse tree taggings in the Penn TreeBank   or named entity taggings for the Genia corpus    this kind of preprocessing does not reduce the number of text tokens actually to be considered 
Veale  used WordNet to answer 374 multiplechoice SAT analogy questions  achieving an accuracy of 43   but the best corpusbased approach attains an accuracy of 56   
The IOB format  introduced in   consistently   ame out as the best format 
They were based on mutual information   conditional probabilities   or on some standard statistical tests  such as the chisquare test or the loglikelihood ratio  
Stateoftheart machine learning techniques including Support Vector Machines   AdaBoost  and Maximum Entropy Models  provide high performance classifiers if one has abundant correctly labeled examples 
2 The BLEU Metric The metric most often used with MERT is BLEU   where the score of a candidate c against a reference translation r is  BLEU  BP  len  c   len  r   exp  4summationdisplay n    4 logpn   where pn is the ngram precision2 and BP is a brevity penalty meant to penalize short outputs  to discourage improving precision at the expense of recall 
Lexicalization can increase parsing performance dramatically for English   and the lexicalized model proposed by Collins  997  has been successfully applied to Czech  and Chinese  
Some NLG researchers are impressed by the success of the BLEU evaluation metric  in Machine Translation  MT   which has transformed the MT field by allowing researchers to quickly and cheaply evaluate the impact of new ideas  algorithms  and data sets 
In addition  the averaged parameters technology  is used to alleviate overfitting and achieve stable performance 
Successful discriminative parsers have used generative models to reduce training time and raise accuracy above generative baselines  
 Introduction A hypergraph  as demonstrated by   is a compact datastructure that can encode an exponential number of hypotheses generated by a regular phrasebased machine translation  MT  system  eg  Koehn et al 
 compares his method to  and shows that for four words the former performs significantly better in distinguishing between two senses 
So far  SCL has been applied successfully in NLP for PartofSpeech tagging and Sentiment Analysis  
 Introduction With the introduction of the BLEU metric for machine translation evaluation   the advantages of doing automatic evaluation for various NLP applications have become increasingly appreciated  they allow for faster implementevaluate cycles  by bypassing the human evaluation bottleneck   less variation in evaluation performance due to errors in human assessor judgment  and  not least  the possibility of hillclimbing on such metrics in order to improve system performance  
SVM has been shown to be useful for text classification tasks   and has previously given good performance in sentiment classification experiments  
Also  in a  stateoftheart English parser  only the words tha  t occur more tha  n d times in training data 
23 The Averaged Perceptron Reranking Model Averaged perceptron  has been successfully applied to several tagging and parsing reranking tasks   and in this paper  we employed it in reranking semantic parses generated by the base semantic parser SCISSOR 
One popular and statistically appealing such measure is LogLikelihood  LL   
Our MT experiments use a reimplementation of Moses  called Phrasal  which provides an easier API for adding features 
Recent several years have witnessed the rapid development of system combination methods based on confusion networks  eg     which show stateoftheart performance in MT benchmarks 
The averaged 555 perceptron has a solid theoretical fundamental and was proved to be effective across a variety of NLP tasks  
23 Classifier Training We chose maximum entropy  as our primary classifier  since it had been successfully applied by the highest performing systems in both the SemEval2007 preposition sense disambiguation task  and the general word sense disambiguation task  
2 Maximum Entropy Models Maximum entropy  ME  models   also known as loglinear and exponential learning models  provideageneralpurposemachinelearningtechnique for classification and prediction which has been successfully applied to natural language processing including part of speech tagging  named entity recognition etc Maximum entropy models can integrate features from many heterogeneous information sources for classification  
 Introduction Raw parallel data need to be preprocessed in the modern phrasebased SMT before they are aligned by alignment algorithms  one of which is the wellknown tool  GIZA     for training IBM models  4  
3 Language modelling with Bloom filters Recentwork  presenteda scheme for associating static frequency information with a set of ngrams in a BF efficiently 3 Logfrequency Bloom filter The efficiency of the scheme for storing ngram statistics within a BF presented in Talbot and Osborne  relies on the Zipflike distribution of ngramfrequencies  mosteventsoccuranextremely small number of times  while a small number are very frequent 
In an experiment on 6800 sentences of ChineseEnglish newswire text with segmentlevel human evaluation from the Linguistic Data Consortium?s LDC Multiple Translation project we compare the LFGbased evaluation method with other popular metrics like BLEU NIST General Text Matcher GTM Turian et al  2003 Translation Error Rate TER Snover et al  2006 and METEOR Banerjee and Lavie 2005 and we show that combining dependency representations with synonyms leads to a more accurate evaluation that correlates better with human judgment
Typical examples of linguistically sophisticated annotation include tagging words with their syntactic category  although this has not been found to be effective for R   lemma of the word  eg ` corpus ' for ` corpora '   phrasal information  eg identifying noun groups and phrases    and subjectpredicate identification  
To scale LMs to larger corpora with higherorder dependencies  researchers Work completed while this author was at Google Inc have considered alternative parameterizations such as classbased models   model reduction techniques such as entropybased pruning   novel represention schemes such as suffix arrays   Golomb Coding  and distributed language models that scale more readily  
 Introduction Robust statistical syntactic parsers  made possible by new statistical techniques  and by the availability of large  handannotated training corpora such as WSJ  and Switchboard   have had a major impact on the field of natural language processing 
For the extraction problem  there have been various methods proposed to date  which are quite adequate  whose training corpus for the noun drug was 9 times bigger than that of Karov and Edelman  reports 94  correct performance improved to impressive 939  when using the ` one sense per discourse ' constraint 
This increase of probabilities is defined as multiplicative change  N  as follows   N   P  E Tprime  \/ P  E T   2  The main innovation of the model in  is the possibility of adding at each step the best relation N   Ri  j  as well as N  I  Ri  j  that is Ri  j with all the relations by the existing taxonomy However  as also pointed out by   this observation does not hold uniformly over all possible cooccurrences of two words 
The MERT module is a highly modular  efficient and customizable implementation of the algorithm described in  Comparison with SSCRFMER When we consider semisupervised SOL methods  SSCRFMER  is the most competitive with HySOL  since both methods are defined based on CRFs WSD is one of the fundamental problems in natural language processing and is important for applications such as machine translation  MT    information retrieval  IR   etc WSD is typically viewed as a classification problem where each ambiguous word is assigned a sense label  from a predefined sense inventory  during the disambiguation process 
 has described an efficient exact onedimensional accuracy maximization technique for a similar search problem in machine translation While these are based on a relatively few number of items and while we have not performed any tests to determine whether the differences in ? are statistically significant the results 7The CzechEnglish conditions were excluded since there were so few systems 46 are nevertheless interesting since three metrics have higher correlation than Bleu ??Semantic role overlap Gimenez and M`arquez 2007 which makes its debut in the proceedings of this workshop ??ParaEval measuring recall Zhou et al  2006 which has a model of allowable variation in translation that uses automatically generated paraphrases CallisonBurch 2007 ??Meteor Banerjee and Lavie 2005 which also allows variation by introducing synonyms and by flexibly matches words using stemming
A number of partofspeech taggers are readily available and widely used  all trained and retrainable on text corpora  To avoid this problem  we adopt crossvalidation training as used in  The default training set of Penn Treebank  was used for the parser because the domain and style of those texts actually matches fairly well with the domain and style of the texts on which a reading level predictor for second language learners might be used 
This source of overcounting is considered and fixed by  and Zens and Ney  2003   which we briefly review here This further supports the claim by  that loglikelihood ratio is much less sensitive than pmi to low counts Inversion transduction grammar   or ITG  is a wellstudied synchronous grammar formalism 
For example   used cooccurrences between verbs and their subjects and objects  and proposed a similarity metric based on mutual information  but no exploration concerning the effectiveness of other kinds of word relationship is provided  although it is extendable to any kinds of contextual information 
Studies on the supervised task have shown that straightforward baselines  eg models based on source only  target only  or the union of the data  achieve a relatively high performance level and are surprisingly difficult to beat  
Results from  show that under these definitions the following guarantee holds  LogLossUpda  k  BestWtk  a C20 BestLossk  a So it can be seen that the update from a to Upda  k  BestWtk  a is guaranteed to decrease LogLoss by at least W k q C0 W C0 k qC6C7 2 From these results  the algorithms in Figures 3 and 4 could be altered to take the revised definitions of W k and W C0 k into account 
The notion of incrementally merging classes of lexical items is intuitively satisfying and is explored in detail in  Interand Intraannotator agreement We measured pairwise agreement among annotators usingthekappacoefficient  K  whichiswidelyused in computational linguistics for measuring agreement in category judgments  
In the supervised setting  a recent paper by  shows that a simple feature augmentation method for SVM is able to effectively use both labeled target and source data to provide the best domainadaptation results in a number of NLP tasks To speed our computations  we use the cube pruning method of  with a fixed beam size informationtheoretic similarity measure is commonly used in lexicon acquisition tasks and has demonstrated good performance in unsupervised WSD  
 Introduction Stateoftheart Statistical Machine Translation  SMT  systems usually adopt a twopass search strategy  as shown in Figure  
Following  we can avoid unnecessary false positives by not querying for the longer ngram in such cases It is explored extensively in  We compare semisupervised LEAF with a previous state of the art semisupervised system  
The main application of these techniques to written input has been in the robust  lexical tagging of corpora with partofspeech labels  Head Lexicalization As previously shown  Charniak      Carroll and Rooth  998   etc   ContextFree Grammars  CFGs  can be transformed to lexicalized CFGs  provided that a headmarking scheme for rules is given Other wellknown metrics are WER   NIST   GTM   ROUGE   METEOR   and TER   just to name a few Studies reveal that statistical alignment models outperform the simple Dice coefficient  One possible approach is to employ stateoftheart techniques for coreference and zeroanaphora resolution  in preprocessing cooccurrence samples 
The most widely used singlewordbased statistical alignment models  SAMs  have been proposed in  Decision lists have already been successfully applied to lexical ambiguity resolution by  where they perfromed well Some of the more popular and more accurate of these approaches to datadriven parsing  have been based on generative models that are closely related to probabilistic contextfree grammars Indeed  recent work has shown that benefits can be made by first separating facts from opinions in a document  eg  Yu and Hatzivassiloglou   and classifying the polarity based solely on the subjective portions of the document  eg    
 Introduction Chinese Word Segmentation  CWS  has been witnessed a prominent progress in the last three Bakeoffs      Of particular interest are lexicalized parsing models such as the ones developed by  and Carroll and Rooth  998  
Recently  graphbased methods have proved useful for a number of NLP and IR tasks such as document reranking in ad hoc IR  and analyzing sentiments in text  For English  after a relatively big jump achieved by   we have seen two significant improvements   and  pushed the results by a significant amount each time In our final comparison  we have also included the results of   because it has surpassed  as well and we have used this tagger in the data preparation phase 
The  algorithm was one of the first bootstrapping algorithms to become widely known in computational linguistics Another widely used discriminative method is the perceptron algorithm   which achieves comparable performance to CRFs with much faster training  so we base this work on the perceptron 
For symmetrization  we found that Och and Neys refined technique described in  produced the best AER for this data set under all experimental conditions Ramshaw and Marcus  successflflly applied Eric Brill 's transformationbased learning method to the chunking problem 
METEOR uses the Porter stemmer and synonymmatching via WordNet to calculate recall and precision more accurately  Recent work  has demonstrated that randomized encodings can be used to represent ngram counts for LMs with signficant spacesavings  circumventing informationtheoretic constraints on lossless data structures by allowing errors with some small probability 
Also  slightly restating the advantages of phrasepairs identified in   these blocks are effective at capturing context including the encoding of noncompositional phrase pairs  and capturing local reordering  but they lack variables  eg embedding between ne pas in French   have sparsity problems  and lack a strategy for global reordering 
The efficient block alignment algorithm in Section 4 is related to the inversion transduction grammar approach to bilingual parsing described in   in both cases the number of alignments is drastically reduced by introducing appropriate reordering restrictions System Overview 3 Translation model The system developed for this years shared task is a stateoftheart  twopass phrasebased statistical machine translation system based on a loglinear translation model  
Perceptron Reranking As  observes  perceptron training involves a simple  online algorithm  with few iterations typically required to achieve good performance In order to estimate the conditional distributions shown in Table   we use the general technique of choosing the MaxEnt distribution that properly estimates the average of each feature over the training data  
4 Features We used a dependency structure as the context for words because it is the most widely used and one of the best performing contextual information in the past studies  
This corpusbased information typically concerns sequences of 3 tags or words  It has been argued that METEOR correlates better with human judgment due to higher weight on recall than precision  Treebanking The Penn Treebank  is annotated with information to make predicateargument structure easy to decode  including function tags and markers of empty categories that represent displaced constituents 
After a brief period following the introduction of generally accepted and widely used metrics BLEU Papineni et al 2002 and NIST Doddington 2002 when it seemed that this persistent problem has finally been solved the researchers active in the field of machine translation MT started to express their worries that although these metrics are simple fast and able to provide consistent results for a particular system during its development they are not sufficiently reliable for the comparison of different systems or different language pairs
However  in   the authors investigate minimum translation units  MTU  which is a refinement over a similar approach by  to eliminate the overlap issue Because of this property  vector space models have been used successfully both in computational linguistics  and in cognitive science  
 Introduction The field of machine translation has seen many advances in recent years  most notably the shift from wordbased  to phrasebased models which use token ngrams as translation units  
The remaining six entries were all fully automatic machine translation systems  in fact  they were all phrasebased statistical machine translation system that had been trained on the same parallel corpus and most used Bleubased minimum error rate training  to optimize the weights of their log linear models feature functions  
 introduced the averaged perceptron  as a way of reducing overfitting  and it has been shown to perform better than the nonaveraged version on a number of tasks 
This averaging effect has been shown to reduce overfitting and produce much more stable results  Probably the most widely used association weight function is  pointwise  Mutual Information  MI          defined by         log    2 fPwP fwPfwMI  A known weakness of MI is its tendency to assign high weights for rare features    and Basque   which pose quite different and in the end less severe problems  there have been attempts at solving this problem for some of the highly inflectional European languages  such as     Slovenian       Czech  and   five Central and Eastern European languages   but so far no system has reached in the absolute terms a performance comparable to English tagging  such as    which stands around or above 97  
In order to overcome this problem  we look to the bootstrapping method outlined in  Much later work  relies on the use of extremely large corpora which allow very precise  but sparse features 
The simplest one is the BIO representation scheme   where a B denotes the first item of an element and an I any noninitial item  and a syllable with tag O is not a part of any element 
Conditional Markov models  CMM   have been successfully used in sequence labeling tasks incorporating rich feature sets 
To compare the output of their shallow parser with the output of the wellknown  parser  Li and Roth applied the chunklink conversion script to extract the shallow constituents from the output of the Collins parser on WSJ section 00 while the former is piecewise constant and thus can not be optimized using gradient techniques   provides an approach that performs such training efficiently 
Successful discriminative parsers have relied on generative models to reduce training time and raise accuracy above generative baselines  procedure is the most widelyused version of MERT for SMT  
Some tasks can thrive on a nearly pure diet of unlabeled data  Machine Translation Experiments 4 Experimental Setting For our MT experiments  we used a reimplementation of Moses   a stateoftheart phrasebased system Motivation The success of Statistical Machine Translation  SMT  has sparked a successful line of investigation that treats paraphrase acquisition and generation essentially as a monolingual machine translation problem   Phrasebased ChinesetoEnglish MT The MT system used in this paper is Moses  a stateoftheart phrasebased system  eports a success rate of 96  disambiguating twelve words with two clear sense distinctions each one  
It is an online training algorithm and has been successfully used in many NLP tasks  such as POS tagging   parsing   Chinese word segmentation   and so on 
One major resource for corpusbased research is the treebanks available in many research organizations   which carry skeletal syntactic structures or ` brackets ' that have been manually verified Successful approaches aimed at trying to overcome the sparse data limitation include backoff   TuringGood variants   interpolation   deleted estimation   similaritybased models   Poslanguage models  and decision tree models  As aptly pointed out in Jean   agreement measures proposed so far in the computational linguistics literature has failed to ask an important question of whether results obtained using agreement data are in any way different from random data 
Many previous studies have shown that the loglikelihood ratio is well suited for this purpose  Recent work    has shown that adding many millions of words of machine parsed and reranked LA Times articles does  in fact  improve performance of the parser on the closely related WSJ data 
In addition to the widely used BLEU  and NIST  scores  we also evaluate translation quality with the recently proposed Meteor  and four editdistance style metrics  Word Error Rate  WER   Positionindependent word Error Rate  PER    CDER  which allows block reordering   and Translation Edit Rate  TER   
Most stateoftheart SMT systems treat grammatical elements in exactly the same way as content words  and rely on generalpurpose phrasal translations and target language models to generate these elements  For instance   shows that a simple feature augmentation method for SVM is able to effectively use both labeled target and source data to provide the best domainadaptation results in a number of NLP tasks 
For English  we use three stateoftheart taggers  the taggers of  and  in Step   and the SVM tagger  in Step 3 We used the average perceptron algorithm of  in our experiments  a variation that has been proven to be more effective than the standard algorithm shown in Figure 2 
More recently  phrasebased models  have been proposed as a highly successful alternative to the IBM models Models that can handle nonindependent lexical features have given very good results both for partofspeech and structural disambiguation  
It is often straightforward to obtain large amounts of unlabeled data  making semisupervised approaches appealing  previous work on semisupervised methods for dependency parsing includes  
Recently  an elegant approach to inference in discourse interpretation has been developed at a number of sites   all based on tim notion of abduction  and we have begun to explore its potential application to machine translation 
The state of the art technology for relation extraction primarily relies on patternbased approaches  Many mainstream systems and formalisms would satisfy these criteria  including ones such as the University of Pennsylvania Treebank  which are purely syntactic  though of course  only syntactic properties could then be extracted  
The Penn Treebank  has until recently been the only such corpus  covering 45M words in a single genre of financial reporting 
Some methods which can offer powerful reordering policies have been proposed like syntax based machine translation  and Inversion Transduction Grammar  
Recent work emphasizes corpusbased unsupervised approach  that avoids the need for costly truthed training data We examine the effectiveness of Structural Correspondence Learning  SCL   for this task  a recently proposed adaptation technique shown to be effective for PoS tagging and Sentiment Analysis 
Online votedperceptrons have been reported to work well in a number of NLP tasks  Introduction Large scale annotated corpora  eg  the Penn TreeBank  PTB  project   have played an important role in textmining 
The fluency models hold promise for actual improvements in machine translation output quality  The notion that nouns have only one sense per discourse\/collocation was also exploited by  in his seminal work on bootstrapping for word sense disambiguation Using the components of the rowvector bm as feature function values for the candidate translation em  m a6    M   the system prior weights can easily be trained using the Minimum Error Rate Training described in  
Bootstrapping a PMTG from a lowerdimensional PMTG and a wordtoword translation model is similar in spirit to the way that regular grammars can help to estimate CFGs   and the way that simple translation models can help to bootstrap more sophisticated ones  
Semantic collocations are harder to extract than cooccurrence patternsthe state of the art does not enable us to find semantic collocations automatically t This paper however argues that if we take advantage of lexicai paradigmatic behavior underlying the lexicon we can at least achieve semiautomatic extraction of semantic collocations see also Calzolari and Bindi 990 I But note the important work by Hindle [HindlegO] on extracting semantically similar nouns based on their substitutability in certain verb contexts
Extracting semantic information from word cooccurrence statistics has been effective  particularly for sense disambiguation  High correlation is reported between the BLEU score and human evaluations for translations from Arabic  Chinese  French  and Spanish to English  
The maximum entropy approach  is known to be well suited to solve the classification problem We compared a baseline system  the stateoftheart phrasebased system Pharaoh   against our system The BLEU metric  in MT has been particularly successful  for example MT05  the 2005 NIST MT evaluation exercise  used BLEU4 as the only method of evaluation 
On the other hand  integrating an additional component into a baseline SMT system is notoriously tricky as evident in the research on integrating word sense disambiguation  WSD  into SMT systems  different ways of integration lead to conflicting conclusions on whether WSD helps MT performance  
This is analogous  and in a certain sense equivalent  to empirical risk minimization  which has been used successfully in related areas  such as speech recognition   language modeling   and machine translation   investigated the use of concurrent parsing of parallel corpora in a transduction inversion framework  helping to resolve attachment ambiguities in one language by the coupled parsing state in the second language 
Sentencelevel subjectivity detection  where training data is easier to obtain than for positive vs negative classification  has been successfully performed using supervised statistical methods alone  or in combination with a knowledgebased approach  
Properly calculated BLEU scores have been shown to correlate reliably with human judgments  The technique is based on word class models  pioneered by   which hierarchically 5 CoNLL03 CoNLL03 MUC7 MUC7 Web Component Test data Dev data Dev Test pages   Baseline 8365 8925 7472 728 74 2      Gazetteer Match 8722 96 8583 8043 7446 3      Word Class Model 8682 9085 8025 7988 7226 4  All External Knowledge 8855 9249 8450 8323 7444 Table 4  Utility of external knowledge 
 Introduction During the last few years  SMT systems have evolved from the original wordbased approach  to phrasebased translation systems  
Recently   have successfully constructed high quality and high coverage gazetteers from Wikipedia Introduction The maximum entropy model  has attained great popularity in the NLP field due to its power  robustness  and successful performance in various NLP tasks  The implementation of MEBA was strongly influenced by the notorious five IBM models described in  
Annotated reference corpora  such as the Brown Corpus   the Penn Treebank   and the BNC   have helped both the development of English computational linguistics tools and English corpus linguistics Introduction Syntactically annotated corpora like the Penn Treebank   the NeGra corpus  or the statistically dismnbiguated parses in  provide a wealth of intbrmation  which can only be exploited with an adequate query language 
Similarly  Structural Correspondence Learning  has proven to be successful for the two tasks examined  PoS tagging and Sentiment Classification A notable exception is the work of  
Inversion Transduction Grammar  ITG   and SyntaxDirected Translation Schema  SDTS   lack both of these properties 
Introduction Michael  parsing models have been quite influential in the field of natural language processing We do not completely rule out the possibility that some more sophisticated  ontologically promiscuous  firstorder analysis  perhaps along the lines of   might account for these kinds of monotonicity inferences It is promising to optimize the model parameters directly with respect to AER as suggested in statistical machine translation  
In the wellknown socalled IBM word alignment models   reestimating the model parameters depends on the empirical probability P  ek  fk  for each sentence pair  ek  fk  
The classification is performed with a statistical approach  built around the maximum entropy  MaxEnt  principle   that has the advantage of combining arbitrary types of information in making a classification decision 
Furthermore  the BLEU score performance suggests that our model is not very powerful  but some interesting hints can be found in Table 3 when we compare our method with a 5gram language model to a stateoftheart system Moses  based on various evaluation metrics  including BLEU score  NIST score   METEOR   TER   WER and PER 
Disambiguation of a limited number of words is not hard  and necessary context information can be carefully collected and handcrafted to achieve high disambiguation accuracy as shown in  
 and Bikel and Chiang  has demonstrated the applicability of the  model for Czech and Chinese One of the most successful metrics for judging machinegenerated text is BLEU  
Among them  the unsupervised algorithm using decisiontrees  has achieved promising performance Recent projects in semisupervised  and unsupervised  tagging also show significant progress Introduction The emergence of phrasebased statistical machine translation  PSMT   has been one of the major developments in statistical approaches to translation 
Their idea has proven effective for estimating the statistics of unknown words in previous studies   We use the popular online learning algorithm of structured perceptron with parameter averaging   This algorithm is referred to as GHKM  and is widely used in SSMT systems  
Stochastic models  have been widely used in POS tagging for simplicity and language independence of the models solved relational similarity problems using the Web as a corpus Albeit simple  the algorithm has proven to be very efficient and accurate for the task of parse selection  
Two popular techniques that incorporate the error criterion are Minimum Error Rate Training  MERT   and Minimum BayesRisk  MBR  decoding  Automated metrics such as BLEU   RED   Weighted Ngram model  WNM    syntactic relation \/ semantic vector model  have been shown to correlate closely with scoring or ranking by different human evaluation parameters 
Arguably the most widely used is the mutual information  An especially wellfounded framework is maximum entropy  2 Related Work Supervised machine learning methods including Support Vector Machines  SVM  are often used in sentiment analysis and shown to be very promising  
Turney also reported good result without domain customization  In our experiments  we follow Lowe and McDonald  in using the wellknown loglikelihood ratio G 2  
It is often straightforward to obtain large amounts of unlabeled data  making semisupervised approaches appealing  previous work on semisupervised methods for dependency parsing includes  
In the II  OO  and OI scenarios   succeeded in improving the parser performance only when a reranker was used to reorder the 50best list of the generative parser  with a seed size of 40K sentences Maximum Entropy Maximum entropy classiflcation  MaxEnt  or ME  for short  is an alternative technique which has proven efiective in a number of natural language processing applications  
Yarowsky has proposed an algorithm that requires as little user input as one seed word per sense to start the training process Thus  over the past few years  along with advances in the use of learning and statistical methods for acquisition of full parsers   significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship  
Such a method alleviates the problem of creating templates from examples which would be used in an ulterior phase of generation  The variance semiring is essential for many interesting training paradigms such as deterministic 40 annealing   minimum risk   active and semisupervised learning  
Among these methods  CRFs is the most common technique used in NLP and has been successfully applied to PartofSpeech Tagging   NamedEntity Recognition  and shallow parsing  Wikipedia first sentence  WikiFS    used Wikipedia as an external knowledge to improve Named Entity Recognition 
However  this is not unprecedented  discriminatively weighted generative models have been shown to outperform purely discriminative competitors in various NLP classification tasks   and remain the standard approach in statistical translation modeling  A more refined algorithm  the incremental feature selection algorithm by   allows one feature being added at each selection and at the same time keeps estimated parameter values for the features selected in the previous stages 
We also plan to apply selftraining of nbest tagger which successfully boosted the performance of one of the best existing English syntactic parser  In machine translation  the rankings from the automatic BLEU method  have been shown to correlate well with human evaluation  and it has been widely used since and has even been adapted for summarization  
Several studies have demonstrated that for instance Statistical Machine Translation  SMT  benefits from incorporating a dedicated WSD module  In our experiments  we have used Averaged Perceptron  and Perceptron with margin  to improve performance In 2004  Conroy  tested Maximal Marginal Relevance  as well as QR decomposition 
Automatically creating or extending taxonomies for specific domains is then a very interesting area of research  SVM has been shown to be useful for text classification tasks   and has previously given good performance in sentiment classification experiments  2 Prior Work Statistical machine translation  as pioneered by IBM   is grounded in the noisy channel model The most notable of these include the trigram HMM tagger   maximum entropy tagger   transformationbased tagger   and cyclic dependency networks  Averaging has been shown to reduce overfitting  as well as reliance on the order of the examples during training 
Synchronous parsing models have been explored with moderate success  Systems based on perceptron have been shown to be competitive in NER and text chunking  We specify the model and the features with the LBJ  modeling language We use maximumentropy models   which are particularly wellsuited for tasks  like ours  with many overlapping features  to harness these linguistic insights by using features in our models which encode  directly or indirectly  the linguistic correlates to SE types
