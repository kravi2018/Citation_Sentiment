'The averaged version of the perceptron <CIT> , like the voted perceptron <OTH> , reduces the effect of over-training '
'Since the use of cluster of machines is not always practical , <CIT> showed a randomized data structure called Bloom filter , that can be used to construct space efficient language models 513 for SMT '
'1 Introduction The Maximum Entropy -LRB- ME -RRB- statistical framework <CIT> has been successfully deployed in several NLP tasks '
'To achieve efficient parsing , we use a beam search strategy like the previous methods <CIT> '
'Successflfl examples of reuse of data resources include : the WordNet thesaurus <OTH> ; the Penn Tree Bank <CIT> ; the Longmans Dictionary of Contemporary English <OTH> '
'Averaging has been shown to help reduce overfitting <CIT> '
'Hypergraphs have been successfully used in parsing <CIT> and machine translation <CIT> '
'1 Introduction <CIT> introduced minimum error rate training -LRB- MERT -RRB- for optimizing feature weights in statistical machine translation -LRB- SMT -RRB- models , and demonstrated that it produced higher translation quality scores than maximizing the conditional likelihood of a maximum entropy model using the same features '
'1 Introduction When data have distinct sub-structures , models exploiting latent variables are advantageous in learning <CIT> '
'<CIT> and <OTH> shows how some of the methods which have been used in the past -LRB- particularly mutual information scores -RRB- are invalid for rare events , and introduce accurate measures of how ` surprising '' rare events are '
'Nowadays , most of the state-of-the-art SMT systems are based on bilingual phrases <CIT> '
'In this paper , we show that a noisy channel model instantiated within the paradigm of Statistical Machine Translation -LRB- SMT -RRB- <CIT> can successfully provide editorial assistance for non-native writers '
'However , formally syntax-based methods propose simple but efficient ways to parse and translate sentences <CIT> '
'441 N-gram Co-Occurrence Statistics for Answer Extraction N-gram co-occurrence statistics have been successfully used in automatic evaluation <OTH> , and more recently as training criteria in statistical machine translation <CIT> '
'3 Experimental Results Whereas stochastic modelling is widely used in speech recognition , there are so far only a few research groups that apply stochastic modelling to language translation <CIT> '
'1 Introduction By exploiting information encoded in human-produced syntactic trees <CIT> , research on probabilistic models of syntax has driven the performance of syntactic parsers to about 90 % accuracy <OTH> '
'Unigram models have been previously shown to give good results in sentiment classification tasks <CIT> : unigram representations can capture a variety of lexical combinations and distributions , including those of emotion words '
'The state-of-the art taggers are using feature sets discribed in the corresponding articles -LRB- <OTH> , <OTH> , <OTH> and <CIT> -RRB- , Morce supervised and Morce semi-supervised are using feature set desribed in section 4 '
'It performed slightly worse on baseNP recognition than the <CIT> experiments -LRB- Fz = 1 = 916 -RRB- '
'21 The averaged perceptron The averaged perceptron algorithm <CIT> was proposed as a way of reducing overfitting on the training data '
'In the statistical NLP community , the most widely used grammatical resource is the Penn Treebank <CIT> '
'<OTH> , Pedersen <OTH> , Yarowsky and Florian <OTH> -RRB- as well as maximum entropy models -LRB- eg , Dang and Palmer <OTH> , <CIT> and Manning <OTH> -RRB- in particular have shown a large degree of success for WSD , and have established challenging state-of-the-art benchmarks '
'1 Introduction Large scale annotated corpora such as the Penn TreeBank <CIT> have played a central role in speech and natural language research '
'<CIT> report an improvement in MT grammaticality on a very restricted test set : short sentences parsable by an LFG grammar without back-off rules '
'We carried out automatic evaluation of our summaries using ROUGE <CIT> toolkit , which has been widely adopted by DUC for automatic summarization evaluation '
'In machine translation , confusion-network based combination techniques -LRB- eg , <CIT> -RRB- have achieved the state-of-theart performance in MT evaluations '
'However , except for <CIT> , none of these advances in alignment quality has improved translation quality of a state-of-the-art system '
'In this paper , we build on recent work <CIT> that demonstrated how the Bloom filter -LRB- Bloom -LRB- 1970 -RRB- ; BF -RRB- , a space-efficient randomised data structure for representing sets , could be used to store corpus statistics efficiently '
'Among all the automatic MT evaluation metrics , BLEU <CIT> is the most widely used '
'To simulate real world scenario , we use n-best lists from ISIs state-of-the-art statistical machine translation system , AlTemp <CIT> , and the 2002 NIST Chinese-English evaluation corpus as the test corpus '
'A quite different approach from our hypotheses testing implemented in the TREQ-AL aligner is taken by the model-estimating aligners , most of them relying on the IBM models -LRB- 1 to 5 -RRB- described in the <CIT> seminal paper '
'In our future work we plan to investigate the effect of more sophisticated and , probably , more accurate filtering methods <CIT> on the QA results '
'In Statistical Machine Translation -LRB- SMT -RRB- , recent work shows that WSD helps translation quality when the WSD system directly uses translation candidates as sense inventories <CIT> '
'Most semi-automated approaches have met with limited success <OTH> and supervised learning models have tended to outperform dictionary-based classi cation schemes <CIT> '
'While studies have shown that ratings of MT systems by BLEU and similar metrics correlate well with human judgments <CIT> , we are not aware of any studies that have shown that corpus-based evaluation metrics of NLG systems are correlated with human judgments ; correlation studies have been made of individual components <OTH> , but not of systems '
'While EM has worked quite well for a few tasks , notably machine translations -LRB- starting with the IBM models 1-5 <CIT> , it has not had success in most others , such as part-of-speech tagging <OTH> , named-entity recognition <OTH> and context-free-grammar induction -LRB- numerous attempts , too many to mention -RRB- '
'Incremental top-down and left-corner parsers have been shown to effectively -LRB- and efficiently -RRB- make use of non-local features from the left-context to yield very high accuracy syntactic parses <CIT> , and we will use such rich models to derive our scores '
'Erk <OTH> compared a number of techniques for creating similar-word sets and found that both the Jaccard coefficient and <CIT> s information-theoretic metric work best '
'4 Extended Minimum Error Rate Training Minimum error rate training <CIT> is widely used to optimize feature weights for a linear model <CIT> '
'When we run our classifiers on resource-tight environments such as cell-phones , we can use a random feature mixing technique <CIT> or a memory-efficient trie implementation based on a succinct data structure <OTH> to reduce required memory usage '
'32 Evaluation Criteria Well-established objective evaluation measures like the word error rate -LRB- WER -RRB- , positionindependent word error rate -LRB- PER -RRB- , and the BLEU score <CIT> were used to assess the translation quality '
'In recent several years , the system combination methods based on confusion networks developed rapidly <CIT> , which show state-of-the-art performance in benchmarks '
'Although bi-alignments are known to exhibit high precision <CIT> , in the face of sparse annotations we use unidirectional alignments as a fallback , as has been proposed in the context of phrase-based machine translation <CIT> '
'It is based on Incremental Sigmoid Belief Networks -LRB- ISBNs -RRB- , a class of directed graphical model for structure prediction problems recently proposed in <CIT> , where they were demonstrated to achieve competitive results on the constituent parsing task '
'In our experience , this approach is advantageous in terms of translation quality , eg by 07 % in BLEU compared to a minimum Bayes risk primary <CIT> '
'To solve this problem , we adopt an idea one sense per collocation which was introduced in word sense disambiguation research <CIT> '
'The results show that , as compared to BLEU , several recently proposed metrics such as Semantic-role overlap <OTH> , ParaEval-recall <OTH> , and METEOR <CIT> achieve higher correlation '
'Parsing models have been developed for different languages and state-of-the-art results have been reported for , eg , English <CIT> '
'For English , after a relatively big jump achieved by <CIT> , we have seen two significant improvements : <OTH> and <OTH> pushed the results by a significant amount each time1 1In our final comparison , we have also included the results of <OTH> , because it has surpassed <CIT> as well and we have used this tagger in the data preparation phase '
'However , evaluations on the widely used WSJ corpus of the Penn Treebank <CIT> show that the accuracy of these parsers still lags behind the state-of-theart '
'22 Maximum Entropy Models Maximum entropy -LRB- ME -RRB- models <CIT> , also known as 928 log-linear and exponential learning models , provide a general purpose machine learning technique for classification and prediction which has been successfully applied to natural language processing including part of speech tagging , named entity recognition etc Maximum entropy models can integrate features from many heterogeneous information sources for classification '
'1 Introduction Many state-of-the-art machine translation -LRB- MT -RRB- systems over the past few years <CIT> rely on several models to evaluate the goodness of a given candidate translation in the target language '
'The use of dependencies in MT evaluation has not been extensively researched before -LRB- one exception here would be <CIT> -RRB- , and requires more research to improve it , but the method shows potential to become an accurate evaluation metric '
'Albeit simple , the algorithm has proven to be very efficient and accurate for the task of parse selection <CIT> '
'It is an online training algorithm and has been successfully used in many NLP tasks , such as POS tagging <CIT> , parsing <CIT> , Chinese word segmentation <OTH> , and so on '
'We wish to minimize this error function , so we select accordingly : argmin summationdisplay a E -LRB- a -RRB- -LRB- a , -LRB- argmax a p -LRB- a , f e -RRB- -RRB- -RRB- -LRB- 4 -RRB- Maximizing performance for all of the weights at once is not computationally tractable , but <CIT> has described an efficient one-dimensional search for a similar problem '
'For French\/English translation we use a state of the art phrase-based MT system similar to <CIT> '
'In agreement with recent results on parsing with lexicalised probabilistic grammars <CIT> , our main result is that statistics over lexical features best correspond to independently established truman intuitive preferences and experimental findings '
'Since human evaluation is costly and difficult to do reliably , a major focus of research has been on automatic measures of MT quality , pioneered by BLEU <CIT> and NIST <OTH> '
'It is interesting to note that, while the study of how the granularity of context-free grammars (CFG) affects the performance of a parser (e.g. in the form 86 n1:IP [=] n2:NP [SUBJ=] n4:NR [=] GSC4ES JiangZemin n3:VP [=] n5:VV [=] ESDO interview n6:NP [OBJ=] n7:NR [ADJUNCT] AIC1 Thai n8:NN [=] D3D2 president f1             PRED ESDO SUBJ f2  PRED GSC4ESNTYPE proper NUM sg   OBJ f3       PRED D3D2 NTYPE common NUM sg ADJUNCT   f4  PRED AIC1NTYPE proper NUM sg                         : N  F (n1)=(n3)=(n5)=f1 (n2)=(n4)=f2 (n6)=(n8)=f3 (n7)=f4 Figure 1: Cand f-structures with  links for the sentence GSC4ESESDOAIC1D3D2 of grammar transforms (Johnson, 1998) and lexicalisation (Collins, 1997)) has attracted substantial attention, to our knowledge, there has been a lot less research on this subject for surface realisation, a process that is generally regarded as the reverse process of parsing.'
'The former term P -LRB- E -RRB- is called a language model , representing the likelihood of E The latter term P -LRB- J E -RRB- is called a translation model , representing the generation probability from E into J As an implementation of P -LRB- J E -RRB- , the word alignment based statistical translation <CIT> has been successfully applied to similar language pairs , such as FrenchEnglish and German English , but not to drastically dierent ones , such as JapaneseEnglish '
'The best previous result is an accuracy of 561 % <CIT> '
'1 Introduction Hierarchical approaches to machine translation have proven increasingly successful in recent years <CIT> , and often outperform phrase-based systems <OTH> on target-language fluency and adequacy '
'Throughout , the likelihood ratio <CIT> is used as significance measure because of its stable performance in various evaluations , yet many more measures are possible '
'However , the study of <CIT> provides interesting insights into what makes a good distributional similarity measure in the contexts of semantic similarity prediction and language modeling '
'While <CIT> does not discuss distinguishing more than 2 senses of a word , there is no immediate reason to doubt that the ` one sense per collocation '' rule <CIT> would still hold for a larger number of senses '
'Similarity-based smoothing <CIT> provides an intuitively appealing approach to language modeling '
'<CIT> , which is the classic work on collocation extraction , uses a two-stage filtering model in which , in the first step , n-gram statistics determine possible collocations and , in the second step , these candidates are submitted to a syntactic valida7Of course , lexical material is always at least partially dependent on the domain in question '
'One of the most effective taggers based on a pure HMM is that developed at Xerox <CIT> '
'The success of recent high-quality parsers <CIT> relies on the availability of such treebank corpora '
'Synchronous binarization <CIT> solves this problem by simultaneously binarizing both source and target-sides of a synchronous rule , making sure of contiguous spans on both sides whenever possible '
'<CIT> reported very high results -LRB- 96 % on the Brown corpus -RRB- for unsupervised POS tagging using Hidden Markov Models -LRB- HMMs -RRB- by exploiting hand-built tag dictionaries and equivalence classes '
'All the enumerated segment pairs are listed in the following table : Feature x , y Feature x , y AM1 +1 c1 , c0 AM2 +1 c2c1 , c0 AM1 +2 c1 , c0c1 AM2 +2 c2c1 , c0c1 AM1 +3 c1 , c0c1c2 AM3 +1 c3c2c1 , c0 We use Dunnings method <CIT> because it does not depend on the assumption of normality and it allows comparisons to be made between the signiflcance of the occurrences of both rare and common phenomenon '
'3 Extending Bleu and Ter with Flexible Matching Many widely used metrics like Bleu <CIT> and Ter <OTH> are based on measuring string level similarity between the reference translation and translation hypothesis , just like Meteor Most of them , however , depend on finding exact matches between the words in two strings '
'Promising features might include those over source side reordering rules <OTH> or source context features <CIT> '
'The BLEU metric <CIT> and the closely related NIST metric <OTH> along with WER and PER 48 have been widely used by many machine translation researchers '
'A later study <CIT> found that performance increased to 872 % when considering only those portions of the text deemed to be subjective '
'1 Introduction We have seen rapid recent progress in machine translation through the use of rich features and the development of improved decoding algorithms , often based on grammatical formalisms1 If we view MT as a machine learning problem , features and formalisms imply structural independence assumptions , which are in turn exploited by efficient inference algorithms , including decoders <CIT> '
'Although state-of-the-art statistical parsers <CIT> are more accurate , the simplicity and efficiency of deterministic parsers make them attractive in a number of situations requiring fast , light-weight parsing , or parsing of large amounts of data '
'<CIT> has been unable to find real examples of cases where hierarchical alignment would fail under these conditions , at least in fixed-word-order languages that are lightly inflected , such as English and Chinese -LRB- p 385 -RRB- '
'1 Introduction Phrase-based method <OTH> and syntaxbased method <CIT> represent the state-of-the-art technologies in statistical machine translation -LRB- SMT -RRB- '
'1 Introduction Phrase-based translation <CIT> and hierarchical phrase-based translation <OTH> are the state of the art in statistical machine translation -LRB- SMT -RRB- techniques '
'Although the Kappa coefficient has a number of advantages over percentage agreement -LRB- eg , it takes into account the expected chance interrater agreement ; see <CIT> for details -RRB- , we also report percentage agreement as it allows us to compare straightforwardly the human performance and the automatic methods described below , whose performance will also be reported in terms of percentage agreement '
'Recent innovations have greatly improved the efficiency of language model integration through multipass techniques , such as forest reranking <CIT> , local search <OTH> , and coarse-to-fine pruning <OTH> '
'However , since most of statistical translation models <CIT> are symmetrical , it is relatively easy to train a translation system to translate from English to Chinese , except that weneed to train aChinese language model from the Chinese monolingual data '
'In terms of applying non-parametric Bayesian approaches to NLP , <CIT> evaluated the clustering properties of DPMMs by performing anaphora resolution with good results '
'The full model yields a stateof-the-art BLEU <CIT> score of 08506 on Section 23 of the CCGbank , which is to our knowledge the best score reported to date 410 using a reversible , corpus-engineered grammar '
'They have been successfully applied in several tasks , such as information retrieval <OTH> and harvesting thesauri <CIT> '
'<CIT> showed that the results for French-English were competitive to state-of-the-art alignment systems '
'Another kind of popular approaches to dealing with query translation based on corpus-based techniques uses a parallel corpus containing aligned sentences whose translation pairs are corresponding to each other <CIT> '
'The corpus-based statistical parsing community has many fast and accurate automated parsing systems , including systems produced by <CIT> , Charniak -LRB- 1997 -RRB- and Ratnaparkhi -LRB- 1997 -RRB- '
'Head-lexicalized stochastic grammars have recently become increasingly popular <CIT> '
'Unsupervised algorit ~ m ~ such as <CIT> have reported good accuracy that rivals that of supervised algorithms '
'<CIT> is one of the most famous work that discussed learning polarity from corpus '
'This method , initially proposed by <CIT> , was successfully evaluated in the context of the SENSEVAL framework <OTH> '
'23 Classifier Training We chose maximum entropy <CIT> as our primary classifier because the highest performing systems in both the SemEval-2007 preposition sense disambiguation task <OTH> and the general word sense disambiguation task <OTH> used it '
'Each component model takes the exponential form: a37a55a38a57a56 a51 a42a6a44a59a58a60a56 a61 a51a64a63a65a53a67a66 a53 a45a46a70 a71a16a72a21a73a75a74a77a76a79a78a81a80 a78a16a82a11a78 a38a83a44a59a58a60a56a84a61 a51a64a63a65a53a67a66 a53 a58a60a56 a51 a45a86a85 a87 a38a83a44a59a58a60a56a84a61 a51a64a63a65a53a67a66 a53 a45 a58 (2) where a87 a38a83a44a59a58a60a56 a61 a51a41a63a65a53a67a66 a53 a45 is a normalization term to ensure that a37a55a38a57a56 a51a42a6a44a88a58a60a56a62a61 a51a41a63a65a53a67a66 a53 a45 is a probability, a82a11a78 a38a83a44a59a58a60a56 a61 a51a64a63a65a53a67a66 a53 a58a60a56 a51 a45 is a feature function (often binary) and a80 a78 is the weight ofa82a21a78 . Given a set of features and a corpus of training data, there exist ef cient training algorithms (Darroch and Ratcliff, 1972; Berger et al. , 1996) to nd the optimal parameters a89 a80 a78a14a90 . The art of building a maximum entropy parser then reduces to choosing good features.'
'A variety of classifiers have been employed for this task <OTH> , the most popular being decision lists <CIT> and naive Bayesian classifiers <CIT> '
'2 Related Work Recently , several successful attempts have been made at using supervised machine learning for word alignment <CIT> '
'Pr -LRB- cJ1 , aJ1 eI1 -RRB- = p -LRB- J I -RRB- -LRB- I + 1 -RRB- J Jproductdisplay j = 1 p -LRB- cj eaj -RRB- -LRB- 8 -RRB- 312 Log-likelihood ratio The log-likelihood ratio statistic has been found to be accurate for modeling the associations between rare events <CIT> '
'In contrast , the idea of bootstrapping for relation and information extraction was first proposed in <OTH> , and successfully applied to the construction of semantic lexicons <OTH> , named entity recognition <OTH> , extraction of binary relations <OTH> , and acquisition of structured data for tasks such as Question Answering <CIT> '
'22 Perceptron-based training To tune the parameters w of the model , we use the averaged perceptron algorithm <CIT> because of its efficiency and past success on various NLP tasks <CIT> '
'But in fact , the issue of editing in text summarization has usually been neglected , notable exceptions being the works by <CIT> and Mani , Gates , and Bloedorn -LRB- 1999 -RRB- '
'We also use Cube Pruning algorithm <CIT> to speed up the translation process '
'An important contribution to interactive CAT technology was carried out around the TransType -LRB- TT -RRB- project <CIT> '
'We conclude by noting that English language models currently used in speech recognition <OTH> and automated language translation <CIT> are much more powerful , employing , for example , 7-gram word models -LRB- not letter models -RRB- trained on trillions of words '
'22 ITG Space Inversion Transduction Grammars , or ITGs <CIT> provide an efficient formalism to synchronously parse bitext '
'18 More recently , <CIT> have proposed methods for automatically extracting from a corpus heads that correlate well with discourse novelty '
'A key component of the parsing system is a Maximum Entropy CCG supertagger <CIT> which assigns lexical categories to words in a sentence '
'They provide pairs of phrases that are used to construct a large set of potential translations for each input sentence , along with feature values associated with each phrase pair that are used to select the best translation from this set1 The most widely used method for building phrase translation tables <CIT> selects , from a word alignment of a parallel bilingual training corpus , all pairs of phrases -LRB- up to a given length -RRB- that are consistent with the alignment '
'In the SMT research community , the second step has been well studied and many methods have been proposed to speed up the decoding process , such as node-based or span-based beam search with different pruning strategies <OTH> and cube pruning <CIT> '
'A promising approach may be to use aligned bilingual corpora , especially for augmenting existing lexicons with domain-specific terminology <CIT> '
'It was found to produce automated scores , which strongly correlate with human judgements about translation fluency <CIT> '
'1 Introduction The most widely used alignment model is IBM Model 4 <CIT> '
'We view this as a particularly promising aspect of our work , given that phrase-based systems such as Pharaoh <CIT> perform better with higher recall alignments '
'To facilitate comparisons with previous work <OTH> , we used the training\/development\/test partition defined in the corpus and we also used the automatically-assigned part of speech tags provided in the corpus10 Czech word clusters were derived from the raw text section of the PDT 10 , which contains about 39 million words of newswire text11 We trained the parsers using the averaged perceptron <CIT> , which represents a balance between strong performance and fast training times '
'This kind of corpus has served as an extremely valuable resource for computational linguistics applications such as machine translation and question answering <OTH> , and has also proved useful in theoretical linguistics research <CIT> '
'1 Motivation Statistical part-of-speech disambiguation can be efficiently done with n-gram models <CIT> '
'For the IBM models defined by a pioneering paper <CIT> , a decoding algorithm based on a left-to-right search was described in <OTH> '
'1 Introduction The emergence of phrase-based statistical machine translation -LRB- PSMT -RRB- <CIT> has been one of the major developments in statistical approaches to translation '
'In the supervised setting , a recent paper by <CIT> shows that , using a very simple feature augmentation method coupled with Support Vector Machines , he is able to effectively use both labeled target and source data to provide the best results in a number of NLP tasks '
'Constraining learning by using document boundaries has been used quite effectively in unsupervised word sense disambiguation <CIT> '
'His results may be improved if more sophisticated methods and larger corpora are used to establish similarity between words <CIT> '
'1 Introduction Statistical parsing models have been shown to be successful in recovering labeled constituencies <CIT> and have also been shown to be adequate in recovering dependency relationships <CIT> '
'1 Introduction Recent works in statistical machine translation -LRB- SMT -RRB- shows how phrase-based modeling <CIT> significantly outperform the historical word-based modeling <OTH> '
'Tools like Xtract <CIT> were based on the work of Church and others , but made a step forward by incorporating various statistical measurements like z-score and variance of distribution , as well as shallow linguistic techniques like part-of-speech tagging and lemmatization of input data and partial parsing of raw output '
'73 224 Minimum Error Rate Training A good way of training is to minimize empirical top-1 error on training data <CIT> '
'2 Parsing Model The Berkeley parser <OTH> is an efficient and effective parser that introduces latent annotations <CIT> to refine syntactic categories to learn better PCFG grammars '
'However , to be more expressive and flexible , it is often easier to start with a general SCFG or tree-transducer <CIT> '
'This is a common technique in machine translation for which the IBM translation models are popular methods <CIT> '
'51 Comparison to self-training For completeness , we also compared our results to the self-learning algorithm , which has commonly been referred to as bootstrapping in natural language processing and originally popularized by the work of Yarowsky in word sense disambiguation <CIT> '
'There are only a few successful studies , such as <OTH> for chunking and <CIT> on constituency parsing '
'To reduce the knowledge engineering burden on the user in constructing and porting an IE system , unsupervised learning has been utilized , eg Riloff <OTH> , Yangarber et al '
'Recently so-called reranking techniques , such as maximum entropy models <CIT> and gradient methods <CIT> , have been applied to machine translation -LRB- MT -RRB- , and have provided significant improvements '
'Previous work for English <CIT> has shown that lexicalization leads to a sizable improvement in parsing performance '
'Tighter integration of semantics into the parsing models , possibly in the form of discriminative reranking models <CIT> , is a promising way forward in this regard '
'The creation of the Penn English Treebank <CIT> , a syntactically interpreted corpus , played a crucial role in the advances in natural language parsing technology <OTH> for English '
'Bleu is fast and easy to run , and it can be used as a target function in parameter optimization training procedures that are commonly used in state-of-the-art statistical MT systems <CIT> '
'For our experiments , we chose GIZA + + <OTH> and the RA approach <CIT> the best known alignment combination technique as our initial aligners1 42 TBL Templates Our templates consider consecutive words -LRB- of size 1 , 2 or 3 -RRB- in both languages '
'It has been known for some years that good performance can be realized with partial tagging and a hidden Markov model <CIT> '
'21 Lexicalized parse trees The first successful work on syntactic disambiguation was based on lexicalized probabilistic context-free grammar -LRB- LPCFG -RRB- <CIT> '
'32 Statistical Learning Model 321 Nave Bayes Learning Nave Bayes learning has been widely used in natural language processing with good results such as statistical syntactic parsing <CIT> , hidden language understanding <OTH> '
'1 Introduction IBM Model 1 <CIT> is a wordalignment model that is widely used in working with parallel bilingual corpora '
'Such methods have also been a key driver of progress in statistical machine translation , which depends heavily on unsupervised word alignments <CIT> '
'<CIT> improves the F1 score from 882 % to 897 % , while Charniak and Johnson -LRB- 2005 -RRB- improve from 903 % to 914 % '
'In <CIT> , anotherstate-of-the-artWSDengine -LRB- acombination of naive Bayes , maximum entropy , boosting and Kernel PCA models -RRB- is used to dynamically determine the score of a phrase pair under consideration and , thus , let the phrase selection adapt to the context of the sentence '
'The creation of the Penn English Treebank <OTH> , a syntactically interpreted corpus , played a crucial role in the advances in natural language parsing technology <CIT> for English '
'1 Introduction As with many other statistical natural language processing tasks , statistical machine translation <CIT> produces high quality results when ample training data is available '
'With non-local features , we can not use efcient procedures such as forward-backward procedures and the Viterbi algorithm that are required in training CRFs <OTH> and perceptrons <CIT> '
'1 Introduction In recent years , phrase-based systems for statistical machine translation <CIT> have delivered state-of-the-art performance on standard translation tasks '
'1 Introduction There is a pressing need for a consensus on a taskoriented level of semantic representation that can enable the development of powerful new semantic analyzers in the same way that the Penn Treebank <CIT> enabled the development of statistical syntactic parsers <OTH> '
'Corpus-based or example-based MT <OTH> and statistical MT <CIT> systems provide the easiest customizability , since users have only to supply a collection of source and target sentence pairs -LRB- a bilingual corpus -RRB- '
'Automated evaluation metrics that rate system behavior based on automatically computable properties have been developed in a number of other fields : widely used measures include BLEU <CIT> for machine translation and ROUGE <OTH> for summarisation , for example '
'Specifically , in the task of word alignment , heuristic approaches such as the Dice coefficient consistently underperform their re-estimated counterparts , such as the IBM word alignment models <CIT> '
'More recent work has achieved state-of-the-art results with Maxi101 mum entropy conditional Markov models -LRB- MaxEnt CMMs , or MEMMs for short -RRB- <CIT> '
'51 The AUGMENT technique for Domain Adaptation The AUGMENT technique introduced by <CIT> is a simple yet very effective approach to performing domain adaptation '
'Thus , as a powerful sequence tagging model , CRF became the dominant method in the Bakeoff 2006 <CIT> '
'<CIT> improves over <OTH> by suggesting a simpler approach '
'Nowadays , most of the state-of-the-art SMT systems are based on bilingual phrases <CIT> '
'Global information is known to be useful in other NLP tasks , especially in the named entity recognition task , and several studies successfully used global features <CIT> '
'All state-of-the-art wide-coverage parsers relax this assumption in some way , for instance by -LRB- i -RRB- changing the parser in step -LRB- 3 -RRB- , such that the application of rules is conditioned on other steps in the derivation process <CIT> , or by -LRB- ii -RRB- enriching the nonterminal labels in step -LRB- 1 -RRB- with context-information <OTH> , along with suitable backtransforms in step -LRB- 4 -RRB- '
'Recent work includes improved model variants <CIT> and applications such as web data extraction <OTH> , scientific citation extraction <OTH> , and word alignment <OTH> '
'2 Related Work To model the syntactic transformation process , researchers in these fieldsespecially in machine translationhave developed powerful grammatical formalisms and statistical models for representing and learning these tree-to-tree relations <CIT> '
'1 Introduction The availability of large amounts of so-called parallel texts has motivated the application of statistical techniques to the problem of machine translation starting with the seminal work at IBM in the early 90s <CIT> '
'This combination of the perceptron algorithm with beam-search is similar to that described by <CIT> 5 The perceptron algorithm is a convenient choice because it converges quickly usually taking only a few iterations over the training set <CIT> '
'In their seminal work , <CIT> demonstrated that supervised learning signi cantly outperformed a competing body of work where hand-crafted dictionaries are used to assign sentiment labels based on relative frequencies of positive and negative terms '
'7Another related measure is <CIT> ''s likelihood ratio tests for binomial and multinomial distributions , which are claimed to be effective even with very much smaller volumes of text than is necessary for other tests based on assumed normal distributions '
'The ROUGE <CIT> suite of metrics are n-gram overlap based metrics that have been shown to highly correlate with human evaluations on content responsiveness '
'<CIT> improve their F-score by 3 % by including a Wikipedia-based feature in their machine learner '
'Support Vector Machines -LRB- SVMs -RRB- <OTH> and Maximum Entropy -LRB- ME -RRB- method <OTH> are powerful learning methods that satisfy such requirements , and are applied successfully to other NLP tasks <CIT> '
'Among these techniques , SCL -LRB- Structural Correspondence Learning -RRB- <CIT> is regarded as a promising method to tackle transfer-learning problem '
'Several general-purpose off-the-shelf -LRB- OTS -RRB- parsers have become widely available <CIT> '
'The best example of such an approach is <CIT> , who proposes a method that automatically identifies collocations that are indicative of the sense of a word , and uses those to iteratively label more examples '
'1 Introduction There has been a great deal of progress in statistical parsing in the past decade <CIT> '
'An especially well-founded framework for doing this is maximum entropy <CIT> '
'State-of-art systems for doing word alignment use generative models like GIZA + + <CIT> '
'5 Comparison with Previous Top Systems and Related Work In POS tagging , the previous best performance was reported by <CIT> as summarized in Table 7 '
'<CIT> work is perhaps one of the most notable examples of unsupervised polarity classification '
'Far from full syntactic complexity , we suggest to go back to the simpler alignment methods first described by <CIT> '
'The latent-annotation model <CIT> is one of the most effective un-lexicalized models '
'Online votedperceptrons have been reported to work well in a number of NLP tasks <CIT> '
'Because it is not feasible here to have humans judge the quality of many sets of translated data , we rely on an array of well known automatic evaluation measures to estimate translation quality : BLEU <CIT> is the geometric mean of the n-gram precisions in the output with respect to a set of reference translations '
'22 Unsupervised Parameter Estimation We can perform maximum likelihood estimation of the parameters of this model in a similar fashion to that of Model 4 <OTH> , described thoroughly in <CIT> '
'To overcome this problem , unsupervised learning methods using huge unlabeled data to boost the performance of rules learned by small labeled data have been proposed recently <OTH> <CIT> <OTH> <OTH> '
'Moreover , the deterministic dependency parser of Yamada and Matsumoto <OTH> , when trained on the Penn Treebank , gives a dependency accuracy that is almost as good as that of <CIT> and Charniak -LRB- 2000 -RRB- '
'David McClosky , Eugene Charniak , and Mark Johnson Brown Laboratory for Linguistic Information Processing -LRB- BLLIP -RRB- Brown University Providence , RI 02912 -LCB- dmcc ec mj -RCB- @ csbrownedu Abstract Self-training has been shown capable of improving on state-of-the-art parser performance <CIT> despite the conventional wisdom on the matter and several studies to the contrary <OTH> '
'We conclude with some challenges that still remain in applying proactive learning for MT 2 Syntax Based Machine Translation In recent years , corpus based approaches to machine translation have become predominant , with Phrase Based Statistical Machine Translation -LRB- PBSMT -RRB- <CIT> being the most actively progressing area '
'A two-tier scheme <CIT> where sentences are rst classi ed as subjective versus objective , and then applying the sentiment classi er on only the subjective sentences further improves performance '
'There is also substantial work in the use of target-side syntax <CIT> '
'Averaged perceptron <CIT> , which has been successfully applied to several tagging and parsing reranking tasks <CIT> , was employed for training rerank267 CLANG GEOQUERY P R F P R F SCISSOR 895 737 808 985 744 848 SCISSOR + 870 780 823 955 772 854 Table 2 : The performance of the baseline model SCISSOR + compared with SCISSOR -LRB- with the best result in bold -RRB- , where P = precision , R = recall , and F = F-measure '
'Substantial improvements have been made to parse western language such as English , and many powerful models have been proposed <CIT> '
'Maximum entropy can be used to improve IBM-style translation probabilities by using features , such as improvements to P -LRB- f e -RRB- in <CIT> '
'22 Statistical Translation Lexicon We use a statistical translation lexicon known as IBM Model-1 in <CIT> for both efficiency and simplicity '
'53 Comparison with System Combination We re-implemented a state-of-the-art system combination method <CIT> '
'-LRB- 3 -RRB- -LRB- -RRB- -LRB- -RRB- 0 log 2 log A LH LH = 1 Problems for an unscaled log approach Although log identifies collocations much better than competing approaches <CIT> in terms of its recall , it suffers from its relatively poor precision rates '
'1 Introduction Phrase-based method <CIT> and syntaxbased method <OTH> represent the state-of-the-art technologies in statistical machine translation -LRB- SMT -RRB- '
'Annealing resembles the popular bootstrapping technique <CIT> , which starts out aiming for high precision , and gradually improves coverage over time '
'41 Complete ambiguity classes Ambiguity classes capture the relevant property we are interested in : words with the same category possibilities are grouped together4 And ambiguity classes have been shown to be successfully employed , in a variety of ways , to improve POS tagging <CIT> '
'1 Introduction Phrase-based translation models <CIT> , which go beyond the original IBM translation models <OTH> 1 by modeling translations of phrases rather than individual words , have been suggested to be the state-of-theart in statistical machine translation by empirical evaluations '
'<CIT> demonstrated that semi-supervised WSD could be successful '
'Second , benefits for sentiment analysis can be realized by decomposing the problem into S\/O -LRB- or neutral versus polar -RRB- and polarity classification <CIT> '
'This similarity score is computed as a max over a number of component scoring functions, some based on external lexical resources, including:  various string similarity functions, of which most are applied to word lemmas  measures of synonymy, hypernymy, antonymy, and semantic relatedness, including a widelyused measure due to Jiang and Conrath (1997), based on manually constructed lexical resources such as WordNet and NomBank  a function based on the well-known distributional similarity metric of Lin (1998), which automatically infers similarity of words and phrases from their distributions in a very large corpus of English text The ability to leverage external lexical resources both manually and automatically constructedis critical to the success of MANLI.'
'The search across a dimension uses the efficient method of <CIT> '
'In the hierarchical phrase-based model <OTH> , and an inversion transduction grammar -LRB- ITG -RRB- <CIT> , the problem is resolved by restricting to a binarized form where at most two non-terminals are allowed in the righthand side '
'In syntactic parse re-ranking supersenses have been used to build useful latent semantic features <CIT> '
'In agreement with recent resuits on parsing with lexicalised probabilistic grammars <CIT> , we find that statistics over lexical , as opposed to structural , features best correspond to human intuitivejudgments and to experimental findings '
'1 Introduction In recent years , statistical machine translation have experienced a quantum leap in quality thanks to automatic evaluation <OTH> and errorbased optimization <CIT> '
'1 Introduction Automatic Metrics for machine translation -LRB- MT -RRB- evaluation have been receiving significant attention in the past two years , since IBM ''s BLEU metric was proposed and made available <CIT> '
'In the classic work on SMT , Brownandhiscolleagues atIBMintroduced the notion of alignment between a sentence f and its translation e and used it in the development of translation models <CIT> '
'<CIT> work is perhaps one of the most notable examples of unsupervised polarity classification '
'a2 Maximum-entropy method The maximum-entropy method is useful with sparse data conditions and has been used by many researchers <CIT> '
'NJ 08903 USA suzanne ~ ruccs , rutgers , edu Empirically-induced models that learn a linguistically meaningflll grammar <CIT> seem to give tile best practical results in statistical natural language processing '
'Freund and Schapire <OTH> originally proposed the averaged parameter method ; it was shown to give substantial improvements in accuracy for tagging tasks in <CIT> '
'Phrase-based decoding <CIT> is a dominant formalism in statistical machine translation '
'However , other types of nonlocal information have also been shown to be effective <CIT> and we will examine the effectiveness of other non-local information which can be embedded into label information '
'The translation quality was evaluated using a well-established automatic measure : BLEU score <CIT> '
'Penn Treebank <CIT> was also used to induce part-of-speech -LRB- POS -RRB- taggers because the corpus contains very precise and detailed POS markers as well as bracket , annotations '
'5 Related Work Discriminative models have recently been proved to be more effective than generative models in some NLP tasks , eg , parsing <CIT> , POS tagging <CIT> and LM for speech recognition <OTH> '
'Usually in 1 In our experiments , we set negative PMI values to 0 , because <CIT> , in their seminal paper on word association ratio , show that negative PMI values are not expected to be accurate unless co-occurrence counts are made from an extremely large corpus '
'1 Introduction Phrase-based Statistical MT -LRB- PB-SMT -RRB- <CIT> has become the predominant approach to Machine Translation in recent years '
'4 Evaluation The purpose of our evaluation is to contrast our proposed feature based approach with a state-ofthe-art sequential learning technique <CIT> '
'This combination of the perceptron algorithm with beam-search is similar to that described by <CIT> 5 The perceptron algorithm is a convenient choice because it converges quickly usually taking only a few iterations over the training set <CIT> '
'1 Motivation Phrase-based statistical machine translation <CIT> has emerged as the dominant paradigm in machine translation research '
'1 Introduction Treebank-based probabilistic parsing has been the subject of intensive research over the past few years , resulting in parsing models that achieve both broad coverage and high parsing accuracy <CIT> '
'The best examples of this approach has been the resent work of Yarowsky <CIT> , <CIT> , <CIT> '
'The IBM models 1-5 <CIT> produce word alignments with increasing algorithmic complexity and performance '
'Probably the most widely used feature weighting function is (point-wise) Mutual Information (MI) (Church and Patrick 1990; Hindle 1990; Luk 1995; Lin 1998; Gauch, Wang, and Rachakonda 1999; Dagan 2000; Baroni and Vegnaduzzo 2004; Chklovski and Pantel 2004; Pantel and Ravichandran 2004; Pantel, Ravichandran, and Hovy 2004; Weeds, Weir, and McCarthy 2004), dened by: weight MI (w,f)=log 2 P(w,f) P(w)P(f) (1) We calculate the MI weights by the following statistics in the space of co-occurrence instances S: weight MI (w,f)=log 2 count(w,f) nrels count(w) count(f) (2) where count(w,f) is the frequency of the co-occurrence pair w,f  in S, count(w)and count(f) are the independent frequencies of w and f in S,andnrels is the size of S.High MI weights are assumed to correspond to strong wordfeature associations.'
'Even robust parsers using linguistically sophisticated formalisms , such as TAG <OTH> , CCG <OTH> , HPSG <OTH> and LFG <CIT> , often use training data derived from the Penn Treebank '
'The NIST MT03 set is used to tune model weights -LRB- eg those of -LRB- 16 -RRB- -RRB- and the scaling factor 17We have also experimented with MERT <CIT> , and found that the deterministic annealing gave results that were more consistent across runs and often better '
'-LRB- 2 -RRB- We note that these posterior probabilities can be computed efficiently for some alignment models such as the HMM <OTH> , Models 1 and 2 <CIT> '
'The averaged perceptron <CIT> is a variant which averages the w across all iterations ; it has demonstrated good generalization especially with data that is not linearly separable , as in many natural language processing problems '
'1 Introduction In this paper , we show how discriminative training with averaged perceptron models <CIT> can be used to substantially improve surface realization with Combinatory Categorial Grammar <OTH> '
'1 Introduction Statistical machine translation <OTH> has seen many improvements in recent years , most notably the transition from wordto phrase-based models <CIT> '
'Point-wise mutual information <CIT> and Relative Feature Focus <OTH> are well-known examples '
'A solution that leverages the complementary strengths of these two approachesdescribed in detail by <CIT> was recently and successfully explored by Nivre and McDonald -LRB- 2008 -RRB- '
'A detailed description of the popular translation\/alignment models IBM-1 to IBM-5 <CIT> , as well as the Hidden-Markov alignment model -LRB- HMM -RRB- <OTH> can be found in <OTH> '
'Our model exploits the same kind of tag-n-gram information that forms the core of many successful tagging models , for example , <OTH> , <OTH> , <CIT> '
'To some extent , this can probably be explained by the strong tradition of constituent analysis in Anglo-American linguistics , but this trend has been reinforced by the fact that the major treebank of American English , the Penn Treebank <CIT> , is annotated primarily with constituent analysis '
'As a side product , we find empirical evidence to suggest that the effectiveness of rule lexicalization techniques <CIT> and parent annotation techniques <OTH> is due to the fact that both lead to a reduction in perplexity in the automata induced from training corpora '
'The dif1The routinely used tool for automatic evaluation ROUGE was adopted exactly because it was demonstrated it is highly correlated with the manual DUC coverage scores <CIT> '
'1 Introduction Phrase-based method <OTH> and syntaxbased method <CIT> represent the state-of-the-art technologies in statistical machine translation -LRB- SMT -RRB- '
'7 Related Work Unannotated texts have been used successfully for a variety of NLP tasks , including named entity recognition <OTH> , subjectivity classification <OTH> , text classification <OTH> , and word sense disambiguation <CIT> '
'2 Previous Work So far , Structural Correspondence Learning has been applied successfully to PoS tagging and Sentiment Analysis <CIT> '
'We use a recently proposed dependency parser <CIT> 1 which has demonstrated state-of-theart performance on a selection of languages from the 1The ISBN parser will be soon made downloadable from the authors web-page '
'Indeed , as for the voted perceptron of <CIT> , we can get performance gains by reducing the support threshold for features to be included in the model '
'In addition to the widely used BLEU <OTH> and NIST <OTH> scores , we also evaluate translation quality with the recently proposed Meteor <CIT> and four edit-distance style metrics , Word Error Rate -LRB- WER -RRB- , Positionindependent word Error Rate -LRB- PER -RRB- <OTH> , CDER , which allows block reordering <OTH> , and Translation Edit Rate -LRB- TER -RRB- <OTH> '
'Along this line , <CIT> present convincing evidence that restricting phrasal translation to syntactic constituents yields poor translation performance the ability to translate nonconstituent phrases -LRB- such as there are , note that , and according to -RRB- turns out to be critical and pervasive '
'One of the most notable examples is <CIT> bootstrapping algorithm for word sense disambiguation '
'There has been considerable skepticism over whether WSD will actually improve performance of applications , but we are now starting to see improvement in performance due to WSD in cross-lingual information retrieval <OTH> and machine translation <CIT> and we hope that other applications such as question-answering , text simplication and summarisation might also benet as WSD methods improve '
'In addition , their system does not classify non-anaphoric pronouns , A third paper that has significantly influenced our work is that of <CIT> '
'of the position infer marion of words at ltlat -LRB- ; hillg pairs of sellte\/lCeS , which turned out useful <CIT> '
'1 Introduction The most widely applied training procedure for statistical machine translation IBM model 4 <CIT> unsupervised training followed by post-processing with symmetrization heuristics <OTH> yields low quality word alignments '
'1 Introduction One of the major approaches to disambiguate word senses is supervised learning <OTH> , <CIT> , <OTH> , <OTH> , <OTH> , <OTH> , <OTH> , <OTH> '
'ROUGE <CIT> has been widely used for summarization evaluation '
'ROUGE version 155 <CIT> was used for evaluation2 Among others , we focus on ROUGE-1 in the discussion of the result , because ROUGE-1 has proved to have strong correlation with human annotation <CIT> '
'We report results using the well-known automatic evaluation metrics Bleu <CIT> '
'Among recent top performing methods are Hidden Markov Models <OTH> , maximum entropy approaches <CIT> , and transformation-based learning <OTH> '
'Another interesting point is the relation to maximum entropy model <CIT> , which is popular in the natural language processing community '
'We evaluated the generator on the Penn Treebank <CIT> , which is highly reliable corpus consisting of real-world texts '
'Averaging parameters is a way to reduce overfitting for perceptron training <CIT> , and is applied to all our experiments '
'2 Maximum Entropy Models Maximum entropy -LRB- ME -RRB- models <CIT> , also known as log-linear and exponential learning models , provideageneralpurposemachinelearningtechnique for classification and prediction which has been successfully applied to natural language processing including part of speech tagging , named entity recognition etc Maximum entropy models can integrate features from many heterogeneous information sources for classification '
'1 Introduction State-of-the-art Statistical Machine Translation -LRB- SMT -RRB- systems usually adopt a two-pass search strategy <CIT> as shown in Figure 1 '
'51 The statistical parser The parsing model is the one proposed in Merlo and Musillo <OTH> , which extends the syntactic parser of Henderson <OTH> and <CIT> with annotations which identify semantic role labels , and has competitive performance '
'Study in collocation extraction using lexical statistics has gained some insights to the issues faced in collocation extraction <CIT> '
'Effective training algorithm exists <CIT> once the set of features a42 a57 a16 a1a33a8 a71a54a8 a71a100a85a68a5 a53 is selected '
'There are several distance measures suitable for this purpose , such as the mutual information <CIT> , the dice coefficient <OTH> , the phi coefficient <OTH> , the cosine measure <OTH> and the confidence <OTH> '
'1 Introduction Co-occurrence statistics extracted from corpora lead to good performance on a wide range of tasks that involve the identification of the semantic relation between two words or concepts <CIT> '
'Maximum entropy models <CIT> are a class of exponential models which require no unwarranted independence assumptions and have proven to be very successful in general for integrating information from disparate and possibly overlapping sources '
'Support Vector Machines -LRB- SVMs -RRB- <OTH> and Maximum Entropy -LRB- ME -RRB- method <CIT> are powerful learning methods that satisfy such requirements , and are applied successfully to other NLP tasks <OTH> '
'42 Support Vector Machines We chose to adopt a tagging perspective for the Simple NP chunking task , in which each word is to be tagged as either B , I or O depending on wether it is in the Beginning , Inside , or Outside of the given chunk , an approach first taken by <CIT> , and which has become the de-facto standard for this task '
'In order to overcome this , some unsupervised learning methods and minimally-supervised methods , eg , <CIT> , have been proposed '
'For the current work , the Log-likelihood coefficient has been employed <CIT> , as it is reported to perform well among other scoring methods <OTH> '
'This averaging effect has been shown to help overfitting <CIT> '
'Finally , to estimate the parameters i of the weighted linear model , we adopt the popular minimum error rate training procedure <CIT> which directly optimizes translation quality as measured by the BLEU metric '
'Indeed , researchers have shown that gigantic language models are key to state-ofthe-art performance <CIT> , and the ability of phrase-based decoders to handle large-size , high-order language models with no consequence on asymptotic running time during decoding presents a compelling advantage over CKYdecoders , whosetimecomplexitygrowsprohibitively large with higher-order language models '
'A novel approach was described in <CIT> , which used an unsupervised training technique , extracting relations that were explicitly and unamibiguously signalled and automatically labeling those examples as the training set '
'The bigram translation probability relies on word context , known to be helpful in translation <CIT> , to improve the identification of target phrases '
'It is the most widely reported metric in MT research , and has been shown to correlate well with human judgment <CIT> '
'This means that the 1 -RRB- roblem of recognizing named entities in those cases can be solved by incorporating techniques of base noun phrase chunking <CIT> '
'These models have achieved state-of-the-art performance in transcript-based speech summarization <CIT> '
'1 Introduction Research in language processing has benefited greatly from the collection of large annotated corpora such as Penn PropBank <OTH> and Penn Treebank <CIT> '
'G-Theory and Agreement Indices Two well-known measures for capturing the quality of manual annotations are agreement percentages and the kappa statistic <CIT> '
'The technique of averaging was introduced in the context of perceptrons as an approximation to taking a vote among all the models traversed during training , and has been shown to work well in practice <CIT> '
'2 Confusion-network-based MT system combination The current state-of-the-art is confusion-networkbased MT system combination as described by 98 Rosti and colleagues <CIT> '
'<CIT> successfully used this observation as an approximate annotation technique in an unsupervised WSD model '
'So far , SCL has been applied successfully in NLP for Part-of-Speech tagging and Sentiment Analysis <CIT> '
'4 Experiments Phrase-based SMT systems have been shown to outperform word-based approaches <CIT> '
'21 The <CIT> sparked considerable interest in bootstrapping with his successful method for word sense disambiguation '
'In statistical machine translation , IBM 1 ~ 5 models <CIT> based on the source-chmmel model have been widely used and revised for many language donmins and applications '
'6 Discussion Noting that adding latent features to nonterminals in unlexicalized context-free parsing has been very successful <CIT> , we were surprised not to see a 3Czech experiments were not done , since the number of features -LRB- more than 14 million -RRB- was too high to multiply out by clusters '
'1 Introduction During the last four years , various implementations and extentions to phrase-based statistical models <CIT> have led to significant increases in machine translation accuracy '
'RANDLM <CIT> performs well and scaled to the full data with improvement -LRB- resulting in our best overall system -RRB- '
'Bleu is fast and easy to run , and it can be used as a target function in parameter optimization training procedures that are commonly used in state-of-the-art statistical MT systems <CIT> '
'1 Introduction Over the past decade , there has been tremendous progress on learning parsing models from treebank data <CIT> '
'On the Hansards data , the simple averaging technique described by <CIT> yields a reasonable model '
'With the in-depth study of opinion mining , researchers committed their efforts for more accurate results : the research of sentiment summarization <OTH> , domain transfer problem of the sentiment analysis <CIT> and finegrained opinion mining <OTH> are the main branches of the research of opinion mining '
'Finally , the translation model can be formalized as the following optimization problem argmax logPr -LRB- D ; -RRB- st mwsummationdisplay j = 1 Pr -LRB- wj ok -RRB- = 1 , k This optimization problem can be solved by the EM algorithm <CIT> '
'First , we compared our system output to human reference translations using Bleu <CIT> , a widelyaccepted objective metric for evaluation of machine translations '
'1 Introduction Phrase-based method <CIT> and syntaxbased method <OTH> represent the state-of-the-art technologies in statistical machine translation -LRB- SMT -RRB- '
'According to our experience , the best performance is achieved when the union of the source-to-target and target-to-source alignment sets <CIT> is used for tuple extraction -LRB- some experimental results regarding this issue are presented in Section 422 -RRB- '
'Nonparametricmodels <CIT> may be appropriate '
'6 Related Work The popular IBM models for statistical machine translation are described in <CIT> '
'We use five sentiment classification datasets , including the widely-used movie review dataset -LRB- MOV -RRB- <CIT> as well as four datasets containing reviews of four different types of products from Amazon -LRB- books -LRB- BOO -RRB- , DVDs -LRB- DVD -RRB- , electronics -LRB- ELE -RRB- , and kitchen appliances -LRB- KIT -RRB- -RRB- <OTH> '
'<CIT> shows that baseNP recognition -LRB- Fz = I = 920 -RRB- is easier than finding both NP and VP chunks -LRB- Fz = 1 = 881 -RRB- and that increasing the size of the training data increases the performance on the test set '
'Movies Reviews : This is a popular dataset in sentiment analysis literature <CIT> '
'36 Parameter Estimation To estimate parameters k -LRB- 1 k K -RRB- , lm , and um , we adopt the approach of minimum error rate training -LRB- MERT -RRB- that is popular in SMT <CIT> '
'1 Introduction In recent years , Bracketing Transduction Grammar -LRB- BTG -RRB- proposed by <CIT> has been widely used in statistical machine translation -LRB- SMT -RRB- '
'While significant time savings have already been reported on the basis of automatic pre-tagging -LRB- eg , for POS and parse tree taggings in the Penn TreeBank <CIT> , or named entity taggings for the Genia corpus <OTH> -RRB- , this kind of pre-processing does not reduce the number of text tokens actually to be considered '
'Veale <OTH> used WordNet to answer 374 multiple-choice SAT analogy questions , achieving an accuracy of 43 % , but the best corpus-based approach attains an accuracy of 56 % <CIT> '
'The IOB1 format , introduced in <CIT> , consistently -LRB- : ame out as the best format '
'They were based on mutual information <OTH> , conditional probabilities <OTH> , or on some standard statistical tests , such as the chi-square test or the loglikelihood ratio <CIT> '
'State-of-theart machine learning techniques including Support Vector Machines <OTH> , AdaBoost <OTH> and Maximum Entropy Models <CIT> provide high performance classifiers if one has abundant correctly labeled examples '
'21 The BLEU Metric The metric most often used with MERT is BLEU <CIT> , where the score of a candidate c against a reference translation r is : BLEU = BP -LRB- len -LRB- c -RRB- , len -LRB- r -RRB- -RRB- exp -LRB- 4summationdisplay n = 1 1 4 logpn -RRB- , where pn is the n-gram precision2 and BP is a brevity penalty meant to penalize short outputs , to discourage improving precision at the expense of recall '
'Lexicalization can increase parsing performance dramatically for English <CIT> , and the lexicalized model proposed by Collins -LRB- 1997 -RRB- has been successfully applied to Czech <CIT> and Chinese <OTH> '
'Some NLG researchers are impressed by the success of the BLEU evaluation metric <CIT> in Machine Translation -LRB- MT -RRB- , which has transformed the MT field by allowing researchers to quickly and cheaply evaluate the impact of new ideas , algorithms , and data sets '
'In addition , the averaged parameters technology <CIT> is used to alleviate overfitting and achieve stable performance '
'Successful discriminative parsers have used generative models to reduce training time and raise accuracy above generative baselines <CIT> '
'1 Introduction A hypergraph , as demonstrated by <CIT> , is a compact data-structure that can encode an exponential number of hypotheses generated by a regular phrase-based machine translation -LRB- MT -RRB- system -LRB- eg , Koehn et al '
'<CIT> compares his method to <OTH> and shows that for four words the former performs significantly better in distinguishing between two senses '
'So far , SCL has been applied successfully in NLP for Part-of-Speech tagging and Sentiment Analysis <CIT> '
'1 Introduction With the introduction of the BLEU metric for machine translation evaluation <CIT> , the advantages of doing automatic evaluation for various NLP applications have become increasingly appreciated : they allow for faster implement-evaluate cycles -LRB- by by-passing the human evaluation bottleneck -RRB- , less variation in evaluation performance due to errors in human assessor judgment , and , not least , the possibility of hill-climbing on such metrics in order to improve system performance <OTH> '
'SVM has been shown to be useful for text classification tasks <OTH> , and has previously given good performance in sentiment classification experiments <CIT> '
'Also , in a , state-ofthe-art English parser <CIT> only the words tha , t occur more tha , n d times in training data '
'23 The Averaged Perceptron Reranking Model Averaged perceptron <CIT> has been successfully applied to several tagging and parsing reranking tasks <CIT> , and in this paper , we employed it in reranking semantic parses generated by the base semantic parser SCISSOR '
'One popular and statistically appealing such measure is Log-Likelihood -LRB- LL -RRB- <CIT> '
'Our MT experiments use a re-implementation of Moses <CIT> called Phrasal , which provides an easier API for adding features '
'Recent several years have witnessed the rapid development of system combination methods based on confusion networks -LRB- eg , <CIT> -RRB- , which show state-of-theart performance in MT benchmarks '
'The averaged 1555 perceptron has a solid theoretical fundamental and was proved to be effective across a variety of NLP tasks <CIT> '
'23 Classifier Training We chose maximum entropy <CIT> as our primary classifier , since it had been successfully applied by the highest performing systems in both the SemEval-2007 preposition sense disambiguation task <OTH> and the general word sense disambiguation task <OTH> '
'It could be shown that such methods , of which BLEU <CIT> is the most common , can deliver evaluation results that show a high agreement with human judgments <CIT> '
'The most widely used are Word Error Rate -LRB- WER -RRB- , Position Independent Word Error Rate -LRB- PER -RRB- , the BLEU score <CIT> and the NIST score <OTH> '
'<OTH> has proved to be a simple yet powerful observation and has been successfully used in word sense disambiguation -LRB- WSD -RRB- and related tasks -LRB- eg , <CIT> ; Agirre and Rigau The author was partially funded by GALE DARPA Contract No '
'Furthermore , end-to-end systems like speech recognizers <OTH> and automatic translators <CIT> use increasingly sophisticated discriminative models , which generalize well to new data that is drawn from the same distribution as the training data '
'Moreover , log likelihood ratios are regarded as a more effective method to identify collocations especially when the occurrence count is very low <CIT> '
'After the success in syntactic -LRB- Penn TreeBank <CIT> -RRB- and propositional encodings -LRB- Penn PropBank <OTH> -RRB- , more sophisticated semantic data -LRB- such as temporal <OTH> or opinion annotations <OTH> -RRB- and discourse data -LRB- eg , for anaphora resolution <OTH> and rhetorical parsing <OTH> -RRB- are being generated '
'We choose those sections because several state-of-thwart parsers <CIT> are trained on Section 2-21 and tested on Section 23 '
'441 N-gram Co-Occurrence Statistics for Answer Extraction N-gram co-occurrence statistics have been successfully used in automatic evaluation <CIT> , and more recently as training criteria in statistical machine translation <OTH> '
'However , since work in this direction has started , a significant progress has also been made in the research on statistical learning of full parsers , both in terms of accuracy and processing time <CIT> '
'Phrases extracted using these heuristics are also shown to perform better than syntactically motivated phrases , the joint model , and IBM model 4 <CIT> '
'Recently , various works have improved the quality of statistical machine translation systems by using phrase translation <CIT> '
'He has achieved state-of-the art results by applying ME to parsing <CIT> , part-of-speech tagging <CIT> , and sentence-boundary detection <CIT> '
'There also have been prior work on maintaining approximate counts for higher-order language models -LRB- LMs -RRB- -LRB- <CIT> -RRB- operates under the model that the goal is to store a compressed representation of a disk-resident table of counts and use this compressed representation to answer count queries approximately '
'By default , the log-likelihood ratio measure -LRB- LLR -RRB- is proposed , since it was shown to be particularly suited to language data <CIT> '
'1 Introduction Very large corpora obtained from the Web have been successfully utilized for many natural languageprocessing -LRB- NLP -RRB- applications , suchasprepositional phrase -LRB- PP -RRB- attachment , other-anaphora resolution , spellingcorrection , confusablewordsetdisambiguation and machine translation <CIT> '
'To estimate the parameters of the MEMM + pred model we turn to the successful Maximum Entropy <CIT> parameter estimation method '
'For the full parser , we use the one developed by Michael Collins <CIT> one of the most accurate full parsers around '
'This algorithm appears fairly widely known : it was described by Goodman <OTH> and <CIT> and used by Ding et al -LRB- 2005 -RRB- , and is very similar to other dynamic programming algorithms for CFGs , so we only summarize it here '
'<CIT> -RRB- are best known and studied '
'Introduction The Penn Treebank <CIT> initiated a new paradigm in corpus-based research '
'<CIT> then extended their method and established a sound probabilistic model series , relying on different parameters describing how words within parallel sentences are aligned to each other '
'On the other hand , the best available parsers trained on the Penn Treebank , those of <CIT> and Charniak -LRB- 2000 -RRB- , use statistical models for disambiguation that make crucial use of dependency relations '
'Experiments show that the resulting rule set significantly improves the speed and accuracy over monolingual binarization -LRB- see Table 1 -RRB- in a stateof-the-art syntax-based machine translation system <CIT> '
'An important aspect of web search is to be able to narrow down search results by distinguishing among people with the same name leading to multiple efforts focusing on web person name disambiguation in the literature <CIT> '
'1 Introduction Statistical approaches to machine translation , pioneered by <CIT> , achieved impressive performance by leveraging large amounts of parallel corpora '
'2 Maximum Entropy Models Maximum entropy -LRB- ME -RRB- models <CIT> , also known as log-linear and exponential learning models , provideageneralpurposemachinelearningtechnique for classification and prediction which has been successfully applied to natural language processing including part of speech tagging , named entity recognition etc Maximum entropy models can integrate features from many heterogeneous information sources for classification '
'In this work we use the averaged perceptron algorithm <CIT> since it is an online algorithm much simpler and orders of magnitude faster than Boosting and MaxEnt methods '
'Still , it is in our next plans and part of our future work to embed in our model some of the interesting WSD approaches , like knowledgebased <CIT> , corpus-based <CIT> , or combinations with very high accuracy <OTH> '
'Thus, we obtain the following second-order model: a36a39a38a41a40 a17 a5a7 a42a4 a5a7 a44 a8 a5a57 a15a27a58 a7 a36a39a38a41a40 a17a20a15a59a42a17 a15a41a49 a7 a7 a60 a4 a5a7 a44 a8 ma61a63a62a65a64a33a66 a5a57 a15a27a58 a7a68a67 a40 a17 a15 a42a17 a15a50a49 a7 a15a50a49a51a48 a60 a4 a15a27a47a55a48 a15a50a49a54a48 a44 a11 A well-founded framework for directly modeling the posterior probability a67 a40 a17 a15 a42a17 a15a50a49 a7 a15a50a49a54a48 a60 a4 a15a12a47a55a48 a15a50a49a54a48 a44 is maximum entropy (Berger et al. , 1996).'
'2 Related Work The popular IBM models for statistical machine translation are described in <CIT> and the HMM-based alignment model was introduced in <OTH> '
'In the news article domain , ROUGE scores have been shown to be generally highly correlated with human evaluation in content match <CIT> '
'1 Introduction Phrase-based systems <CIT> are probably the most widespread class of Statistical Machine Translation systems , and arguably one of the most successful '
'21 Log-Linear Models The log-linear model -LRB- LLM -RRB- , or also known as maximum-entropy model <CIT> , is a linear classifier widely used in the NLP literature '
'Although we see statistically significant improvements -LRB- at the 05 level on a paired permutation test -RRB- , the quality of the parsers is still quite poor , in contrast to other applications of bootstrapping which rival supervised methods <CIT> '
'This paper demonstrates several of the characteristics and benefits of SemFrame <CIT> , a system that produces such a resource '
'3 The Perceptron The perceptron algorithm introduced into NLP by <CIT> , is a simple but effective discriminative training method '
'3 CLaC-NB System : Nave Bayes Supervised statistical methods have been very successful in sentiment tagging of texts and in subjectivity detection at sentence level : on movie review texts they reach an accuracy of 85-90 % <CIT> and up to 92 % accuracy on classifying movie review snippets into subjective and objective using both Nave Bayes and SVM <CIT> '
'1 Introduction Raw parallel data need to be preprocessed in the modern phrase-based SMT before they are aligned by alignment algorithms , one of which is the wellknown tool , GIZA + + <CIT> , for training IBM models -LRB- 1-4 -RRB- '
'3 Language modelling with Bloom filters Recentwork <CIT> presenteda scheme for associating static frequency information with a set of n-grams in a BF efficiently1 31 Log-frequency Bloom filter The efficiency of the scheme for storing n-gram statistics within a BF presented in Talbot and Osborne <OTH> relies on the Zipf-like distribution of n-gramfrequencies : mosteventsoccuranextremely small number of times , while a small number are very frequent '
'In the past five years , important research on the automatic acquisition of word classes based on lexical distribution has been published <CIT> '
'Ranking algorithms , such as Kleinbergs HITS algorithm <OTH> or Googles PageRank <OTH> , have been traditionally and successfully used in Web-link analysis <OTH> , social networks , and more recently in text processing applications <CIT> , <CIT> , <OTH> '
'Some notable efforts in this direction for other languages have been the Penn Tree Bank <CIT> for English and the Prague Dependency Bank <OTH> for Czech '
'In informal experiments described elsewhere <OTH> , I found that the G 2 statistic suggested by <CIT> slightly outperforms 2 '
'Introduction The creation of the Penn Treebank <CIT> and the word sense-annotated SEMCOR <OTH> have shown how even limited amounts of annotated data can result in major improvements in complex natural language understanding systems '
'The Penn TreeBank -LRB- PTB -RRB- is an example of such a resource with worldwide impact on natural language processing <CIT> '
'Exponential family models are a mainstay of modern statistical modeling <OTH> and they are widely and successfully used for example in text classification <CIT> '
'1 Introduction Recently linguistically-motivated syntax-based translation method has achieved great success in statistical machine translation -LRB- SMT -RRB- <CIT> '
'The abduction-based approach <CIT> has provided a simple and elegant way to realize such a task '
'Recent comparisons of approaches that can be trained on corpora <OTH> have shown that in most cases statistical aproaches <CIT> yield better results than finite-state , rule-based , or memory-based taggers <OTH> '
'For a full derivation of the modified updates and for quite technical convergence proofs , see <CIT> '
'State-of-the-art measures such as BLEU <CIT> or NIST <OTH> aim at measuring the translation quality rather on the document level1 than on the level of single sentences '
'As a result , the good results of <CIT> with large seed sets do not immediately imply success with small seed sets '
'Its also worth noting that <CIT> saw a LFMS improvement of 08 % over their baseline discriminative parser after adding punctuation features , one of which encoded the sentence-final punctuation '
'It is often straightforward to obtain large amounts of unlabeled data , making semi-supervised approaches appealing ; previous work on semisupervised methods for dependency parsing includes <CIT> '
'In an experiment on 16,800 sentences of Chinese-English newswire text with segment-level human evaluation from the Linguistic Data Consortium?s (LDC) Multiple Translation project, we compare the LFG-based evaluation method with other popular metrics like BLEU, NIST, General Text Matcher (GTM) (Turian et al. , 2003), Translation Error Rate (TER) (Snover et al. , 2006)1, and METEOR (Banerjee and Lavie, 2005), and we show that combining dependency representations with synonyms leads to a more accurate evaluation that correlates better with human judgment.'
'Typical examples of linguistically sophisticated annotation include tagging words with their syntactic category -LRB- although this has not been found to be effective for 1R -RRB- , lemma of the word -LRB- eg ` corpus '' for ` corpora '' -RRB- , phrasal information -LRB- eg identifying noun groups and phrases <OTH> -RRB- , and subject-predicate identification <CIT> '
'Coming from the other direction , such observations about phrase reordering between different languages are precisely thekindsoffactsthatparsingapproachestomachine translation are designed to handle and do successfully handle <CIT> '
'This translation model differs from the well known phrase-based translation approach <CIT> in two basic issues : rst , training data is monotonously segmented into bilingual units ; and second , the model considers n-gram probabilities instead of relative frequencies '
'A = adjoin , T = attach , C = conjoin , G = generate In this paper , we use the perceptron-like algorithm proposed in <CIT> which does not suffer from the label bias problem , and is fast in training '
'Eigenvector centrality in particular has been successfully applied to many different types of networks , including hyperlinked web pages <OTH> , lexical networks <CIT> , and semantic networks <CIT> '
'Discriminative methods such as Conditional Random Fields -LRB- CRFs -RRB- <OTH> , Semi-Markov Random Fields <OTH> , and perceptrons <CIT> have been popular approaches for sequence labeling because of their excellent performance , which is mainly due to their ability to incorporate many kinds of overlapping and non-independent features '
'First , several of the best-performing parsers on the WSJ treebank <CIT> are cases of history-based models '
'Recently , we can see an important development in natural language processing and computational linguistics towards the use of empirical learning methods -LRB- for instance , <CIT> -RRB- '
'Whereas language generation has benefited from syntax <CIT> , the performance of statistical phrase-based machine translation when relying solely on syntactic phrases has been reported to be poor <OTH> '
'This type of direct optimization is known as Minimum Error Rate Training <CIT> in the MT community , and is an essential component in building the stateof-art MT systems '
'In particular , the use of SVMs in <CIT> initially sparked interest in using machine learning methods for sentiment classi cation '
'Our training and test corpora , for instance , are lessthan-gargantuan compared to such collections as the Penn Treebank <CIT> '
'The IBM models have shown good performance in machine translation , and especially so within certain families of languages , for example in translating between French and English or between Sinhalese and Tamil <CIT> '
'Such word-based lexicalizations of probability models are used successfully in the statistical parsing models of , eg , <CIT> , Charniak -LRB- 1997 -RRB- , or Ratnaparkhi -LRB- 1997 -RRB- '
'To scale LMs to larger corpora with higher-order dependencies , researchers Work completed while this author was at Google Inc have considered alternative parameterizations such as class-based models <OTH> , model reduction techniques such as entropy-based pruning <OTH> , novel represention schemes such as suffix arrays <OTH> , Golomb Coding <OTH> and distributed language models that scale more readily <CIT> '
'Furthermore , use of the self-training techniques described in <CIT> raise this to 878 % -LRB- an error reduction of 28 % -RRB- again without any use of labeled Brown data '
'When efficient techniques have been proposed <CIT> , they have been mostly evaluated on safe pairs of languages where the notion of word is rather clear '
'Note that our result on Dataset A is as strong as that obtained by <CIT> via their subjectivity summarization algorithm , which retains only the subjective portions of a document '
'Several classification models can be adopted here , however , we choose the averaged perceptron algorithm <CIT> because of its simplicity and high accuracy '
'In a later study , <CIT> present a loglinear combination of the HMM and IBM Model 4 that produces better alignments than either of those '
'354 supervised induction techniques that have been successfully developed for English -LRB- eg , Schutze <OTH> , Clark <OTH> -RRB- , including the recentlyproposed prototype-driven approach <CIT> and Bayesian approach <OTH> '
'For example , <CIT> shows that training a learning algorithm on the weighted union of different data sets -LRB- which is basically what we did -RRB- performs almost as well as more involved domain adaptation approaches '
'Similar models have been successfully applied in the past to other tasks including parsing <CIT> , chunking <OTH> , and machine translation <OTH> '
'To perform translation , state-of-the-art MT systems use a statistical phrase-based approach <CIT> by treating phrases as the basic units of translation '
'We also note that there are a number of bootstrapping methods successfully applied to text eg , word sense disambiguation <CIT> , named entity instance classification <OTH> , and the extraction of parts word given the whole word <OTH> '
'2 Motivation and Prior Work While several authors have looked at the supervised adaptation case , there are less -LRB- and especially less successful -RRB- studies on semi-supervised domain adaptation <CIT> '
'The maximum entropy model <CIT> provides us with a well-founded framework for this purpose , which has been extensively used in natural lan guage processing tasks ranging from part-ofspeech tagging to machine translation '
'Here , we use the more established ROUGE-W measure <CIT> instead '
'<CIT> discussed efficient implementation '
'The current state-of-the-art is to optimize these parameters with respect to the final evaluation criterion ; this is the so-called minimum error rate training <CIT> '
'Compared to a basic treebank grammar <OTH> , the grammars of highaccuracy parsers weaken independence assumptions by splitting grammar symbols and rules with either lexical <OTH> or nonlexical <CIT> conditioning information '
'Weight averaging was also employed <CIT> , which helped improve performance '
'As a result of this tuning , our -LRB- fully supervised -RRB- version of the Morce tagger gives the best accuracy among all single taggers for Czech and also very good results for English , being beaten only by the tagger <CIT> -LRB- by 010 % absolute -RRB- and -LRB- not significantly -RRB- by <OTH> '
'One widely used model is the IBM model <CIT> '
'Word segmentation and POS tagging in a joint process have received much attention in recent research and have shown improvements over a pipelined fashion <CIT> '
'Our method follows and substantially extends the earlier work of <CIT> , who use syntactic features and unlabelled dependencies to evaluate MT quality , outperforming BLEU on segment-level correlation with human judgement '
'Among these advances , forest-based modeling <OTH> and tree sequence-based modeling <CIT> are two interesting modeling methods with promising results reported '
'Some recent work on incremental parsing <CIT> showed another way to handle this problem '
'Similar to WSD , <CIT> used contextual information to solve the ambiguity problem for phrases '
'1 Introduction State-of-the-art part of speech -LRB- POS -RRB- tagging accuracy is now above 97 % for newspaper text <CIT> '
'However , the only known work which automates part of a customer service center using natural language dialogue is the one by <CIT> '
'1 Introduction Robust statistical syntactic parsers , made possible by new statistical techniques <OTH> and by the availability of large , hand-annotated training corpora such as WSJ <CIT> and Switchboard <OTH> , have had a major impact on the field of natural language processing '
'For the extraction problem , there have been various methods proposed to date , which are quite adequate <CIT> '
'robust mforrmatlon extractlon, and readlly-avmlable on-hne NLP resources These techtuques and resources allow us to create a richer indexed source of Imgmstlc and domain knowledge than other frequency approaches Our approach attempts to apprommate text dlscourse structure through these multlple layers of mformatlon, ohtinned from automated methods m contrast to labor-lntenslve, discourse-based approaches Moreover, our planned training methodology will also allow us to explmt thin productlve infrastructure m ways whlch model human performance whde avoidmg hand-crafting domain-dependent rules of the knowledge-based approaches Our ultlmate goal m to make our summarlzatlon system scalable and portable by learning summarization rules from easily extractable text features 2 System Description Our summarization system DlmSum consmts of the Summarization Server and the Summarlzatzon Chent The Server extracts features (the Feature Extractor) from a document using various robust NLP techmques, described In Sectzon 2 1, and combines these features (the Feature Combiner) to basehne multiple combinations of features, as described m Section 2 2 Our work m progress to automattcally tram the Feature Combiner based upon user and apphcatlon needs m presented in Section 2 2 2 The Java-based Chent, which wdl be dmcnssed In Section 4, provides a graphical user interface (GUI) for the end user to cnstomlze the summamzatlon preferences and see multiple views of generated sumInarles 2.1 Extracting Stlmmarization Features In this section, we describe how we apply robust NLP technology to extract summarization features Our goal IS to add more mtelhgence to frequencybased approaches, to acqmre domain knowledge In a more automated fashion, and to apprommate text structure by recogmzing sources of dmcourse cohesion and coherence 2.1.1 Going Beyond a Word Frequency-based summarization systems typically use a single word stnng as a umt for counting frequencies Whde such a method IS very robust, it ignores the semantic content of words and their potential membership m multi-word phrases For example, zt does not dmtmgumh between ''bill'' m ''Bdl Table 1 Collocations with ''chlps'' {potato tortdla corn chocolate b~gle} chips {computer pentmm Intel macroprocessor memory} chips {wood oak plastlc} cchlps bsrgmmng clups blue clups mr chips Clmton'' and ''bill'' in ''reform bill'' This may introduce noise m frequency counting as the same strmgs are treated umformly no matter how the context may have dmamblguated the sense or regardless of membership in multl-word phrases For DlrnSum, we use term frequency based on tf*Idf (Salton and McGdl, 1983, Brandow, Mitze, and Rau, 1995) to derive ssgnature words as one of the summarization features If single words were the sole basra of countmg for our summarization application, nome would be introduced both m term frequency and reverse document frequency However, recent advances in statmtlcal NLP and information extraction make it possible to utilize features which go beyond the single word level Our approach is to extract multi-word phrases automatlcally with high accuracy and use them as the basic unit in the summarization process, including frequency calculation Ftrst, just as word association methods have proven effective m lemcal analysis, e g (Church and Hanks, 1990), we are exploring whether frequently occurring Collocatlonal reformation can improve on simple word-based approaches We have preprocessed about 800 MB of LA tlmes/Wastnngton Post newspaper articles nsmg a POS tagger (Bnll, 1993) and derived two-word noun collocations using mutual information The.'
'<CIT> , whose training corpus for the noun drug was 9 times bigger than that of Karov and Edelman , reports 914 % correct performance improved to impressive 939 % when using the ` one sense per discourse '' constraint '
'This increase of probabilities is defined as multiplicative change -LRB- N -RRB- as follows : -LRB- N -RRB- = P -LRB- E Tprime -RRB- \/ P -LRB- E T -RRB- -LRB- 2 -RRB- The main innovation of the model in <CIT> is the possibility of adding at each step the best relation N = -LCB- Ri , j -RCB- as well as N = I -LRB- Ri , j -RRB- that is Ri , j with all the relations by the existing taxonomy '
'However , as also pointed out by <CIT> , this observation does not hold uniformly over all possible co-occurrences of two words '
'The MERT module is a highly modular , efficient and customizable implementation of the algorithm described in <CIT> '
'53 Comparison with SS-CRF-MER When we consider semi-supervised SOL methods , SS-CRF-MER <CIT> is the most competitive with HySOL , since both methods are defined based on CRFs '
'WSD is one of the fundamental problems in natural language processing and is important for applications such as machine translation -LRB- MT -RRB- <CIT> , information retrieval -LRB- IR -RRB- , etc WSD is typically viewed as a classification problem where each ambiguous word is assigned a sense label -LRB- from a pre-defined sense inventory -RRB- during the disambiguation process '
'<CIT> has described an efficient exact onedimensional accuracy maximization technique for a similar search problem in machine translation '
'While these are based on a relatively few number of items, and while we have not performed any tests to determine whether the differences in ? are statistically significant, the results 7The Czech-English conditions were excluded since there were so few systems 146 are nevertheless interesting, since three metrics have higher correlation than Bleu: ??Semantic role overlap (Gimenez and M`arquez, 2007), which makes its debut in the proceedings of this workshop ??ParaEval measuring recall (Zhou et al. , 2006), which has a model of allowable variation in translation that uses automatically generated paraphrases (Callison-Burch, 2007) ??Meteor (Banerjee and Lavie, 2005) which also allows variation by introducing synonyms and by flexibly matches words using stemming.'
'A number of part-of-speech taggers are readily available and widely used , all trained and retrainable on text corpora <CIT> '
'To avoid this problem , we adopt cross-validation training as used in <CIT> '
'The default training set of Penn Treebank <CIT> was used for the parser because the domain and style of those texts actually matches fairly well with the domain and style of the texts on which a reading level predictor for second language learners might be used '
'This source of overcounting is considered and fixed by <CIT> and Zens and Ney -LRB- 2003 -RRB- , which we briefly review here '
'This further supports the claim by <CIT> that loglikelihood ratio is much less sensitive than pmi to low counts '
'Inversion transduction grammar <CIT> , or ITG , is a wellstudied synchronous grammar formalism '
'For example , <CIT> used cooccurrences between verbs and their subjects and objects , and proposed a similarity metric based on mutual information , but no exploration concerning the effectiveness of other kinds of word relationship is provided , although it is extendable to any kinds of contextual information '
'Studies on the supervised task have shown that straightforward baselines -LRB- eg models based on source only , target only , or the union of the data -RRB- achieve a relatively high performance level and are surprisingly difficult to beat <CIT> '
'Results from <CIT> show that under these definitions the following guarantee holds : LogLossUpda , k , BestWtk , a C20 BestLossk , a So it can be seen that the update from a to Upda , k , BestWtk , a is guaranteed to decrease LogLoss by at least W k q C0 W C0 k qC16C17 2 From these results , the algorithms in Figures 3 and 4 could be altered to take the revised definitions of W k and W C0 k into account '
'The notion of incrementally merging classes of lexical items is intuitively satisfying and is explored in detail in <CIT> '
'61 Interand Intra-annotator agreement We measured pairwise agreement among annotators usingthekappacoefficient -LRB- K -RRB- whichiswidelyused in computational linguistics for measuring agreement in category judgments <CIT> '
'In the supervised setting , a recent paper by <CIT> shows that a simple feature augmentation method for SVM is able to effectively use both labeled target and source data to provide the best domainadaptation results in a number of NLP tasks '
'To speed our computations , we use the cube pruning method of <CIT> with a fixed beam size '
'<CIT> information-theoretic similarity measure is commonly used in lexicon acquisition tasks and has demonstrated good performance in unsupervised WSD <OTH> '
'1 Introduction State-of-the-art Statistical Machine Translation -LRB- SMT -RRB- systems usually adopt a two-pass search strategy <CIT> as shown in Figure 1 '
'Following <CIT> we can avoid unnecessary false positives by not querying for the longer n-gram in such cases '
'It is explored extensively in <CIT> '
'We compare semisupervised LEAF with a previous state of the art semi-supervised system <CIT> '
'The main application of these techniques to written input has been in the robust , lexical tagging of corpora with part-of-speech labels <CIT> '
'2 Head Lexicalization As previously shown -LRB- Charniak <OTH> , <CIT> <OTH> , Carroll and Rooth -LRB- 1998 -RRB- , etc -RRB- , ContextFree Grammars -LRB- CFGs -RRB- can be transformed to lexicalized CFGs , provided that a head-marking scheme for rules is given '
'Other well-known metrics are WER <OTH> , NIST <OTH> , GTM <OTH> , ROUGE <OTH> , METEOR <CIT> , and TER <OTH> , just to name a few '
'Studies reveal that statistical alignment models outperform the simple Dice coefficient <CIT> '
'One possible approach is to employ state-of-the-art techniques for coreference and zeroanaphora resolution <CIT> in preprocessing cooccurrence samples '
'The evaluation results also confirm the argument of <CIT> , who suggested G2 as a more robust alternative to X2 '
'We decided to use the class of maximum entropy models , which are probabilistically sound , can make use of possibly many overlapping features , and can be trained efficiently <CIT> '
'Whereas dependency based semantic spaces have been shown to surpass other word space models for a number of problems <CIT> , for the task of categorisation simple pattern based spaces have been shown to perform equally good if not better <OTH> '
'In addition , parsing re-ranking <CIT> has also been shown to be another effective technique to improve parsing performance '
'However , attempts to retrofit syntactic information into the phrase-based paradigm have not met with enormous success <CIT> 1 , and purely phrase-based machine translation systems continue to outperform these syntax\/phrase-based hybrids '
'In their seminal paper on SMT , Brownand his colleagues highlighted the problems weface aswe go from IBM Models 1-2 to 3-5 <CIT> 3 : Asweprogress from Model1toModel5 , evaluating the expectations that gives us counts becomes increasingly difficult '
'1 Introduction Phrase-based modeling method <CIT> is a simple , but powerful mechanism to machine translation since it can model local reorderings and translations of multiword expressions well '
'And again , we see this insight informing statistical machine translation systems , for instance , in the phrase-based approaches of Och <OTH> and <CIT> et al '
'<CIT> reports interesting results of this kind based on literal collocations , where he parses the corpus <CIT> into predicate-argument structures and applies a mutual information measure <OTH> to weigh the association between the predicate and each of its arguments '
'31 A History-Based Model The history-based -LRB- HB -RRB- approach which incorporates more context information has worked well in parsing <CIT> '
'In previous work <OTH> , I described a Maximum Entropy\/Minimum Divergence -LRB- MEMD -RRB- model <OTH> for p -LRB- w -LRB- hi , s -RRB- which incorporates a trigram language model and a translation component which is an analog of the well-known IBM translation model 1 <CIT> '
'a list of pilot terms ranked from the most representative of the corpus to the least thanks to the Loglikelihood coefficient introduced by <CIT> '
'This paper is heavily indebted to prior work on unsupervised learning of position categories such as <CIT> et al 1992 , Schtze 1997 , Higgins 2002 , and others cited there '
'Recent work by <CIT> shows a practically ef cient approach that binarizes linguistically SCFG rules when possible '
'Among the grammar formalisms successfully put into use in syntaxbased SMT are synchronous context-free grammars -LRB- SCFG -RRB- <CIT> and synchronous treesubstitutiongrammars -LRB- STSG -RRB- <OTH> '
'Hidden Markov models are simple and effective , but unlike discriminative models , such as Maximum Entropy models <CIT> and Conditional Random Fields <OTH> , they have more difficulty utilizing a rich set of conditionally dependent features '
'8Interestingly , in work on the automated classification of nouns , <CIT> also noted problems with ''em pty '' words that depend on their complements for meaning '
'Several studies have reported alignment or translation performance for syntactically augmented translation models <CIT> and these results have been promising '
'22 Perceptron-based training To tune the parameters w of the model , we use the averaged perceptron algorithm <CIT> because of its efficiency and past success on various NLP tasks <CIT> '
'For French\/English translation we use a state of the art phrase-based MT system similar to <CIT> '
'Maximum entropy taggers have been shown to be highly competitive on a number of tagging tasks , such as partof-speech tagging <CIT> , and namedentity recognition -LRB- Borthwick et '
'Lexicalized PCFGs use the structural features on the lexical head of phrasal node in a tree , and get significant improvements for parsing <CIT> '
'The most widely used single-word-based statistical alignment models -LRB- SAMs -RRB- have been proposed in <CIT> '
'Decision lists have already been successfully applied to lexical ambiguity resolution by <CIT> where they perfromed well '
'Some of the more popular and more accurate of these approaches to data-driven parsing <CIT> have been based on generative models that are closely related to probabilistic contextfree grammars '
'Indeed , recent work has shown that benefits can be made by first separating facts from opinions in a document -LRB- eg , Yu and Hatzivassiloglou <OTH> -RRB- and classifying the polarity based solely on the subjective portions of the document -LRB- eg , <CIT> -RRB- '
'1 Introduction Chinese Word Segmentation -LRB- CWS -RRB- has been witnessed a prominent progress in the last three Bakeoffs <OTH> , <OTH> , <CIT> '
'Of particular interest are lexicalized parsing models such as the ones developed by <CIT> and Carroll and Rooth -LRB- 1998 -RRB- '
'Recently , graph-based methods have proved useful for a number of NLP and IR tasks such as document re-ranking in ad hoc IR <OTH> and analyzing sentiments in text <CIT> '
'For English , after a relatively big jump achieved by <OTH> , we have seen two significant improvements : <OTH> and <CIT> pushed the results by a significant amount each time1 1In our final comparison , we have also included the results of <OTH> , because it has surpassed <OTH> as well and we have used this tagger in the data preparation phase '
'The <CIT> algorithm was one of the first bootstrapping algorithms to become widely known in computational linguistics '
'Another widely used discriminative method is the perceptron algorithm <CIT> , which achieves comparable performance to CRFs with much faster training , so we base this work on the perceptron '
'For symmetrization , we found that Och and Neys refined technique described in <CIT> produced the best AER for this data set under all experimental conditions '
'Ramshaw and Marcus <CIT> successflflly applied Eric Brill ''s transformation-based learning method to the chunking problem '
'METEOR uses the Porter stemmer and synonymmatching via WordNet to calculate recall and precision more accurately <CIT> '
'Recent work <CIT> has demonstrated that randomized encodings can be used to represent n-gram counts for LMs with signficant space-savings , circumventing information-theoretic constraints on lossless data structures by allowing errors with some small probability '
'Also , slightly restating the advantages of phrase-pairs identified in <CIT> , these blocks are effective at capturing context including the encoding of non-compositional phrase pairs , and capturing local reordering , but they lack variables -LRB- eg embedding between ne pas in French -RRB- , have sparsity problems , and lack a strategy for global reordering '
'The efficient block alignment algorithm in Section 4 is related to the inversion transduction grammar approach to bilingual parsing described in <CIT> : in both cases the number of alignments is drastically reduced by introducing appropriate re-ordering restrictions '
'3 System Overview 31 Translation model The system developed for this years shared task is a state-of-the-art , two-pass phrase-based statistical machine translation system based on a log-linear translation model <CIT> '
'Introduction Automatic word alignment <CIT> is a vital component of all statistical machine translation -LRB- SMT -RRB- approaches '
'4 Data and Evaluation For the CoNLL shared task , we have chosen to work with the same sections of the Penn Treebank as the widely used data set for base noun phrase recognition <CIT> : WSJ sections 15-18 of the Penn Treebank as training material and section 20 as test material 3 '
'Many efficient techniques exist to extract multiword expressions , collocations , lexical units and idioms <CIT> '
'1 Introduction The maximum entropy model <CIT> has attained great popularity in the NLP field due to its power , robustness , and successful performance in various NLP tasks <OTH> '
'This was recently followed by <CIT> who introduce state-of-the-art nearly unlexicalized PCFG parsers '
'Recent comparisons of approaches that can be trained on corpora <OTH> have shown that in most cases statistical aproaches <CIT> yield better results than finite-state , rule-based , or memory-based taggers <OTH> '
'Nowadays , most state-of-the-art SMT systems are based on bilingual phrases <CIT> '
'52 Results We use a Maximum Entropy -LRB- ME -RRB- classi er <CIT> which allows an e cient combination of many overlapping features '
'1 Introduction Syntactic methods are an increasingly promising approach to statistical machine translation , being both algorithmically appealing <CIT> and empirically successful <OTH> '
'3 Perceptron Reranking As <CIT> observes , perceptron training involves a simple , on-line algorithm , with few iterations typically required to achieve good performance '
'In order to estimate the conditional distributions shown in Table 1 , we use the general technique of choosing the MaxEnt distribution that properly estimates the average of each feature over the training data <CIT> '
'41 Features We used a dependency structure as the context for words because it is the most widely used and one of the best performing contextual information in the past studies <CIT> '
'This corpus-based information typically concerns sequences of 1-3 tags or words <CIT> '
'It has been argued that METEOR correlates better with human judgment due to higher weight on recall than precision <CIT> '
'2 Treebanking The Penn Treebank <CIT> is annotated with information to make predicate-argument structure easy to decode , including function tags and markers of empty categories that represent displaced constituents '
'For example , factored translation models <CIT> retain the simplicity of phrase-based SMT while adding the ability to incorporate additional features '
'Many machine learning techniques have been successfully applied to chunking tasks , such as Regularized Winnow <OTH> , SVMs <OTH> , CRFs <OTH> , Maximum Entropy Model <CIT> , Memory Based Learning <OTH> and SNoW <OTH> '
'Nowadays , most state-of-the-art SMT systems are based on bilingual phrases <CIT> '
'3 Probabilistic Parsing Models 31 Probabilistic Context-Free Grammars Lexicalization has been shown to improve parsing performance for the Penn Treebank <CIT> '
'The Perceptron style for natural language processing problems as initially proposed by <CIT> can provide state of the art results on various domains including text chunking , syntactic parsing , etc The main drawback of the Perceptron style algorithm is that it does not have a mechanism for attaining the maximize margin of the training data '
'The current state-of-the-art is to use minimum error rate training -LRB- MERT -RRB- as described in <CIT> '
'In this work , we propose two models that can be categorized as extensions of standard word lexicons : A discriminative word lexicon that uses global , ie sentence-level source information to predict the target words using a statistical classifier and a trigger-based lexicon model that extends the well-known IBM model 1 <CIT> with a second trigger , allowing for a more finegrained lexical choice of target words '
'<CIT> describe a more efficient algorithm that can compute all edge expectations in O -LRB- n3 -RRB- time using the inverse of the Kirchoff matrix K1 '
'Both <CIT> , and Tillmann and Zhang -LRB- 2006 -RRB- report on effective machine translation -LRB- MT -RRB- models involving large numbers of features with discriminatively trained weights '
'32 Comparison between SVM , Bootstrapping and LP For WSD , SVM is one of the state of the art supervised learning algorithms <OTH> , while bootstrapping is one of the state of the art semi-supervised learning algorithms <CIT> '
'Turning off the extensions to GIZA + + and training p0 as in <CIT> produces a substantial increase in AER '
'22 Maximum Entropy Models Maximum entropy -LRB- ME -RRB- models <CIT> , also known as 928 log-linear and exponential learning models , provide a general purpose machine learning technique for classification and prediction which has been successfully applied to natural language processing including part of speech tagging , named entity recognition etc Maximum entropy models can integrate features from many heterogeneous information sources for classification '
'Several recent real-world parsers have improved state-of-the-art parsing accuracy by relying on probabilistic or weighted versions of bilexical grammars <CIT> '
'<CIT> has described an ef cient exact one-dimensional error minimization technique for a similar search problem in machine translation '
'<CIT> do not achieve higher BLEU scores , but do score better according to human grammaticality judgments for in-coverage cases '
'Although they obtained consistent and stable performance gains for MT , these were inferior to the gains yielded by Ochs procedure in <CIT> '
'2 Evaluation Metrics Currently , the most widely used automatic MT evaluation metric is the NIST BLEU-4 <CIT> '
'Hiero Search Refinements <CIT> offer several refinements to cube pruning to improve translation speed '
'After a brief period following the introduction of generally accepted and widely used metrics, BLEU (Papineni et al., 2002) and NIST (Doddington, 2002), when it seemed that this persistent problem has finally been solved, the researchers active in the field of machine translation (MT) started to express their worries that although these metrics are simple, fast and able to provide consistent results for a particular system during its development, they are not sufficiently reliable for the comparison of different systems or different language pairs.'
'However , in <CIT> , the authors investigate minimum translation units -LRB- MTU -RRB- which is a refinement over a similar approach by <OTH> to eliminate the overlap issue '
'Because of this property , vector space models have been used successfully both in computational linguistics <CIT> and in cognitive science <OTH> '
'1 Introduction The field of machine translation has seen many advances in recent years , most notably the shift from word-based <OTH> to phrasebased models which use token n-grams as translation units <CIT> '
'The remaining six entries were all fully automatic machine translation systems ; in fact , they were all phrase-based statistical machine translation system that had been trained on the same parallel corpus and most used Bleubased minimum error rate training <CIT> to optimize the weights of their log linear models feature functions <CIT> '
'<CIT> introduced the averaged perceptron , as a way of reducing overfitting , and it has been shown to perform better than the non-averaged version on a number of tasks '
'This averaging effect has been shown to reduce overfitting and produce much more stable results <CIT> '
'Probably the most widely used association weight function is -LRB- point-wise -RRB- Mutual Information -LRB- MI -RRB- <OTH> , <CIT> , <OTH> , <OTH> , defined by : -RRB- -LRB- -RRB- -LRB- -RRB- , -LRB- log -RRB- , -LRB- 2 fPwP fwPfwMI = A known weakness of MI is its tendency to assign high weights for rare features '
'<OTH> -RRB- , and Basque <OTH> , which pose quite different and in the end less severe problems , there have been attempts at solving this problem for some of the highly inflectional European languages , such as <OTH> , <OTH> -LRB- Slovenian -RRB- , <OTH> , <OTH> -LRB- Czech -RRB- and <OTH> -LRB- five Central and Eastern European languages -RRB- , but so far no system has reached in the absolute terms a performance comparable to English tagging -LRB- such as <CIT> -RRB- , which stands around or above 97 % '
'In order to overcome this problem , we look to the bootstrapping method outlined in <CIT> '
'Much later work <CIT> relies on the use of extremely large corpora which allow very precise , but sparse features '
'The simplest one is the BIO representation scheme <CIT> , where a B denotes the first item of an element and an I any non-initial item , and a syllable with tag O is not a part of any element '
'Conditional Markov models -LRB- CMM -RRB- <CIT> have been successfully used in sequence labeling tasks incorporating rich feature sets '
'To compare the output of their shallow parser with the output of the well-known <CIT> parser , Li and Roth applied the chunklink conversion script to extract the shallow constituents from the output of the Collins parser on WSJ section 00 '
'While the former is piecewise constant and thus can not be optimized using gradient techniques , <CIT> provides an approach that performs such training efficiently '
'Successful discriminative parsers have relied on generative models to reduce training time and raise accuracy above generative baselines <CIT> '
'Ochs procedure is the most widely-used version of MERT for SMT <CIT> '
'Some tasks can thrive on a nearly pure diet of unlabeled data <CIT> '
'4 Machine Translation Experiments 41 Experimental Setting For our MT experiments , we used a reimplementation of Moses <CIT> , a state-of-the-art phrase-based system '
'g2 2 Motivation The success of Statistical Machine Translation -LRB- SMT -RRB- has sparked a successful line of investigation that treats paraphrase acquisition and generation essentially as a monolingual machine translation problem <CIT> '
'22 Phrase-based Chinese-to-English MT The MT system used in this paper is Moses , a stateof-the-art phrase-based system <CIT> '
'<CIT> reports a success rate of 96 % disambiguating twelve words with two clear sense distinctions each one -RRB- '
'It is an online training algorithm and has been successfully used in many NLP tasks , such as POS tagging <OTH> , parsing <OTH> , Chinese word segmentation <CIT> , and so on '
'One major resource for corpus-based research is the treebanks available in many research organizations <CIT> , which carry skeletal syntactic structures or ` brackets '' that have been manually verified '
'Successful approaches aimed at trying to overcome the sparse data limitation include backoff <OTH> , Turing-Good variants <OTH> , interpolation <OTH> , deleted estimation <OTH> , similarity-based models <CIT> , Pos-language models <OTH> and decision tree models <OTH> '
'As aptly pointed out in Jean <CIT> , agreement measures proposed so far in the computational linguistics literature has failed to ask an important question of whether results obtained using agreement data are in any way different from random data '
'Many previous studies have shown that the log-likelihood ratio is well suited for this purpose <CIT> '
'Recent work , <CIT> , has shown that adding many millions of words of machine parsed and reranked LA Times articles does , in fact , improve performance of the parser on the closely related WSJ data '
'In addition to the widely used BLEU <CIT> and NIST <OTH> scores , we also evaluate translation quality with the recently proposed Meteor <OTH> and four edit-distance style metrics , Word Error Rate -LRB- WER -RRB- , Positionindependent word Error Rate -LRB- PER -RRB- <OTH> , CDER , which allows block reordering <OTH> , and Translation Edit Rate -LRB- TER -RRB- <OTH> '
'Most stateof-the-art SMT systems treat grammatical elements in exactly the same way as content words , and rely on general-purpose phrasal translations and target language models to generate these elements <CIT> '
'For instance , <CIT> shows that a simple feature augmentation method for SVM is able to effectively use both labeled target and source data to provide the best domainadaptation results in a number of NLP tasks '
'For English , we use three state-of-the-art taggers : the taggers of <OTH> and <CIT> in Step 1 , and the SVM tagger <OTH> in Step 3 '
'We used the average perceptron algorithm of <CIT> in our experiments , a variation that has been proven to be more effective than the standard algorithm shown in Figure 2 '
'More recently , phrase-based models <CIT> have been proposed as a highly successful alternative to the IBM models '
'Models that can handle non-independent lexical features have given very good results both for part-of-speech and structural disambiguation <CIT> '
'It is often straightforward to obtain large amounts of unlabeled data , making semi-supervised approaches appealing ; previous work on semisupervised methods for dependency parsing includes <CIT> '
'Recently , an elegant approach to inference in discourse interpretation has been developed at a number of sites <OTH> , all based on tim notion of abduction , and we have begun to explore its potential application to machine translation '
'The state of the art technology for relation extraction primarily relies on pattern-based approaches <CIT> '
'Many mainstream systems and formalisms would satisfy these criteria , including ones such as the University of Pennsylvania Treebank <CIT> which are purely syntactic -LRB- though of course , only syntactic properties could then be extracted -RRB- '
'The Penn Treebank <CIT> has until recently been the only such corpus , covering 45M words in a single genre of financial reporting '
'Some methods which can offer powerful reordering policies have been proposed like syntax based machine translation <OTH> and Inversion Transduction Grammar <CIT> '
'Recent work emphasizes corpus-based unsupervised approach <CIT> that avoids the need for costly truthed training data '
'We examine the effectiveness of Structural Correspondence Learning -LRB- SCL -RRB- <CIT> for this task , a recently proposed adaptation technique shown to be effective for PoS tagging and Sentiment Analysis '
'Online votedperceptrons have been reported to work well in a number of NLP tasks <CIT> '
'1 Introduction Large scale annotated corpora , eg , the Penn TreeBank -LRB- PTB -RRB- project <CIT> , have played an important role in text-mining '
'The fluency models hold promise for actual improvements in machine translation output quality <CIT> '
'The notion that nouns have only one sense per discourse\/collocation was also exploited by <CIT> in his seminal work on bootstrapping for word sense disambiguation '
'Using the components of the row-vector bm as feature function values for the candidate translation em -LRB- m a16 1 , , M -RRB- , the system prior weights can easily be trained using the Minimum Error Rate Training described in <CIT> '
'Bootstrapping a PMTG from a lower-dimensional PMTG and a word-to-word translation model is similar in spirit to the way that regular grammars can help to estimate CFGs <OTH> , and the way that simple translation models can help to bootstrap more sophisticated ones <CIT> '
'Semantic collocations are harder to extract than cooccurrence patterns--the state of the art does not enable us to find semantic collocations automatically t. This paper however argues that if we take advantage of lexicai paradigmatic behavior underlying the lexicon, we can at least achieve semi-automatic extraction of semantic collocations (see also Calzolari and Bindi (1990) I But note the important work by Hindle [HindlegO] on extracting semantically similar nouns based on their substitutability in certain verb contexts.'
'Extracting semantic information from word co-occurrence statistics has been effective , particularly for sense disambiguation <CIT> '
'High correlation is reported between the BLEU score and human evaluations for translations from Arabic , Chinese , French , and Spanish to English <CIT> '
'The maximum entropy approach <CIT> is known to be well suited to solve the classification problem '
'We compared a baseline system , the state-of-the-art phrase-based system Pharaoh <CIT> , against our system '
'The BLEU metric <CIT> in MT has been particularly successful ; for example MT05 , the 2005 NIST MT evaluation exercise , used BLEU-4 as the only method of evaluation '
'On the other hand , integrating an additional component into a baseline SMT system is notoriously tricky as evident in the research on integrating word sense disambiguation -LRB- WSD -RRB- into SMT systems : different ways of integration lead to conflicting conclusions on whether WSD helps MT performance <CIT> '
'This is analogous , and in a certain sense equivalent , to empirical risk minimization , which has been used successfully in related areas , such as speech recognition <OTH> , language modeling <OTH> , and machine translation <CIT> '
'<CIT> investigated the use of concurrent parsing of parallel corpora in a transduction inversion framework , helping to resolve attachment ambiguities in one language by the coupled parsing state in the second language '
'Sentence-level subjectivity detection , where training data is easier to obtain than for positive vs negative classification , has been successfully performed using supervised statistical methods alone <CIT> or in combination with a knowledgebased approach <OTH> '
'Properly calculated BLEU scores have been shown to correlate reliably with human judgments <CIT> '
'The technique is based on word class models , pioneered by <CIT> , which hierarchically 151 CoNLL03 CoNLL03 MUC7 MUC7 Web Component Test data Dev data Dev Test pages 1 -RRB- Baseline 8365 8925 7472 7128 7141 2 -RRB- -LRB- 1 -RRB- + Gazetteer Match 8722 9161 8583 8043 7446 3 -RRB- -LRB- 1 -RRB- + Word Class Model 8682 9085 8025 7988 7226 4 -RRB- All External Knowledge 8855 9249 8450 8323 7444 Table 4 : Utility of external knowledge '
'1 Introduction During the last few years , SMT systems have evolved from the original word-based approach <OTH> to phrase-based translation systems <CIT> '
'Recently , <CIT> have successfully constructed high quality and high coverage gazetteers from Wikipedia '
'1 Introduction The maximum entropy model <OTH> has attained great popularity in the NLP field due to its power , robustness , and successful performance in various NLP tasks <CIT> '
'The implementation of MEBA was strongly influenced by the notorious five IBM models described in <CIT> '
'Annotated reference corpora , such as the Brown Corpus <OTH> , the Penn Treebank <CIT> , and the BNC <OTH> , have helped both the development of English computational linguistics tools and English corpus linguistics '
'1 Introduction Syntactically annotated corpora like the Penn Treebank <CIT> , the NeGra corpus <OTH> or the statistically dismnbiguated parses in <OTH> provide a wealth of intbrmation , which can only be exploited with an adequate query language '
'Similarly , Structural Correspondence Learning <CIT> has proven to be successful for the two tasks examined , PoS tagging and Sentiment Classification '
'A notable exception is the work of <CIT> '
'Inversion Transduction Grammar -LRB- ITG -RRB- <CIT> and Syntax-Directed Translation Schema -LRB- SDTS -RRB- <OTH> lack both of these properties '
'Introduction Michael <CIT> parsing models have been quite influential in the field of natural language processing '
'We do not completely rule out the possibility that some more sophisticated , ontologically promiscuous , first-order analysis -LRB- perhaps along the lines of <CIT> -RRB- might account for these kinds of monotonicity inferences '
'It is promising to optimize the model parameters directly with respect to AER as suggested in statistical machine translation <CIT> '
'Recently , <CIT> suggest to mine semantic relatedness from Wikipedia , which can deal with the data sparseness problem suffered by using WordNet '
'2 Previous Work So far , Structural Correspondence Learning has been applied successfully to PoS tagging and Sentiment Analysis <CIT> '
'The well-known BLEU <CIT> is based on the number of common n-grams between the translation hypothesis and human reference translations of the same sentence '
'They are latent variable models which are not tractable to compute exactly , but two approximations exist which have been shown to be effective for constituent parsing <CIT> '
'Here we choose to work with stupid backoff smoothing <CIT> since this is significantly more efficient to train and deploy in a distributed framework than a contextdependent smoothing scheme such as Kneser-Ney '
'We can credit DUC with the emergence of automatic methods for evaluation such as ROUGE <CIT> which allow quick measurement of systems during development and enable evaluation of larger amounts of data '
'Preparing tagged corpora either by hand is labour-intensive and potentially error-prone , and although a semi-automatic approach can be used <CIT> , it is a good thing to reduce the human involvement as much as possible '
'Research in this direction was pioneered by <CIT> , who developed Inversion Transduction Grammars to capture crosslingual grammar variations such as phrase reorderings '
'Since its introduction to the Natural Language Processing -LRB- NLP -RRB- community <CIT> , ME-based classifiers have been shown to be effective in various NLP tasks '
'We employ a robust statistical parser <CIT> to determine the constituent structure for each sentence , from which subjects -LRB- s -RRB- , objects -LRB- o -RRB- , and relations other than subject or object -LRB- x -RRB- are identified '
'Unlike Choueka <OTH> , <CIT> identify as collocations both interrupted and uninterrupted sequences of words '
'The state-of-the-art SMT system Moses implements a distance-based reordering model <CIT> and a distortion model , operating with rewrite patterns extracted from a phrase alignment table <OTH> '
'Maximum Entropy -LRB- MaxEnt -RRB- principle has been successfully applied in many classification and tagging tasks <CIT> '
'In the well-known so-called IBM word alignment models <CIT> , re-estimating the model parameters depends on the empirical probability P -LRB- ek , fk -RRB- for each sentence pair -LRB- ek , fk -RRB- '
'The classification is performed with a statistical approach , built around the maximum entropy -LRB- MaxEnt -RRB- principle <CIT> , that has the advantage of combining arbitrary types of information in making a classification decision '
'Furthermore , the BLEU score performance suggests that our model is not very powerful , but some interesting hints can be found in Table 3 when we compare our method with a 5-gram language model to a state-of-the-art system Moses <CIT> based on various evaluation metrics , including BLEU score , NIST score <OTH> , METEOR <OTH> , TER <OTH> , WER and PER '
'Disambiguation of a limited number of words is not hard , and necessary context information can be carefully collected and hand-crafted to achieve high disambiguation accuracy as shown in <CIT> '
'<OTH> and Bikel and Chiang <OTH> has demonstrated the applicability of the <CIT> model for Czech and Chinese '
'One of the most successful metrics for judging machine-generated text is BLEU <CIT> '
'Among them , the unsupervised algorithm using decisiontrees <CIT> has achieved promising performance '
'Recent projects in semisupervised <CIT> and unsupervised <OTH> tagging also show significant progress '
'1 Introduction The emergence of phrase-based statistical machine translation -LRB- PSMT -RRB- <CIT> has been one of the major developments in statistical approaches to translation '
'Their idea has proven effective for estimating the statistics of unknown words in previous studies <CIT> '
'We use the popular online learning algorithm of structured perceptron with parameter averaging <CIT> '
'This algorithm is referred to as GHKM <CIT> and is widely used in SSMT systems <CIT> '
'Stochastic models <CIT> have been widely used in POS tagging for simplicity and language independence of the models '
'<CIT> solved relational similarity problems using the Web as a corpus '
'Albeit simple , the algorithm has proven to be very efficient and accurate for the task of parse selection <CIT> '
'Two popular techniques that incorporate the error criterion are Minimum Error Rate Training -LRB- MERT -RRB- <CIT> and Minimum BayesRisk -LRB- MBR -RRB- decoding <OTH> '
'Besides relative frequencies , lexical weights <CIT> are widely used to estimate how well the words in f translate the words in e To do this , one needs first to estimate a lexical translation probability distribution w -LRB- e f -RRB- by relative frequency from the same word alignments in the training corpus : w -LRB- e f -RRB- = count -LRB- f , e -RRB- summationtext e count -LRB- f , e -RRB- -LRB- 3 -RRB- Note that a special source NULL token is added to each source sentence and aligned to each unaligned target word '
'Perhaps more importantly , discriminative models have been shown to offer competitive performance on a variety of sequential and structured learning tasks in NLP that are traditionally tackled via generative models , such as letter-to-phoneme conversion <OTH> , semantic role labeling <OTH> , syntactic parsing <OTH> , language modeling <OTH> , and machine translation <CIT> '
'While error-driven training techniques are commonly used to improve the performance of phrasebased translation systems <CIT> , this paper presents a novel block sequence translation approach to SMT that is similar to sequential natural language annotation problems 727 such as part-of-speech tagging or shallow parsing , both in modeling and parameter training '
'5 Bidirectional Sequence Classification Bidirectional POS tagging <CIT> , the current state of the art for English , has some properties that make it appropriate for Icelandic '
'We chose the perceptron for the training algorithm because it has shown good performance on other NLP tasks ; in particular , <CIT> reported good performance for a perceptron tagger compared to a Maximum Entropy tagger '
'The state-of-the art taggers are using feature sets discribed in the corresponding articles -LRB- <CIT> , <OTH> , <OTH> and <OTH> -RRB- , Morce supervised and Morce semi-supervised are using feature set desribed in section 4 '
'<CIT> saw a LFMS improvement of 08 % over their baseline discriminative parser after adding punctuation features , one of which encoded the sentence-final punctuation '
'The current state of the art is represented by the so-called phrase-based translation approach <CIT> '
'In addition to the classical window-based technique , some studies investigated the use of lexico-syntactic patterns -LRB- eg , X or Y -RRB- to get more accurate co-occurrence statistics <CIT> '
'Among the various knowledge-based <OTH> and data-driven <CIT> word sense disambiguation methods that have been proposed to date , supervised systems have been constantly observed as leading to the highest performance '
'Discriminative taggers and chunkers have been the state-of-the-art for more than a decade <CIT> '
'To alleviate this effort , various semi-supervised learning algorithms such as self-training <CIT> , cotraining <OTH> , transductive SVM <OTH> and many others have been proposed and successfully applied under different assumptions and settings '
'Then the same system weights are applied to both IncHMM and Joint Decoding - based approaches , and the feature weights of them are trained using the max-BLEU training method proposed by Och <OTH> and refined by <CIT> '
'Aside from purely linguistic interest , bracket structure has been empirically shown to be highly effective at constraining subsequent training of , for example , stochastic context-free grammars <OTH> '
'Word segmentation and POS tagging in a joint process have received much attention in recent research and have shown improvements over a pipelined fashion <CIT> '
'35 Anaphoricity Determination Finally , several coreference systems have successfully incorporated anaphoricity determination 660 modules -LRB- eg Ng and Cardie <OTH> and <CIT> -RRB- '
'1 Introduction Phrase-based statistical machine translation models <CIT> have achieved significant improvements in translation accuracy over the original IBM word-based model '
'1 Introduction Todays statistical machine translation systems rely on high quality phrase translation pairs to acquire state-of-the-art performance , see <CIT> '
'There is usually not a considerable difference between the two methods in terms of the accuracy of the resulting model <CIT> , but L1 regularization has a significant advantage in practice '
'We have chosen the Maximum Entropy tagger <CIT> for a comparison with our universal tagger , since it achieved -LRB- by a small margin -RRB- the best overall result on Slovene as reported there <OTH> of taggers available to us -LRB- MBT , the best overall , was not freely available to us at the time of writing -RRB- '
'The pioneering work of <CIT> introduced NP chunking as a machine-learning problem , with standard datasets and evaluation metrics '
'Furthermore , good results have been produced in other areas of NLP research using maximum entropy techniques <CIT> '
'3 Experiments We evaluated the effect of random feature mixing on four popular learning methods : Perceptron , MIRA <OTH> , SVM and Maximum entropy ; with 4 NLP datasets : 20 Newsgroups1 , Reuters <OTH> , Sentiment <CIT> and Spam <OTH> '
'Currently , the best-performing English NP interpretation methods in computational linguistics focus mostly on two consecutive noun instances -LRB- noun compounds -RRB- and are either -LRB- weakly -RRB- supervised , knowledge-intensive <OTH> , <OTH> , <OTH> , <OTH> , <OTH> , <OTH> , <CIT> , <OTH> , or use statistical models on large collections of unlabeled data <OTH> , <OTH> , <OTH> , <OTH> '
'Empirically the BLEU score has a high correlation with human evaluation when N = 4 for English translation evaluations <CIT> '
'Brute-force methods -LRB- ie those that exploit the massive raw computing power currently available cheaply -RRB- may well produce some useful results <CIT> '
'Recent work has applied Bayesian non-parametric models to anaphora resolution <CIT> , lexical acquisition <OTH> and language modeling <OTH> with good results '
'However , the learning curve for Negra -LRB- see Figure 1 -RRB- indicates that the performance of the <CIT> model is stable , even for small training sets '
'This approach took inspiration from the pioneering work by <CIT> , but it is also fundamentally different , because instead of grouping similar senses together , the CoreLex approach groups together words according to all of their senses '
'They have been successfully applied to accent restoration , word '' sense disambiguation 209 and homograph disambiguation <CIT> '
'Generative word alignment models , initially developed at IBM <CIT> , and then augmented by an HMM-based model <OTH> , have provided powerful modeling capability for word alignment '
'The limitations of the generative approach to sequence tagging , i e Hidden Markov Models , have been overcome by discriminative approaches proposed in recent years <CIT> '
'GIZA + + <CIT> is a very popular system within SMT for creating word alignment from parallel corpus , in fact , the Moses training scripts uses it '
'For English , self-training contributes 083 % absolute improvement to the PCFG-LA parser , which is comparable to the improvement obtained from using semi-supervised training with the twostage parser in <CIT> '
'They are central to many parsing models <CIT> , and despite their simplicity n-gram models have been very successful '
'1 Introduction In recent years , statistical machine translation have experienced a quantum leap in quality thanks to automatic evaluation <CIT> and errorbased optimization <OTH> '
'First , such a system makes use of lexical information when modeling reordering <CIT> , which has previously been shown to be useful in German-to-English translation <OTH> '
'This representation , being contiguous on both sides , successfully reduces the decoding complexity to a low polynomial and significantly improved the search quality <CIT> '
'We report on ROUGE-1 -LRB- unigrams -RRB- , ROUGE-2 -LRB- bigrams -RRB- , ROUGE W-12 -LRB- weighted LCS -RRB- , and ROUGE-S \* -LRB- skip bigrams -RRB- as they have been shown to correlate well with human judgments for longer multidocument summaries <CIT> '
'<OTH> , but we use a maximum entropy classifier <CIT> to determine parser actions , which makes parsing considerably faster '
'Recently , <CIT> have successfully applied self-training to various parser adaptation scenarios using the reranking parser of <OTH> '
'An efficient algorithm for performing this tuning for a larger number of model parameters can be found in <CIT> '
'Beam-search has been successful in many NLP tasks -LRB- Koehn et al , 2003 ; 562 Inputs : training examples -LRB- xi , yi -RRB- Initialization : set vectorw = 0 Algorithm : \/ \/ R training iterations ; N examples for t = 1R , i = 1N : zi = argmaxyGEN -LRB- xi -RRB- -LRB- y -RRB- vectorw if zi negationslash = yi : vectorw = vectorw + -LRB- yi -RRB- -LRB- zi -RRB- Outputs : vectorw Figure 1 : The perceptron learning algorithm <CIT> and Roark , 2004 -RRB- , and can achieve accuracy that is close to exact inference '
'By having the advantage of leveraging large parallel corpora , the statistical MT approach outperforms the traditional transfer based approaches in tasks for which adequate parallel corpora is available <CIT> '
'More sophisticated first-order accounts <CIT> may be extendable to bear this load '
'It us widely acknowledged that word sense d~samblguatmn (WSD) us a central problem m natural language processing In order for computers to be able to understand and process natural language beyond simple keyword matching, the problem of d~samblguatmg word sense, or dlscermng the meamng of a word m context, must be effectively dealt with Advances in WSD v, ill have slgmficant Impact on apphcatlons hke information retrieval and machine translation For natural language subtasks hke part-of-speech tagging or s)ntactm parsing, there are relatlvely well defined and agreed-upon cnterm of what it means to have the ''correct'' part of speech or syntactic structure assigned to a word or sentence For instance, the Penn Treebank corpus (Marcus et al, 1993) pro~ide~,t large repo.~tory of texts annotated w~th partof-speech and s}ntactm structure mformatlon Tv.o independent human annotators can achieve a high rate of agreement on assigning part-of-speech tags to words m a g~ven sentence Unfortunately, th~s us not the case for word sense assignment F~rstly, it is rarely the case that any two dictionaries will have the same set of sense defimtmns for a g~ven word Different d~ctlonanes tend to carve up the ''semantic space'' m a different way, so to speak Secondly, the hst of senses for a word m a typical dmtmnar~ tend to be rather refined and comprehensive This is especmlly so for the commonly used words which have a large number of senses The sense dustmctmn between the different senses for a commonly used word m a d~ctmnary hke WoRDNET (Miller, 1990) tend to be rather fine Hence, two human annotators may genuinely dusagree m their sense assignment to a word m context The agreement rate between human annotators on word sense assignment us an Important concern for the evaluatmn of WSD algorithms One would prefer to define a dusamblguatlon task for which there us reasonably hlgh agreement between human annotators The agreement rate between human annotators will then form the upper ceiling against whmh to compare the performance of WSD algorithms For instance, the SENSEVAL exerclse has performed a detaded study to find out the raterannotator agreement among ~ts lexicographers taggrog the word senses (Kllgamff, 1998c, Kllgarnff, 1998a, Kflgarrlff, 1998b) 2 A Case Study In this-paper, we examine the ~ssue of raterannotator agreement by comparing the agreement rate of human annotators on a large sense-tagged corpus of more than 30,000 instances of the most frequently occurring nouns and verbs of Enghsh This corpus is the intersection of the WORDNET Semcor corpus (Miller et al, 1993) and the DSO corpus (Ng and Lee, 1996, Ng, 1997), which has been independently tagged wlth the refined senses of WORDNET by two separate groups of human annotators The Semcor corpus us a subset of the Brown corpus tagged with ~VoRDNET senses, and consists of more than 670,000 words from 352 text files Sense taggmg was done on the content words (nouns, ~erbs, adjectives and adverbs) m this subset The DSO corpus consists of sentences drawn from the Brown corpus and the Wall Street Journal For each word w from a hst of 191 frequently occurring words of Enghsh (121 nouns and 70 verbs), sentences containing w (m singular or plural form, and m its various reflectional verb form) are selected and each word occurrence w ~s tagged w~th a sense from WoRDNET There ~s a total of about 192,800 sentences in the DSO corpus m which one word occurrence has been sense-tagged m each sentence The intersection of the Semcor corpus and the DSO corpus thus consists of Brown corpus sentences m which a word occurrence w is sense-tagged m each sentence, where w Is one of.the 191 frequently oc-,currmg English nouns or verbs Since this common pomon has been sense-tagged by two independent groups of human annotators, ~t serves as our data set for investigating inter-annotator agreement in this paper 3 Sentence Matching To determine the extent of inter-annotator agreement, the first step ~s to match each sentence m Semcor to its corresponding counterpart In the DSO corpus This step ~s comphcated by the following factors 1 Although the intersected portion of both corpora came from Brown corpus, they adopted different tokemzatmn convention, and segmentartan into sentences differed sometimes 2 The latest versmn of Semcor makes use of the senses from WORDNET 1 6, whereas the senses used m the DSO corpus were from WoRDNET 15 1 To match the sentences, we first converted the senses m the DSO corpus to those of WORDNET 1 6 We ignored all sentences m the DSO corpus m which a word is tagged with sense 0 or -1 (A word is tagged with sense 0 or -1 ff none of the given senses m WoRDNFT applies ) 4, sentence from Semcor is considered to match one from the DSO corpus ff both sentences are exactl) ldent~cal or ff the~ differ only m the pre~ence or absence of the characters '' (permd) or -'' (hyphen) For each remaining Semcor sentence, taking into account word ordering, ff 75% or more of the words m the sentence match those in a DSO corpus sentence, then a potential match ~s recorded These i -kctua[ly, the WORD~q''ET senses used m the DSO corpus were from a shght variant of the official WORDNE''I 1 5 release Th~s ssas brought to our attention after the pubhc release of the DSO corpus potential matches are then manually verffied to ensure that they are true matches and to ~eed out any false matches Using this method of matching, a total of 13,188 sentence-palrs contasnmg nouns and 17,127 sentence-pa~rs containing verbs are found to match from both corpora, ymldmg 30,315 sentences which form the intersected corpus used m our present study 4 The Kappa Statistic Suppose there are N sentences m our corpus where each sentence contains the word w Assume that w has M senses Let 4 be the number of sentences which are assigned identical sense b~ two human annotators Then a simple measure to quantify the agreement rate between two human annotators Is Pc, where Pc, = A/N The drawback of this simple measure is that it does not take into account chance agreement between two annotators The Kappa statistic a (Cohen, 1960) is a better measure of rater-annotator agreement which takes into account the effect of chance agreement It has been used recently w~thm computatmnal hngu~stlcs to measure raterannotator agreement (Bruce and Wmbe, 1998, Carletta, 1996, Veroms, 1998) Let Cj be the sum of the number of sentences which have been assigned sense 3 by annotator 1 and the number of sentences whmh have been assigned sense 3 by annotator 2 Then P~-P~ 1-P~ where M j=l and Pe measures the chance agreement between two annotators A Kappa ~alue of 0 indicates that the agreement is purely due to chance agreement, whereas a Kappa ~alue of 1 indicates perfect agreement A Kappa ~alue of 0 8 and above is considered as mdmatmg good agreement (Carletta, 1996) Table 1 summarizes the inter-annotator agreement on the mtersected corpus The first (becond) row denotes agreement on the nouns (xerbs), wh~le the lass row denotes agreement on all words combined The a~erage ~ reported m the table is a s~mpie average of the individual ~ value of each word The agreement rate on the 30,315 sentences as measured by P= is 57% This tallies with the figure reported ~n our earlier paper (Ng and Lee, 1996) where we performed a quick test on a subset of 5,317 sentences,n the intersection of both the Semcor corpus and the DSO corpus 10 [] mm m m m m m mm m m m m mm m m m Type Num of v, ords A N [ P~ Avg Nouns 121 7,676 13,188 I 0 582 0 300 Verbs 70 9,520 17,127 I 0 555 0 347 All I 191 I 17,196 30,315 I 056T 0317 Table 1 Raw inter-annotator agreement 5 Algorithm Since the rater-annotator agreement on the intersected corpus is not high, we would like to find out how the agreement rate would be affected if different sense classes were in use In this section, we present a greedy search algorithm that can automatmalb derive coarser sense classes based on the sense tags assigned by two human annotators The resulting derived coarse sense classes achmve a higher agreement rate but we still maintain as many of the original sense classes as possible The algorithm is given m Figure 1 The algorithm operates on a set of sentences where each sentence contains an occurrence of the word w whmh has been sense-tagged by two human annotators At each Iteration of the algorithm, tt finds the pair of sense classes Ct and Cj such that merging these two sense classes results in the highest t~ value for the resulting merged group of sense classes It then proceeds to merge Cz and C~ Thin process Is repeated until the ~ value reaches a satisfactory value ~,~t,~, which we set as 0 8 Note that this algorithm is also applicable to deriving any coarser set of classes from a refined set for any NLP tasks in which prior human agreement rate may not be high enough Such NLP tasks could be discourse tagging, speech-act categorization, etc 6 Results For each word w from the list of 121 nouns and 70 verbs, ~e applied the greedy search algorithm to each set of sentences in the intersected corpus contaming w For a subset of 95 words (53 nouns and 42 verbs), the algorithm was able to derive a coarser set of 2 or more senses for each of these 95 words such that the resulting Kappa ~alue reaches 0 8 or higher For the other 96 words, m order for the Kappa value to reach 0 8 or higher, the algorithm collapses all senses of the ~ord to a single (trivial) class Table 2 and 3 summarizes the results for the set of 53 nouns and 42 ~erbs, respectively Table 2 md~cates that before the collapse of sense classes, these 53 nouns have an average of 7 6 senses per noun There is a total of 5,339 sentences in the intersected corpus containing these nouns, of which 3,387 sentences were assigned the same sense by the two groups of human annotators The average Kappa statistic (computed as a simple average of the Kappa statistic of ~he mdlwdual nouns) is 0 463 After the collapse of sense classes by the greedy search algorithm, the average number of senses per noun for these 53 nouns drops to 40 Howe~er, the number of sentences which have been asmgned the same coarse sense by the annotators increases to 5,033 That is, about 94 3% of the sentences have been assigned the same coarse sense, and that the average Kappa statistic has improved to 0 862, mgmfymg high rater-annotator agreement on the derived coarse senses Table3 gl~es the analogous figures for the 42 verbs, agmn mdmatmg that high agreement is achieved on the coarse sense classes den~ed for verbs 7 Discussion Our findings on rater-annotator agreement for word sense tagging indicate that for average language users, it is quite dl~cult to achieve high agreement when they are asked to assign refned sense tags (such as those found in WORDNET) given only the scanty definition entries m the WORDNET dlctionary and a few or no example sentences for the usage of each word sense Thin observation agrees wlth that obtmned m a recent study done by (Veroms, 1998), where the agreement on sense-tagging by naive users was also not hlgh Thus It appears that an average language user is able to process language wlthout needing to perform the task of dlsamblguatmg word sense to a very fine-grained resolutmn as formulated m a tradltlonal dmtlonary In contrast, expert lexicographers tagged the ~ ord sense in the sentences used m the SENSEVAL exerclse, where high rater-annotator agreement was reported There are also fuller dlctlonary entries m the HECTOR dlctlonary used and more e<amples showing the usage of each word sense m HECTOR These factors are likely to have contributed to the difference in rater-annotator agreement observed m the three studies conducted We also examined the coarse sense classes derived by the greedy search algorithm Vv''e found some interesting groupings of coarse senses for nouns which ~e hst in Table 4 From Table 4, it is apparent that the greedy search algorithm can derive interesting groupings of word senses that correspond to human mtmtwe judgment of sense graz}.ulanty It Is clear that some of the disagreement between the two groups of human annotators can be attributed solely to the overly refined senses of WoRDNET As an example, there is a total Ii loop: let Ct,, C M denote the current M sense classes ~* +--oo for all z,3 such that 1 <, < 3 < M let C[,,C~w_ 1 denote the resulting M 1 sense classes by mergmg C, and C 3 compute ~(C[,, C~/_t) ff ~(C,, C~4_x) > ~* then ~'' +~(C~,,C~_t), z* +~, ~* +end for merge the sense class C,.'
'There are basically two kinds of systems working at these segmentation levels : the most widespread rely on statistical models , in particular the IBM ones <CIT> ; others combine simpler association measures with different kinds of linguistic information <OTH> '
'1 Introduction For statistical machine translation -LRB- SMT -RRB- , phrasebased methods <OTH> and syntax-based methods <CIT> outperform word-based methods <OTH> '
'Using the IBM translation models IBM-1 to IBM-5 <CIT> , as well as the Hidden-Markov alignment model <OTH> , we can produce alignments of good quality '
'A popular metric for evaluating machine translation quality is the Bleu score <CIT> '
'2 Method Maximum Entropy Markov Models -LRB- MEMMs -RRB- <CIT> and their extensions <OTH> have been successfully applied to English POS tagging '
'2 The Problem of Coverage in SMT Statistical machine translation made considerable advances in translation quality with the introduction of phrase-based translation <CIT> '
'It has been used in a variety of difficult classification tasks such as part-of-speech tagging <CIT> , prepositional phrase attachment <CIT> and named entity tagging <OTH> , and achieves state of the art performance '
'Recently , Cabezas and Resnik <OTH> experimented with incorporating WSD translations into Pharaoh , a state-of-the-art phrase-based MT system <CIT> '
'In particular , <CIT> presents very strong results using a distributional-similarity module and achieve impressive tagging accuracy while starting with a mere 116 prototypical words '
'A pioneer work in online training is the perceptron-like algorithm used in training a hidden Markov model -LRB- HMM -RRB- <CIT> '
'Aligning tokens in parallel sentences using the IBM Models <CIT> , <OTH> may require less information than full-blown translation since the task is constrained by the source and target tokens present in each sentence pair '
'To improve the unknown word model , featurebased approach such as the maximum entropy method <CIT> might be useful , because we don '' t have to divide the training data into several disjoint sets -LRB- like we did by part of speech and word type -RRB- and we can incorporate more linguistic and morphological knowledge into the same probabilistic framework '
'Recently , many syntax-based models have been proposed to address the above deficiencies <CIT> '
'The field of statistical machine translation has been blessed with a long tradition of freely available software tools such as GIZA + + <CIT> and parallel corpora such as the Canadian Hansards2 '
'3 Space-Efficient Approximate Frequency Estimation Prior work on approximate frequency estimation for language models provide a no-false-negative guarantee , ensuring that counts for n-grams in the model are returned exactly , while working to make sure the false-positive rate remains small <CIT> '
'<CIT> gave a good description of ME model '
'One of the largest and earliest such efforts is the Penn Treebank <CIT> , which contains a one-million word Institute for Research in Cognitive Science , University of Pennsylvania , 3401 Walnut Street , Suite 400A , Philadelphia , PA 19104-6228 , USA '
'<CIT> has recently proposed a simpler SVM-based algorithm for analogical classification called PairClass '
'We chose a dataset that would be enjoyable to reannotate : the movie review dataset of <CIT> 3 The dataset consists of 1000 positive and 1000 negative movie reviews obtained from the Internet Movie Database -LRB- IMDb -RRB- review archive , all written before 2002 by a total of 312 authors , with a cap of 20 reviews per author per 2Taking Ccontrast to be constant means that all rationales are equally valuable '
'To reduce the time complexity , we adapted the lazy update proposed in <CIT> , which was also used in <OTH> '
'<CIT> showed that adding a small set of prototypes to the unlabeled data can improve tagging accuracy significantly '
'The statistical machine translation community relies on the Bleu metric for the purposes of evaluating incremental system changes and optimizing systems through minimum error rate training <CIT> '
'A detailed description of the popular translation models IBM-1 to IBM-5 <CIT> , aswellastheHidden-Markovalignmentmodel -LRB- HMM -RRB- <OTH> can be found in <OTH> '
'One of the best efforts to quantify the performance of a term-recognition system <CIT> does so only for one processing stage , leaving unassessed the text-to-output performance of the system '
'More suitable ways could be bilingual chunk parsing , and refining the bracketing grammar as described in <CIT> '
'-LRB- 2 -RRB- We note that these posterior probabilities can be computed efficiently for some alignment models such as the HMM <CIT> , Models 1 and 2 <OTH> '
'Widely used alignment models , such as IBM Model serial <CIT> and HMM , all assume one-to-many alignments '
'This approach has been shown to be accurate , relatively efficient , and robust using both generative and discriminative models <CIT> '
'3 The Syntactic and Semantic Parser Architecture To achieve the complex task of joint syntactic and semantic parsing , we extend a current state-of-theart statistical parser <CIT> to learn semantic role annotation as well as syntactic structure '
'This results also agree with Dunning ''s argument about overestimation on the infrequent occurrences in which many infrequent pairs tend to get higher estimation <CIT> '
'An online learning algorithm considers a single training instance for each update to the weight vector w We use the common method of setting the final weight vector as the average of the weight vectors after each iteration <CIT> , which has been shown to alleviate overfitting '
'ROUGE-L , ROUGE-W , and ROUGE-S have also been applied in automatic evaluation of summarization and achieved very promising results <CIT> '
'Given the parameters -LCB- pi0 , pi , , K -RCB- of the HMM , the joint distribution over hidden states s and observationsy can be written -LRB- with s0 = 0 -RRB- : p -LRB- s , y pi0 , pi , , K -RRB- = Tproductdisplay t = 1 p -LRB- st st1 -RRB- p -LRB- yt st -RRB- As <CIT> clearly explained , training the HMM with EM leads to poor results in PoS tagging '
'Unlike <OTH> , one interesting idea proposed by <CIT> is to cluster similar pairs of paraphrases to apply multiplesequence alignment '
'Automated metrics such as BLEU <CIT> , RED <OTH> , Weighted N-gram model -LRB- WNM -RRB- <OTH> , syntactic relation \/ semantic vector model <OTH> have been shown to correlate closely with scoring or ranking by different human evaluation parameters '
'Arguably the most widely used is the mutual information <CIT> '
'An especially well-founded framework is maximum entropy <CIT> '
'2 Related Work Supervised machine learning methods including Support Vector Machines -LRB- SVM -RRB- are often used in sentiment analysis and shown to be very promising <CIT> '
'Turney also reported good result without domain customization <CIT> '
'In our experiments , we follow Lowe and McDonald <OTH> in using the well-known log-likelihood ratio G 2 <CIT> '
'It is often straightforward to obtain large amounts of unlabeled data , making semi-supervised approaches appealing ; previous work on semisupervised methods for dependency parsing includes <CIT> '
'In the II , OO , and OI scenarios , <CIT> succeeded in improving the parser performance only when a reranker was used to reorder the 50-best list of the generative parser , with a seed size of 40K sentences '
'52 Maximum Entropy Maximum entropy classiflcation -LRB- MaxEnt , or ME , for short -RRB- is an alternative technique which has proven efiective in a number of natural language processing applications <CIT> '
'Yarowsky has proposed an algorithm that requires as little user input as one seed word per sense to start the training process <CIT> '
'Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers <OTH> , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship <CIT> '
'Such a method alleviates the problem of creating templates from examples which would be used in an ulterior phase of generation <OTH> '
'The variance semiring is essential for many interesting training paradigms such as deterministic 40 annealing <OTH> , minimum risk <OTH> , active and semi-supervised learning <CIT> '
'Among these methods , CRFs is the most common technique used in NLP and has been successfully applied to Part-of-Speech Tagging <OTH> , Named-Entity Recognition <CIT> and shallow parsing <OTH> '
'Wikipedia first sentence -LRB- WikiFS -RRB- : <CIT> used Wikipedia as an external knowledge to improve Named Entity Recognition '
'However , this is not unprecedented : discriminatively weighted generative models have been shown to outperform purely discriminative competitors in various NLP classification tasks <OTH> , and remain the standard approach in statistical translation modeling <CIT> '
'A more refined algorithm , the incremental feature selection algorithm by <CIT> , allows one feature being added at each selection and at the same time keeps estimated parameter values for the features selected in the previous stages '
'We also plan to apply self-training of n-best tagger which successfully boosted the performance of one of the best existing English syntactic parser <CIT> '
'In machine translation , the rankings from the automatic BLEU method <CIT> have been shown to correlate well with human evaluation , and it has been widely used since and has even been adapted for summarization <OTH> '
'Several studies have demonstrated that for instance Statistical Machine Translation -LRB- SMT -RRB- benefits from incorporating a dedicated WSD module <CIT> '
'In our experiments , we have used Averaged Perceptron <CIT> and Perceptron with margin <OTH> to improve performance '
'In 2004 , Conroy <OTH> tested Maximal Marginal Relevance <CIT> as well as QR decomposition '
'Automatically creating or extending taxonomies for specific domains is then a very interesting area of research <CIT> '
'SVM has been shown to be useful for text classification tasks <OTH> , and has previously given good performance in sentiment classification experiments <CIT> '
'2 Prior Work Statistical machine translation , as pioneered by IBM <CIT> , is grounded in the noisy channel model '
'The most notable of these include the trigram HMM tagger <OTH> , maximum entropy tagger <CIT> , transformation-based tagger <OTH> , and cyclic dependency networks <OTH> '
'Averaging has been shown to reduce overfitting <CIT> as well as reliance on the order of the examples during training '
'Synchronous parsing models have been explored with moderate success <CIT> '
'Systems based on perceptron have been shown to be competitive in NER and text chunking <CIT> We specify the model and the features with the LBJ <OTH> modeling language '
'Weusemaximumentropy models <CIT> , which are particularly well-suited for tasks -LRB- like ours -RRB- with many overlapping features , to harness these linguistic insights by using features in our models which encode , directly or indirectly , the linguistic correlates to SE types '
