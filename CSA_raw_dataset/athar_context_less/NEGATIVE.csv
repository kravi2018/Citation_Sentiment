'Unfortunately , this is not the case for such widely used MT evaluation metrics as BLEU <CIT> and NIST <OTH> '
'<OTH> applied the parser of <CIT> developed for English , to Czech , and found thatthe performance wassubstantially lower when compared to the results for English '
'Hanks and <CIT> proposed using pointwise mutual information to identify collocations in lexicography ; however , the method may result in unacceptable collocations for low-count pairs '
'Unlike <CIT> , Smadja -LRB- 1993 -RRB- goes beyond the ` two-word '' limitation and deals with ` collocations of arbitrary length '' '
'2This can explain why previous attempts to use WordNet for generating sentence-level paraphrases <CIT> were unsuccessful '
'We have also illustrated that ASIA outperforms three other English systems <CIT> , even though many of these use more input than just a semantic class name '
'For unknown words , SCL gives a relative reduction in error of 195 % over <CIT> , even with 40,000 sentences of source domain training data '
'For comparison purposes , we revisit a fullygenerative Bayesian model for unsupervised coreference resolution recently introduced by <CIT> , discuss its potential weaknesses and consequently propose three modifications to their model -LRB- Section 3 -RRB- '
'Unfortunately , there is no straightforward generalization of the method of <CIT> to the two edge marginal problem '
'Previous literature on GB parsing \/ Wehrli , 1984 ; Sharp , 1985 ; <CIT> , 1986 ; Kuhns , 1986 ; Abney , 1986\/has not addressed the issue of implementation of the Binding theory -RRB- The present paper intends in part to fill this gap '
'In general , these authors have found that existing lexicalized parsing models for English <CIT> do not straightforwardly generalize to new languages ; this typically manifests itself in a severe reduction in parsing performance compared to the results for English '
'This method was shown to outperform the class based model proposed in <CIT> and can thus be expected to discover better clusters of words '
'Although a rich literature covers bootstrapping methods applied to natural language problems <CIT> several questions remain unanswered when these methods are applied to syntactic or semantic pattern acquisition '
'It also differs from previous proposals on lexical acquisition using statistical measures such as <CIT> which either deny the prior existence of linguistic knowledge or use linguistic knowledge in ad hoc ways '
'In pursuit of better translation , phrase-based models <OTH> havesignificantlyimprovedthe quality over classical word-based models <CIT> '
'Several studies have shown that large-margin methods can be adapted to the special complexities of the task <CIT> However , the capacity of these algorithms to improve over state-of-the-art baselines is currently limited by their lack of robust dimensionality reduction '
'The 746 % final accuracy on apartments is higher than any result obtained by <CIT> -LRB- the highest is 741 % -RRB- , higher than the supervised HMM results reported by Grenager et al '
'One prominent constraint of the IBM word alignment models <CIT> is functional alignment , that is each target word is mapped onto at most one source word '
'Although the first three are particular cases where N = 1 and\/or M = 1 , the distinction is relevant , because most word-based translation models -LRB- eg IBM models <CIT> -RRB- can typically not accommodate general M-N alignments '
'Inside-out alignments <CIT> , such as the one in Example 13 , can not be induced by any of these theories ; in fact , there seems to be no useful synchronous grammar formalisms available that handle inside-out alignments , with the possible exceptions of synchronous tree-adjoining grammars <OTH> , Bertsch and Nederhof <OTH> and generalized multitext grammars <OTH> , which are all way more complex than ITG , STSG and -LRB- 2,2 -RRB- - BRCG '
'Bilexical context-free grammars have been presented in <OTH> as an abstraction of language models that have been adopted in several recent real-world parsers , improving state-of-the-art parsing accuracy <CIT> '
'For the Penn Treebank , <CIT> reports an accuracy of 966 % using the Maximum Entropy approach , our much simpler and therefore faster HMM approach delivers 967 % '
'While SCL has been successfully applied to PoS tagging and Sentiment Analysis <CIT> , its effectiveness for parsing was rather unexplored '
'V B N P  J J R ( a ) ( b ) V 2 V 1 V 2 '' V 1 '' V P V B N P w ill b e J J R  Figure 1: Two different binarizations (a) and (b) of the same SCFG rule distinguished by the solid lines and dashed lines                         ( W e  h o p e  t h e  s i t u a t i o n  w i l l  b e  b e t t e r  . )           N P        J J R     d e c o d i n g m a t c h  8 7 4  r u l e s m a t c h  6 2  r u l e s c o m p e t i n g  e d g e s :  8 0 1 c o m p e t i n g  e d g e s :  5 7 Figure 2: Edge competitions caused by different binarizations  The edge competition problem for SMT decoding is not addressed in previous work (Zhang et al., 2006; Huang, 2007) in which each SCFG rule is binarized in a fixed way.'
'Although such approaches have been employed effectively <CIT> , there appears to remain considerable room for improvement '
'In terms of alignment , this wordnumber difference means that multiword connections must be considered , a task which 334 Sue J Ker and Jason S Chang Word Alignment is beyond the reach of methods proposed in recent alignment works based on <CIT> Model 1 and 2 '
'2 Motivation and Prior Work While several authors have looked at the supervised adaptation case , there are less -LRB- and especially less successful -RRB- studies on semi-supervised domain adaptation <CIT> '
'In particular , the model in <CIT> failed to generate punctuation , a deficiency of the model '
'1 Introduction Phrase-based translation models <OTH> , which go beyond the original IBM translation models <CIT> 1 by modeling translations of phrases rather than individual words , have been suggested to be the state-of-theart in statistical machine translation by empirical evaluations '
'Table 2 : Figures about clustering algorithms Algorithm # Sentences \/ # Clusters S-HAC 6,23 C-HAC 2,17 QT 2,32 EM 4,16 In fact , table 2 shows that most of the clusters have less than 6 sentences which leads to question the results presented by <CIT> who only keep the clusters that contain more than 10 sentences '
'These methods go beyond the original IBM machine translation models <CIT> , by allowing multi-word units -LRB- phrases -RRB- in one language to be translated directly into phrases in another language '
'1 Introduction Recent works in statistical machine translation -LRB- SMT -RRB- shows how phrase-based modeling <OTH> significantly outperform the historical word-based modeling <CIT> '
'Our system outperforms competing approaches , including the standard machine translation alignment models <CIT> and the state-of-the-art Cut and Paste summary alignment technique <OTH> '
'For example , we would like to know that if a -LRB- JJ , JJ -RRB- 7We also tried using word clusters <CIT> instead of POS but found that POS was more helpful '
'This is because their training data , the Penn Treebank <CIT> , does not fully annotate NP structure '
'Although this Wikipedia gazetteer is much smaller than the English version used by <CIT> that has over 2,000,000 entries , it is the largest gazetteer that can be freely used for Japanese NER '
'As the tagger of <CIT> can not tag a word lattice , we can not back off to this tagging '
'Point-wise mutual information -LRB- PMI -RRB- is commonly used for computing the association of two terms <CIT> , which is defined as : nullnullnull null null , null null nullnullnull nullnullnullnull , nullnull nullnull null null null nullnullnullnullnull However , we argue that PMI is not a suitable measure for our purpose '
'Moreover , the parameters of the model must be estimated using averaged perceptron training <CIT> , which can be unstable '
'This method has the advantage that it is not limited to the model scaling factors as the method described in <CIT> '
'They reported that their method is superior to BLEU <CIT> in terms of the correlation between human assessment and automatic evaluation '
'While the idea of exploiting multiple news reports for paraphrase acquisition is not new , previous efforts <CIT> have been restricted to at most two news sources '
'Furthermore , we provide a 638 % error reduction compared to IBM Model 4 <CIT> '
'With all but two formats IBI-IG achieves better FZ = l rates than the best published result in <CIT> '
'A maximum entropy approach has been applied to partof-speech tagging before <CIT> , but the approach ''s ability to incorporate nonlocal and non-HMM-tagger-type evidence has not been fully explored '
'If we consider these probabilities as a vector , the similarities of two English words can be obtained by computing the dot product of their corresponding vectors2 The formula is described below : similarity -LRB- ei , ej -RRB- = Nsummationdisplay k = 1 p -LRB- ei fk -RRB- p -LRB- ej fk -RRB- -LRB- 3 -RRB- Paraphrasing methods based on monolingual parallel corpora such as <CIT> can also be used to compute the similarity ratio of two words , but they dont have as rich training resources as the bilingual methods do '
'32 Evaluation Metrics AER -LRB- Alignment Error Rate -RRB- <CIT> is the most widely used metric of alignment quality , but requires gold-standard alignments labeled with sure\/possible annotations to compute ; lacking such annotations , we can compute alignment fmeasure instead '
'Clustering algorithms have been previously shown to work fairly well for the classification of words into syntactic and semantic classes <CIT> , but determining the optimum number of classes for a hierarchical cluster tree is an ongoing difficult problem , particularly without prior knowledge of the item classification '
'Although , there are various manual\/automatic evaluation methods for these systems , eg , BLEU <CIT> , these methods are basically incapable of dealing with an MTsystem and a w\/p-MT-system at the same time , as they have different output forms '
'However , reordering models in traditional phrase-based systems are not sufficient to treat such complex cases when we translate long sentences <CIT> '
'Current tree-based models that integrate linguistics and statistics , such as GHKM <CIT> , are not able to generalize well from a single phrase pair '
'Our syntactic-relation-based thesaurus is based on the method proposed by <CIT> , although Hindle did not apply it to information retrieval '
'Both Charniak <OTH> and Bikel <OTH> were trained using the goldstandard tags , as this produced higher accuracy on the development set than using <CIT> s tags '
'The utility of ITG as a reordering constraint for most language pairs , is well-known both empirically <OTH> and analytically <CIT> , howeverITGsstraight -LRB- monotone -RRB- andinverted -LRB- reverse -RRB- rules exhibit strong cohesiveness , which is inadequate to express orientations that require gaps '
'This latter point is a critical difference that contrasts to the major weakness of the work of <CIT> which uses a top-N list of translations to select the maximum BLEU sentence as a target for training -LRB- so called local update -RRB- '
'Even the creators of BLEU point out that it may not correlate particularly well with human judgment at the sentence level <CIT> '
'1 Introduction In recent years , various phrase translation approaches <OTH> have been shown to outperform word-to-word translation models <CIT> '
'Sentence-level approximations to B exist <CIT> , but we found it most effective to perform B computations in the context of a setOof previously-translated sentences , following Watanabe et al '
'In such a process , original phrase-based decoding <CIT> does not take advantage of any linguistic analysis , which , however , is broadly used in rule-based approaches '
'However , many of these models are not applicable to parallel treebanks because they assume translation units where either the source text , the target text or both are represented as word sequences without any syntactic structure <CIT> '
'2 Statistical Word Alignment Statistical translation models <CIT> only allow word to word and multi-word to word alignments '
'Even the 3 A demo of the parser can be found at http://lfgdemocomputingdcuie/lfgparserhtml creators of BLEU point out that it may not correlate particularly well with human judgment at the sentence level <CIT> '
'Our method , extending this line of research with the use of labeled LFG dependencies , partial matching , and n-best parses , allows us to considerably outperform <CIT> highest correlations with human judgement -LRB- they report 0144 for the correlation with human fluency judgement , 0202 for the correlation with human overall judgement -RRB- , although it has to be kept in mind that such comparison is only tentative , as their correlation is calculated on a different test set '
'Unlike well-known bootstrapping approaches <CIT> , EM and CE have the possible advantage of maintaining posteriors over hidden labels -LRB- or structure -RRB- throughout learning ; bootstrapping either chooses , for each example , a single label , or remains completely agnostic '
'1 Introduction Recent approaches to statistical machine translation -LRB- SMT -RRB- piggyback on the central concepts of phrasebased SMT <CIT> and at the same time attempt to improve some of its shortcomings by incorporating syntactic knowledge in the translation process '
'For the results in this paper , we have used Pointwise Mutual Information -LRB- PMI -RRB- instead of IBM Model 1 <CIT> , since <OTH> found it to be as effective on Springer , but faster to compute '
'Numbers in the table correspond to the percentage of experiments in which the condition at the head of the column was true -LRB- for example figure in the first row and first column means that for 989 percent of the language pairs the BLEU score for the bidirectional decoder was better than that of the forward decoder -RRB- proach <CIT> -RRB- '
'This provides a compelling advantage over previous dependency language models for MT <CIT> , whichusea5-gramLMonlyduringreranking '
'Experimental results indicate that our model outperforms <CIT> coreference model by a large margin on the ACE data sets and compares favorably to a modified version of their model '
'This method was preferred against other related methods , like the one introduced in <CIT> , since it embeds all the available semantic information existing in WordNet , even edges that cross POS , thus offering a richer semantic representation '
'Both the global models <CIT> use fairly small training sets , and there is no evidence that their techniques will scale to larger data sets '
'Our model improves the baseline provided by <CIT> : -LRB- i -RRB- accuracy is increased by creating a lexicalised PCFG grammar and enriching conditioning context with parent f-structure features ; and -LRB- ii -RRB- coverage is increased by providing lexical smoothing and fuzzy matching techniques for rule smoothing '
'While the model of <CIT> significantly outperforms the constrained model of <OTH> , they both are well below the state-of-the-art in constituent parsing '
'By doing so we must emphasize that , as described in the previous section , the BLEU score was not designed to deliver satisfactory results at the sentence level <CIT> , and this also applies to the closely related NIST score '
'<CIT> focus on alignment and do not present MT results , while May and Knight -LRB- 2007 -RRB- takesthesyntacticre-alignmentasaninputtoanEM algorithm where the unaligned target words are insertedintothetemplatesandminimumtemplatesare combinedintobiggertemplates <OTH> '
'But without the global normalization , the maximumlikelihood criterion motivated by the maximum entropy principle <CIT> is no longer a feasible option as an optimization criterion '
'? ? word class : <CIT> measures polarity using only adjectives , however in our approach we consider the noun , the verb , the adverb and the adjective content words '
'While most parsing methods are currently supervised or semi-supervised <CIT> , they depend on hand-annotated data which are difficult to come by and which exist only for a few languages '
'To analyze our methods on IV and OOV words , we use a detailed evaluation metric than Bakeoff 2006 <CIT> which includes Foov and Fiv '
'Surprisingly , although JESS-CM is a simpler version of the hybrid model in terms of model structure and parameter estimation procedure , JESS-CM provides F-scores of 9445 and 8803 for CoNLL00 and 03 data , respectively , which are 015 and 083 points higher than those reported in <CIT> for the same configurations '
'We presented some theoretical arguments for not limiting extraction to minimal rules , validated them on concrete examples , and presented experiments showing that contextually richer rules provide a 363 BLEU point increase over the minimal rules of <CIT> '
'Our study also shows that the simulated-annealing algorithm <OTH> is more effective 1552 than the perceptron algorithm <CIT> for feature weight tuning '
'By segmenting words into morphemes , we can improve the performance of natural language systems including machine translation <CIT> and information retrieval <OTH> '
'Statistical disambiguation such as <CIT> for PP-attachment or <CIT> for generative parsing greatly improve disambiguation , but as they model by imitation instead of by understanding , complete soundness has to remain elusive '
'It can be applied to complicated models such IBM Model-4 <CIT> '
'By 17 0 10 20 30 40 50 60 70 80 90 100 10000 100000 1e+06 1e+07 Test Set Items with Translations (%) Training Corpus Size (num words) unigrams bigrams trigrams 4-grams Figure 1: Percent of unique unigrams, bigrams, trigrams, and 4-grams from the Europarl Spanish test sentences for which translations were learned in increasingly large training corpora increasing the size of the basic unit of translation, phrase-based machine translation does away with many of the problems associated with the original word-based formulation of statistical machine translation (Brown et al. , 1993).'
'2 21 Word Alignment Adaptation Bi-directional Word Alignment In statistical translation models <CIT> , only one-to-one and more-to-one word alignment links can be found '
'The method was intended as a replacement for sentence-based methods -LRB- eg , <CIT> -RRB- , which are very sensitive to noise '
'This approach addresses the problematic aspects of both pure knowledge-based generation -LRB- where incomplete knowledge is inevitable -RRB- and pure statistical bag generation <CIT> -LRB- where the statistical system has no linguistic guidance -RRB- '
'In addition , the clustering methods used , such as HMMs and Browns algorithm <CIT> , seem unable to adequately capture the semantics of MNs since they are based only on the information of adjacent words '
'We preferred the log-likelihood ratio to other statistical scores , such as the association ratio <CIT> or ; -LRB- 2 , since it adequately takes into account the frequency of the co-occurring words and is less sensitive to rare events and corpussize <OTH> '
'The ubiquitous minimum error rate training -LRB- MERT -RRB- approach optimizes Viterbi predictions , but does not explicitly boost the aggregated posterior probability of desirable n-grams <CIT> '
'However , work in that direction has so far addressed only parse reranking <CIT> '
'The combination is significantly better than <CIT> at a very high level , but more importantly , Shens results -LRB- currently representing the replicable state-of-the-art in POS tagging -RRB- have been significantly surpassed also by the semisupervised Morce -LRB- at the 99 % confidence level -RRB- '
'a time-consuming process <CIT> '
'Other statistical machine translation systems such as <CIT> and <OTH> also produce a tree a15 given a sentence a16 Their models are based on mechanisms that generate two languages at the same time , so an English tree a15 is obtained as a subproduct of parsing a16 However , their use of the LM is not mathematically motivated , since their models do not decompose into Pa4a5a2a9a8a3a10a6 and a12a14a4a5a3a7a6 unlike the noisy channel model '
'WSD systems have been far more successful in distinguishing coarsegrained senses than fine-grained ones <CIT> , but does that approach neglect necessary meaning differences ? '
'Secondly , while most pronoun resolution evaluations simply exclude non-referential pronouns , recent unsupervised approaches <CIT> must deal with all pronouns in unrestricted text , and therefore need robust modules to automatically handle non-referential instances '
'12Poon and Domingos <OTH> outperformed <CIT> '
'As with similar work <CIT> , the size of the corpus makes preprocessing such as lemmatization , POS tagging or partial parsing , too costly '
'The size of the development set used to generate 1 and 2 <OTH> compensates the tendency of the unsmoothed MERT algorithm to overfit <CIT> by providing a high ratio between number of variables and number of parameters to be estimated '
'<CIT> have proposed a rule-based algorithm for sentence combination , but no results have been reported '
'<CIT> provides anecdotal evidence that only incorrect alignments are eliminated by ITG constraints '
'By segmenting words into morphemes , we can improve the performance of natural language systems including machine translation <CIT> and information retrieval <OTH> '
'It has been difficult to identify all and only those cases where a token functions as a discourse connective , and in many cases , the syntactic analysis in the Penn TreeBank <CIT> provides no help '
'For example , 10 million words of the American National Corpus <OTH> will have manually corrected POS tags , a tenfold increase over the Penn Treebank <CIT> , currently used for training POS taggers '
'The process of phrase extraction is difficult to optimize in a non-discriminative setting : many heuristics have been proposed <CIT> , but it is not obvious which one should be chosen for a given language pair '
'While several methods have been proposed to automatically extract compounds <CIT> , we know of no successful attempt to automatically make classes of compounds '
'Many approaches for POS tagging have been developed in the past , including rule-based tagging <OTH> , HMM taggers <CIT> , maximum-entropy models <OTH> , cyclic dependency networks <OTH> , memory-based learning <OTH> , etc All of these approaches require either a large amount of annotated training data -LRB- for supervised tagging -RRB- or a lexicon listing all possible tags for each word -LRB- for unsupervised tagging -RRB- '
'Our focus is on the sentence level , unlike <OTH> and <CIT> ; we employ a significantly larger set of seed words , and we explore as indicators of orientation words from syntactic classes other than adjectives -LRB- nouns , verbs , and adverbs -RRB- '
'This additional conditioning has the effect of making the choice of generation rules sensitive to the history of the generation process , and , we argue , provides a simpler , more uniform , general , intuitive and natural probabilistic generation model obviating the need for CFG-grammar transforms in the original proposal of <CIT> '
'1 Introduction The most widely applied training procedure for statistical machine translation IBM model 4 <OTH> unsupervised training followed by post-processing with symmetrization heuristics <CIT> yields low quality word alignments '
'This is in contrast to purely statistical systems <CIT> , which are difficult to inspect and modify '
'In addition , the semi-supervised Morce performs -LRB- on single CPU and development data set -RRB- 77 times faster than the combination and 23 times faster than <CIT> '
'In a recent study by <CIT> , nonlocal information is encoded using an independence model , and the inference is performed by Gibbs sampling , which enables us to use a stateof-the-art factored model and carry out training efficiently , but inference still incurs a considerable computational cost '
'<CIT> suggests use of an approximation summing over the training data , which does not sum over possible tags : '' h E f j = 2 P -LRB- ~ -RRB- p -LRB- ti l hi -RRB- f j -LRB- hi , ti -RRB- i = 1 However , we believe this passage is in error : such an estimate is ineffective in the iterative scaling algorithm '
'IBM Model1 <CIT> is a simplistic model which takes no account of the subtler aspects of language translation including the way word order tends to differ across languages '
'A number of studies have investigated sentiment classification at document level , eg , <CIT> , and at sentence level , eg , <OTH> ; however , the accuracy is still less than desirable '
'Automatic evaluation methods such as BLEU <CIT> , RED <OTH> , or the weighted N-gram model proposed here may be more consistent in judging quality as compared to human evaluators , but human judgments remain the only criteria for metaevaluating the automatic methods '
'For comparison purposes , we revisit <CIT> fully-generative Bayesian model for unsupervised coreference resolution , discuss its potential weaknesses and consequently propose three modifications to their model '
'The class-based kappa statistic of <CIT> can not be applied here , as the classes vary depending on the number of ambiguities per entry in the lexicon '
'While the amount of parallel data required to build such systems is orders of magnitude smaller than corresponding phrase based statistical systems <CIT> , the variety of linguistic annotation required is greater '
'Although this method is comparatively easy to be implemented , it just achieves the same performance as the synchronous binarization method <CIT> for syntaxbased SMT systems '
'Among the applications of collocational analysis for lexical acquisition are: the derivation of syntactic disambiguation cues (Basili et al. 1991, 1993a; Hindle and Rooths 1991,1993; Sekine 1992) (Bogges et al. 1992), sense preference (Yarowski 1992), acquisition of selectional restrictions (Basili et al. 1992b, 1993b; Utsuro et al. 1993), lexical preference in generation (Smadjia 1991), word clustering (Pereira 1993; Hindle 1990; Basili et al. 1993c), etc. In the majority of these papers, even though the (precedent or subsequent) statistical processing reduces the number of accidental associations, very large corpora (10,000,000 words) are necessary to obtain reliable data on a ''large enough'' number of words.'
'Due to limited variations in the N-Best list , the nature of ranking , and more importantly , the non-differentiable objective functions used for MT -LRB- such as BLEU <CIT> -RRB- , one often found only local optimal solutions to , with no clue to walk out of the riddles '
'We also compare our performance against <OTH> and <CIT> and find that we manage higher relative improvement than existing work despite starting from a very competitive baseline CRF '
'Although several methods have already been proposed to incorporate non-local features <CIT> , these present a problem that the types of non-local features are somewhat constrained '
'The morphological processing in PairClass <OTH> is more sophisticated than in <CIT> '
'In addition , the performance of the adapted model for Joint S&T obviously surpass that of <CIT> , which achieves an F1 of 9341 % for Joint S&T , although with more complicated models and features '
'Some are the result of inconsistency in labeling in the training data <CIT> , which usually reflects a lack of linguistic clarity or determination of the correct part of speech in context '
'Therefore , sublanguage techniques such as Sager <OTH> and <CIT> do not work '
'2Mutual information , though potentially of interest as a measure of collocational status , was not tested due to its well-known property of overemphasising the significance of rare events <CIT> '
'While in traditional word-based statistical models <CIT> the atomic unit that translation operates on is the word , phrase-based methods acknowledge the significant role played in language by multiword expressions , thus incorporating in a statistical framework the insight behind Example-Based Machine Translation <OTH> '
'Note that the minimum error rate training <CIT> uses only the target sentence with the maximum posterior probability whereas , here , the whole probability distribution is taken into account '
'A word order correlation bias , as well as the phrase structure biases in <CIT> Models 4 and 5 , would be less beneficial with noisier training bitexts or for language pairs with less similar word order '
'Although various approaches to SMT system combination have been explored , including enhanced combination model structure <CIT> , better word alignment between translations <OTH> and improved confusion network construction <CIT> , most previous work simply used the ensemble of SMT systems based on different models and paradigms at hand and did not tackle the issue of how to obtain the ensemble in a principled way '
'1 Introduction The field of machine translation has seen many advances in recent years , most notably the shift from word-based <CIT> to phrasebased models which use token n-grams as translation units <OTH> '
'Although ITA rates and system performance both significantly improve with coarse-grained senses <CIT> , the question about what level of granularity is needed remains '
'three models in <CIT> are susceptible to the O -LRB- n 3 -RRB- method -LRB- cf '
'Even with the current incomplete set of semantic templates , the hypertagger brings realizer performance roughly up to state-of-the-art levels , as our overall test set BLEU score <OTH> slightly exceeds that of <CIT> , though at a coverage of 96 % insteadof98 % '
'An alternative method <CIT> makes decisions at the end but has a high computational requirement '
'Several teams had approaches that relied (to varying degrees) on an IBM model of statistical machine translation (Brown et al. , 1993), with different improvements brought by different teams, consisting of new submodels, improvements in the HMM model, model combination for optimal alignment, etc. Se-veral teams used symmetrization metrics, as introduced in (Och and Ney, 2003) (union, intersection, refined), most of the times applied on the alignments produced for the two directions sourcetarget and targetsource, but also as a way to combine different word alignment systems.'
'In the thriving area of research on automatic analysis and processing of product reviews <CIT> , little attention has been paid to the important task studied here assessing review helpfulness '
'In such tasks , feature calculation is also very expensive in terms of time required ; huge sets of extracted rules must be sorted in two directions for relative frequency calculation of such features as the translation probability p -LRB- f e -RRB- and reverse translation probability p -LRB- e f -RRB- <CIT> '
'Other statistical systems that address word classification probleans do not emphasize the use of linguistic knowledge and do not deal with a specific word class <CIT> , or do not exploit as much linguistic knowledge as we do <OTH> '
'As one can see in Table 4 , the resulting parser ranks among the best lexicalized parsers , beating those of Collins <OTH> and Charniak and Johnson <OTH> 8 Its F1 performance is a 27 % reduction in error over <CIT> et al '
'Our experiments on the Canadian Hansards show that our unsupervised technique is significantly more effective than picking seeds by hand <CIT> , which in turn is known to rival supervised methods '
'2 Related Work One of the major problems with the IBM models <CIT> and the HMM models <OTH> is that they are restricted to the alignment of each source-language word to at most one targetlanguage word '
'This implies that the complexity of structure divergence between two languages is higher than suggested in literature <CIT> '
'<OTH> report better perplexity results on the Verbmobil Corpus with their HMMbased alignment model in comparison to Model 2 of <CIT> '
'<OTH> have implemented a dependency parser with good accuracy -LRB- it is almost as good at dependency parsing as Charniak <OTH> -RRB- and very impressive speed -LRB- it is about ten times faster than <CIT> and four times faster than Charniak <OTH> -RRB- '
'Unlike minimum error rate training <CIT> , our system is able to exploit large numbers of specific features in the same manner as static reranking systems <CIT> '
'However , to what extent that assumption holds is tested only on a small number of language pairs using hand aligned data <CIT> '
'Although evaluated on a different test set , our method also outperforms the correlation with human scores reported in <CIT> '
'However , it seems unrealistic to expect a one-size-fits-all approach to be achieve uniformly high performance across varied languages , and , in fact , it doesnt Though the system presented in <CIT> outperforms the best systems in the 2006 PASCAL challenge for Turkish and Finnish , it still does significantly worse on these languages than English -LRB- F-scores of 662 and 665 , compared to 794 -RRB- '
'For comparison purposes , three additional heuristically-induced alignments are generated for each system : -LRB- 1 -RRB- Intersection of both directions -LRB- Aligner -LRB- int -RRB- -RRB- ; -LRB- 2 -RRB- Union of both directions -LRB- Aligner -LRB- union -RRB- -RRB- ; and -LRB- 3 -RRB- The previously bestknown heuristic combination approach called growdiag-final <CIT> -LRB- Aligner -LRB- gdf -RRB- -RRB- '
'Brill ''s results demonstrate that this approach can outperform the Hidden Markov Model approaches that are frequently used for part-of-speech tagging <CIT> , as well as showing promise for other applications '
'It has the advantage of naturally capturing local reorderings and is shown to outperform word-based machine translation <CIT> '
'Again the best result was obtained with IOB1 <OTH> which is an imI -RRB- rovement of the best reported F , ~ = 1 rate for this data set -LRB- <CIT> : 9203 -RRB- '
'This is well illustrated by the Collins parser <CIT> , scrutinized by Bikel -LRB- 2004 -RRB- , where several transformations are applied in order to improve the analysis of noun phrases , coordination and punctuation '
'It is faster and more mnemonic than the one in <CIT> '
'1 Introduction Translations tables in Phrase-based Statistical Machine Translation -LRB- SMT -RRB- are often built on the basis of Maximum-likelihood Estimation -LRB- MLE -RRB- , being one of the major limitations of this approach that the source sentence context in which phrases occur is completely ignored <CIT> '
'In what concerns the evaluation process , although ROUGE <CIT> is the most common evaluation metric for the automatic evaluation of summarization , since our approach might introduce in the summary information that it is not present in the original input source , we found that a human evaluation was more adequate to assess the relevance of that additional information '
'The program takes the output of char_align <OTH> , a robust alternative to sentence-based alignment programs , and applies word-level constraints using a version of Brown el al ''s Model 2 <CIT> , modified and extended to deal with robustness issues '
'6 Conclusion Traditional approaches for devising parsing models , smoothing techniques and evaluation metrics are not well suited for MH , as they presuppose 13The lack of head marking , for instance , precludes the use of lexicalized models a la <CIT> '
'The experimental results show that our method outperforms the synchronous binarization method <CIT> with over 08 BLEU scores on both NIST 2005 and NIST 2008 Chinese-to-English evaluation data sets '
'<OTH> produced a corpus of 4,000 questions annotated with syntactic trees , and obtained an improvement in parsing accuracy for Bikels reimplementation of the Collins parser <CIT> by training a new parser model with a combination of newspaper and question data '
'1 Introduction Statistical phrase-based systems <CIT> have consistently delivered state-of-the-art performance in recent machine translation evaluations , yet these systems remain weak at handling word order changes '
'With these linguistic annotations , we expect the LABTG to address two traditional issues of standard phrase-based SMT <CIT> in a more effective manner '
'At the same time , we believe our method has advantages over the approach developed initially at IBM <CIT> for training translation systems automatically '
'Despite relying on a the same concept , our approach outperforms BE in most comparisons , and it often achieves higher correlations with human judgments than the string-matching metric ROUGE <CIT> '
'More recent work <OTH> has considered methods for speeding up the feature selection methods described in <CIT> , Ratnaparkhi -LRB- 1998 -RRB- , and Della Pietra , Della Pietra , and Lafferty <OTH> '
'1 Introduction Currently , most of the phrase-based statistical machine translation -LRB- PBSMT -RRB- models <CIT> adopt full matching strategy for phrase translation , which means that a phrase pair -LRB- tildewidef , tildewidee -RRB- can be used for translating a source phrase f , only if tildewidef = f Due to lack of generalization ability , the full matching strategy has some limitations '
'1 Introduction For statistical machine translation -LRB- SMT -RRB- , phrasebased methods <OTH> and syntax-based methods <OTH> outperform word-based methods <CIT> '
'To our knowledge no systems directly address Problem 1 , instead choosing to ignore the problem by using one or a small handful of reference derivations in an n-best list <CIT> , or else making local independence assumptions which side-step the issue <OTH> '
'For example , the statistical word alignment in IBM translation models <CIT> can only handle word to word and multi-word to word alignments '
'Our graphical representation has two advantages over previous work <CIT> : unifying sentence relations and incorporating question interactions '
'<OTH> presented a history-based generation model to overcome some of the inappropriate independence assumptions in the basic generation model of <CIT> '
'Unfortunately , longer sentences -LRB- up to 100 tokens , rather than 40 -RRB- , longer phrases -LRB- up to 10 tokens , rather than 7 -RRB- , two LMs -LRB- rather than just one -RRB- , higher-order LMs -LRB- order 7 , rather than 3 -RRB- , multiple higher-order lexicalized re-ordering models -LRB- up to 3 -RRB- , etc all contributed to increased system ? s complexity , and , as a result , time limitations prevented us from performing minimum-error-rate training -LRB- MERT -RRB- <CIT> for ucb3 , ucb4 and ucb5 '
'Unsupervised methods have been developed for WSD , but despite modest success have not always been well understood statistically <CIT> '
'In addition , uniform conditioning on mother grammatical function is more general than the case-phenomena specific generation grammar transform of <CIT> , in that it applies to each and every sub-part of a recursive input f-structure driving generation , making available relevant generation history -LRB- context -RRB- to guide local generation decisions '
'Section 5 presents an error analysis for <CIT> lexicalized model , which shows that the head-head dependencies used in this model fail to cope well with the flat structures in Negra '
'Even the creators of BLEU point out that it may not correlate particularly well with human judgment at the sentence level <CIT> , a problem also noted by <OTH> and <OTH> '
'Methods like McDonalds , including the wellknown Maximal Marginal Relevance -LRB- MMR -RRB- algorithm <CIT> , are subject to another problem : Summary-level redundancy is not always well modeled by pairwise sentence-level redundancy '
'While other systems , such as <CIT> , have addressed these tasks to some degree , OPINE is the first to report results '
'Thirdly , <CIT> deploys the dependency language model to augment the lexical language model probability be1183 tween two head words but never seek a full dependency graph '
'METEOR was chosen since , unlike the more commonly used BLEU metric <CIT> , it provides reasonably reliable scores for individual sentences '
'Our method is a natural extension of those proposed in <CIT> and <OTH> , and overcomes their drawbacks while retaining their advantages '
'Another consequence of not generating posthead conjunctions and punctuation as first-class words is that they 19 In fact , if punctuation occurs before the head , it is not generated at alla deficiency in the parsing model that appears to be a holdover from the deficient punctuation handling in the model of <CIT> '
'One conclusion that we can draw is that at present the additional word features used in <CIT> looking at words more than one position away from the current do not appear to be helping the overall performance of the models '
'In comparison , the 2D model in Figure 2 -LRB- c -RRB- used in previous work <CIT> can only model the interaction between adjacent questions '
'The problem is typically presented in log-space , which simplifies computations , but otherwise does not change the problem due to the monotonicity of the log function -LRB- hm = log hprimem -RRB- log p -LRB- t s -RRB- = summationdisplay m m hm -LRB- t , s -RRB- -LRB- 3 -RRB- Phrase-based models <CIT> are limited to the mapping of small contiguous chunks of text '
'Though taggers based on dependency networks <OTH> , SVM <OTH> , MaxEnt <CIT> , CRF <OTH> , and other methods may reach slightly better results , their train\/test cycle is orders of magnitude longer '
'Several papers have looked at higher-order representations , but have not examined the equivalence of syn\/para distributions when formalized as Markov chains <CIT> '
'Of the methods we compare against , only the WordNet-based similarity measures , <OTH> , and <CIT> provide a method for predicting verb similarities ; our learned measure widely outperforms these methods , achieving a 136 % F-score improvement over the LESK similarity measure '
'In order to capture the dependency relationship between lexcial heads <CIT> breaks down the rules from head outwards , which prevents us from factorizing them in other ways '
'Besides , our model , as being linguistically motivated , is also more expressive than the formally syntax-based models of Chiang <OTH> and <CIT> '
'String alignment with synchronous grammars is quite expensive even for simple synchronous formalisms like ITG <CIT> but Duchi et al '
'1 Introduction Phrase-based systems , flat and hierarchical alike <CIT> , have achieved a much better translation coverage than wordbased ones <OTH> , but untranslated words remain a major problem in SMT '
'Such a quasi-syntactic structure can naturally capture the reordering of phrases that is not directly modeled by a conventional phrase-based approach <CIT> '
'Although generating training examples in advance without a working parser <OTH> is much faster than using inference <CIT> , our training time can probably be decreased further by choosing a parsing strategy with a lower branching factor '
'This cost can often be substantial , as with the Penn Treebank <CIT> '
'2 Previous work on Sentiment Analysis Some prior studies on sentiment analysis focused on the document-level classification of sentiment <CIT> where a document is assumed to have only a single sentiment , thus these studies are not applicable to our goal '
'Since Czech is a language with relatively high degree of word-order freedom , and its sentences contain certain syntactic phenomena , such as discontinuous constituents -LRB- non-projective constructions -RRB- , which can not be straightforwardly handled using the annotation scheme of Penn Treebank <CIT> , based on phrase-structure trees , we decided to adopt for the PCEDT the dependency-based annotation scheme of the Prague Dependency Treebank PDT <OTH> '
'13 <CIT> give an informal example , but do not elaborate on it '
'Like WASP1 , the phrase extraction algorithm of PHARAOH is based on the output of a word alignment model such as GIZA + + <CIT> , which performs poorly when applied directly to MRLs -LRB- Section 32 -RRB- '
'This restriction is necessary because the problem of optimizing many-to-many alignments 5 Our preliminary experiments with n-gram-based overlap measures , such as BLEU <CIT> and ROUGE <OTH> , show that these metrics do not correlate with human judgments on the fusion task , when tested against two reference outputs '
'Although the authors of <CIT> stated that they would discuss the search problem in a follow-up arti cle , so far there have no publications devoted to the decoding issue for statistical machine translation '
'This strategy is commonly used in MT evaluation , because of BLEUs well-known problems with documents of small size <CIT> '
'Our system improves over the latent named-entity tagging in <CIT> , from 61 % to 87 % '
'The generalized perceptron proposed by <CIT> is closely related to CRFs , but the best CRF training methods seem to have a slight edge over the generalized perceptron '
'2 Previous Work It is helpful to compare this approach with recent efforts in statistical MT Phrase-based models <CIT> are good at learning local translations that are pairs of -LRB- consecutive -RRB- sub-strings , but often insufficient in modeling the reorderings of phrases themselves , especially between language pairs with very different word-order '
'When compared to other kernel methods , our approach performs better than those based on the Tree kernel <CIT> , and is only 02 % worse than the best results achieved by a kernel method for parsing <OTH> '
'Lexical relationships under the standard IBM models <CIT> do not account for many-to-many mappings , and phrase extraction relies heavily on the accuracy of the IBM word-toword alignment '
'Despite ME theory and its related training algorithm <OTH> do not set restrictions on the range of feature functions1 , popular NLP text books <OTH> and research papers <CIT> seem to limit them to binary features '
'4 Conclusions Compared with other word alignment algorithms <CIT> , word_align does not require sentence alignment as input , and was shown to produce useful alignments for small and noisy corpora '
'1 Introduction Statistical phrase-based systems <CIT> have consistently delivered state-of-the-art performance in recent machine translation evaluations , yet these systems remain weak at handling word order changes '
'Our approach not only outperformed a notoriously difficult baseline but also achieved similar performance to the approach of <CIT> , without requiring their third-party data resources '
'While we have shown an increase in performance over a purely syntactic baseline model -LRB- the algorithm of <CIT> -RRB- , there are a number of avenues to pursue in extending this work '
'Presently , there exist methods for learning oppositional terms <CIT> and paraphrase learning has been thoroughly studied , but successfully extending these techniques to learn incompatible phrases poses difficulties because of the data distribution '
'Formal complexity analysis has not been carried out , but my algorithm is simpler , at least conceptually , than the variable-word-order parsers of Johnson <OTH> , <CIT> , and Abramson and Dahl -LRB- 1989 -RRB- '
'These scores are higher than those of several other parsers <CIT> , but remain behind tim scores of Charniak -LRB- 2000 -RRB- who obtains 901 % LP and 901 % LR for sentences _ -LRB- 40 words '
'In contrast to existing approaches <CIT> , the context of the whole corpus rather than a single sentence is considered in this iterative , unsupervised procedure , yielding a more reliable alignment '
'While minimum error training <CIT> has by now become a standard tool for interpolating a small number of aggregate scores , it is not well suited for learning in high-dimensional feature spaces '
'<CIT> and Collins and Duffy -LRB- 2002 -RRB- rerank the top N parses from an existing generative parser , but this kind of approach 1Dynamic programming methods <OTH> can sometimes be used for both training and decoding , but this requires fairly strong restrictions on the features in the model '
'Ever since its introduction in general <OTH> and in computational linguistics <CIT> , many researchers have pointed out that there are quite some problems in using -LRB- eg '
'1 Introduction The dominance of traditional phrase-based statistical machine translation -LRB- PBSMT -RRB- models <CIT> has recently been challenged by the development and improvement of a number of new models that explicity take into account the syntax of the sentences being translated '
'1 Introduction Hierarchical approaches to machine translation have proven increasingly successful in recent years <OTH> , and often outperform phrase-based systems <CIT> on target-language fluency and adequacy '
'While we do not have a direct comparison , we note that <CIT> performs worse on movie reviews than on his other datasets , the same type of data as the polarity dataset '
'At any rate , regularized conditional loglinear models have not previously been applied to the problem of producing a high quality part-of-speech tagger : Ratnaparkhi <OTH> , Toutanova and Manning <OTH> , and <CIT> all present unregularized models '
'The most commonly used metric , BLEU , correlates well over large test sets with human judgments <CIT> , but does not perform as well on sentence-level evaluation <OTH> '
'We will show that some achieve significantly better results than the standard minimum error rate training of <CIT> '
'Allomorphs -LRB- eg , deni and deny -RRB- are also automatically identified in <CIT> , but the general problem of recognizing highly irregular forms is examined more extensively in <OTH> '
'Most recently , <OTH> published their Semi-supervised sequential labeling method , whose results on POS tagging seem to be optically better than <CIT> , but no significance tests were given and the tool is not available for download , ie for repeating the results and significance testing '
'By increasing the size of the basic unit of translation , phrase-based machine translation does away with many of the problems associated with the original word-based formulation of statistical machine translation <CIT> , in particular : The Brown et al '
'<OTH> examine the FS of the weighted log-likelihood ratio -LRB- WLLR -RRB- on the movie review dataset and achieves an accuracy of 871 % , which is higher than the result reported by <CIT> with the same dataset '
'While both <OTH> and <CIT> propose models which use the parameters of the generative model but train to optimize a discriminative criteria , neither proposes training algorithms which are computationally tractable enough to be used for broad coverage parsing '
'Turneys method did not work well although they reported 80 % accuracy in <CIT> '
'<OTH> tried a different generative phrase translation model analogous to IBM word-translation Model 3 <CIT> , and again found that the standard model outperformed their generative model '
'The automatically generated patterns in PairClass are slightly more general than the patterns of <CIT> '
'Although previous work <CIT> has tackled the bootstrapping approach from both the theoretical and practical point of view , many key problems still remain unresolved , such as the selection of initial seed set '
'Unlike <CIT> , who found optimal performance when was approximately 104 , we observed monotonic increases in performance as dropped '
'While these approaches have had som e success to date <CIT> , their usability as parsers in systems for natural language understanding is suspect '
'2 Motivation and Prior Work While several authors have looked at the supervised adaptation case , there are less -LRB- and especially less successful -RRB- studies on semi-supervised domain adaptation <CIT> '
