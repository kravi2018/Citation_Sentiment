'In <OTH> it was observed that a significant percent of the queries made by a user in a search engine are associated to a repeated search '
'Output sequence optimization Rather than basing classifications only on model parameters estimated from co-occurrences between input and output symbols employed for maximizing the likelihood of point-wise single-label predictions at the output level , classifier output may be augmented by an optimization over the output sequence as a whole using optimization techniques such as beam searching in the space of a conditional markov models output <CIT> or hidden markov models <OTH> '
'Dredze et al yielded the second highest score1 in the domain adaptation track <CIT> '
'The IBM models <CIT> search a version of permutation space with a one-to-many constraint '
'<CIT> propose the use of language models for sentiment analysis task and subjectivity extraction '
'In training process , we use GIZA + + 4 toolkit for word alignment in both translation directions , and apply grow-diag-final method to refine it <CIT> '
'The models in the comparative study by <CIT> did not include such features , and so , again for consistency of comparison , we experimentally verified that our maximum entropy model -LRB- a -RRB- consistently yielded higher scores than when the features were not used , and -LRB- b -RRB- consistently yielded higher scores than nave Bayes using the same features , in agreement with <CIT> '
'<OTH> and <CIT> et al '
'We used the WordNet : : Similarity package <CIT> to compute baseline scores for several existing measures , noting that one word pair was not processed in WS-353 because one of the words was missing from WordNet '
'We use MER <CIT> to tune the decoders parameters using a development data set '
'The training set is extracted from TreeBank <CIT> section 1518 , the development set , used in tuning parameters of the system , from section 20 , and the test set from section 21 '
'For non-local features , we adapt cube pruning from forest rescoring <CIT> , since the situation here is analogous to machine translation decoding with integrated language models : we can view the scores of unit nonlocal features as the language model cost , computed on-the-fly when combining sub-constituents '
'31 Agreement for Emotion Classes The kappa coefficient of agreement is a statistic adopted by the Computational Linguistics community as a standard measure for this purpose <CIT> '
'ITGs translate into simple -LRB- 2,2 -RRB- - BRCGs in the following way ; see <CIT> for a definition of ITGs '
'This may be because their system was not tuned using minimum error rate training <CIT> '
'However , most of the existing models have been developed for English and trained on the Penn Treebank <CIT> , which raises the question whether these models generalize to other languages , and to annotation schemes that differ from the Penn Treebank markup '
'Following <CIT> , we used sections 0-18 of the Wall Street Journal -LRB- WSJ -RRB- corpus for training , sections 19-21 for development , and sections 22-24 for final evaluation '
'In <CIT> , the authors provide some sample subtrees resulting from such a 1,000-word clustering '
'We took part the Multilingual Track of all ten languages provided by the CoNLL-2007 shared task organizer <CIT> '
'To set the weights , m , we carried out minimum error rate training <CIT> using BLEU <OTH> as the objective function '
'Use of global features for structured prediction problem has been explored by several NLP applications such as sequential labeling <CIT> and dependency parsing <OTH> with a great deal of success '
'It is important to realize that the output of all mentioned processing steps is noisy and contains plenty of mistakes , since the data has huge variability in terms of quality , style , genres , domains etc , and domain adaptation for the NLP tasks involved is still an open problem <CIT> '
'They are also used for inducing alignments <CIT> '
'In recent work , <CIT> proposed a general framework for including morphological features in a phrase-based SMT system by factoring the representation of words into a vector of morphological features and allowing a phrase-based MT system to work on any of the factored representations , which is implemented in the Moses system '
'2 Architecture of the system The goal of statistical machine translation -LRB- SMT -RRB- is to produce a target sentence e from a source sentence f It is today common practice to use phrases as translation units <CIT> and a log linear framework in order to introduce several models explaining the translation process : e ? ? = argmaxp -LRB- e f -RRB- = argmaxe -LCB- exp -LRB- summationdisplay i ihi -LRB- e , f -RRB- -RRB- -RCB- -LRB- 1 -RRB- The feature functions hi are the system models and the i weights are typically optimized to maximize a scoring function on a development set <OTH> '
'1 Introduction Sentiment analysis have been widely conducted in several domains such as movie reviews , product reviews , news and blog reviews <CIT> '
'Their approaches include the use of a vector-based information retrieval technique <CIT> \/ bin\/bash : line 1 : a : command not found Our do - mains are more varied , which may results in more recognition errors '
'The corpus was aligned with GIZA + + <OTH> and symmetrized with the grow-diag-finaland heuristic <CIT> '
'BLEU <CIT> was devised to provide automatic evaluation of MT output '
'Statistics in linguistics , Oxford : Basil Blackwell </rawString> </citation> <citation valid=\\\''true\\\''> <authors> <author> N Chinchor </author> </authors> <title> Evaluating message understanding systems : an analysis of the third Message Understanding Conference -LRB- MUC-3 </title> <date> 1993 </date> <journal> Computational Linguistics </journal> <volume> 19 </volume> <pages> 409 -- 449 </pages> <marker> Chinchor , 1993 </marker> <rawString> Chinchor , N , et al , 1993 '
'Note that it is straightforward to calculate these expected counts using a variant of the inside-outside algorithm <OTH> applied to the <OTH> dependency-parsing data structures <OTH> for projective dependency structures , or the matrix-tree theorem <CIT> for nonprojective dependency structures '
'Following <CIT> , we consider an anaphoric reference , NPi , correctly resolved if NPi and its closest antecedent are in the same coreference chain in the resulting partition '
'This algorithm adjusts the log-linear weights so that BLEU <CIT> is maximized over a given development set '
'We discriminatively trained our parser in an on-line fashion using a variant of the voted perceptron <CIT> '
'In fact , we found that it doesnt do so badly at all : the bitag HMM estimated by EM achieves a mean 1-to1 tagging accuracy of 40 % , which is approximately the same as the 413 % reported by <CIT> for their sophisticated MRF model '
'2 Motivation and Prior Work While several authors have looked at the supervised adaptation case , there are less -LRB- and especially less successful -RRB- studies on semi-supervised domain adaptation <CIT> '
'There are other types of variations for phrases ; for example , insertion , deletion or substitution of words , and permutation of words such as view point and point of view are such variations <OTH> '
'? ? search engines : <CIT> uses the Altavista web browser , while we consider and combine the frequency information acquired from three web search engines '
'Many methods have been proposed to measure the co-occurrence relation between two words such as 2 <OTH> , mutual information <OTH> , t-test <OTH> , and loglikelihood <CIT> '
'It has been argued that the reliability of a coding schema can be assessed only on the basis of judgments made by naive coders <CIT> '
'to the pair-wise TER alignment described in <CIT> '
'We obtain aligned parallel sentences and the phrase table after the training of Moses , which includes running GIZA + + <OTH> , grow-diagonal-final symmetrization and phrase extraction <CIT> '
'From the above discussion , we can see that traditional tree sequence-based method uses single tree as translation input while the forestbased model uses single sub-tree as the basic translation unit that can only learn tree-to-string <CIT> rules '
'Our baseline method for ambiguity resolution is the Collins parser as implemented by Bikel <CIT> '
'We then built separate English-to-Spanish and Spanish-to-English directed word alignments using IBM model 4 <OTH> , combined them using the intersect + grow heuristic <CIT> , and extracted phrase-level translation pairs of maximum length 7 using the alignment template approach <CIT> '
'For instance , both Pang and Lee <OTH> and <CIT> <OTH> consider the thumbs up\/thumbs down decision : is a film review positive or negative ? '
'Binarizing the syntax trees for syntax-based machine translation is similar in spirit to generalizing parsing models via markovization <CIT> '
'4 Options from the Translation Table Phrase-based statistical machine translation methods acquire their translation knowledge in form of large phrase translation tables automatically from large amounts of translated texts <CIT> '
'For example , <CIT> collected reviews from a movie database and rated them as positive , negative , or neutral based on the rating -LRB- eg , number of stars -RRB- given by the reviewer '
'The earliest work in this direction are those of <OTH> , <OTH> , <OTH> , <OTH> , <OTH> and <CIT> '
'<CIT> established that it is important to tune -LRB- the trade-off between Precision and Recall -RRB- to maximize performance '
'Moreover , under this view , SMT becomes quite similar to sequential natural language annotation problems such as part-of-speech tagging and shallow parsing , and the novel training algorithm presented in this paper is actually most similar to work on training algorithms presented for these task , eg the on-line training algorithm presented in <OTH> and the perceptron training algorithm presented in <CIT> '
'Feature weights vector are trained discriminatively in concert with the language model weight to maximize the BLEU <OTH> automatic evaluation metric via Minimum Error Rate Training -LRB- MERT -RRB- <CIT> '
'<CIT> and Wiebe -LRB- 2000 -RRB- focused on learning adjectives and adjectival phrases and Wiebe et al '
'A number of alignment techniques have been proposed , varying from statistical methods <CIT> to lexical methods <OTH> '
'Even though there are some studies that compare the results from statistically computed association measures with word association norms from psycholinguistic experiments <CIT> there has not been any research on the usage of a digital , network-based dictionary reflecting the organization of the mental lexicon to our knowledge '
'Finally , we use as a feature the mappings produced in <CIT> of WordNet senses to Oxford English Dictionary senses '
'3 Semantic Representation 31 The Need for Dependencies Perhaps the most common representation of text for assessing content is Bag-Of-Words or Bag-of-NGrams <CIT> '
'First , we adopt an ONTOLOGICALLY PROMISCUOUS representation <CIT> that includes a wide variety of types of entities '
'For each training direction , we run GIZA + + <CIT> , specifying 5 iterations of Model 1 , 4 iterations of the HMM model <OTH> , and 4 iterations of Model 4 '
'To set the weight vector w , we train twenty averaged perceptrons <CIT> on different shuffles of data drawn from sections 0221 of the Penn Treebank '
'In the refined model 2 <CIT> alignment probabilities a -LRB- ilj , l , m -RRB- are included to model the effect that the position of a word influences the position of its translation '
'6 Conclusions and Future Directions In previous work , statistical NLP computation over large corpora has been a slow , of ine process , as in KNOWITALL <OTH> and also in PMI-IR applications such as sentiment classi cation <CIT> '
'The first solution might also introduce errors elsewhere As <CIT> already noted : ` While this automatic derivation process introduced a small percentage of errors on its own , it was the only practical way both to provide the amount of training data required and to allow for fully-automatic testing '' '
'1 To train their system , R&M used a 200k-word chunk of the Penn Treebank Parsed Wall Street Journal <OTH> tagged using a transformation-based tagger <OTH> and extracted base noun phrases from its parses by selecting noun phrases that contained no nested noun phrases and further processing the data with some heuristics -LRB- like treating the possessive marker as the first word of a new base noun phrase -RRB- to flatten the recursive structure of the parse '
'This can either be semi-supervised parsing , using both annotated and unannotated data <CIT> or unsupervised parsing , training entirely on unannotated text '
'In <OTH> , as well as other similar works <CIT> , only left-toright search was employed '
'This iterative optimiser , derived from a word disambiguation technique <CIT> , finds the nearest local maximum in the lexical cooccurrence network from each concept seed '
'This model is related to the averaged perceptron algorithm of <CIT> '
'a22 a14 is the sufficient statistic of a16 a14 Then , we can rewrite a2a24a3 a10a27 a42a7 a25 as : a5a7a6a9a8a11a10 a23 a3 a10 a7 a15 a27 a25a18a17a26a25 a12a28a27 a5a7a6a29a8a30a10 a23 a3 a10 a7 a15 a27 a25a18a17 3 Loss Functions for Label Sequences Given the theoretical advantages of discriminative models over generative models and the empirical support by <CIT> , and that CRFs are the state-of-the-art among discriminative models for label sequences , we chose CRFs as our model , and trained by optimizing various objective functions a31 a3 a10a36 a25 with respect to the corpus a36 The application of these models to the label sequence problems vary widely '
'Pustejovsky confronted with the problem of automatic acquisition more extensively in <OTH> '
'PropBank encodes propositional information by adding a layer of argument structure annotation to the syntactic structures of the Penn Treebank <CIT> '
'However , while similarity measures -LRB- such as WordNet distance or Lins similarity metric -RRB- only detect cases of semantic similarity , association measures -LRB- such as the ones used by Poesio et al , or by Garera and Yarowsky -RRB- also find cases of associative bridg497 Lin98 RFF TheY TheY : G2 PL03 Land -LRB- country\/state\/land -RRB- Staat Staat Kemalismus Regierung Kontinent state state Kemalism government continent Stadt Stadt Bauernfamilie Prasident Region city city agricultural family president region Region Landesregierung Bankgesellschaft Dollar Stadt region country government banking corporation dollar city Bundesrepublik Bundesregierung Baht Albanien Staat federal republic federal government Baht Albania state Republik Gewerkschaft Gasag Hauptstadt Bundesland republic trade union -LRB- a gas company -RRB- capital state Medikament -LRB- medical drug -RRB- Arzneimittel Pille RU Patient Arzneimittel pharmaceutical pill -LRB- a drug -RRB- patient pharmaceutical Praparat Droge Abtreibungspille Arzt Lebensmittel preparation drug -LRB- non-medical -RRB- abortion pill doctor foodstuff Pille Praparat Viagra Pille Praparat pill preparation Viagra pill preparation Hormon Pestizid Pharmakonzern Behandlung Behandlung hormone pesticide pharmaceutical company treatment treatment Lebensmittel Lebensmittel Praparat Abtreibungspille Arznei foodstuff foodstuff preparation abortion pill drug highest ranked words , with very rare words removed : RU 486 , an abortifacient drug Lin98 : Lins distributional similarity measure <OTH> RFF : Geffet and Dagans Relative Feature Focus measure <OTH> TheY : association measure introduced by Garera and Yarowsky <OTH> TheY : G2 : similar method using a log-likelihood-based statistic <CIT> this statistic has a preference for higher-frequency terms PL03 : semantic space association measure proposed by Pado and Lapata <OTH> Table 1 : Similarity and association measures : most similar items ing like 1a , b ; the result of this can be seen in table -LRB- 2 -RRB- : while the similarity measures -LRB- Lin98 , RFF -RRB- list substitutable terms -LRB- which behave like synonyms in many contexts -RRB- , the association measures -LRB- Garera and Yarowskys TheY measure , Pado and Lapatas association measure -RRB- also find non-compatible associations such as countrycapital or drugtreatment , which is why they are commonly called relationfree '
'Jiao et al propose semi-supervised conditional random fields <CIT> that try to maximize the conditional log-likelihood on the training data and simultaneously minimize the conditional entropy of the class labels on the unlabeled data '
'<CIT> report extracting database records by learning record field compatibility '
'Unfortunately , a counterexample illustrated in <OTH> shows that the max function does not produce valid kernels in general '
'2 Detecting Discourse-New Definite Descriptions 21 Vieira and Poesio Poesio and Vieira <OTH> carried out corpus studies indicating that in corpora like the Wall Street Journal portion of the Penn Treebank <CIT> , around 52 % of DDs are discourse-new <OTH> , and another 15 % or so are bridging references , for a total of about 66-67 % firstmention '
'The distinction between lexical and relational similarity for word pair comparison is recognized by <CIT> -LRB- hecallstheformer attributional similarity -RRB- , though the methods he presents focus on relational similarity '
'The POS disambiguation has usually been performed by statistical approaches mainly using hidden markov model -LRB- HMM -RRB- -LRB- <CIT> et al , 1992 ; Kupiec '
'As a baseline model we used a maximum entropy tagger , very similar to the one described in <CIT> '
'We assign tags of part-of-speech -LRB- POS -RRB- to the words with MXPOST that adopts the Penn Treebank tag set <CIT> '
'Such coarse-grained inventories can be produced manually from scratch <OTH> or by automatically relating <OTH> or clustering <CIT> existing word senses '
'Given a contextual word cw that occurs in the paragraphs of bc , a log-likelihood ratio -LRB- G2 -RRB- test is employed <CIT> , which checks if the distribution of cw in bc is similar to the distribution of cw in rc ; p -LRB- cw bc -RRB- = p -LRB- cw rc -RRB- -LRB- null hypothesis -RRB- '
'In this paper we use the so-called Model 4 from <CIT> '
'We would expect the opposite effect with hand-aligned data <CIT> '
'Extensions to Hiero Several authors describe extensions to Hiero , to incorporate additional syntactic information <CIT> , or to combine it with discriminative latent models <OTH> '
'The other form of hybridization ? ? a statistical MT model that is based on a deeper analysis of the syntactic 33 structure of a sentence ? ? has also long been identified as a desirable objective in principle -LRB- consider <CIT> -RRB- '
'272 Similarity-based estimation was first used for language modeling in the cooccurrence smoothing method of Essen and Steinbiss <OTH> , derived from work on acoustic model smoothing by Sugawara et al '
'Following the setup in <CIT> , we initialize the transition and emission distributions to be uniform with a small amount of noise , and run EM and VB for 1000 iterations '
'Our method uses assumptions similar to <CIT> et al 1996 but is naturally suitable for distributed parallel computations '
'The agreement on identifying the boundaries of units , using the statistic discussed in <CIT> , was = 9 -LRB- for two annotators and 500 units -RRB- ; the agreement on features -LRB- 2 annotators and at least 200 units -RRB- was as follows : UTYPE : = 76 ; VERBED : = 9 ; FINITE : = 81 '
'In comparison we introduce 28 several metrics coefficients reported in Albrecht and Hwa <OTH> including smoothed BLEU <OTH> , METEOR <OTH> , HWCM <CIT> , and the metric proposed in Albrecht and Hwa <OTH> using the full feature set '
'This is one manifestation of what is commonly referred to as the data sparseness problem , and was discussed by <CIT> as a side-effect of specificity '
'Techniques for weakening the independence assumptions made by the IBM models 1 and 2 have been proposed in recent work <CIT> '
'C3BTC5 and CCCDCA were used in <OTH> and <CIT> , respectively '
'In this work we will use structured linear classifiers <CIT> '
'This is the best automatically learned part-of-speech tagging result known to us , representing an error reduction of 44 % on the model presented in <CIT> , using the same data splits , and a larger error reduction of 121 % from the more similar best previous loglinear model in Toutanova and Manning <OTH> '
'The template we use here is similar to <CIT> , but we have added extra context words before the X and after the Y Our morphological processing also differs from <CIT> '
'The algorithm employs the OpenNLP MaxEnt implementation of the maximum entropy classification algorithm <CIT> to develop word sense recognition signatures for each lemma which predicts the most likely sense for the lemma according to the context in which the lemma occurs '
'Alternatively , order is modelled in terms of movement of automatically induced hierarchical structure of sentences <CIT> '
'Parameters used to calculate P -LRB- D -RRB- are trained using MER training <CIT> on development data '
'Chiang <OTH> distinguishes statistical MT approaches that are syntactic in a formal sense , going beyond the nite-state underpinnings of phrasebased models , from approaches that are syntactic in a linguistic sense , ie taking advantage of a priori language knowledge in the form of annotations derived from human linguistic analysis or treebanking1 The two forms of syntactic modeling are doubly dissociable : current research frameworks include systems that are nite state but informed by linguistic annotation prior to training -LRB- eg , <CIT> -RRB- , and also include systems employing contextfree models trained on parallel text without bene t of any prior linguistic analysis -LRB- eg '
'This can be done in a supervised <CIT> , a semi-supervised <CIT> or a fully unsupervised way <OTH> '
'Equation -LRB- 3 -RRB- reads If the target noun appears , then it is distinguished by the majority The log-likelihood ratio <CIT> decides in which order rules are applied to the target noun in novel context '
'Statistic-based algorithms based on Belief Network <OTH> such as Hidden-MarkovModel -LRB- HMM -RRB- <OTH> <OTH> , Lexicalized HMM <OTH> and Maximal-Entropy model <CIT> use the statistical information of a manually tagged corpus as background knowledge to tag new sentences '
'One way of resolving query ambiguities is to use the statistics , such as mutual information <CIT> , to measure associations of query terms , on the basis of existing corpora <OTH> '
'The wn : : similarity package <CIT> to compute the Jiang & Conrath -LRB- J&C -RRB- distance <OTH> as in <OTH> '
'<OTH> propose using a statistical word alignment algorithm as a more robust way of aligning -LRB- monolingual -RRB- outputs into a confusion network for system com2 <CIT> construct lattices over paraphrases using an iterative pairwise multiple sequence alignment -LRB- MSA -RRB- algorithm '
'One of the first large scale hand tagging efforts is reported in <OTH> , where a subset of the Brown corpus was tagged with WordNet July 2002 , pp '
'42 Experiments To build all alignment systems , we start with 5 iterations of Model 1 followed by 4 iterations of HMM <OTH> , as implemented in GIZA + + <CIT> '
'The usual recall and precision metrics -LRB- eg , how many of the interesting bits of information were detected , and how many of the found bits were actually correct -RRB- require either a test corpus previously annotated with the required information , or manual evaluation <CIT> '
'The CRF tagger was implemented in MALLET <OTH> using the original feature templates from <CIT> '
'Given a set of terms with unknown sentiment orientation , <CIT> then uses the PMI-IR algorithm <CIT> to issue queries to the web and determine , for each of these terms , its pointwise mutual information -LRB- PMI -RRB- with the two seed words across a large set of documents '
'This is similartothegraphconstructionmethodof <CIT> and Rao et al '
'We distinguish two main approaches to domain adaptation that have been addressed in the literature <CIT> : supervised and semi-supervised '
'Smadja,Frank.(1993).'
'The experimental results in <CIT> show a negative impact on the parsing accuracy from too long dependency relation '
'k - ~ P -LRB- A -RRB- P -LRB- E -RRB- -LRB- 3 -RRB- 1P -LRB- E -RRB- <CIT> suggests that the units over which the kappa statistic is computed affects the outcome '
'Among the chunk types , NP chunking is the first to receive the attention <CIT> , than other chunk types , such as VP and PP chunking <OTH> '
'This model shares some similarities with the stochastic inversion transduction grammars -LRB- SITG -RRB- presented by Wu in <CIT> '
'-LRB- <CIT> makes a similar point , noting that for reviews , the whole is not necessarily the sum of the parts '' -RRB- '
'Identifying transliteration pairs is an important component in many linguistic applications which require identifying out-of-vocabulary words , such as machine translation and multilingual information retrieval <CIT> '
'<CIT> and Chan et al '
'Based on these grammars , a great number of SMT models have been recently proposed , including string-to-string model -LRB- Synchronous FSG -RRB- <CIT> , tree-to-string model -LRB- TSG-string -RRB- <OTH> , string-totree model -LRB- string-CFG\/TSG -RRB- <OTH> , tree-to-tree model -LRB- Synchronous CFG\/TSG , Data-Oriented Translation -RRB- <OTH> and so on '
'We use a standard maximum entropy classifier <CIT> implemented as part of MALLET <OTH> '
'8412 only PTB -LRB- baseline -RRB- 8358 1st <OTH> 8342 2nd <CIT> 8338 3rd <OTH> 8308 third row lists the three highest scores of the domain adaptation track of the CoNLL 2007 shared task '
'(Johnson [1997] notes that this structure has a higher probability than the correct, flat structure, given counts taken from the treebank for a standard PCFG).'
'We used a loglinear model with no Markov dependency between adjacent tags ,3 and trained the parameters of the model with the perceptron algorithm , with averaging to control for over-training <CIT> '
'In Turneys work , the co-occurrence is considered as the appearance in the same window <CIT> '
'Named entities also pose another problem with the <CIT> coreference model ; since it models only the heads of NPs , it will fail to resolve some references to named entities : -LRB- Ford Motor Co , Ford -RRB- , while erroneously merging others : -LRB- Ford Motor Co , Lockheed Martin Co -RRB- '
'Translation rules can : look like phrase pairs with syntax decoration : NPB -LRB- NNP -LRB- prime -RRB- NNP -LRB- minister -RRB- NNP -LRB- keizo -RRB- NNP -LRB- obuchi -RRB- -RRB- BUFDFKEUBWAZ carry extra contextual constraints : VP -LRB- VBD -LRB- said -RRB- x0 : SBAR-C -RRB- DKx0 -LRB- according to this rule , DK can translate to said only if some Chinese sequence to the right ofDK is translated into an SBAR-C -RRB- be non-constituent phrases : VP -LRB- VBD -LRB- said -RRB- SBAR-C -LRB- IN -LRB- that -RRB- x0 : S-C -RRB- -RRB- DKx0 VP -LRB- VBD -LRB- pointed -RRB- PRT -LRB- RP -LRB- out -RRB- -RRB- x0 : SBAR-C -RRB- DXGPx0 contain non-contiguous phrases , effectively phrases with holes : PP -LRB- IN -LRB- on -RRB- NP-C -LRB- NPB -LRB- DT -LRB- the -RRB- x0 : NNP -RRB- -RRB- NN -LRB- issue -RRB- -RRB- -RRB- -RRB- GRx0 EVABG6 PP -LRB- IN -LRB- on -RRB- NP-C -LRB- NPB -LRB- DT -LRB- the -RRB- NN -LRB- issue -RRB- -RRB- x0 : PP -RRB- -RRB- GRx0 EVEVABABG6 be purely structural -LRB- no words -RRB- : S -LRB- x0 : NP-C x1 : VP -RRB- x0 x1 re-order their children : NP-C -LRB- NPB -LRB- DT -LRB- the -RRB- x0 : NN -RRB- PP -LRB- IN -LRB- of -RRB- x1 : NP-C -RRB- -RRB- x1 DFx0 Decoding with this model produces a tree in the target language , bottom-up , by parsing the foreign string using a CYK parser and a binarized rule set <CIT> '
'7 Model Structure In our statistical model , trees are generated according to a process similar to that described in <CIT> '
'For the constituent-based models , constituent information was obtained from the output of <CIT> for English and Dubeys parser -LRB- 2004 -RRB- for German '
'This finding has been previously reported , among others , in <CIT> '
'In order increase the likelihood that 909 only true paraphrases were considered as phraselevel alternations for an example , extracted sentences were clustered using complete-link clustering using a technique proposed in <CIT> '
'As reported in <CIT> , parameter averaging can effectively avoid overfitting '
'Several representations to encode region information are proposed and examined <CIT> '
'One important application of bitext maps is the construction of translation lexicons <OTH> and , as discussed , translation lexicons are an important information source for bitext mapping '
'This method is described hereafter , while the subsequent steps , that use deeper -LRB- rulebased -RRB- levels of knowledge , are implemented into the ARIOSTO_LEX lexical learning system , described in <OTH> '
'This can be the base of a principled method for detecting structural contradictions <CIT> '
'6 Related Work Several works attempt to extend WordNet with additional lexical semantic information <CIT> '
'In order to improve sentence-level evaluation performance , several metrics have been proposed , including ROUGE-W , ROUGE-S <OTH> and METEOR <CIT> '
'Besides the the case-sensitive BLEU-4 <CIT> used in the two experiments , we design another evaluation metrics Reordering Accuracy -LRB- RAcc -RRB- for forced decoding evaluation '
'2 Syntactic-oriented evaluation metrics We investigated the following metrics oriented on the syntactic structure of a translation output : POSBLEU The standard BLEU score <CIT> calculated on POS tags instead of words ; POSP POS n-gram precision : percentage of POS ngrams in the hypothesis which have a counterpart in the reference ; POSR Recall measure based on POS n-grams : percentage of POS n-grams in the reference which are also present in the hypothesis ; POSF POS n-gram based F-measure : takes into account all POS n-grams which have a counter29 part , both in the reference and in the hypothesis '
'4 Building Noun Similarity Lists A lot of work has been done in the NLP community on clustering words according to their meaning in text <CIT> '
'In other words , -LRB- 4b -RRB- can be used in substitution of -LRB- 4a -RRB- , whereas -LRB- 5b -RRB- can not , so easily 41n <CIT> , a value of K between 8 and I indicates good agreement ; a value between 6 and 8 indicates some agreement '
'For example , <CIT> have studied synchronous context free grammar '
'<CIT> Peter F Brown , Vincent J Della Pietra , Petere V deSouza , Jenifer C Lai , and Robert L Mercer '
'Table 1 reports values for the Kappa -LRB- K -RRB- coefficient of agreement <CIT> for Forward and Backward Functions 6 The columns in the tables read as follows : if utterance Ui has tag X , do coders agree on the subtag ? '
'When we have a junction tree for each document , we can efficiently perform belief propagation in order to compute argmax in Equation -LRB- 1 -RRB- , or the marginal probabilities of cliques and labels , necessary for the parameter estimation of machine learning classifiers , including perceptrons <CIT> , and maximum entropy models <OTH> '
'<OTH> -RRB- , concordancing for bilingual lexicography <OTH> , computerassisted language learning , corpus linguistics -LRB- Melby '
'In particular , previous work <OTH> has investigated the use of Markov random fields -LRB- MRFs -RRB- or log-linear models as probabilistic models with global features for parsing and other NLP tasks '
'The model consists of a set of word-pair parameters p -LRB- t -LRB- s -RRB- and position parameters p -LRB- j -LRB- i , \/ -RRB- ; in model 1 -LRB- IBM1 -RRB- the latter are fixed at 1 \/ -LRB- 1 + 1 -RRB- , as each position , including the empty position 0 , is considered equally likely to contain a translation for w Maximum likelihood estimates for these parameters can be obtained with the EM algorithm over a bilingual training corpus , as described in <CIT> '
'We perform minimum error rate training <CIT> to tune the feature weights for the log-linear modeltomaximizethesystemssBLEUscoreonthe development set '
'73 ID Participant BBN-COMBO BBN system combination (Rosti et al., 2008) CMU-COMBO Carnegie Mellon University system combination (Jayaraman and Lavie, 2005) CMU-GIMPEL Carnegie Mellon University Gimpel (Gimpel and Smith, 2008) CMU-SMT Carnegie Mellon University SMT (Bach et al., 2008) CMU-STATXFER Carnegie Mellon University Stat-XFER (Hanneman et al., 2008) CU-TECTOMT Charles University TectoMT (Zabokrtsky et al., 2008) CU-BOJAR Charles University Bojar (Bojar and Hajic, 2008) CUED Cambridge University (Blackwood et al., 2008) DCU Dublin City University (Tinsley et al., 2008) LIMSI LIMSI (Dechelotte et al., 2008) LIU Linkoping University (Stymne et al., 2008) LIUM-SYSTRAN LIUM / Systran (Schwenk et al., 2008) MLOGIC Morphologic (Novak et al., 2008) PCT a commercial MT provider from the Czech Republic RBMT16 Babelfish, Lingenio, Lucy, OpenLogos, ProMT, SDL (ordering anonymized) SAAR University of Saarbruecken (Eisele et al., 2008) SYSTRAN Systran (Dugast et al., 2008) UCB University of California at Berkeley (Nakov, 2008) UCL University College London (Wang and Shawe-Taylor, 2008) UEDIN University of Edinburgh (Koehn et al., 2008) UEDIN-COMBO University of Edinburgh system combination (Josh Schroeder) UMD University of Maryland (Dyer, 2007) UPC Universitat Politecnica de Catalunya, Barcelona (Khalilov et al., 2008) UW University of Washington (Axelrod et al., 2008) XEROX Xerox Research Centre Europe (Nikoulina and Dymetman, 2008) Table 2: Participants in the shared translation task.'
'The candidates of unknown words can be generated by heuristic rules <OTH> or statistical word models which predict the probabilities for any strings to be unknown words <OTH> '
'For each differently tokenized corpus , we computed word alignments by a HMM translation model <OTH> and by a word alignment refinement heuristic of grow-diagfinal <CIT> '
'On the other hand , purely statistical systems <CIT> extract discriminating MWUs from text corpora by means of association measure regularities '
'The chunker is trained on the answer side of the Training corpus in order to learn 2 and 3word collocations , defined using the likelihood ratio of <CIT> '
'We then tested the best models for each vocabulary size on the testing set4 Standard measures of performance are shown in table 15 3We used a publicly available tagger <CIT> to provide the tags used in these experiments , rather than the handcorrected tags which come with the corpus '
'First , we can let the number of nonterminals grow unboundedly , as in the Infinite PCFG , where the nonterminals of the grammar can be indefinitely refined versions of a base PCFG <CIT> '
'The tree-to-string model <CIT> views the translation as a structure mapping process , which first breaks the source syntax tree into many tree fragments and then maps each tree fragment into its corresponding target translation using translation rules , finally combines these target translations into a complete sentence '
'It is important because a wordaligned corpus is typically used as a first step in order to identify phrases or templates in phrase-based Machine Translation <OTH> , <OTH> , -LRB- <CIT> et al , 2003 , sec '
'Parse Parse score from Model 2 of the statistical parser <CIT> , normalized by the number of words '
'But Koehn , <CIT> find that phrases longer than three words improve performance little for training corpora of up to 20 million words , suggesting that the data may be too sparse to learn longer phrases '
'To do this , we first identify initial phrase pairs using the same criterion as previous systems <CIT> : Definition 1 '
'The first is a novel stochastic search strategy that appears to make better use of <CIT> s algorithm for finding the global minimum along any given search direction than either coordinate descent or Powells method '
'However , as <CIT> do not propose any evaluation of which clustering algorithm should be used , we experiment a set of clustering algorithms and present the comparative results '
'Transformation-based error-driven learning has been applied to a number of natural language problems , including part of speech tagging , prepositional phrase attachment disambiguation , speech generation and syntactic parsing <CIT> '
'To measure interannotator agreement , we compute Cohens Kappa <CIT> from the two sets of annotations , obtaining a Kappa value of only 043 '
'Many 412 <CIT> Similarity of Semantic Relations researchers have argued that metaphor is the heart of human thinking <OTH> '
'We use Entropy Regularization -LRB- ER -RRB- <CIT> to leverage unlabeled instances7 We weight the ER term by choosing the best8 weight in -LCB- 103,102,101,1,10 -RCB- multiplied by # labeled # unlabeled for each data set and query selection method '
'<OTH> applied to the output of the reranking parser of Charniak and Johnson <OTH> , whereas in BE -LRB- in the version presented here -RRB- dependencies are generated by the Minipar parser <CIT> '
'Traditionally , generative word alignment models have been trained on massive parallel corpora <CIT> '
'There are other approaches in which the generation grammars are extracted semiautomatically <OTH> or automatically -LRB- such as HPSG <OTH> , LFG <CIT> and CCG <OTH> -RRB- '
'<CIT> 1993 -RRB- or else -LRB- as with mutual information -RRB- eschew significance testing in favor of a generic information-theoretic approach '
'We automatically converted the phrase structure output of the <CIT> parser into the syntactic dependency representation used by our syntactic realizer , RealPro <OTH> '
'3 We then run <CIT> , using just the sentence pairs where parsing succeeds with a negative log likelihood below 200 '
'On the one hand using 1 human reference with uniform results is essential for our methodology , since it means that there is no more trouble with Recall <CIT> a systems ability to avoid under-generation of N-grams can now be reliably measured '
'2 Linguistic and Context Features 21 Non-terminal Labels In the original string-to-dependency model <OTH> , a translation rule is composed of a string of words and non-terminals on the source side and a well-formed dependency structure on the target side '
'Results in terms of word-error-rate -LRB- WER -RRB- and BLEU score <CIT> are reported in Table 4 for those sentences that contain at least one unknown word '
'This was used , for example , by <CIT> in information extraction , and by <OTH> in POS tagging '
'5 Combining In-Domain and Out-of-Domain Data for Training In this section , we will first introduce the AUGMENT technique of <CIT> , before showing the performance of our WSD system with and without using this technique '
'<OTH> use hand-coded slot-filling rules to determine the semantic roles of the arguments of a nominalization '
'Previous work on linguistic annotation pipelines <CIT> has enforced consistency from one stage to the next '
'<CIT> suggested comparing the frequency of phrase co-occurrences with words predetermined by the sentiment lexicon '
'<CIT> has called attention to the log-likelihood ratio , G 2 , as appropriate for the analysis of such contingency tables , especially when such contingency tables concern very low frequency words '
'The original <CIT> publication evaluated their NP chunker on two data sets , the second holding a larger amount of training data -LRB- Penn Treebank sections 02-21 -RRB- while using 00 as test data '
'In addition to precision and recall , we also evaluate the Bleu score <CIT> changes before and after applying our measure word generation method to the SMT output '
'We used a bottom-up , CKY-style decoder that works with binary xRs rules obtained via a synchronous binarization procedure <CIT> '
'Alternatively , one can train them with respect to the final translation quality measured by an error criterion <CIT> '
'53 Performance of Taxonomy Induction In this section , we compare the following automatic taxonomy induction systems : HE , the system by Hearst <OTH> with 6 hypernym patterns ; GI , the system by Girju et al '
'Examples are Andersen <OTH> , <CIT> , Sun et al '
'To address this , standard measures like precision and recall could be used , as in some previous research <CIT> '
'Many statistical translation models <CIT> try to model word-to-word correspondences between source and target words '
'The models are trained using the Margin Infused Relaxed Algorithm or MIRA <OTH> instead of the standard minimum-error-rate training or MERT algorithm <CIT> '
'Phrases are then extracted from the word alignments using the method described in <CIT> '
'<CIT> is the first , to the best of our knowledge , to raise the issue of a unified approach '
'IBM constraints <OTH> , the lexical word reordering model <OTH> , and inversion transduction grammar -LRB- ITG -RRB- constraints <CIT> belong to this type of approach '
'The effectiveness of these features for recognition of discourse relations has been previously shown by <CIT> '
'However , such methods require the existence of either a parallel corpus\/machine translation engine for projecting\/translating annotations\/lexica from a resource-rich language to the target language <OTH> , or a domain that is similar enough to the target domain <CIT> '
'Given an input sentence x , the correct output segmentation F -LRB- x -RRB- satisfies : F -LRB- x -RRB- = argmax yGEN -LRB- x -RRB- Score -LRB- y -RRB- where GEN -LRB- x -RRB- denotes the set of possible segmentations for an input sentence x , consistent with notation from <CIT> '
'For the statistics-based approaches , <CIT> developed a statistics-based method for automatically identifying existential definite NPs which are non-anaphoric '
'A large database of human judgments might also be useful as an objective function for minimum error rate training <CIT> or in other system development tasks '
'During evaluation two performance metrics , BLEU <CIT> and NIST , were computed '
'-LRB- 1 -RRB- Here , the candidate generator gen -LRB- s -RRB- enumerates candidates of destination -LRB- correct -RRB- strings , and the scorer P -LRB- t s -RRB- denotes the conditional probability of the string t for the given s The scorer was modeled by a noisy-channel model <OTH> and maximum entropy framework <CIT> '
'Meanwhile , translation grammars have grown in complexity from simple inversion transduction grammars <OTH> to general tree-to-string transducers <CIT> and have increased in size by including more synchronous tree fragments <CIT> '
'220 <CIT> ; they can overlap5 Additionally , since phrase features can be any function of words and alignments , we permit features that consider phrase pairs in which a target word outside the target phrase aligns to a source word inside the source phrase , as well as phrase pairs with gaps <OTH> '
'There are three major types of models : Heuristic models as in <OTH> , generative models as the IBM models <CIT> and discriminative models <OTH> '
'<OTH> invented heuristic symmetriza57 FRENCH\/ENGLISH ARABIC\/ENGLISH SYSTEM F-MEASURE -LRB- = 04 -RRB- BLEU F-MEASURE -LRB- = 01 -RRB- BLEU GIZA + + 735 3063 758 5155 <OTH> 741 3140 791 5289 LEAF UNSUPERVISED 745 723 LEAF SEMI-SUPERVISED 763 3186 845 5434 Table 3 : Experimental Results tion of the output of a 1-to-N model and a M-to-1 model resulting in a M-to-N alignment , this was extended in <OTH> '
'The limited contexts used in this model are similar to the previous methods <CIT> '
'Distributional measures of distance , such as those proposed by <CIT> , quantify how similar the two sets of contexts of a target word pair are '
'GIZA + + <CIT> , an implementation of the IBM <OTH> and HMM -LRB- ? -RRB- '
'2 Block Orientation Bigrams This section describes a phrase-based model for SMT similar to the models presented in <CIT> '
'A perceptron algorithm gives 9711 % <CIT> '
'The f are trained using a held-out corpus using maximum BLEU training <CIT> '
'Standard CI Model 1 training , initialised with a uniform translation table so that t -LRB- ejf -RRB- is constant for all source\/target word pairs -LRB- f , e -RRB- , was run on untagged data for 10 iterations in each direction <CIT> '
'According to the document , it is the output of Ratnaparkhis tagger <CIT> '
'1 Introduction Current methods for large-scale information extraction take advantage of unstructured text available from either Web documents <CIT> or , more recently , logs of Web search queries <OTH> to acquire useful knowledge with minimal supervision '
'6 The Experimental Results We used the Penn Treebank <CIT> to perform empirical experiments on this parsing model '
'Given a source sentence f , the preferred translation output is determined by computing the lowest-cost derivation -LRB- combination of hierarchical and glue rules -RRB- yielding f as its source side , where the cost of a derivation R1 Rn with respective feature vectors v1 , , vn Rm is given by msummationdisplay i = 1 i nsummationdisplay j = 1 -LRB- vj -RRB- i Here , 1 , , m are the parameters of the loglinear model , which we optimize on a held-out portion of the training set <OTH> using minimum-error-rate training <CIT> '
'The dependency trees induced when each rewrite rule in an i-th order LCFRS distinguish a unique head can similarly be characterized by being of gap-degree i , so that i is the maximum number of gaps that may appear between contiguous substrings of any subtree in the dependency tree <CIT> '
'This improvement is close to that of one sense per discourse <CIT> -LRB- improvement ranging from 13 % to 17 % -RRB- , which seems to be a sensible upper bound of the proposed method '
'Besides precision , recall and -LRB- balanced -RRB- F-measure , we also include an F-measure variant strongly biased towards recall -LRB- # 0B = 01 -RRB- , which <CIT> found to be best to tune their LEAF aligner for maximum MT accuracy '
'1 Introduction Sentiment detection and classification has received considerable attention recently <CIT> '
'4 -RRB- , it constitutes a bijection between source and target sentence positions , since the intersecting alignments are functions according to their definition in <CIT> 3 '
'In fact , in <CIT> it was shown that this neural network can be viewed as a coarse approximation to the corresponding ISBN model '
'The evaluation metric is case-sensitive BLEU-4 <CIT> '
'To this end , we adopt techniques from statistical machine translation <CIT> and use statistical alignment to learn the edit patterns '
'Indeed , in the II scenario , <CIT> reported no improvement of the base parser for small -LRB- 500 sentences , in the first paper -RRB- and large -LRB- 40K sentences , in the last two papers -RRB- seed datasets respectively '
'The basic phrase-based model is an instance of the noisy-channel approach <CIT> '
'Here , ppicker shows the accuracy when phrases are extracted by using the N-best phrase alignment method described in Section 41 , while growdiag-final shows the accuracy when phrases are extracted using the standard phrase extraction algorithm described in <CIT> '
'The word sense disambiguation method proposed in <CIT> can also be viewed as a kind of co-training '
'These joint counts are estimated using the phrase induction algorithm described in <OTH> , with symmetrized word alignments generated using IBM model 2 <CIT> '
'have been proposed <CIT> '
'Also related are the areas of word alignment for machine translation <OTH> , induction of translation lexicons <OTH> , and cross-language annotation projections to a second language <CIT> '
'In Table 6 we report our results , together with the state-of-the-art from the ACL wiki5 and the scores of <CIT> -LRB- PairClass -RRB- and from Amac Herdagdelens PairSpace system , that was trained on ukWaC '
'The features used in this study are : the length of t ; a single-parameter distortion penalty on phrase reordering in a , as described in <CIT> ; phrase translation model probabilities ; and 4-gram language model probabilities logp -LRB- t -RRB- , using Kneser-Ney smoothing as implemented in the SRILM toolkit '
'We have computed the BLEU score -LRB- accumulated up to 4-grams -RRB- <OTH> , the NIST score -LRB- accumulated up to 5-grams -RRB- <OTH> , the General Text Matching -LRB- GTM -RRB- F-measure -LRB- e = 1,2 -RRB- <OTH> , and the METEOR measure <CIT> '
'As <CIT> , we adopted an evaluation of mutual information as a cohesion measure of each cooccurrence '
'The last two counts -LRB- CAUS and ANIM -RRB- were performed on a 29-million word parsed corpus -LRB- gall Street Journal 1988 , provided by Michael Collins <CIT> -RRB- '
'The tagger used is thus one that does not need tagged and disambiguated material to be trained on , namely the XPOST originally constructed at Xerox Parc <CIT> '
'We perform word alignment using GIZA + + <CIT> , symmetrize the alignments using the grow-diag-final-and heuristic , and extract phrases up to length 3 '
'Congress of the Italian Association for Artificial Intelligence , Palermo , 1991 B Boguraev , Building a Lexicon : the Contribution of Computers , IBM Report , TJ Watson Research Center , 1991 M Brent , Automatic Aquisition of Subcategorization frames from Untagged Texts , in <OTH> N Calzolari , R Bindi , Acquisition of Lexical Information from Corpus , in <OTH> K W <CIT> , P Hanks , Word Association Norms , Mutual Information , and Lexicography , Computational Linguistics , vol '
'4 Maximum Entropy To explain our method , we l -RRB- riefly des -LRB- : ribe the con -LRB- : ept of maximum entrol -RRB- y Recently , many al -RRB- lnoaches l -RRB- ased on the maximum entroi -RRB- y lnodel have t -RRB- een applied to natural language processing <CIT> '
'These are most directly presented in <CIT> '
'Modeling reordering as the inversion in order of two adjacent blocks is similar to the approach taken by the Inverse Transduction Model -LRB- ITG -RRB- <CIT> , except that here we are not limited to a binary tree '
'EM-HMM tagger provided with good initial conditions <CIT> 914 \* -LRB- \* uses linguistic constraints and manual adjustments to the dictionary -RRB- Figure 1 : Previous results on unsupervised POS tagging using a dictionary <OTH> on the full 45-tag set '
'Dialogs Speakers Turns Words Fragments Distinct Words Distinct Words\/POS Singleton Words Singleton Words\/POS Intonational Phrases Speech Repairs 98 34 6163 58298 756? 859? 1101 252? 350? 1094 7 2396 Table 1 : Size of the Trains Corpus 21 POS Annotations Our POS tagset is based on the Penn Treebank tagset <CIT> , but modified to include tags for discourse markers and end-of-turns , and to provide richer syntactic information <OTH> '
'html 162 311 Penn Treebank 3 The Penn Treebank 3 corpus <CIT> consists of hand-coded parses of the Wall Street Journal -LRB- test , development and training -RRB- and a small subset of the Brown corpus <OTH> -LRB- test only -RRB- '
'Default parameters were used for all experiments except for the numberofiterationsforGIZA + <CIT> '
'The baseline we measure against in all of these experiments is the state-of-the-art grow-diag-final -LRB- gdf -RRB- alignment refinement heuristic commonly used in phrase-based SMT <CIT> '
'<CIT> , 1993 -RRB- : 1 '
'This decomposition applies both to discriminative linear models and to generative models such as HMMs and CRFs , in which case the linear sum corresponds to log likelihood assigned to the input\/output pair by the model -LRB- for details see <OTH> for the classi cation case and <CIT> for the structured case -RRB- '
'So far , pivot features on the word level were used <CIT> , eg Does the bigram not buy occur in this document ? <CIT> '
'In order to get a better understanding of these matters , we replicate parts of the error analysis presented by <CIT> , where parsing errors are related to different structural properties of sentences and their dependency graphs '
'Dubey et al proposed an unlexicalized PCFG parser that modied PCFG probabilities to condition the existence of syntactic parallelism <CIT> '
'We use the cosine similarity measure for windowbased contexts and the following commonly used similarity measures for the syntactic vector space : <CIT> measure , the weighted Lin measure <OTH> , the - Skew divergence measure <OTH> , the Jensen-Shannon -LRB- JS -RRB- divergence measure <OTH> , Jaccards coef cient <OTH> and the Confusion probability <OTH> '
'Note that this early discarding is related to ideas behind cube pruning <CIT> , which generates the top n most promising hypotheses , but in our method the decision not to generate hypotheses is guided by the quality of hypotheses on the result stack '
'Examples of such methods are the introduction of information weights as in the NIST measure or the comparison of stems or synonyms , as in METEOR <CIT> '
'Supervision for simple features has been explored in the literature <CIT> '
'In general , they can be divided into two major categories , namely lexicalized models <OTH> and un-lexicalized models <CIT> '
'1 Introduction Many state-of-the-art machine translation -LRB- MT -RRB- systems over the past few years <CIT> rely on several models to evaluate the goodness of a given candidate translation in the target language '
'Initial results show the potential benefit of factors for statistical machine translation , <CIT> and <CIT> '
'Part-of-speech tagging is an active area of research ; a great deal of work has been done in this area over the past few years <CIT> '
'Evaluation We evaluate translation output using three automatic evaluation measures : BLEU <OTH> , NIST <OTH> , and METEOR <CIT> 5 All measures used were the case-sensitive , corpuslevel versions '
'In the early statistical translation model work at IBM , these representations were called cepts , short for concepts <CIT> '
'We adopt an approach , similar to <OTH> , in which the meaning representation , in our case XML , is transformed into a sorted flat list of attribute-value pairs indicating the core contentful concepts of each command '
'An alternative to tercom , considered in this paper , is to use the Inversion Transduction Grammar -LRB- ITG -RRB- formalism <CIT> which allows one to view the problem of alignment as a problem of bilingual parsing '
'We obtain weights for the combinations of the features by performing minimum error rate training <CIT> on held-out data '
'Actually , it is defined similarly to the translation model in SMT <CIT> '
'2 Related Work There has been extensive research in opinion mining at the document level , for example on product and movie reviews <CIT> '
'2 Experimental System and Data HMIHY is a spoken dialogue system based on the notion of call routing <CIT> '
'Our results on Chinese data confirm previous findings on English data shown in <CIT> '
'We use the log-likelihood ratio for determining significance as in <CIT> , but other measures are possible as well '
'First , word frequencies , context word frequencies in surrounding positions -LRB- here three-words window -RRB- are computed following a statistics-based metrics , the log-likelihood ratio <CIT> '
'The two annotators agreed on the annotations of 385\/453 turns , achieving 8499 % agreement , with Kappa = 0682 This inter-annotator agreement exceeds that of prior studies of emotion annotation in naturally occurring speech 2a3a5a4a7a6a8a6a9a4a11a10a13a12a15a14a17a16a19a18a21a20a22a12a23a14a25a24a26a18 a27 a20a22a12a23a14a25a24a26a18 <CIT> '
',.~.eqmvalent ot duty in a parallel French text, the correct sense of the Enghsh word is identified These studies exploit th~s lnformatmn m order to gather co-occurrence data for the different senses, which ts then used to dtsamb~guate new texts In related work, Dywk (1998) used patterns of translational relatmns in an EnghshNorwegian paralle ! corpus (ENPC, Oslo Umverslty) to define semantic propemes such as synonymy, ambtgmty, vagueness, and semantic helds and suggested a derivation otsemantic representations for signs (eg, lexemes), captunng semantm relatmnshlps such as hyponymy etc, fiom such translatmnal relatmns Recently, Resnlk and Yarowsky (1997) suggested that fol the purposes ot WSD, the different senses of a wo~d could be detelmlned by considering only sense d~stmctmns that are lextcahzed cross-hngmstlcally In particular, they propose that some set of target languages be ~dent~fied, and that the sense d~stmctmns to be considered for language processing appllcatmns and evaluatmn be restricted to those that are reahzed lexlcally in some minimum subset of those languages This idea would seem to p~ovtde an answer, at least m part, to the problem of determining different senses of a word mtumvely, one assumes that ff another language lexlcahzes a word m two or more ways, there must be a conceptual monvatmn If we look at enough languages, we would be likely to fred the s~gmficant lexlcal differences that dehmtt different senses of a word However, th~s suggestmn raises several questions Fo~ instance, ~t ~s well known that many amb~gumes are preserved across languages (for example, the French tntdrYt and the Enghsh interest), especmlly languages that are relatively closely related Assuming this problem can be overcome, should differences found m closely related languages be given lesser (or greater) weight than those found m more distantly related languages 9 More generally, which languages should be considered for this exermse 9 All languages 9 Closely related languages9 Languages from different language famlhes ''~ A mixture of the two 9 How many languages, and of which types, would be ''enough'' to provide adequate lnfotmanon tot this purpose~ There ts also the questmn ot the crlterm that would be used to estabhsh that a sense distinction is ''lexlcahzed cross-hngu~stmally'' How consistent must the d~stlnCtlOn be 9 Does it mean that two concepts are expressed by mutually non-lntetchangeable lexmal items in some slgmficant number ot other languages, or need tt only be the case that the option ot a different lexlcahzatlon exists m a certain percentage of cases 9 Another conslderatmn ts where the cross-hngual mformatlon to answer these questmns would come from Using bdmgual dictionaries would be extremely tedmus and error-prone, g~ven the substantial d~vergence among d~ctlonanes in terms of the kinds and degree of sense dlstmctmns they make Resmk and Yalowsky (1997) suggest EutoWordNet (Vossen, 1998) as a possible somce of mformatmn, but, given that EuroWordNet ts pttmatdy a lexmon and not a corpus, ~t is subject to many of the same objections as for bl-hngual dictionaries An alternative would be to gather the reformation from parallel, ahgned corpma Unlike bilingual and muttt-hngual dictionaries, translatmn eqmvalents xn parallel texts a~e determined by experienced translatols, who evaluate each instance ot a word''s use m context rather than as a part of the meta-hngmst~c actlvlty of classifying senses for mclusmn in a dictionary However, at present very few parallel ahgned corpora exist The vast majority ot these are bl-texts, mvolwng only two languages, one of which is very often English Ideally, a serious 53 evaluation of Resnik and Yarowsky''s proposal would include parallel texts m languages from several different language families, and, to maximally ensure that the word m question is used in the exact same sense across languages, ~t would be preferable that the same text were used over all languages in the study The only currently avadable parallel corpora for more than two languages are Olwell''s Nmeteen Eighty-Four (Erjavec and Ide, 1998), Plato''s Repubhc (Erjavec, et al, 1998), the MULTEXT Journal .o/ the Commt.~ston corpus (Ide and V6roms, 1994), and the Bible (Resnlk, et al, m press) It is likely that these corpora do not provide enough appropriate data to reliably determine sense distinctions Also, ~t Is not clear how the lexlcahzatlon of sense distractions across languages Is affected by genre, domain, style, etc Thls paper attempts to provide some prehmlnary answers to the questions outhned above, In order to eventually determine the degree to which the use of parallel data ts vmble to determine sense distinctions, and, ff so, the ways in which th~s reformation might be used Given the lack of lalge parallel texts across multiple languages, the study is necessarily hmlted, however, close exammanon of a small sample of parallel data can, as a first step, provide the basis and dlrectmn for more extensive studies 1 Methodology I have conducted a small study using parallel, aligned versmns ot George Orwell''s Nineteen Etghtv-Fo,lr (Euavec and Ide, 1998)m five languages Enghsh, Slovene, Estonian, Romanlan, and Czech I The study therefole Involves languages from four language families The O~well parallel corpus also includes vers|ons o) Ntneteen-E~gho Four m Hungarian, Bulgarmn, Latwan, Llthuaman, Se~bmn, and Russmn (Germanic, Slavic, Fmno-Ugrec, and Romance), two languages from the same family (Czech and Slovene), as well as one non-Indo-European language (Estoman) Nmeteen Eighty-Four Is a text of about 100,000 words, translated directly from the original English m each of the other languages The parallel versions of the text are sentence-aligned to the English and tagged for part of speech Although Nineteen Eighty-Four is a work of fiction, Orwell''s prose IS not highly stylized and, as such, it provides a reasonable sample ot modern, ordinary language that ~s not tied to a given topic or sub-domain (such as newspapers, technical reports, etc ) Furthermore, the translations of the text seem to be relatively faithful to the original for instance, over 95% ot the sentence alignments in the full pmallel corpus of seven languages are one-to-one (Prlest-Dorman, et al, 1997) Nine ambiguous English words were considered hard, head, country, hne, promise, shght, seize, scrap, float The first four were chosen because they have been used in other dlsamb~guatlon studies, the latter five were chosen from among the words used m the Senseval dlsamblguatlon exercise (Kllgamff and Palmer, forthcoming) In all cases, the study was necessarily hmlted to words that occurred frequently enough in the Orwell text to warrant consideration F~ve hundred forty-two sentences conta|nmg an occurrence or occurrences (Including morphological variants) of each of the nine words were extracted from the Enghsh text, together w~th the parallel sentences m which they occur m the texts ot the four comparison languages (Czech, Estonian, Romantan, Slovene) As Walks and Stevenson (1998) have pointed out, pa~t-of-speech tagging accomplishes a good portion of the work ot semantic dlsamb~guatmn, therefore occmrences of wolds that appemed in the data in more than 54 one part of speech were grouped separately 2 The Enghsh occurrences were then grouped usmg the sense distinctions m WordNet, (version 1 6) [Miller et al, 1990, Fellbaum, 1998]) The sense categonzatmn was performed by the author and two student assistants, results from the three were compared and a final, mutually agreeable set of sense assignments was estabhshed For each of the four comparison languages, the corpus of sense-grouped parallel sentences were sent to a llngmst and natl,ve speaker of the comparison language The hngmsts were asked to provide the lexlcal item m each parallel sentence that corresponds to the ambiguous Enghsh word If inflected, they were asked to provide both the inflected form and the root form In addttmn, the lmgmsts were asked to indicate the type of translatmn, according to the dtstmctmns given m Table 1 For over 85% of the Enghsh word occurrences (corresponding to types 1 and 2 m Table 1), a specific lexlcal item or items could be identified as the translation equivalent for the corresponding Enghsh word For comparison purposes, each translanon equivalent was represented by ~ts lemma (or the lemma of the toot form in the case of derivatives) and associated w~th the WordNet sense to which it corresponds In order to determine the degree to which the assigned sense dlstlncttons correspond to translation eqmvalents, a coherence index ( Cl) was computed that measures how often each pmr of senses is translated usmg the same word as well as the consistency with which a g~ven se,ls,z ~s translated with the same word ~ Note that the z The adJective and adverb senses of hard are consadeied together because the distinction is not consistent across the translations used m the study Note that the CI ~s similar to semanuc entropy (Melamed, 1997) However, Melamed computes CIs do not determine whether or not a sense dtstmctton can be lextcahzed in the target language, but only the degree to whmh they are lexicahzed differently m the translated text However, tt can be assumed that the CIs provide a measure of the tendency to lex~cahze different WordNet senses differently, which can m turn be seen as an mdtcatmn of the degree to which the distraction ts vahd For each ambiguous word, the CI Is computed for each pair of senses, as follows S<q t> Cl(sqS, ) = ''=1 m rnrt where @ n ~s the number of comparison languages under consideration, nl~q and m,, are the nt~mber of occurrences olsense sqand sense s~ m the Enghsh corpus, respectively, including occurrences that have no idenufiable translation, s<~ ~>m ts the number of times that senses q and r are translated by the same lex~cal Item m language t, i e, x=y t ~tJan ~( q ), r~oan~( r ) The CI ts a value between 0 and 1, computed by examining clusters of occurrences translated by the same word In the othel languages If sense and sense ) are consistently translated w~th the same wo~d in each comparison language, then Cl(s, s~) = 1, if they are translated with a different word m every occurrence, Cl(s, ~) = 0 In general, the CI for pans of different senses provides an index of thmr relatedness, t e, the greater the value of Cl(s, sj), the more frequently occurrences of-sense t and sense j are translated with the same lextcal item When t = j, we entropy tOl wold types, lather than word senses 55 obtain a measure of the coherence of a ~lven sense Type Meaning 1 A slngle lexlcal Item is used to translate the En@izsh equivalent (possibly a 2 The English word is translated by a phrase of two or more words or a compound, meaning as the slngle English word 3 The En@izsh word is not lexzcalized in the translation 4 A pronoun is substituted for the English word In the translation An English phrase contalnmng the ambiguous word Is translated by a single language which has a broader or more specific meanlng, or by a phrase in whl corresponding to the English word Is not explicltl~ lexlcallzed Table 1 Translation types and their trequencles % dizen whl%h h 6% 6% 6% of s p same Word # Description hard 1 1 difficult 2 head i i i 1 Table 2 1 2 _meta~horlcally hard _] 3 not yielding to pressure, 1 4 very strong or ~lgorous, ar 2 I wlth force or vigor (adv) 3 earnestly, intently (adv) i_ ~art of the body  3 intellect 4 _r~le_!r, ch,%ef 7 front, front part WoldNet senses ot hard and head CIs were also computed for each language individually as well as for different language groupings Romaman, Czech, and Estonian (three different language families) Czech and Slovene (same family), Romaman, Czech, Slovene (Indo-European, and Estonian (nonIndo-European) To better visualize the relationship between senses, a hierarchical clustering algorithm was applied to the CI data to generate trees reflecting sense proximity 4 Finally, in order to determine the degree to which the linguistic relaUon between languages may affect coherence, a correlation was run among CIs for all pairs of the four target languages Fol example, Table 2 gives the senses of hard and head that occurred in the data s The CI data .s ''sobS'' hard and head are given in Tables 3 and 4 ~uous CIs measuring the aff, mty of a sense with itself--that is, the tendency for all occurrences of that sense to be translated wlth the same word--show that all of the s,x senses of ha,d have greatel internal consistency tfian athmty with other senses, with senses 1 1 (''dlff|cult'' CI = 56) and 13 (,''not soft,, ci = 63) registenng the h,ghest internal consistency 6 The same holds true for three of the four senses of head, while the CI for senses 1 3 (''Intellect'') and 1 1 (''part of the body'') is higher than the CI for 1 3/1 3 WordNet Sense 2 1 2 3 1 4 1 3 1 1 1 2 21 23 1 4 13 0 50 o 13 i ool 0 O0 0 25 i O0 0 04 0 50 0 17 0 56 0 19 0 00 0 00 0 00 0 00 0 00 0 25 0 21 Table 3 CIs for hard I i 12 0,,63 0 00 0 50 2 Results Although the data sample is small, It gives some insight into ways m which a larger sample might contribute to sense discrimination 4 Developed by Andleas Stolcke Results tor all words m the study are avadable at http//www cs vassar edu/~~de/wsd/cross-hng html 6 Senses 2 3 and 1 4 have CIs ot 1 because each ot these senses exists m a single occurrence m the corpus, and have theretote been dlscarded horn consideration ot CIs to~ individual senses We a~e currently mvesugatmg the use oI the Kappa staUst~c (Carletta, 1996) to normahze these sparse data 56 WordNet Sense 1 1 1 3 1 4 1 7 1 1 0 69 1 3 0 53 0 45 1 4 0 12 0 07, 0 50 1 7 0 40 0 001 0 00 1 00 Table 4 CIs for head Figure 2 shows the sense clusters for hard generated from the CI data 7 The senses fall into two mare clusters, w~th the two most internally consistent senses (1 1 and 1 3) at the deepest level of each ot the respecuve groups The two adverbml forms 8 are placed in separate groups, leflectmg thmr semantic proximity to the different adjecuval meanings of hard The clusters for head (Figure 2) stmdarly show two dlstmct groupings, each anchored in the two senses with the h~ghest internal consistency and the lowest mutual CI (''part of the body'' (1 1) and ''ruler, chief'' (1 4)) The h~erarchtes apparent m the cluster graphs make intuitive sense Structured hke dictmnary enmes, the clusters for hard and head might appeal as m F~gure 1 This ts not dissimilar to actual dlctLonary entries for hard and head, for example, the enmes for hard in four differently constructed dlctmnanes ( Colhns Enghsh (CED), Longman''s (LDOCE), OxJotd Advanced Learner''s (OALD), and COBUILD) all hst the ''''d~fficult'' and ''not soft'' senses first and second, whmh, since most dictionaries hst the most common Ol frequently used senses hrst, reflects the gross dlwslon apparent m the clusters Beyond this, ~t ~s difficult to assess the 7 Foi the purposes ot the cluster analys~s, CIs of l 00 resulting from a single occurrrence were normahzed to 5 8 Because ~oot to, ms were used m the analysis, no dzstlncUon m UanslaUon eqmvalents was made tor part ot speech correspondence between the senses In the dictionary entries and the clusters The remamlng WordNet senses are scattered at various places within the entries or, m some cases, split across various senses The h~erarchlcal relatmns apparent m the clusters are not reflected m the d~cttonary enmes, smce the senses are for the most part presented in flat, hnear hsts However, It is interesting to note that the first five senses of hard In the COBUILD d~cuonary, which is the only d~cttonary in the group constructed on the bas~s of colpus examples 9 and presents senses m ruder of frequency, correspond to hve of the six WordNet senses in thls study WordNet''s ''metaphorically hard'' is spread over multiple senses in the COB UILD, as it.is In the other d~ctlonarles HARD HEAD I 1 dlfflcult 2 vlgorously II 1 a not soft b strong 2 a earnestly b metaphorlcally hard I 1 a part of the body b zntellect 2 front, front part II ruler, chlef Flgme 1 Clusteis tol hard and head suuctured as dlcuonary entt ~es The results tor dlftment language groupings show that the tendency to lextcahze senses differently is not aftected by language d~stance (Table 5) In fact, the mean CI fol Estonian, the only non-Indo-European language m the study, ~s lower than that for any other group, mdmatmg that WordNet sense dtstmctmns are slightly less hkely to be lexlcahzed differently m Estonian 9 Edmons ot the LDOCE (1987 vexsmn) and OALD (1985 version) dictlonalles consulted m this study ple-date edmons ol those same d~ctlonanes based on colpus evidence 57 Correlations of CIs for each language pair (Table 5) also show no relationship between the degree to which sense d~stmcuons are lexlcahzed differently and language distance This is contrary to results obtained by Resmk and Yarowsky (subm,tted), who, using a memc slmdar to the one used in this study, found that that non-Indo-European languages tended to lexlcallze English sense d~stmctlons more than Indo-European languages, especially at finergrained levels However, their translation data was generated by native speakers presented with Isolated sentences in English, who were asked to provide the translation for a given word In the sentence It is not clear how this data compares to translations generated by trained translators working with full context Lanquaqe qroup Averaqe CI ALL 0 27 RO/ES/SL 0 28 SL/CS 0 28 RO/SL/CS 0 27 ES 0 26 Table 5 Average CI values Lanqs Hard Country Llne Head Ave ES/CS 0 86 0 72 0 68 0 69 0 74 RO/SL 0 73 0 78 0 68 1 00 0 80 RO/CS 0 83 0 66 0 67 0 72 0 72 SL/CS 0 88 0 51 0 72 0 71 0 71 RO/ES 0 97 0 26 0 70 0 98 0 73 ES/SL 0 73 0 59 0 90 0 99 0 80 Table 6 CI correlauon tor the tour target languages I -I I  I I m~nlmum dlstance = 0 249399 m~nlmum d~stance = 0 434856 mlnlmum dlstance = 0 555158 mlnlmum dlstance = 0 602972 m~nlmum dlstance = 0 761327 I  >21 I  >ii I  >23 l  >13 l  >14 I  >12 (13) (23) (12) (1,4) (ii) (21) (1412) (2313) ( 2 3 1 3 1 4 1 2 ) ( 2 111 ) Figure 2 Cluster tree and distance measures tor the sm senses of hard I  >14 -i I  > i i I--- 1 J  > i 3 I  >17 mlnlmum dlstance = 0 441022 mlnlmum dlstance = 0 619052 mln~mum dlstance = 0 723157 (13) (ll) (17) (1113) (111317) (14) F,gure 3 Cluster tree and dmtance measures tot the tout senses ot head 58 Conclusion The small sample m this study suggests that cross-hngual lexlcahzat~on can be used to define and structure sense d~stmct~ons The cluster graphs above provide mformat~on about relations among WordNet senses that could be used, for example, to determine the granularity of sense differences, whtch m turn could be used in tasks such as machine translatton, mtormaUon retrieval, etc For example, it is hkely that as sense dtstmcttons become finer, the degree of error ~s less severe Resmk and Yarowsky (1997) suggest that confusing freer-grained sense dtstmctlons should be penahzed less severely than confusing grosser d~stmct~ons when evaluatmg the performance of sense dtsambtguatt0n systems The clusters also provide insight into the lexlcallzatlon of sense dtstmcttons related by various semantic relations (metonymy, meronymy, etc ) across languages, for instance, the ''part of the body'' and ''intellect'' senses of head are lex~cahzed with the same ~tem a s~gnlficant portion of the t~me across all languages, reformation that could be used m machine translatton In addtt~on, cluster data such as that presented here could be used m lexicography, to determine a mole detaded hierarchy of relations among senses in dtct~onary entries It is less clear how cross-hngual reformation can be used to determine sense d~st~nctlons independent of a pre-deflned set, such as the WordNet senses used here In an effort to explore how thts mlght be done, I have used the small sample from thts study to create word groupmgs from ''back translations'' (l e, additional translations m the original language ot the translations m the target language) and developed a metric that uses th~s mformatton to determine relatedness between occurrences, whtch ~s m turn used to cluster occurrences into sense groups I have also compared sets of back translations for words representing the various WordNet senses, which provtde word groups s~mdar to WordNet synsets Interestingly, there ts virtually no overlap between the WordNet synsets and word groups generated from back translations The results show, however, that sense dlstmctlons useful for natural language processing tasks such as machme translanon could potentsally be determined, ot at least influenced, by constdeHng this mformatton The automatically generated synsets themselves may also be useful m the same apphcatlons; where WordNet synsets (and ontologtes) have been used tn the past More work needs to be done on the topic of cross-hngual sense determination, utthzmg substantially larger parallel corpora that include a variety ot language types as well as texts fiom several genres This small study explores a possible methodology to apply when such resources become avatlable Acknowledgements The author would hke to gratefully acknowledge the contrtbut~on of those who provided the translatton mfotmat~on Tomaz Eua~ec (Slovene), Kadrt Muxschnek (Estonian), Vladtmlr Petkevtc (Czech), and Dan Tubs (Romanlan), as well as Dana Fleut and Darnel Khne, who helped to transcrtbe and evaluate the data Special thanks to Dan Melamed and Hlnrtch Schutze for their helpful comments 59 [] [] in [] in i i Hg nn i an i am References Ca~letta, Jean (1996) Assessing Agreement on Classthcatton Tasks The Kappa Stat~st~t. Computational Lmgulstlcs, 22(2), 249-254 Dagan, Ido and Ita~, Alon (1994) Wo~d sense dlsambxguat~on using a second language monohngual corpus Computattonal Ltngmsttcs, 20(4), 563-596 Dagan, Ido, Ital, Alon, and Schwall, Ulnke (1991) Two languages a~e more mformattve than one Proceedings of the 29th Annual Meettng of the Assoctatton for Computattonal Ltngutsttcs, 18-21 June 1991, Berkeley, Cahfornm, 130-137 Dyvtk, Helge (1998) Translations as Semantic Mirrors Proceedmgs of Workshop W13 Multzlmguahty in the Lextcon II, The 13th Biennial European Conference on Arttftctal lntelhgence (ECA198), Brighton, UK, 24-44 Eqavec, Tomaz and Ide, Nancy (1998) The MULTEXT-EAST Corpus Proceedlng~ of the Fltst International Conference on Language Resources and Evaluatton, 27-30 May 1998, Granada, 971-74 Erjavec, Tomaz, Lawson, Ann, and Romary, Laurent (1998) East meets West Producing Multflmgual Resources m a European Context Pioceedtngs of the Ftrst Internattonal Conference on Language Resources and Evaluation, 27-30 May 1998, Gtanada, 981-86 Fellbaum, Chttstmne (ed) (1998) WordNet An Electrontc Lexlcal Database MIT Press, Cambridge, Massachusetts Gale, Wdham A, Church, Kenneth W and Yatowsky, Davtd (1993) A method tor dlsamblguatmg word senses m a large cmpus Computers and the Humamtles, 26, 415-439, Hearst, M''attl A (1991) Noun homograph  '' dlsamblguatlon using local:''~.''0ntext m large corpora Proceedtngs of the 7th Annual Conference of the Umver~lt~ of Waterloo Centre for the New OED and Text ReaeaJch, Oxford, Umted Kingdom, 1-19 Ide, Nancy and V61oms, Jean (1998) Word sense d~samb~guat~on The state of the alt Computational Lmgut~ttc~, 24 1, 1-40 Kdgar~ttt, Adam and Palmer, Ma~tha, Eds (forthcoming) Proceedmgs ot the Senseval Word Sense D~samb~guatlon Workshop, Specml double ~ssue otComputer~ and the Humamttes, 33 4-5 Leacock, Claudia, Towell, Geoffrey and Voorhees, Ellen (1993) Corpus-based stattstlcal sense resolution Proceedtng~ of the ARPA Human Language Technology Worsl~shop, San Francisco, Morgan Kautman Melamed, I Dan (1997) Measuring Semantic Entropy ACL-SIGLEX Workshop Taggmg Tert wtth Lextcal Semanttcs Why, What, and How ~ April 4-5, 1997, Washington, D C, 41-46 Mtllet, George A, Beckwlth, Richard T Fellbaum.'
'We build a subset S C ~ '' incrementally by iterating to adjoin a feature f E ~ '' which maximizes loglikelihood of the model to S This algorithm is called the Basic Feature Selection <CIT> '
'From this point of view , some of the measures used in the evaluation of Machine Translation systems , such as BLEU <CIT> , have been imported into the summarization task '
'The linear kernel derived from the L1 distance is the same as the difference-weighted token-based similarity measure of <CIT> '
'Thus , it may not suffer from the issues of non-isomorphic structure alignment and non-syntactic phrase usage heavily <CIT> '
'For example , <CIT> applied multiple-sequence alignment -LRB- MSA -RRB- to parallel news sentences and induced paraphrasing patterns for generating new sentences '
'Many authors claim that class-based methods are more robust against data sparseness problems <OTH> , <OTH> , <CIT> '
'It achieves 901 % average precision\/recall for sentences with maximum length 40 and 895 % for sentences with maximum length 100 when trained and tested on the standard sections of the Wall Street Journal Treebank <CIT> '
'The usefulness of prosody was found to be very limited by itself , if the effect of utterance length is not considered <CIT> '
'In our search procedure , we use a mixture-based alignment model that slightly differs from the model introduced as Model 2 in <CIT> '
'One can imagine the same techniques coupled with more informative probability distributions , such as lexicalized PCFGs <OTH> , or even grammars not based upon literal rules , but probability distributions that describe how rules are built up from smaller components <CIT> '
'4 Related Work 41 Acquisition of Classes of Instances Although some researchers focus on re-organizing or extending classes of instances already available explicitly within manually-built resources such as Wikipedia <OTH> or WordNet <CIT> or both <OTH> , a large body of previous work focuses on compiling sets of instances , not necessarily labeled , from unstructured text '
'An alternative method we considered was to estimate certain conditional probabilities , similarly to the formula used in <CIT> : SW -LRB- t -RRB- log P -LRB- p C A\/t -RRB- f -LRB- t , A -RRB- f -LRB- A -RRB- = ~ log -LRB- 2 -RRB- P -LRB- p C R\/t -RRB- f -LRB- t , R -RRB- f -LRB- l ~ -RRB- Here f -LRB- A -RRB- is -LRB- an estimate of -RRB- the probability that any given candidate phrase will be accepted by the spotter , and f -LRB- R -RRB- is the probability that this phrase is rejected , ie , f -LRB- R -RRB- = l-f -LRB- A -RRB- '
'3 Automatic Evaluation of MT Quality We utilize BLEU <CIT> for the automatic evaluation of MT quality in this paper '
'Since the texts in the RST Treebank are taken from the syntactically annotated Penn Treebank <CIT> , it is natural to ask what the relation is between the discourse structures in the RST Treebank and the syntactic structures of the Penn Treebank '
'Syntactic context information is used <CIT> to compute term similarities , based on which similar words to a particular word can directly be returned '
'B-X I-X 0 first word of a chunk of type X non-initial word in an X chunk word outside of any chunk This representation type is based on a representation proposed by <CIT> for noun phrase chunks '
'The approach is evaluated by cross-validation on the WSJ treebank corpus <CIT> '
'The block set is generated using a phrase-pair selection algorithm similar to <CIT> , which includes some heuristic filtering to mal statement here '
'For details please refer to <CIT> '
'5 Translation performance was measured using the automatic BLEU evaluation metric <CIT> on four reference translations '
'The elementary trees were extracted from the parse trees in sections 02-21 of the Wall Street Journal in Penn Treebank <CIT> , which is transformed by using parent-child annotation and left factoring <OTH> '
'The probabilities of derivation decisions are modelled using the neural network approximation <OTH> to a type of dynamic Bayesian Network called an Incremental Sigmoid Belief Network -LRB- ISBN -RRB- <CIT> '
'To test whether a better set of initial parameter estimates can improve Model 1 alignment accuracy , we use a heuristic model based on the loglikelihood-ratio -LRB- LLR -RRB- statistic recommended by <CIT> '
'The intuition is that the produced clusters will be less sense-conflating than those produced by other graph-based approaches , since collocations provide strong and consistent clues to the senses of a target word <CIT> '
'At the sentence level , <CIT> employed an unsupervised learning approach to cluster sentences and extract lattice pairs from comparable monolingual corpora '
'Similarly , if the task is to distinguish between binary , coarse sense distinction , then current WSD techniques can achieve very high accuracy -LRB- in excess of 96 % when tested on a dozen words in <CIT> -RRB- '
'These algorithms are usually applied to sequential labeling or chunking , but have also been applied to parsing <OTH> , machine translation <CIT> and summarization <OTH> '
'A variety of methods are used to account for the re-ordering stage : word-based <CIT> , templatebased <OTH> , and syntax-based <OTH> , to name just a few '
'For the multilingual dependency parsing track , which was the other track of the shared task , Nilsson et al achieved the best performance using an ensemble method <OTH> '
'However , by exploiting the fact that the underlying scores assigned to competing hypotheses , w -LRB- e , h , f -RRB- , vary linearly wrt changes in the weight vector , w , <CIT> proposed a strategy for finding the global minimum along any given search direction '
'11 However , modeling word order under translation is notoriously difficult <CIT> , and it is unclear how much improvement in accuracy a good model of word order would provide '
'<CIT> present a system called BABAR that uses contextual role knowledge to do coreference resolution '
'The system described in <CIT> also makes use of syntactic heuristics '
'<OTH> , <CIT> <OTH> , Dave et al '
'Our MT baseline system is based on Moses decoder <CIT> with word alignment obtained from GIZA + + <OTH> '
'1 Motivation A major component in phrase-based statistical Machine translation -LRB- PBSMT -RRB- <CIT> is the table of conditional probabilities of phrase translation pairs '
'<OTH> , <CIT> <OTH> , Dave et al '
'12 Related Work Recently , discriminative methods for alignment have rivaled the quality of IBM Model 4 alignments <CIT> '
'The disambiguation algorithms also require that the semantic relatedness measures WordNet : : Similarity <CIT> be installed '
'An analysis of the alignments shows that smoothing the fertility probabilities significantly reduces the frequently occurring problem of rare words forming garbage collectors in that they tend to align with too many words in the other language <CIT> '
'Moses provides BLEU <OTH> and NIST <OTH> , but Meteor <CIT> and TER <OTH> can easily be used instead '
'In an evaluation on the PENN treebank <CIT> , the parser outperformed other unlexicalized PCFG parsers in terms of labeled bracketing fscore '
'Since the word support model and triple context matching model have been proposed in our previous work <OTH> at the SIGHAN bakeoff 2005 <OTH> and 2006 <CIT> , the major descriptions of this paper is on the WBT model '
'612 ROUGE evaluation Table 4 presents ROUGE scores <CIT> of each of human-generated 250-word surveys against each other '
'Two are conditionalized phrasal models , each EM trained until performance degrades : C-JPTM3 as described in <OTH> Phrasal ITG as described in Section 41 Three provide alignments for the surface heuristic : GIZA + + with grow-diag-final -LRB- GDF -RRB- Viterbi Phrasal ITG with and without the noncompositional constraint We use the Pharaoh decoder <CIT> with the SMT Shared Task baseline system <CIT> '
'These words and phrases are usually compiled using different approaches -LRB- Hatzivassiloglou and McKeown , 1997 ; Kaji and Kitsuregawa , 2006 ; <CIT> and Nasukawa , 2006 ; Esuli and Sebastiani , 2006 ; Breck et al , 2007 ; Ding , Liu and Yu '
'3 Online Learning Again following <OTH> , we have used the single best MIRA <OTH> , which is a variant of the voted perceptron <CIT> for structured prediction '
'CP-STM -LRB- i -RRB- - l This metric corresponds to the STM metric presented by <CIT> '
'Our experiments created translation modules for two evaluation corpora : written news stories from the Penn Treebank corpus <CIT> and spoken task-oriented dialogues from the TRAINS93 corpus <OTH> '
'CIT -RRB- '
'31 A Note on State-Splits Recent studies <CIT> suggest that category-splits help in enhancing the performance of treebank grammars , and a previous study on MH <OTH> outlines specific POS-tags splits that improve MH parsing accuracy '
'Given this , the mutual information ratio <CIT> is expressed by Formula 1 '
'In addition to sentence fusion , compression algorithms <CIT> and methods for expansion of a multiparallel corpus <OTH> are other instances of such methods '
'by diag-and symmetrization <CIT> '
'There are many research directions , eg , sentiment classification -LRB- classifying an opinion document as positive or negative -RRB- <OTH> , subjectivity classification -LRB- determining whether a sentence is subjective or objective , and its associated opinion -RRB- <OTH> , feature\/topic-based sentiment analysis -LRB- assigning positive or negative sentiments to topics or product features -RRB- -LRB- Hu and Liu 2004 ; Popescu and Etzioni , 2005 ; Carenini et al , 2005 ; Ku et al , 2006 ; Kobayashi , Inui and Matsumoto , 2007 ; Titov and <CIT> '
'23 Online Learning Again following <OTH> , we have used the single best MIRA <OTH> , which is a margin aware variant of perceptron <CIT> for structured prediction '
'Reported work includes improved model variants <CIT> and applications such as web data extraction <OTH> , scientific citation extraction <OTH> , word alignment <OTH> , and discourselevel chunking <OTH> '
'They are not used in LN , but they are known to be useful for WSD <CIT> '
'3 Building the CatVar The CatVar database was developed using a combination of resources and algorithms including the Lexical Conceptual Structure -LRB- LCS -RRB- Verb and Preposition Databases <OTH> , the Brown Corpus section of the Penn Treebank <CIT> , an English morphological analysis lexicon developed for PC-Kimmo -LRB- Englex -RRB- <OTH> , NOMLEX <OTH> , Longman Dictionary of Contemporary English 2For a deeper discussion and classification of Porter stemmers errors , see <OTH> '
'POS tag the text using the tagger of <CIT> '
'The learning algorithm used for each stage of the classification task is a regularized variant of the structured Perceptron <CIT> '
'An alternative is to create an automatic system that uses a set of training question-answer pairs to learn the appropriate question-answer matching algorithm <CIT> '
'<CIT> compared two Bayesian inference algorithms , Variational Bayes and what we call here a point-wise collapsed Gibbs sampler , and found that Variational Bayes produced the best solution , and that the Gibbs sampler was extremely slow to converge and produced a worse solution than EM '
'<OTH> , Pereira and Tishby <OTH> , and Pereira , Tishby , and Lee <OTH> propose methods that derive classes from the distributional properties of the corpus itself , while other authors use external information sources to define classes : Resnik <OTH> uses the taxonomy of WordNet ; <CIT> <OTH> uses the categories of Roget ''s Thesaurus , Slator <OTH> and Liddy and Paik <OTH> use the subject codes in the LDOCE ; Luk <OTH> uses conceptual sets built from the LDOCE definitions '
'For the Brown corpus , we based our division on <CIT> '
'Much of the work in subjectivity analysis has been applied to English data , though work on other languages is growing : eg , Japanese data are used in <CIT> , Chinese data are used in <OTH> , and German data are used in <OTH> '
'Morphosyntacticinformationhas in fact been shown to significantlyimprove the extractionresults <CIT> '
'For the chunk part of the code , we adopt the Inside , Outside , and Between -LRB- IOB -RRB- encoding originating from <CIT> '
'Following <CIT> , Iusevariational Bayes EM <OTH> during the M-step for the transition distribution : l +1 j i = f -LRB- E -LRB- ni , j -RRB- + i -RRB- f -LRB- E -LRB- n i -RRB- + C i -RRB- -LRB- 3 -RRB- f -LRB- v -RRB- = exp -LRB- -LRB- v -RRB- -RRB- -LRB- 4 -RRB- 60 -LRB- v -RRB- = braceleftBigg g -LRB- v 1 2 -RRB- ifv -RRB- 7 -LRB- v + 1 -RRB- 1v ow '
'Our results are similar to those for conventional phrase-based models <CIT> '
'While Kazama and Torisawa used a chunker , we parsed the definition sentence using Minipar <CIT> '
'Perhaps the most well-known method is maximum marginal relevance -LRB- MMR -RRB- <CIT> , as well as cross-sentence informational subsumption <OTH> , mixture models <OTH> , subtopic diversity <OTH> , diversity penalty <OTH> , and others '
'Before training the classifiers , we perform feature ablation by imposing a count cutoff of 10 , and by limiting the number of features to the top 75K features in terms of log likelihood ratio <CIT> '
'32 The parsers The parsers that we chose to evaluate are the C&C CCG parser <OTH> , the Enju HPSG parser <OTH> , the RASP parser <OTH> , the Stanford parser <OTH> , and the DCU postprocessor of PTB parsers <CIT> , based on LFG and applied to the output of the Charniak and Johnson reranking parser '
'Training via the voted perceptron algorithm <CIT> or using a max-margin criterion also correspond to the first option -LRB- eg McCallum and Wellner <OTH> , Finley and Joachims <OTH> -RRB- '
'Furthermore , early work on class-based language models was inconclusive <CIT> '
'Measures of cross-language relatedness are useful for a large number of applications , including cross-language information retrieval <OTH> , cross-language text classification <OTH> , lexical choice in machine translation <OTH> , induction of translation lexicons <OTH> , cross-language annotation and resource projections to a second language <CIT> '
'Related Work The recent availability of large amounts of bilingual data has attracted interest in several areas , including sentence alignment <CIT> , word alignment <CIT> , alignment of groups of words <OTH> , and statistical translation <CIT> '
'51 The baseline System used for comparison was Pharaoh <CIT> , which uses a beam search algorithm for decoding '
'1 Introduction Base noun phrases -LRB- baseNPs -RRB- , broadly the initial portions of non-recursive noun phrases up to the head <CIT> , are valuable pieces of linguistic structure which minimally extend beyond the scope of named entities '
'As modern systems move toward integrating many features <CIT> , resources such as this will become increasingly important in improving translation quality '
'Metrics in the Rouge family allow for skip n-grams <OTH> ; Kauchak and Barzilay <OTH> take paraphrasing into account ; metrics such as METEOR <OTH> and GTM <OTH> calculate both recall and precision ; METEOR is also similar to SIA <CIT> in that word class information is used '
'We then piped the text through a maximum entropy sentence boundary detector <CIT> and performed text normalization using NSW tools <OTH> '
'For English , we have used sections 03-06 of the WSJ portion of the Penn Treebank <OTH> distributed by the Linguistic Data Consortium -LRB- LDC -RRB- , which have frequently been used to evaluate sentence boundary detection systems before ; compare Section 7 '
'In the concept extension part of our algorithm we adapt our concept acquisition framework <CIT> to suit diverse languages , including ones without explicit word segmentation '
'<CIT> , or -LRB- S+T - -RRB- , where no labeled target domain data is available , eg '
'Moreover , rather than predicting an intrinsic metric such as the PARSEVAL Fscore , the metric that the predictor learns to predict can be chosen to better fit the final metric on which an end-to-end system is measured , in the style of <CIT> '
'A possible solution to his problem might be the use of more general morphological rules like those used in part-of-speech tagging models -LRB- eg , 1 2 3 4 530 40 50 60 70 80 90 100 level error RAND BASE Boost_S NNtfidf NB Boost_M Figure 6 : Comparison of all models for a129 a48a51a95a66a97a98a97a180a222 <CIT> -RRB- , where all suffixes up to a certain length are included '
'4 Experiments The experiments described here were conducted using the Wall Street Journal Penn Treebank corpus <CIT> '
'Such studies follow the empiricist approach to word meaning summarized best in the famous dictum of the British 3 linguist J.R. Firth: You shall know a word by the company it keeps. (Firth, 1957, p. 11) Context similarity has been used as a means of extracting collocations from corpora, e.g. by Church & Hanks (1990) and by Dunning (1993), of identifying word senses, e.g. by Yarowski (1995) and by Schutze (1998), of clustering verb classes, e.g. by Schulte im Walde (2003), and of inducing selectional restrictions of verbs, e.g. by Resnik (1993), by Abe & Li (1996), by Rooth et al.'
'For both experiments , we used dependency trees extracted from the Penn Treebank <CIT> using the head rules and dependency extractor from Yamada and Matsumoto -LRB- 2003 -RRB- '
'411 Lexical co-occurrences Lexical co-occurrences have previously been shown to be useful for discourse level learning tasks <CIT> '
'Our evaluation metric is BLEU <CIT> '
'Similar to , eg , <CIT> , we use a Naive Bayes algorithm trained on word features cooccurring with the subjective and the objective classifications '
'44 Experiment 2 : <CIT> s Words We also conducted translation on seven of the twelve English words studied in <CIT> <OTH> '
'Researchers extracted opinions from words , sentences , and documents , and both rule-based and statistical models are investigated <CIT> '
'Since there is no well-agreed to definition of what an utterance is , we instead focus on intonational phrases <OTH> , which end with an acoustically signaled boundary lone '
'Running words 1,864 14,437 Vocabulary size 569 1,081 Table 2 : ChineseEnglish corpus statistics <OTH> using Phramer <OTH> , a 3-gram language model with Kneser-Ney smoothing trained with SRILM <OTH> on the English side of the training data and Pharaoh <CIT> with default settings to decode '
'These tags are drawn from a tagset which is constructed by 363 extending each argument label by three additional symbols a80a44a81a83a82a84a81a86a85 , following <CIT> '
'The first SMT systems were developed in the early nineties <CIT> '
'Document level sentiment classification is mostly applied to reviews , where systems assign a positive or negative sentiment for a whole review document <CIT> '
'A description of the flat featurized dependency-style syntactic representation we use is available in <OTH> , which describes how the entire Penn Treebank <CIT> was converted to this representation '
'According to this model , when translating a stringf in the source language into the target language , a string e is chosen out of all target language strings e if it has the maximal probability given f <CIT> : e = arg maxe -LCB- Pr -LRB- e f -RRB- -RCB- = arg maxe -LCB- Pr -LRB- f e -RRB- Pr -LRB- e -RRB- -RCB- where Pr -LRB- f e -RRB- is the translation model and Pr -LRB- e -RRB- is the target language model '
'<CIT> evaluates both estimation techniques on the Bayesian bitag model ; Goldwater and Griffiths <OTH> emphasize the advantage in the MCMC approach of integrating out the HMM parameters in a tritag model , yielding a tagging supported by many different parameter settings '
'Conjunctions are a major source of errors for English chunking as well <CIT> 9 , and we plan to address them in future work '
'2.1 The Evaluator The evaluator is a function p(t[t'', s) which assigns to each target-text unit t an estimate of its probability given a source text s and the tokens t'' which precede t in the current translation of s. 1 Our approach to modeling this distribution is based to a large extent on that of the IBM group (Brown et al. , 1993), but it differs in one significant aspect: whereas the IBM model involves a ''noisy channel'' decomposition, we use a linear combination of separate predictions from a language model p(tlt ~) and a translation model p(tls ).'
'<CIT> shows that setting those weights should take into account the evaluation metric by which the MT system will eventually be judged '
'Two more recent investigations are by Yarowsky , <CIT> , and later , Mihalcea , <OTH> '
'F (Cahill et al. , 2004) overall 95.98 57.86 72.20 73.00 40.28 51.91 90.16 54.35 67.82 65.54 36.16 46.61 args only 98.64 42.03 58.94 82.69 30.54 44.60 86.36 36.80 51.61 66.08 24.40 35.64 Basic Model overall 92.44 91.28 91.85 63.87 62.15 63.00 63.12 62.33 62.72 42.69 41.54 42.10 args only 89.42 92.95 91.15 60.89 63.45 62.15 47.92 49.81 48.84 31.41 32.73 32.06 Basic Model with Subject Path Constraint overall 92.16 91.36 91.76 63.72 62.20 62.95 75.96 75.30 75.63 50.82 49.61 50.21 args only 89.04 93.08 91.02 60.69 63.52 62.07 66.15 69.15 67.62 42.77 44.76 44.76 Table 7: Evaluation of trace insertion and antecedent recovery for C04 algorithm, our basic algorithm and basic algorithm with the subject path constraint.'
'In general , Agold \/ Acandidates ; following <OTH> and <OTH> for parse reranking and <CIT> for translation reranking , we define Aoracle as alignment in Acandidates that is most similar to Agold8 We update each feature weight i as follows : i = i + hAoraclei hA1-besti 9 Following <OTH> , after each training pass , we average all the feature weight vectors seen during the pass , and decode the discriminative training set using the vector of averaged feature weights '
'Here , under the ITG constraint <CIT> , we need to consider just two kinds of reorderings , straight and inverted between two consecutive blocks '
'1 word w 2 word bigram w1w2 3 single-character word w 4 a word of length l with starting character c 5 a word of length l with ending character c 6 space-separated characters c1 and c2 7 character bigram c1c2 in any word 8 the first / last characters c1 / c2 of any word 9 word w immediately before character c 10 character c immediately before word w 11 the starting characters c1 and c2 of two consecutive words 12 the ending characters c1 and c2 of two consecutive words 13 a word of length l with previous word w 14 a word of length l with next word w Table 1: Feature templates for the baseline segmentor 2 The Baseline System We built a two-stage baseline system, using the perceptron segmentation model from our previous work (Zhang and Clark, 2007) and the perceptron POS tagging model from Collins (2002).'
'A number of systems for automatically learning semantic parsers have been proposed <CIT> '
'791 and score the alignment template models phrases <CIT> '
'The pervading method for estimating these probabilities is a simple heuristic based on the relative frequency of the phrase pair in the multi-set of the phrase pairs extracted from the word-aligned corpus <CIT> '
'Metrics based on syntactic similarities such as the head-word chain metric -LRB- HWCM -RRB- <CIT> '
'4 Filtering with the CFG Rule Dictionary We use an idea that is similar to the method proposed by Ratnaparkhi <CIT> for partof-speech tagging '
'<OTH> used the Base-NP tag set as presented in <CIT> : I for inside a Base-NP , O for outside a Base-NP , and B for the first word in a Base-NP following another Base-NP '
'After maximum BLEU tuning <CIT> on a held-out tuning set , we evaluate translation quality on a held-out test set '
'SEP\/epsilon a\/A # epsilon \/ # a\/epsilon a\/epsilon b\/epsilon b\/B UNK\/epsilon c\/C b\/epsilon c\/BC e \/ + E epsilon \/ + d\/epsilon d\/epsilon epsilon\/epsilon b\/AB # b\/A # B # e \/ + DE c\/epsilon d\/BCD e \/ + D+E Figure 1 : Illustration of dictionary based segmentation finite state transducer 31 Bootstrapping In addition to the model based upon a dictionary of stems and words , we also experimented with models based upon character n-grams , similar to those used for Chinese segmentation <OTH> '
'The results evaluated by BLEU score <CIT> is shown in Table 2 '
'2 Evaluating Heterogeneous Parser Output Two commonly reported shallow parsing tasks are Noun-Phrase -LRB- NP -RRB- Chunking <CIT> and the CoNLL-2000 Chunking task <OTH> , which extends the NPChunking task to recognition of 11 phrase types1 annotated in the Penn Treebank '
'There are many possible methods for combining unlabeled and labeled data <CIT> , but we simply concatenate unlabeled data with labeled data to see the effectiveness of the selected reliable parses '
'We ran the decoder with its default settings and then used Moses implementation of minimum error rate training <CIT> to tune the feature weights on the development set '
'55 Dependency validity features Like <OTH> , we extract the dependency path from the question word to the common word -LRB- existing in both question and sentence -RRB- , and the path from candidate answer -LRB- such as CoNLL NE and numerical entity -RRB- to the common word for each pair of question and candidate sentence using Stanford dependency parser <CIT> '
'Assuming that the parameters P -LRB- etk fsk -RRB- are known , the most likely alignment is computed by a simple dynamic-programming algorithm1 Instead of using an Expectation-Maximization algorithm to estimate these parameters , as commonly done when performing word alignment <CIT> , we directly compute these parameters by relying on the information contained within the chunks '
'We use binary Synchronous ContextFree Grammar -LRB- bSCFG -RRB- , based on Inversion Transduction Grammar -LRB- ITG -RRB- <CIT> , to define the set of eligible segmentations for an aligned sentence pair '
'1 Introduction Over the past decade , researchers at IBM have developed a series of increasingly sophisticated statistical models for machine translation <OTH> '
'Given phrase p1 and its paraphrase p2 , we compute Score3 -LRB- p1 , p2 -RRB- by relative frequency <CIT> : Score3 -LRB- p1 , p2 -RRB- = p -LRB- p2 p1 -RRB- = count -LRB- p2 , p1 -RRB- P pprime count -LRB- pprime , p1 -RRB- -LRB- 7 -RRB- People may wonder why we do not use the same method on the monolingual parallel and comparable corpora '
'These include cube pruning <OTH> , cube growing <CIT> , early pruning <OTH> , closing spans <OTH> , coarse-to-fine methods <OTH> , pervasive laziness <OTH> , and many more '
'In recent years , sentiment classification has drawn much attention in the NLP field and it has many useful applications , such as opinion mining and summarization <CIT> '
'To counteract this , we introduce two brevity penalty measures -LRB- BP -RRB- inspired by BLEU <CIT> which we incorporate into the loss function , using a product , loss = 1PrecBP : BP1 = exp -LRB- 1max -LRB- 1 , rc -RRB- -RRB- -LRB- 6 -RRB- BP2 = exp -LRB- 1max -LRB- cr , rc -RRB- -RRB- where r is the reference length and c is the candidate length '
'129 5 Active learning Whereas a passive supervised learning algorithm is provided with a collection of training examples that are typically drawn at random , an active learner has control over the labeled data that it obtains <OTH> '
'The Penn Treebank documentation <CIT> defines a commonly used set of tags '
'One of the main directions is sentiment classification , which classifies the whole opinion document -LRB- eg , a product review -RRB- as positive or negative <CIT> '
'Finally , we are investigating several avenues for using this system output for Machine Translation -LRB- MT -RRB- including : -LRB- 1 -RRB- aiding word alignment for other MT system <OTH> ; and -LRB- 2 -RRB- aiding the creation various MT models involving analyzed text , eg , <CIT> '
'Hence we use a beam-search decoder during training and testing ; our idea is similar to that of <CIT> who used a beam-search decoder as part of a perceptron parsing model '
'2.2 Corpus occurrence In order to get a feel for the relative frequency of VPCs in the corpus targeted for extraction, namely 0 5 10 15 20 25 30 35 40 0 10 20 30 40 50 60 70 VPC types (%) Corpus frequency Figure 1: Frequency distribution of VPCs in the WSJ Tagger correctextracted Prec Rec Ffl=1 Brill 135135 1.000 0.177 0.301 Penn 667800 0.834 0.565 0.673 Table 1: POS-based extraction results the WSJ section of the Penn Treebank, we took a random sample of 200 VPCs from the Alvey Natural Language Tools grammar (Grover et al. , 1993) and did a manual corpus search for each.'
'However , the pb features yields no noticeable improvement unlike in prefect lexical choice scenario ; this is similar to the findings in <CIT> '
'<CIT> gave a systematic examination of the efficacy of unigram , bigram and trigram features drawn from different representations surface text , constituency parse tree and dependency parse tree '
'Consider the lexical model pw -LRB- ry rx -RRB- , defined following <CIT> , with a denoting the most frequent word alignment observed for the rule in the training set '
'Statistical Model In SIFTs statistical model , augmented parse trees are generated according to a process similar to that described in <CIT> '
'3.5 Regularization We apply lscript1 regularization (Ng, 2004; Gao et al., 2007) to make learning more robust to noise and control the effective dimensionality of the feature spacebysubtractingaweightedsumofabsolutevalues of parameter weights from the log-likelihood of the training data w = argmaxw LL(w) summationdisplay i Ci|wi| (6) We optimize the objective using a variant of the orthant-wise limited-memory quasi-Newton algorithm proposed by Andrew & Gao (2007).3 All values Ci are set to 1 in most of the experiments below, although we apply stronger regularization (Ci = 3) to reordering features.'
'Our method does not suppose a uniform distribution over all possible phrase segmentationsas <CIT> since each phrase tree has a probability '
'After this conversion , we had 1000 positive and 1000 negative examples for each domain , the same balanced composition as the polarity dataset <CIT> '
'The MT community has developed not only an extensive literature on alignment <CIT> , but also standard , proven alignment tools such as GIZA + + <OTH> '
'51 Evaluation of Translation Translations are evaluated on two automatic metrics : Bleu <CIT> and PER , position independent error-rate <OTH> '
'These methods have been used in machine translation <CIT> , terminology research and translation aids <OTH> , bilingual lexicography <OTH> , collocation studies <OTH> , word-sense disambiguation <CIT> and information retrieval in a multilingual environment <OTH> '
'For instance , for Maximum Entropy , I picked <CIT> for the basic theory , <OTH> for an application -LRB- POS tagging in this case -RRB- , and <OTH> for more advanced topics such as optimization and smoothing '
'In comparison with shallow semantic analysis tasks, such as wordsense disambiguation (Ide and Jeaneronis, 1998) and semantic role labeling (Gildea and Jurafsky, 2002; Carreras and M`arquez, 2005), which only partially tackle this problem by identifying the meanings of target words or finding semantic roles of predicates, semantic parsing (Kate et al. , 2005; Ge and Mooney, 2005; Zettlemoyer and Collins, 2005) pursues a more ambitious goal  mapping natural language sentences to complete formal meaning representations (MRs), where the meaning of each part of a sentence is analyzed, including noun phrases, verb phrases, negation, quantifiers and so on.'
'The kappa value <CIT> was used to evaluate the agreement among the judges and to estimate how difficult the evaluation task was '
'Table look-up using an explicit translation lexicon is sufficient and preferable for many multilingual NLP applications , including ` crummy '' MT on the World Wide Web <OTH> , certain machine-assisted translation tools -LRB- eg '
'The local dependencies between sentiment labels on sentences is similar to the work of <CIT> where soft local consistency constraints were created between every sentence in adocument and inference wassolved using a min-cut algorithm '
'Training Set -LRB- Labeled English Reviews -RRB- : There are many labeled English corpora available on the Web and we used the corpus constructed for multi-domain sentiment classification <CIT> 9 , because the corpus was large-scale and it was within similar domains as the test set '
'Atthefinestlevel , thisinvolvesthealignment of words and phrases within two sentences that are known to be translations <CIT> '
'The parser is coupled with an on-line averaged perceptron <CIT> as the learning method '
'Statistical parsers have been developed for TAG <OTH> , LFG <CIT> , and HPSG <OTH> , among others '
'For the give source text , S , it finds the most probable alignment set , A , and target text , T = Aa SaTpSTp -RRB- , -LRB- -RRB- -LRB- -LRB- 1 -RRB- Brown <CIT> proposed five alignment models , called IBM Model , for an English-French alignment task based on equa68 tion -LRB- 1 -RRB- '
'Large volumes of training data of this kind are indispensable for constructing statistical translation models <CIT> , acquiring bilingual lexicon <OTH> , and building example-based machine translation -LRB- EBMT -RRB- systems <OTH> '
'We use the default configuration of the measure in WordNet : : Similarity-012 package <CIT> , and , with a single exception , the measure performed below Gic ; see BP in table 1 '
'The previous studies , with the exception of <CIT> , used smaller gazetteers than ours '
'This approach is similar to that of seed words -LRB- eg , <OTH> -RRB- or hook words -LRB- eg , <CIT> -RRB- in previous work '
'The features we use are shown in Table 2 , which are based on the features used by <CIT> and Uchimoto et al '
'Consequently , here we employ multiple references to evaluate MT systems like BLEU <CIT> and NIST <OTH> '
'31 A simple solution <CIT> suggests that in order to have an ITG take advantage of a known partial structure , one can simply stop the parser from using any spans that would violate the structure '
'We then rank-order the P X|Y MI XY M Z Pr Z|Y MI ZY G092log [P X P Y P X P Y ] f Y [P XY P XY ] f XY [P XY P XY ] f XY M iG13X,X} jG13Y,Y} (f ij G09 ij ) 2 ij f XY G09 XY XY (1G09( XY /N)) f XY G09 XY f XY (1G09(f XY /N)) Table 1: Probabilistic Approaches METHOD FORMULA Frequency (Guiliano, 1964) f XY Pointwise Mutual Information (MI) (Fano, 1961; Church and Hanks, 1990) log (P / PP) 2XY XY Selectional Association (Resnik, 1996) Symmetric Conditional Probability (Ferreira and Pereira, 1999) P / PP XY X Y 2 Dice Formula (Dice, 1945) 2 f / (f +f ) XY X Y Log-likelihood (Dunning, 1993; (Daille, 1996).'
'Similarly , <CIT> propose a relative distortion model to be used with a phrase decoder '
'An extension to WordNet was presented by <CIT> '
'Discovering orientations of context dependent opinion comparative words is related to identifying domain opinion words <CIT> '
'Given a weight vector w , the score wf -LRB- x , y -RRB- ranks possible labelings of x , and we denote by Yk , w -LRB- x -RRB- the set of k top scoring labelings for x We use the standard B , I , O encoding for named entities <CIT> '
'To use the data from NANC , we use self-training <CIT> '
'2 Related work <CIT> recently advocated the need for a uniform approach to corpus-based semantic tasks '
'Similar to work in image retrieval <OTH> , we cast the problem in terms of Machine Translation : given a paired corpus of words and a set of video event representations to which they refer , we make the IBM Model 1 assumption and use the expectation-maximization method to estimate the parameters <CIT> : = + = m j ajm jvideowordpl Cvideowordp 1 -RRB- -LRB- -RRB- 1 -LRB- -RRB- -LRB- -LRB- 1 -RRB- This paired corpus is created from a corpus of raw video by first abstracting each video into the feature streams described above '
'In this paper , sentence pairs are extracted by a simple model that is based on the so-called IBM Model1 <CIT> '
'The piecewise linearity observation made in <CIT> is no longer applicable since we can not move the log operation into the expected value '
'More specifically, by using translation probabilities, we can rewrite equation (11) and (12) as follow: nullnullnullnullnull null nullnull null nullnullnull null null nullnullnullnull null nullnull null null nullnull null   nullnull null null | null null null null nullnull null nullnull null nullnull null null null null nullnull null nullnull null  null null 1nullnull null nullnull null null null nullnull|nullnull (13) nullnullnullnullnull null nullnull null nullnullnull null null nullnullnullnull null nullnull null null nullnull null   nullnull null null | null null null null nullnull null nullnull null nullnull null null null null nullnull null nullnull null  null null 1nullnull null nullnull null null null nullnull|nullnull  (14) where nullnullnullnull|null null null  denotes the probability that topic term null  is the translation of null null . In our experiments, to estimate the probability nullnullnullnull|null null null , we used the collections of question titles and question descriptions as the parallel corpus and the IBM model 1 (Brown et al., 1993) as the alignment model.'
'Table 4 shows the linguistic features of the resulting model compared to the models of Carroll and Rooth <OTH> , <CIT> , and Charniak -LRB- 2000 -RRB- '
'2 Related Work There has been a large and diverse body of research in opinion mining , with most research at the text <CIT> , sentence <OTH> or word <OTH> level '
'A hierarchical alignment algorithm is a type of synchronous parser where , instead of constraining inferences by the production rules of a grammar , the constraints come from word alignments and possibly other sources <CIT> '
'1 Introduction on measures for inter-rater reliability <CIT> , on frameworks for evaluating spoken dialogue agents <OTH> and on the use of different corpora in the development of a particular system -LRB- The Carnegie-Mellon Communicator , Eskenazi et al '
'In the future , we will experiment with semantic -LRB- rather than positional -RRB- clustering of premoditiers , using techniques such as those proposed in <OTH> '
'Our learning method is an extension of Collinss perceptron-based method for sequence labeling <CIT> '
'These feature vectors and the associated parser actions are used to train maximum entropy models <CIT> '
'prime 1 1 1 05 05 1 05 05 01 01 01 00001 00001 01 00001 00001 Further , we ran each setting of each estimator at least 10 times -LRB- from randomly jittered initial starting points -RRB- for at least 1,000 iterations , as <CIT> showed that some estimators require many iterations to converge '
'We use the discriminative perceptron learning algorithm <CIT> to train the values of vectorw '
'7 Experiments To show the effectiveness of cross-language mention propagation information in improving mention detection system performance in Arabic , Chinese and Spanish , we use three SMT systems with very competitive performance in terms of BLEU11 <CIT> '
'The tagger described in this paper is based on the standard Hidden Markov Model architecture <OTH> '
'We measured associations using the log-likelihood measure <CIT> for each combination of target category and semantic class by converting each cell of the contingency into a 22 contingency table '
'32 Rare Word Accuracy For these experiments , we use the Wall Street Journal portion of the Penn Treebank <CIT> '
'<CIT> extracts rules from non-anaphoric noun phrases and noun phrases patterns , which are then applied to test data to identify existential noun phrases '
'2 Previous Approaches <CIT> method of estimating phrasetranslation probabilities is very simple '
'The form of the maximum entropy probability model is identical to the one used in <CIT> : k f $ -LRB- wi,wi-1 , wi-2 , at ~ ri -RRB- YIj = I Otj p -LRB- wilwi-l , wi-2 , attri -RRB- = Z -LRB- Wi-l , wi-2 , attri -RRB- k to t j = l where wi ranges over V t3 stop '
'Instead , researchers routinely use automatic metrics like Bleu <CIT> as the sole evidence of improvement to translation quality '
'Rapp <OTH> , <CIT> -RRB- but using cosine rather than cityblock distance to measure profile similarity '
'32 Maximum Entropy ME models implement the intuition that the best model will be the one that is consistent with the set of constrains imposed by the evidence , but otherwise is as uniform as possible <CIT> '
'The first model , referred to as Maxent1 below , is a loglinear combination of a trigram language model with a maximum entropy translation component that is an analog of the IBM translation model 2 <CIT> '
'In Yarowsky ''s experiment <CIT> , an average of 3936 examples were used to disambiguate between two senses '
'Err:510'
'For Hw6 , students compared their POS tagging results with the ones reported in <CIT> '
'A statistical language model a lexicalized PCFG <CIT> is derived from the analysis grammar by processing a corpus using the same grammar with no statistical model and recording frequencies of substructures built by each rule '
'We use GIZA + + <CIT> to do m-to-n word-alignment and adopt heuristic grow-diag-final-and to do refinement '
'The surface heuristic can define consistency according to any word alignment ; but most often , the alignment is provided by GIZA + + <CIT> '
'154 2 Translation Models 21 Standard Phrase-based Model Most phrase-based translation models <CIT> rely on a pre-existing set of word-based alignments from which they induce their parameters '
'Overall % agreement among judges for 250 propositions 601 A commonly used metric for evaluating interrater reliability in categorization of data is the kappa statistic <CIT> '
'A more optimistic view can be found in <CIT> ; they argue that a near-100 % interjudge agreement is possible , provided the part-of-speech annotation is done carefully by experts '
'From the extracted n-grams , those with a flequc ` ncy of 3 or more were kept -LRB- other approaches get rid of n-grams of such low frequencies <CIT> -RRB- '
'This was expected , as it has been observed before that very simple smoothing techniques can perform well on large data sets , such as web data <CIT> '
'On the other hand , <OTH> proposed an algorithm , borrowed to the field of dynamic programming and based on the output of their previous work , to find the best alignment , subject to certain constraints , between words in parallel sentences '
'21 Minimum Error Rate Training The predominant approach to reconciling the mismatch between the MAP decision rule and the evaluation metric has been to train the parameters of the exponential model to correlate the MAP choice with the maximum score as indicated by the evaluation metric on a development set with known references <CIT> '
'In earlier work <CIT> only singletons were used as seed words ; varying their number allows us to test whether multiple seed words have a positive effect in detection performance '
'stituent alignments <CIT> '
'Many strategies have been proposed to integrate morphology information in SMT , including factored translation models <CIT> , adding a translation dictionary containing inflected forms to the training data <OTH> , entirely replacing surface forms by representations built on lemmas and POS tags <OTH> , morphemes learned in an unsupervised manner <OTH> , and using Porter stems and even 4-letter prefixes for word alignment <OTH> '
'Manually defined heuristics are used to automatically annotate each tree in the treebank with partially specified HPSG derivation trees: Head/argument/modifier distinctions are made for each node in the tree based on Magerman (1994) and Collins (1997); 336 ODonovan et al. Large-Scale Induction and Evaluation of Lexical Resources the whole tree is then converted to a binary tree; heuristics are applied to deal with phenomena such as LDDs and coordination and to correct some errors in the treebank, and finally an HPSG category is assigned to each node in the tree in accordance with its CFG category.'
'A synchronous 363 binarization method is proposed in <CIT> whose basic idea is to build a left-heavy binary synchronous tree <OTH> with a left-to-right shift-reduce algorithm '
'This hypothesized relationship between distributional similarity and semantic similarity has given rise to a large body of work on automatic thesaurus generation <CIT> '
'The details of the algorithm can be found in the literature for statistical translation models , such as <CIT> '
'For comparing the sentence generator sample to the English sample , we compute log-likelihood statistics <CIT> on neighboring words that at least co-occur twice '
'To support distributed computation <CIT> , we further split the N-gram data into shards by hash values of the first bigram '
'See <CIT> for an application of the boosting approach to named entity recognition , and Walker , Rambow , and Rogati <OTH> for the application of boosting techniques for ranking in the context of natural language generation '
'The first stage parser is a best-first PCFG parser trained on sections 2 through 22 , and 24 of the Penn WSJ treebank <CIT> '
'35 The Experiments We have ran LexTract on the one-millionword English Penn Treebank <CIT> and got two Treebank grammars '
'The results are comparable to other results reported using the Inside\/Outside method <CIT> -LRB- see Table 7 '
'Meanwhile , it is common for NP chunking tasks to represent a chunk -LRB- eg , NP -RRB- with two labels , the begin -LRB- eg , B-NP -RRB- and inside -LRB- eg , I-NP -RRB- of a chunk <CIT> '
'Portage is a statistical phrase-based SMT system similar to Pharaoh <CIT> '
'Like <CIT> , we used mutual information to measure the cohesion between two words '
'Workshop Towards Genre-Enabled Search Engines </booktitle> <pages> 13 -- 20 </pages> <editor> In G Rehm and M Santini , editors </editor> <contexts> <context> ork on an intra-document , or page segment level because a single document can contain instances of multiple genres , eg , contact information , list of publications , CV , see <OTH> '
'Again , we find the clearest patterns in the graphs for precision , where Malt has very low precision near the root but improves with increasing depth , while MST shows the opposite trend <CIT> '
'Fortunately , using distributional characteristics of term contexts , it is feasible to induce part-of-speech categories directly from a corpus of suf cient size , as several papers have made clear <CIT> '
'Abney <OTH> notes important problems with the soundness of the approach when a unification-based grammar is actually determining the derivations , motivating the use of log-linear models <OTH> for parse ranking that Johnson and colleagues further developed <OTH> '
'6 Discourse Context <CIT> pointed out that the sense of a target word is highly consistent within any given document -LRB- one sense per discourse -RRB- '
'3 Model As an extension to commonly used lexical word pair probabilities p -LRB- f e -RRB- as introduced in <CIT> , we define our model to operate on word triplets '
'To derive the joint counts c -LRB- ? s , ? t -RRB- from which p -LRB- ? s ? t -RRB- and p -LRB- ? t ? s -RRB- are estimated , we use the phrase induction algorithm described in <CIT> , with symmetrized word alignments generated using IBM model 2 <OTH> '
'We utilise the automatic annotation algorithm of <CIT> to derive a version of Penn-II where each node in each tree is annotated with an LFG functional annotation -LRB- ie an attribute value structure equation -RRB- '
'Most previous work with CRFs containing nonlocal dependencies used approximate probabilistic inference techniques , including TRP <OTH> and Gibbs sampling <CIT> '
'We tokenized sentences using the standard treebank tokenization script , and then we performed part-of-speech tagging using MXPOST tagger <CIT> '
'Training of the phrase translation model builds on top of a standard statistical word alignment over the training corpus of parallel text <CIT> for identifying corresponding word blocks , assuming no further linguistic analysis of the source or target language '
'Section 7 considers recent efforts to induce effective procedures for automated sense labeling of discourse relations that are not lexically marked <CIT> '
'However , most of them fail to utilize non-syntactic phrases well that are proven useful in the phrase-based methods <CIT> '
'Words are encoded through an automatic clustering algorithm <CIT> while tags , labels and extensions are normally encoded using diagonal bits '
'4 Features For our experiments we use the features proposed , motivated and described in detail by <CIT> '
'<CIT> presented a thorough discussion on the Yarowsky algorithm '
'In the field of statistical analysis of natural language data , it is common to use measures of lexical association , such as the informationtheoretic measure of mutual information , to extract useful relationships between words -LRB- eg <CIT> -RRB- '
'We hence chose transformation-based learning to create this -LRB- shallow -RRB- segmentation grammar , converting the segmentation task into a tagging task -LRB- as is done in 85 <CIT> , inter alia -RRB- '
'In particular , previous work <OTH> has investigated the use of Markov random fields -LRB- MRFs -RRB- or log-linear models as probabilistic models with global features for parsing and other NLP tasks '
'<CIT> reports results for different numbers of hidden states but it is unclear how to make this choice a priori , while Goldwater & Griffiths <OTH> leave this question as future work '
'1 Introduction Word Sense Disambiguation -LRB- WSD -RRB- competitions have focused on general domain texts , as attested in the last Senseval and Semeval competitions <OTH> '
'History-based models for predicting the next parser action <CIT> 3 '
'33 Features Similar to the default features in Pharaoh <CIT> , we used following features to estimate the weight of our grammar rules '
'Following the framework of global linear models in <CIT> , we cast this task as learning a mapping F from input verses x X to a text-reuse hypothesis y Y -LCB- epsilon1 -RCB- '
'We obtained 47,025 50-dimensional reduced vectors from the SVD and clustered them into 200 classes using the fast clustering algorithm Buckshot <CIT> -LRB- group average agglomeration applied to a sample -RRB- '
'On the other hand , according to the data-driven approach , a frequency-based language model is acquired from corpora and has the forms of ngrams <CIT> , rules <OTH> , decision trees <OTH> or neural networks <OTH> '
'Several frameworks for finding translation equivalents or translation units in machine translation , such as <OTH> and other example-based MT approaches , might be used to select the preferred mapping '
'-LRB- b -RRB- MEDLINE DT JJ VBN NNS IN DT NN NNS VBP The oncogenic mutated forms of the ras proteins are RB JJ CC VBP IN JJ NN NN constitutively active and interfere with normal signal transduction Figure 1 : Part of speech-tagged sentences from both corpora we investigate its use in part of speech -LRB- PoS -RRB- tagging <CIT> '
'Introduction There has been considerable recent interest in the use of statistical methods for grouping words in large on-line corpora into categories which capture some of our intuitions about the reference of the words we use and the relationships between them <CIT> '
'Our intuition is that we can not apply our binarization to <CIT> '
'Table 3 : Example compressions Compression AvgLen Rating Baseline 970 193 BT-2-Step 2206 321 Spade 1909 310 Humans 2007 383 Table 4 : Mean ratings for automatic compressions nally , we added a simple baseline compression algorithm proposed by <CIT> which removed all prepositional phrases , clauses , toinfinitives , and gerunds '
'Church and Hanks <CIT> employed mutual information to extract both adjacent and distant bi-grams that tend to co-occur within a fixed-size window '
'Previous research has addressed revision in single-document summaries <CIT> <OTH> and has suggested that revising summaries can make them more informative and correct errors '
'2 Three New Features for MT Evaluation Since our source-sentence constrained n-gram precision and discriminative unigram precision are both derived from the normal n-gram precision , it is worth describing the original n-gram precision metric , BLEU <CIT> '
'This concept of alignment has been also used for tasks like authomatic vocabulary derivation and corpus alignment <OTH> '
'We use the Stanford parser <OTH> with its default Chinese grammar , the GIZA + + <CIT> alignment package with its default settings , and the ME tool developed by <OTH> '
'The idea is that the translation of a sentence x into a sentence y can be performed in the following steps1 : -LRB- a -RRB- If x is small enough , IBMs model 1 <CIT> is employed for the translation '
'1 A cept is defined as the set of target words connected to a source word <CIT> '
'Measures of attributional similarity have been studied extensively , due to their applications in problems such as recognizing synonyms <OTH> , information retrieval <OTH> , determining semantic orientation <CIT> , grading student essays <OTH> , measuring textual cohesion <OTH> , and word sense disambiguation <OTH> '
'Note that the algorithm from <CIT> was designed for discriminatively training an HMM-style tagger '
'In this years shared task we evaluated a number of different automatic metrics : Bleu <CIT> Bleu remains the de facto standard in machine translation evaluation '
'In computational linguistics , our pattern discovery procedure extends over previous approaches that use surface patterns as indicators of semantic relations between nouns or verbs -LRB- <CIT> inter alia -RRB- '
'SMT has evolved from the original word-based approach <OTH> into phrase-based approaches <CIT> and syntax-based approaches <OTH> '
'Various clustering techniques have been proposed <CIT> which perform automatic word clustering optimizing a maximum-likelihood criterion with iterative clustering algorithms '
'<OTH> , <CIT> and Lee <OTH> , Wilson et al '
'The approach is able to achieve 94 % precision and recall for base NPs derived from the Penn Treebank Wall Street Journal <CIT> '
'org\/pubs\/citations \/ j ournals\/toms\/1986 -12 -2 \/ p154-meht a \/ Mutual Information Given the definition of Mutual Information <CIT> , I -LRB- x , y -RRB- = log 2 P -LRB- x , y -RRB- P -LRB- x -RRB- P -LRB- y -RRB- '' we consider the distribution of a window word according to the contingency table -LRB- a -RRB- in Table 4 '
'There has been recent work on discovering allomorphic phenomena automatically <CIT> '
'Thus the alignment set is denoted as -RCB- & -RRB- ,1 -LRB- -RRB- , -LCB- -LRB- ialiaiA ii = We adapt the bilingual word alignment model , IBM Model 3 <OTH> , to monolingual word alignment '
'4 Experimental Work A part of the Wall Street Journal -LRB- WSJ -RRB- which had been processed in the Penn Treebanck Project <CIT> was used in the experiments '
'In <CIT> , features are selected according to part-of-speech labels '
'In particular , since we treat each individual speech within a debate as a single document , we are considering a version of document-level sentiment-polarity classification , namely , automatically distinguishing between positive and negative documents <CIT> '
'In this paper we show how the extraction process can be scaled to the complete Wall Street Journal -LRB- WSJ -RRB- section of the Penn-II treebank , with about 1 million words in 50,000 sentences , based on the automatic LFG f-structure annotation algorithm described in <CIT> '
'1 A bilingual language model ITG <CIT> has proposed a bilingual language model called Inversion Transduction Grammar -LRB- ITG -RRB- , which can be used to parse bilingual sentence pairs simultaneously '
'5http : \/ \/ opennlpsourceforgenet \/ We use the standard four-reference NIST MTEval data sets for the years 2003 , 2004 and 2005 -LRB- henceforth MT03 , MT04 and MT05 , respectively -RRB- for testing and the 2002 data set for tuning6 BLEU4 <OTH> , METEOR <CIT> and multiple-reference Word Error Rate scores are reported '
'4 Experiments 41 Experiment Settings A series of experiments were run to compare the performance of the three SWD models against the baseline , which is the standard phrase-based approach to SMT as elaborated in <CIT> '
'We used the heuristic combination described in <OTH> and extracted phrasal translation pairs from this combined alignment as described in <CIT> '
'Parse selection constitutes an important part of many parsing systems <CIT> '
'To generate phrase pairs from a parallel corpus , we use the ` diag-and '' phrase induction algorithm described in <OTH> , with symmetrized word alignments generated using IBM model 2 <CIT> '
'In NLP community , it has been shown that having more data results in better performance <CIT> '
'Such a coding procedure covers , for example , how segmentation of a corpus is performed , if multiple tagging is allowed and if so , is it unlimited or are there just certain combinations of tags not allowed , is look ahead permitted , etc For further information on coding procedures we want to refer to <OTH> and for good examples of coding books see , for example , <CIT> , <OTH> , or <OTH> '
'2 Statistical Word Alignment According to the IBM models <CIT> , the statistical word alignment model can be generally represented as in Equation -LRB- 1 -RRB- '
'2 Data 21 The US Congressional Speech Corpus The text used in the experiments is from the United States Congressional Speech corpus <OTH> , which is an XML formatted version of the electronic United States Congressional Record from the Library of Congress1 '
'Among the four steps , the hypothesis alignment presents the biggest challenge to the method due to the varying word orders between outputs from different MT systems <CIT> '
'<CIT> has proposed a bootstrapping method for word sense disambiguation '
'Language modeling <OTH> , noun-clustering <OTH> , constructing syntactic rules for SMT <OTH> , and finding analogies <CIT> are examples of some of the problems where we need to compute relative frequencies '
'This has been now an active research area for a couple of decades <CIT> '
'Giza + + is a freely available implementation of IBM Models 1-5 <CIT> and the HMM alignment <OTH> , along with various improvements and modifications motivated by experimentation by Och & Ney <OTH> '
'Although the BLEU <CIT> score from Finnish to English is 218 , the score in the reverse direction is reported as 130 which is one of the lowest scores in 11 European languages scores <OTH> '
'<OTH> for English , but not identical to strictly anaphoric ones5 <CIT> , since a non-anaphoric NP can corefer with a previous mention '
'We report case sensitive Bleu <CIT> scoreBleuCforallexperiments '
'Using GIZA + + model 4 alignments and Pharaoh <CIT> , we achieved a BLEU score of 03035 '
'Making such an assumption is reasonable since POS taggers that can achieve accuracy of 96 % are readily available to assign POS to unrestricted English sentences <CIT> '
'3 OverviewofExtractionWork 31 English As one mightexpect , the bulk of the collocation extractionwork concernsthe English language : <CIT> , amongmany others1 '
'6 Experiments We evaluated the translation quality of the system using the BLEU metric <CIT> '
'The loglinear model feature weights were learned using minimum error rate training -LRB- MERT -RRB- <OTH> with BLEU score <CIT> as the objective function '
'Sentiment classification at the sentence-level has also been studied <CIT> '
'This difference was highlighted in the 3http : \/ \/ w3msivxuse\/jha\/maltparser \/ studyof <CIT> , whichshowed that the difference is reflected directly in the error distributions of the parsers '
'43 Relaxing Length Restrictions Increasing the maximum phrase length in standard phrase-based translation does not improve BLEU <CIT> '
'The importance of including single nonheadwords is now also uncontroversial <CIT> , and the current paper has shown the importance of including two and more nonheadwords '
'We annotated with the BIO tagging scheme used in syntactic chunkers <CIT> '
'We follow the approach of bootstrapping from a model with a narrower parameter space as is done in , eg Och and Ney <OTH> and <CIT> '
'6 Results We trained on the standard Penn Treebank WSJ corpus <CIT> '
'For instance , BLEU and ROUGE <OTH> are based on n-gram precisions , METEOR <OTH> and STM <CIT> use word-class or structural information , Kauchak -LRB- 2006 -RRB- leverages on paraphrases , and TER <OTH> uses edit-distances '
'Rulesize and lexicalization affect parsing complexity whether the grammar is binarized explicitly <CIT> or implicitly binarized using Early-style intermediate symbols <OTH> '
'Many methods for calculating the similarity have been proposed <CIT> '
'This idea of employing n-gram co-occurrence statistics to score the output of a computer system against one or more desired reference outputs has its roots in the BLEU metric for machine translation <CIT> and the ROUGE <OTH> metric for summarization '
'Nakagawa <OTH> and <CIT> <OTH> also showed the effectiveness of global features in improving the accuracy of graph-based parsing , using the approximate Gibbs sampling method and a reranking approach , respectively '
'One interesting approach to extending the current system is to introduce a statistical translation model <CIT> to filter out irrelevant translation candidates and to extract the most appropriate subpart from a long English sequence as the translation by locally aligning the Japanese and English sequences '
'294 Fraser and Marcu Measuring Word Alignment Quality for Statistical Machine Translation 22 Measuring Translation Performance Changes Caused By Alignment In phrased-based SMT <CIT> the knowledge sources which vary with the word alignment are the phrase translation lexicon -LRB- which maps source phrases to target phrases using counts from the word alignment -RRB- and some of the word level translation parameters -LRB- sometimes called lexical smoothing -RRB- '
'Such techniques are currently being applied in many areas , including language identification , authorship attribution <OTH> , text genre classification <OTH> , topic identification <OTH> , and subjective sentiment classification <CIT> '
'<OTH> present a probabilistic model for pronoun resolution trained on a small subset of the Penn Treebank Wall Street Journal corpus <CIT> '
'This feature is implemented by using the IBM-1 lexical parameters <CIT> '
'In this paper we present results on using a recent phrase-based SMT system , PHARAOH <CIT> , for NLG1 Although moderately effec1We also tried IBM Model 4\/REWRITE <OTH> , a word-based SMT system , but it gave much worse results '
'In phrase-based SMT systems <CIT> , foreign sentences are firstly segmented into phrases which consists of adjacent words '
'Automatic identification of subjective content often relies on word indicators , such as unigrams <CIT> or predetermined sentiment lexica <OTH> '
'Some regarded Wikipedia as the corpora and applied hand-crafted or machine-learned rules to acquire semantic relations <CIT> '
'Since Soon <OTH> started the trend of using the machine learning approach by using a binary classifier in a pairwise manner for solving co-reference resolution problem , many machine learning-based systems have been built , using both supervised and , unsupervised learning methods <CIT> '
'We used these weights in a beam search decoder to produce translations for the test sentences , which we compared to the WMT07 gold standard using Bleu <CIT> '
'The training methods of LRM-F and SVM-F were useful to improve the F M - scores of LRM and SVM , as reported in <CIT> '
'These categories were automatically generated using the labeled parses in Penn Treebank <CIT> and the labeled semantic roles of PropBank <OTH> '
'or cooking , which agrees with the knowledge presented in previous work <CIT> '
'This ITG constraint is characterized by the two forbidden structures shown in Figure 1 <CIT> '
'One is distortion model <CIT> which penalizes translations according to their jump distance instead of their content '
'-LRB- ii -RRB- Apply some statistical tests such as the Binomial Hypothesis Test <OTH> and loglikelihood ratio score <CIT> to SCCs to filter out false SCCs on the basis of their reliability and likelihood '
'Second , it can be applied to control the quality of parallel bilingual sentences mined from the Web , which are critical sources for a wide range of applications , such as statistical machine translation <CIT> and cross-lingual information retrieval <OTH> '
'For example , the word alignment computed by GIZA + + and used as a basis to extract the TTS templates in most SSMT systems has been observed to be a problem for SSMT <CIT> , due to the fact that the word-based alignment models are not aware of the syntactic structure of the sentences and could produce many syntax-violating word alignments '
'Baseline We use the Moses MT system <OTH> as a baseline and closely follow the example training procedure given for the WMT-07 and WMT-08 shared tasks4 In particular , we perform word alignment in each direction using GIZA + + <CIT> , apply the grow-diag-finaland heuristic for symmetrization and use a maximum phrase length of 7 '
'2 Related Work This method is similar to block-orientation modeling <CIT> and maximum entropy based phrase reordering model <OTH> , in which local orientations -LRB- left\/right -RRB- of phrase pairs -LRB- blocks -RRB- are learned via MaxEnt classifiers '
'The straight-forward way is to first generate the best BTG tree for each sentence pair using the way of <CIT> , then annotate each BTG node with linguistic elements by projecting source-side syntax tree to BTG tree , and finally extract rules from these annotated BTG trees '
'Measurement of Beliability The Kappa Statistic Following Jean <CIT> , we use the kappa statistic <OTH> to measure degree of agreement among subjects '
'As <CIT> point out , WordNet does not encode antonymy across part-of-speech -LRB- for example , legallyembargo -RRB- '
'In comparison , we deployed the GIZA + + MT modeling tool kit , which is an implementation of the IBM Models 1 to 4 <CIT> '
'This is the shared task baseline system for the 2006 NAACL\/HLT workshop on statistical machine translation <OTH> and consists of the Pharaoh decoder <OTH> , SRILM <OTH> , GIZA + + <CIT> , mkcls <CIT> , Carmel ,1 and a phrase model training code '
'<CIT> claimed that this approximation achieved essentially equivalent performance to that obtained when directly using the loss as the objective , O = lscript '
'There also have been prior work on maintaining approximate counts for higher-order language models -LRB- LMs -RRB- -LRB- <CIT> -RRB- operates under the model that the goal is to store a compressed representation of a disk-resident table of counts and use this compressed representation to answer count queries approximately '
'Therefore the probability of alignment aj for position j should have a dependence on the previous alignment position O j_l : P -LRB- -LRB- \/ j -LRB- -LRB- \/ j-1 -RRB- A similar approach has been chosen by <OTH> and <OTH> '
'54 Domain Adaptation 541 Feature-Based Approaches Onewayofadaptingalearnertoanewdomainwithout using any unlabeled data is to only include features that are expected to transfer well <CIT> '
'Our test set is 3718 sentences from the English Penn treebank <OTH> which were translated into German '
'Given a set of evidences E over all the relevant word pairs , in <CIT> , the probabilistic taxonomy learning task is defined as the problem of finding the taxonomy hatwideT that maximizes the 67 probability of having the evidences E , ie : hatwideT = arg max T P -LRB- E T -RRB- In <CIT> , this maximization problem is solved with a local search '
'Inter-annotator agreement was measured using the kappa -LRB- K -RRB- statistics <CIT> on 1,502 instances -LRB- three Switchboard dialogues -RRB- marked by two annotators who followed specific written guidelines '
'<CIT> noted that the unigram unpredictable might have a positive sentiment in a movie review -LRB- eg unpredictable plot -RRB- , but could be negative in the review of an automobile -LRB- eg unpredictable steering -RRB- '
'To prune away those pairs , we used the log-likelihood-ratio algorithm <CIT> to compute the degree of association between the verb and the noun in each pair '
'An alternative approach to extracting the informal phrases is to use a bootstrapping algorithm -LRB- eg , <CIT> -RRB- '
'Accuracy on sentiment classification in other domains exceeds 80 % <CIT> '
'Unlike <CIT> , the ke ~ vord rnay be part of a Chinese word '
'Re-ordering effects across languages have been modeled in several ways , including word-based <CIT> , template-based <OTH> and syntax-based <OTH> '
'<CIT> generate ill-formed sentences by sampling a probabilistic language model and end up with pseudo-negative examples which resemble machine translation output more than they do learner texts '
'For the combined set -LRB- ALL -RRB- , we also show the 95 % BLEU confidence interval computed using bootstrap resampling <CIT> '
'33 Model Construction The head transducer model was trained and evaluated on English-to-Mandarin Chinese translation of transcribed utterances from the ATIS corpus <OTH> '
'We have applied it to the two data sets mentioned in <CIT> '
'Bilingual bracketing methods were used to produce a word alignment in <CIT> '
'Probabilistic generative models like IBM 1-5 (Brown et al., 1993), HMM (Vogel et al., 1996), ITG (Wu, 1997), and LEAF (Fraser and Marcu, 2007) define formulas for P(f | e) or P(e, f), with ok-voon ororok sprok at-voon bichat dat erok sprok izok hihok ghirok totat dat arrat vat hilat ok-drubel ok-voon anok plok sprok at-drubel at-voon pippat rrat dat ok-voon anok drok brok jok at-voon krat pippat sat lat wiwok farok izok stok totat jjat quat cat lalok sprok izok jok stok wat dat krat quat cat lalok farok ororok lalok sprok izok enemok wat jjat bichat wat dat vat eneat lalok brok anok plok nok iat lat pippat rrat nnat wiwok nok izok kantok ok-yurp totat nnat quat oloat at-yurp lalok mok nok yorok ghirok clok wat nnat gat mat bat hilat lalok nok crrrok hihok yorok zanzanok wat nnat arrat mat zanzanat lalok rarok nok izok hihok mok wat nnat forat arrat vat gat Figure 1: Word alignment exercise (Knight, 1997).'
'Still , however , such techniques often require seeds , or prototypes -LRB- cf , <CIT> -RRB- which are used to prune search spaces or direct learners '
'As resolving direct anaphoric descriptions -LRB- the ones where anaphor and antecedent have the same head noun -RRB- is a much simpler problem with high performance rates as shown in previous results <CIT> , these heuristics should be applied first in a system that resolves definite descriptions '
'et al , 2004 ; Collins-Thompson and Callan , 2005 ; <CIT> and Ramage , 2007 -RRB- '
'We benchmark our results against a model -LRB- Hiero -RRB- which was directly trained to optimise BLEUNIST using the standard MERT algorithm <CIT> and the full set of translation and lexical weight features described for the Hiero model <OTH> '
'Although to a lesser extent , measures of word relatedness have also been applied on other languages , including German <CIT> , Chinese <OTH> , Dutch <OTH> and others '
'This method is very similar to some ideas in domain adaptation <CIT> , but we argue that the underlying problems are quite different '
'This wrong translation of content words is similar to the incorrect omission reported in <CIT> , which both hurt translation adequacy '
'1 Introduction In global linear models -LRB- GLMs -RRB- for structured prediction , -LRB- eg , <CIT> -RRB- , the optimal label y for an input x is y = arg max yY -LRB- x -RRB- w f -LRB- x , y -RRB- -LRB- 1 -RRB- where Y -LRB- x -RRB- is the set of possible labels for the input x ; f -LRB- x , y -RRB- Rd is a feature vector that represents the pair -LRB- x , y -RRB- ; and w is a parameter vector '
'We use the same preprocessing steps as Turian and Melamed <OTH> : during both training and testing , the parser is given text POS-tagged by the tagger of <CIT> , with capitalization stripped and outermost punctuation removed '
'2 Phrase-based SMT We use a phrase-based SMT system , Pharaoh , <OTH> , which is based on a log-linear formulation <CIT> '
'To quickly -LRB- and approximately -RRB- evaluate this phenomenon , we trained the statistical IBM wordalignment model 4 <CIT> ,1 using the GIZA + + software <OTH> for the following language pairs : ChineseEnglish , Italian English , and DutchEnglish , using the IWSLT-2006 corpus <OTH> for the first two language pairs , and the Europarl corpus <OTH> for the last one '
'Alignment spaces can emerge from generative stories <CIT> , from syntactic notions <OTH> , or they can be imposed to create competition between links <OTH> '
'1 Introduction Many different statistical tests have been proposed to measure the strength of word similarity or word association in natural language texts <CIT> '
'For example , it has been observed that texts often contain multiple opinions on different topics <CIT> , which makes assignment of the overall sentiment to the whole document problematic '
'In each experiment , performance IMutu '' , d Information provides an estimate of the magnitude of the ratio t -RRB- ctw -LRB- -LRB- - n the joint prol -RRB- ability P -LRB- verb\/noun ,1 -RRB- reposition -RRB- , and the joint probability a ~ suming indcpendcnce P -LRB- verb\/noun -RRB- P -LRB- prcl -RRB- osition -RRB- s -LRB- : -LRB- , <CIT> '
'We use the IBM Model 1 <CIT> -LRB- uniform distribution -RRB- and the Hidden Markov Model -LRB- HMM , first-order dependency , <OTH> -RRB- to estimate the alignment model '
'The parameters , j , were trained using minimum error rate training <CIT> to maximize the BLEU score <OTH> on a 150 sentence development set '
'For these first SMT systems , translation-model probabilities at the sentence level were approximated from word-based translation models that were trained by using bilingual corpora <CIT> '
'<CIT> binarize grammars into CNF normal form , while <OTH> allow only Griebach-Normal form grammars '
'Equation -LRB- 10 -RRB- is of interest because the ratio p -LRB- C v , r -RRB- \/ p -LRB- C r -RRB- can be interpreted as a measure of association between the verb v and class C This ratio is similar to pointwise mutual information <CIT> and also forms part of Resniks association score , which will be introduced in Section 6 '
'In <CIT> , lexical 72 features were limited on each single side due to the feature space problem '
'The SPECIALIST minimal commitment parser relies on the SPECIALIST Lexicon as well as the Xerox stochastic tagger <CIT> '
'The idea caught on very quickly : Suhm and Waibel <OTH> , Mast et aL <OTH> , Warnke et al '
'<CIT> integrated a WSD system into a phrase-based SMT system , Pharaoh <OTH> '
'Recently , it has gained renewed attention as empirical methods in parsing have emphasized the importance of relations between words -LRB- see , eg , <CIT> -RRB- , which is what dependency grammars model explicitly , but context-free phrase-structure grammars do not '
'It is clear that Appendix B contains far fewer true non-compositional phrases than Appendix A 7 Related Work There have been numerous previous research on extracting collocations from corpus , eg , <OTH> and <OTH> '
'model reranking has also been established , both for synchronous binarization <CIT> and for target-only binarization <OTH> '
'From wordlevel alignments , such systems extract the grammar rules consistent either with the alignments and parse trees for one of languages <CIT> , or with the the word-level alignments alone without reference to external syntactic analysis <OTH> , which is the scenario we address here '
'A remedy is to aggressively limit the feature space , eg to syntactic labels or a small fraction of the bi-lingual features available , as in <OTH> , but that reduces the benefit of lexical features '
'<OTH> Classification allows a word to align with a target word using the collective translation tendency of words in the same class '
'51 The Prague Dependency Tree Bank -LRB- PDT in the sequel -RRB- , which has been inspired by the build-up of the Penn Treebank <CIT> , is aimed at a complex annotation of -LRB- a part of -RRB- the Czech National Corpus -LRB- CNC in the sequel -RRB- , the creation of which is under progress at the Department of Czech National Corpus at the Faculty of Philosophy , Charles University -LRB- the corpus currently comprises about 100 million tokens of word forms -RRB- '
'The overall POS tag distribution learned by EM is relatively uniform , as noted by <CIT> , and it tends to assign equal number of tokens to each tag label whereas the real tag distribution is highly skewed '
'1 Introduction Since 1995 , a few statistical parsing algorithms <CIT> demonstrated a breakthrough in parsing accuracy , as measured against the University of Pennsylvania TREEBANK as a gold standard '
'The results are quite promising : our extraction method discovered 89 % of the WordNet cousins , and the sense partitions in our lexicon yielded better values <CIT> than arbitrary sense groupings on the agreement data '
'We use the following features for our rules:  sourceand target-conditioned neg-log lexical weights as described in (Koehn et al. , 2003b)  neg-log relative frequencies: left-handside-conditioned, target-phrase-conditioned, source-phrase-conditioned  Counters: n.o. rule applications, n.o. target words  Flags: IsPurelyLexical (i.e. , contains only terminals), IsPurelyAbstract (i.e. , contains only nonterminals), IsXRule (i.e. , non-syntactical span), IsGlueRule 139  Penalties: rareness penalty exp(1  RuleFrequency); unbalancedness penalty |MeanTargetSourceRatio  n.o. source words n.o. target words| 4 Parsing Our SynCFG rules are equivalent to a probabilistic context-free grammar and decoding is therefore an application of chart parsing.'
'We examine Structural Correspondence Learning -LRB- SCL -RRB- <OTH> for this task , and compare it to several variants of Self-training <CIT> '
'We observe that AER is loosely correlated to BLEU -LRB- = 081 -RRB- though the relation is weak , as observed earlier by <CIT> '
'1 Introduction Robust statistical syntactic parsers , made possible by new statistical techniques <OTH> and by the availability of large , hand-annotated training corpora such as WSJ <OTH> and Switchboard <OTH> , have had a major impact on the field of natural language processing '
'The data set consisting of 249,994 TFSs was generated by parsing the Figure 3 : The size of Dpi ; for the size of the data set 800 bracketed sentences in the Wall Street Journal corpus -LRB- the first 800 sentences in Wall Street Journal 00 -RRB- in the Penn Treebank <CIT> with the XHPSG grammar <OTH> '
'In this method , the decision list -LRB- DL -RRB- learning algorithm <CIT> is used '
'<CIT> dealt with this problem largely by producing an unsupervised learning algorithm that generates probabilistic decision list models of word senses from seed collocates '
'and CAUS ate slgmficantly different for unaccusattve and object-dtop verbs, indicating that we need additional featules that have different values across these two classes In Section 2 1, we noted the differing semantic role asmgnments for the verb classes, and hypothesized that these differences would affect the expression of syntactic features that ate countable in a corpus For example, the c ~bs feature approximates sen]antic role reformation b.~ encoding the oxerlap beh~een nouns that can occur m the ~ubject and object positions of a cau~ative xetb Here x~e suggest another feature, that of ammacy of subject, that is intended to distinguish nouns that receive an Agent role flora those that receive a Theme role Recall that objectdrop verbs assign Agent to their subject in both the transitive and intransitive alternations, while unaccusattves assign Agent to their subject only in the transitive, and Theme m the intransitive We expect then that object-drop verbs will occur more often with an animate subject Note again that ~e are 20 II Features [Acc% SE% II I VBD ACT INTR CAUS I 63 7 0 6 ] VBD ACT INTR CAUS PRO 70 7 0 4 Table 6 Percentage Accuracy (Acc%) and Standard Error (SE%) of C5 0, W~th and W~thout New PRO Feature, All Verb Classes (33 8% basehne) making use of frequency dmtnbutmns--the clatm ~s not that only Agents can be ammate, but rather that nouns that receive the Agent role will more often be ammate than nouns that receive the Theme role A problem w~th a feature hke ammacy ~s that ~t requires etther manual determmatmn of the antmacy of extracted subjects, or reference to an on-hne resource such as WordNet for determining ammacy To approximate ammacy w~th a feature that can be extracted automatically, and w~thout reference to a resource external to the corpus, we instead count pronouns (other than ~t) m subject positron The assumptmn ~s that the words I, we, you, she, he, and they most often refer to ammate ent~tms The values for the new feature, P~.O, were determined by automatmally extracting all subject/verb tuples including our 59 examples verbs (from the WSJ88 parsed corpus), and computing the ratm of occurrences of pronouns to all subjects We again apply t-tests to our new data to determine whether the sets of PRo values d~ffer across the verb classes Interestingly, we find that the Prto values for unaccusat~ve verbs (the only class to ass~gn Theme role to the sub tect m one of tts alternatmns) are s~gmficantly dtffe~ent from those for both unergatlve and object-drop verbs (p< 05) Moreover, the PRo values for unergat~ve and object-drop verbs (whose subjects are Agents m bo~h alternatmns) are not s~gmficantly d~fferent Th~s pattern confirms the abd~ty of the feature to capture the thematm d~stmctmn between unaccusat~ve verbs and the other two classes Table 6 shows the result of applying C5 0 (10-fold eross-vahdatmn repeated 50 t~mes) to the three-x~ay classfficatmn task using the PRo feature m conjunctmn w~th the four previous features ~.ccuracy ranproves to over 70%, a teductmn m the error rate of almost 20% due to th~s single nex~ feature Moteover, classifying the unaccusat~ve an2 object-drop verbs using the new feature m conjunctmn w~th the prevmus four leads to accuracy of over 68% (compared to 58% w~thout PRo) We conclude that this feature ~s ~mportant in d~stmgmshlng unaccusat~ve and object-drop verbs, and hkely contributes to the tmprovement m the three-way classtficatton because of th~s Future work wdl examine the performance w~thm the verb classes of th~s new set of features to see whether accuracy has also tmproved for unergatire verbs 5 Conclusions In thin paper, we have presented an m-depth case study, m whmh we investigate varmus machine learnmg techmques to automatically classify a set of verbs, based on dlstnbutmnal features extracted from a very large corpus Results show that a small number of hngmstlcally motivated grammatical features are sufficmnt to reduce the error rate by mote than 50% over chance, acluevmg a 70% acctuacy rate m a three-way classfficatmn task Tins leads us to conclude that corpus data is a usable repository of verb class mformatmn On one hand ~e observe that semantlc propemes of verb classes (such as causatlvlty, or ammacy of subject) may be usefully approximated through countable syntactic features Even with some noise, lexmal propertms are reflected m the corpus robustly enough to positively contribute m classlficatmn On the other hand, however, we remark that deep hngumtm analysis cannot be ehmmated--m our approach, it is embedded m the selection of the features to count We also think that using hngumtlcally motivated features makes the approach very effective and easdy scalable we report a 56% reductmn m error rate, w~th only five features that are relatwely straightforward to count Acknowledgements This research was partly sponsored by the S~ lss Natmnal Scmnce Foundatmn, under fello~slup 821046569 to Paola Merlo, by the US Natmnal Scmnce Foundatmn, under grants #9702331 and #9818322 to $uzanne Stevenson, and by the Infotmatton Sciences Councd of Rutgers Umverslty ~,~,e thank Martha Palmer for getting us started on tlus ~ork and Mmhael Colhns for gwmg us access to the output of his parser We gratefully acknowledge the help of Ixlva Dickinson, ~ho calculated no~mahzatmns of the corpus data Appendix A The une~gatx~es are manner of morton ~erbs jumptd rushed, malched, leaped floated, laced, huslwd uandered, vaulted, paraded, galloped, gl,ded, hzked hopped jogged, scooted, ncurlzed, ~kzpped, hptoed, trotted The unaccusau~es are verbs of change of state opened, exploded, flooded, dzs~olved, cracked, hardened bozled, melted,.fractured,,ol,dzfied, collapsed cooled folded, w~dened, changed, clealed, dzwded, ~,mmered stabdzzed The object-dlop verbs are unspecffied object altelnatron verbs played, painted, k,cked, carved, reaped, washed, danced, yelled, typed, kmtted bolrowed mhet21 tted, organtzed, rented, sketched, cleaned, packed, studted, swallowed, called References Thomas G Bever 1970 The cogmtwe basis for hngmstlc structure In J R Hayes, e&tor, Cognttson and the Development of Language John Wdey, New York Michael Brent 1993 From grammar to le~con Unsupervmed learmng of [ex~cal syntax Computational Linguistics, 19(2) 243-262 Edward Bnscoe and Ann Copestake 1995 Lex~cal rules m the TDFS framework Techmcal report, AcquflexI I Working'' Papers Anne-Marm Brousseau and Ehzabeth R~tter 1991 A non-umfied analysis of agent~ve verbs In West Coast Conference on Formal Lmgutstzcs, number 20, pages 53-64 M~chael John Colhns 1997 Three generaUve, lexacahsed models for statistical parsmg In Proc of the ~5th Annual Meeting of the ACL, pages 16-23 Hoa Trang Dang, Kann K~pper, Martha Palmer, and Joseph Rosenzwe~g 1998 Investtgatmg regular sense extenmons based on mteresecttve Levm classes In Proc of the 361h Annual Meeting of the ACL and the 171h [nternatwnal Conference on Computatwnal L,ngu,st,cs (COLING-A CL ''98), pages 293-299, Montreal, Canada Umvers~t6 de Montreal Bonme Dorr and Doug Jones 1996 Role of word sense d~samb~guatmn m lexacal acqms~tmn Predmtmg semantics from syntactic cues In Proc of the 161h Internattonal Conference on Computat*onal Lmgutsttcs, pages 322-327, Copenhagen, Denmark COLING Bonnie Dorr 1997 Large-scale chctmnary constructmn for foreign language tutonng and mterhngual machine translatmn Machine Translatton, 12 1-55 Hana Fd~p M~chael Tanenhaus, Greg Carlson, Paul AIlopenna, and Joshua Blatt 1999 Reduced relatives judged hard require constraint-based analyses In P Merlo and S Stevenson, echtors, Sentence Processmg and the Lextcon Formal, Computational, and Ezpertmental Perspectives, John Benjamms, Holland Ken Hale and Jay Keyser 1993 On argument structure and the lexacal representatmn of s:~ ntact~c relatmns In K Hale and J Keyser, editors, The t'',ew from Budding ~0, pages 53-110 MIT Press Juchth L Ixlavans and Martin Chodorow 1992 Degrees of stat~vlty The lexacal representatmn of verb aspect In Proceedmg~ of the Fourteenth International Conference on Computahonal Lmgmst,cs Juchth Ixlavans and Mm-Yen Kan 1998 Role of ~erbs m document analysis In Proc of the 361h Annual Meeting of the ACL and the 171h [nternatzonal Conference on Computational Lmgutsttcs ( C O L L''v G4 C L ''98), pages 680-686, Montreal, Canada Umvers~te de Montreal Beth Levm and/Vlalka Rappapti(t''Hovav 1995 (Jnaccusatwlty MIT Press, Cambridge, MA Beth Le~m 1993 Enghsh Verb Clas~e~ and 4lternatwns Chacago Umvers~ty Press, Chicago, IL Maryellen C MacDonald 1994 Probablhstlc constramts and syntactic amblgtuty resolution Language and Cognltzve Processes, 9(2) 157-201 Paola Merlo and Suzanne Stevenson 1998 What grammars tell us about corpora the case of reduced relative clauses In P1oceedmgs of the Slzth Workshop on Very Large Corpora, pages 134-142, Montreal, CA George Miller, R Beckw~th, C Fellbaum, D Gross, and Ix I~hller 1990 Fwe papers on Wordnet Techmcal report, Cogmtzve Scmnce Lab, Princeton Ual~erstt~ Martha Palmer 1999 Coasmtent criteria for sense distmctmns Computmg ]or the Hamamttes Fernando Perelra, Naftah Tlshby, and Ldhan Lee 1993 Dlstrabutmnal clustering of enghsh words [n Proc of the 31th 4nnual Meeting of the 4CL, pages 183-190 Fernando Perexra, Ido Dagan, and Lalhan Lee 1997 Slmdanty-based methods for word sense dlsamblguatmn In Proc of the 35th Annual Meeting of the 4 CL and the 8th Conf of the E 4 CL (A CL/EA CL ''97) pages 56 -63 Geoffrey K Pullum 1996 Learnabthty, hyperlearnrag, and the poverty of the sttmulus In Jan Johnson, Matthew L Jute, and Jen L Moxley, editors, ~nd Annual Meeting of the Berkeley Lmgutstzcs Soctety General Sesston and Parasesswn on the Role of Learnabdzty m Grammatzcal Theory, pages 498-513, Berkeley, Cahforma Berkeley Linguistics Socmty James Pustejovsky 1995 The Generatwe Lexicon MIT Press J Ross Qumlan 1992 C$ 5 Programs fo~ Machine Learning Series m Machme Learning Morgan Ixaufmann, San Mateo, C 4.'
'Algorithm 1 SCL <CIT> 1 : Select m pivot features '
'<CIT> applies this approach to the so-called IBM Candide system to build context dependent models , compute automatic sentence splitting and to improve word reordering in translation '
'The techniques examined are Structural Correspondence Learning -LRB- SCL -RRB- <CIT> and Self-training <OTH> '
'The perceptron has been used in many NLP tasks , such as POS tagging <CIT> , Chinese word segmentation <OTH> and so on '
'However, in order to cope with the prediction errors of the classi er, we approximate a74a51a18a77a76 a28 with an a80 -gram language model on sequences of the re ned tag labels: a38a58a39 a41 a81 a43a82a44a47a46a83a48a47a50a75a44a15a52 a53a9a54a49a84 a53a9a54a83a84a49a85a9a86a13a87a89a88a91a90 a55a57a56 a38a40a39 a81 a59a60a42a61 (2) a92 a44a47a46a83a48a47a50a75a44a15a52 a53a9a54 a84 a53a9a54a83a84a49a85a9a86a13a87a89a88a91a90 a93 a94a96a95 a55a57a56a98a97a66a99 a95 a59a100a27a61 (3) In order to estimate the conditional distribution a101 a18a20a19a15a21 a1 a68 a72 a28 we use the general technique of choosing the maximum entropy (maxent) distribution that properly estimates the average of each feature over the training data (Berger et al. , 1996).'
'Presently , many systems <OTH> , <OTH> , <OTH> , <OTH> focus on online recognition of proper nouns , and have achieved inspiring results in newscorpus but will be deteriorated in special text , such as spoken corpus , novels '
'For MCE learning , we selected the reference compression that maximize the BLEU score <CIT> -LRB- = argmax rR BLEU -LRB- r , R r -RRB- -RRB- from the set of reference compressions and used it as correct data for training '
'In addition to adapting the idea of Head Word Chains <CIT> , we also compared the input sentences argument structures against the treebank for certain syntactic categories '
'312 Kappa Kappa <OTH> is an evaluation measure which is increasingly used in NLP annotation work <CIT> '
'1 Introduction Text-to-text generation is an emerging area of research in NLP <CIT> '
'Having a single , canonical tree structure for each possible alignment can help when flattening binary trees , as it indicates arbitrary binarization decisions <CIT> '
'We use GIZA + + <OTH> to train generative directed alignment models : HMM and IBM Model4 <CIT> from training record-text pairs '
'<CIT> introduced a transformationbased learning method which considered chunking as a kind of tagging problem '
'The Penn Treebank annotation <CIT> was chosen to be the first among equals : it is the starting point for the merger and data from other annotations are attached at tree nodes '
'We can then use this newly identified set to : -LRB- 1 -RRB- use <CIT> s method to find the orientation for the terms and employ the terms and their scores in a classifier , and -LRB- 2 -RRB- use <CIT> s method to find the orientation for the terms and add the new terms as additional seed terms for a second iteration As opposed to <CIT> <OTH> , we do not use the web as a resource to find associations , rather we apply the method directly to in-domain data '
'The second uses Lin dependency similarity , a syntacticdependency based distributional word similarity resource described in <CIT> 9 '
'A monotonous segmentation copes with monotonous alignments , that is , j -LRB- k aj -LRB- ak following the notation of <CIT> '
'We also can not use prior graph construction methods for the document level -LRB- such as physical proximity of sentences , used in <CIT> -RRB- at the word sense level '
'The model scaling factors are optimized on the development corpus with respect to mWER similar to <CIT> '
'For these classications , we calculated a kappa statistic of 0528 <CIT> '
'Feature weights of both systems are tuned on the same data set3 For Pharaoh , we use the standard minimum error-rate training <CIT> ; and for our system , since there are only two independent features -LRB- as we always fix = 1 -RRB- , we use a simple grid-based line-optimization along the language-model weight axis '
'In most statistical machine translation -LRB- SMT -RRB- models <CIT> , some of measure words can be generated without modification or additional processing '
'The resulting corpus contains 385 documents of American English selected from the Penn Treebank <CIT> , annotated in the framework of Rhetorical Structure Theory '
'We solve SAT analogies with a simplified version of the method of <CIT> '
'Also , the aspect of generalizing features across different products is closely related to fully supervised domain adaptation <CIT> , and we plan to combine our approach with the idea from Daume III -LRB- 2007 -RRB- to gain insights into whether the composite back-off features exhibit different behavior in domain-general versus domain-specific feature sub-spaces '
'1 Introduction The task of sentence compression -LRB- or sentence reduction -RRB- can be defined as summarizing a single sentence by removing information from it <CIT> '
'Because of these kinds of results , the vast majority of statistical parsing work has focused on parsing as a supervised learning problem <CIT> '
'5 The SemCor collection <OTH> is a subset of the Brown Corpus and consists of 352 news articles distributed into three sets in which the nouns , verbs , adverbs , and adjectives have been manually tagged with their corresponding WordNet senses and part-of-speech tags using Brills tagger <OTH> '
'There are also approaches to anaphora resolution using unsupervised methods to extract useful information , such as gender and number <OTH> , or contextual role-knowledge <CIT> '
'<CIT> discuss the influence of bias towards highor low-frequency items for different tasks -LRB- correlation with WordNet-derived neighbor sets and pseudoword disambiguation -RRB- , and it would not be surprising if the different high-frequency bias were leading to different results '
'Some of these have been previously employed for various tasks by Gabrilovich and Markovitch , <OTH> ; Overell and Ruger <OTH> , <CIT> , and Suchanek et al '
'Model 4 of <CIT> is also a first-order alignment model -LRB- along the source positions -RRB- like the HMM , trot includes also fertilities '
', ie : -LRB- ll -RRB- Lj = ~ maz -LRB- zi -LRB- j , u -RRB- -RRB- i = I where xi -LRB- j , u -RRB- E Qi and max -LRB- xi -LRB- j , u -RRB- -RRB- is the highest score in the line of the matrix Qi which corresponds to the head word sense j n is the number of modifiers of the head word h at the current tree level , and k i Lj = j ~ l Lj where k is the number of senses of the head word h The reason why gj -LRB- I0 -RRB- is calculated as a sum of the best scores -LRB- ll -RRB- , rather than by using the traditional maximum likelihood estimate <CIT> -LRB- Gah eta -LRB- '
'SIGHAN , the Special Interest Group for Chinese Language Processing of the Association for Computational Linguistics , conducted three prior word segmentation bakeoffs , in 2003 , 2005 and 2006 <CIT> , which established benchmarks for word segmentation and named entity recognition '
'We show translation results in terms of the automatic BLEU evaluation metric <CIT> on the MT03 Arabic-English DARPA evaluation test set consisting of a212a89a212a89a87 sentences with a98a89a212a161a213a89a214a89a215 Arabic words with a95 reference translations '
'Wu and Weld <OTH> and <CIT> <OTH> calculate the overlap between contexts of named entities and candidate articles from Wikipedia , using overlap ratios or similarity scores in a vector space model , respectively '
'In our approach , equation -LRB- 1 -RRB- is further normalized so that the probability for different lengths of F is comparable at the word level : m m j n i ijm eft l EFP \/ 1 10 -RRB- -LRB- -RRB- 1 -LRB- 1 -RRB- -LRB- + = = = -LRB- 2 -RRB- The alignment models described in <CIT> are all based on the notion that an alignment aligns each source word to exactly one target word '
'3http : \/ \/ wwwopenofficeorg Another corpora based method due to Turney and Littman <OTH> tries to measure the semantic orientation O -LRB- t -RRB- for a term t by O -LRB- t -RRB- = summationdisplay tiS + PMI -LRB- t , ti -RRB- summationdisplay tjS PMI -LRB- t , tj -RRB- where S + and S are minimal sets of polar terms that contain prototypical positive and negative terms respectively , and PMI -LRB- t , ti -RRB- is the pointwise mutual information <CIT> between the terms t and ti '
'To evaluate sentence automatically generated with taking consideration word concatenation into by using references varied among humans , various metrics using n-gram precision and word accuracy have been proposed : word string precision <OTH> for summarization through word extraction , ROUGE <OTH> for abstracts , and BLEU <CIT> for machine translation '
'3 Evaluation We trained our model parameters on a subset of the provided dev2006 development set , optimizing for case-insensitive IBM-style BLEU <CIT> with several iterations of minimum error rate training on n-best lists '
'Levin <OTH> assumes that the syntactic realization of a verb ''s arguments is directly correlated with its meaning -LRB- cf '
'The f-structures are created automatically by annotating nodes in the gold standard WSJ trees with LFG functional equations and then passing these equations through a constraint solver <CIT> '
'First , we trained a finitestate shallow parser on base phrases extracted from the Penn Wall St Journal -LRB- WSJ -RRB- Treebank <CIT> '
'-LRB- Termbased versions of this premise have motivated much sentiment-analysis work for over a decade <CIT> -RRB- '
'In particular , knowing a little about the structure of a language can help in developing annotated corpora and tools , since a little knowledge can go a long way in inducing accurate structure and annotations <CIT> '
'As shown in <CIT> , using this representation , a linear classifier can not distinguish sentences sampled from a trigram and real sentences '
'Researchers have focused on learning adjectives or adjectival phrases <CIT> and verbs <OTH> , but no previous work has focused on learning nouns '
'Probabilistic translation models generally seek to find the translation string e that maximizes the probability Pra5 ea6fa7 , given the source string f <CIT> '
'joint likelihood (JL) productdisplay i p parenleftBig xi,yi | vector parenrightBig conditional likelihood (CL) productdisplay i p parenleftBig yi | xi,vector parenrightBig classification accuracy (Juang and Katagiri, 1992) summationdisplay i (yi, y(xi)) expected classification accuracy (Klein and Manning, 2002) summationdisplay i p parenleftBig yi | xi,vector parenrightBig negated boosting loss (Collins, 2000)  summationdisplay i p parenleftBig yi | xi,vector parenrightBig1 margin (Crammer and Singer, 2001)  s.t. bardbl vectorbardbl  1;i,y negationslash= yi, vector  (vectorf(xi,yi )  vectorf(xi,y))   expected local accuracy (Altun et al. , 2003) productdisplay i productdisplay j p parenleftBig lscriptj(Y ) = lscriptj(yi ) | xi,vector parenrightBig Table 1: Various supervised training criteria.'
'Most work on discriminative training for SMT has focussed on linear models , often with margin based algorithms <OTH> , or rescaling a product of sub-models <CIT> '
'Bikel and Chiang <OTH> in fact contains two parsers : one is a lexicalized probabilistic contextfree grammar -LRB- PCFG -RRB- similar to <CIT> ; the other is based on statistical TAG <OTH> '
'Finally , it should be noted that in the current implementation , we have not applied any of the possible optimizations that appear in the literature <OTH> to speed up normalization of the probability distribution q These improvements take advantage of a models structure to simplify the evaluation of the denominator in -LRB- 1 -RRB- '
'1510 5 Related Work In recent years , many research has been done on extracting relations from free text -LRB- eg , <CIT> -RRB- ; however , almost all of them require some language-dependent parsers or taggers for English , which restrict the language of their extractions to English only -LRB- or languages that have these parsers -RRB- '
'To compare the performance of system , we recorded the total training time and the BLEU score , which is a standard automatic measurement of the translation qualit <CIT> '
'Typically , a small set of seed polar phrases are prepared , and new polar phrases are detected based on the strength of co-occurrence with the seeds <CIT> '
'The basic LCS has a problem that it does not differentiate LCSes of different spatial relations within their embedding sequences <CIT> '
'<OTH> , <CIT> -RRB- '
'3 Parse Tree Features We tagged each candidate transcription with -LRB- 1 -RRB- part-of-speech tags , using the tagger documented in <CIT> ; and -LRB- 2 -RRB- a full parse tree , using the parser documented in Collins -LRB- 1999 -RRB- '
'We use the by now standard a0 statistic <CIT> to quantify the degree of above-chance agreement between multiple annotators , and the a1 statistic for analysis of sources of unreliability <OTH> '
'Other methods that have been proposed are one based on using the gain <CIT> and an approximate method for selecting informative features <OTH> , and several criteria for feature selection were proposed and compared with other criteria <CIT> '
'As our basic data source , we use 500 000 sentences from the Wikipedia XML corpus <OTH> ; this is the corpus used by Akhmatova and Dras <OTH> , and related to one used in one set of experiments by <CIT> et al '
'Among all possible target strings , we will choose the one with the highest probability which is given by Bayes '' decision rule <CIT> : , ~ = argmaxP , '' -LRB- e -RRB- ~ lfg ~ -RRB- -RCB- = argmax -LCB- P , '' -LRB- ef -RRB- '
'In the English all-words task of the previous SENSEVAL evaluations <OTH> , the best performing English all-words task systems with the highest WSD accuracy were trained on SEMCOR <OTH> '
'One is a phrase-based translation in which a phrasal unit is employed for translation <CIT> '
'Other languagesfor which this is the case include English -LRB- with the Penn treebank <CIT> , the Susanne Corpus <OTH> , and the British section of the ICE Corpus <OTH> -RRB- and Italian -LRB- with ISST <OTH> and TUT <OTH> -RRB- '
'The algorithm is based on the Machine Learning method for word categorisation , inspired by the well known study on basic-level categories <OTH> , presented in <OTH> '
'Automated evaluation will utilize the standard DUC evaluation metric ROUGE <CIT> which representsrecallovervariousn-gramsstatisticsfrom asystem-generatedsummaryagainstasetofhumangenerated peer summaries5 We compute ROUGE scores with and without stop words removed from peer and proposed summaries '
'Furthermore , our model is not necessarily nativist ; these biases may be innate , but they may also be the product of some other earlier learning algorithm , as the results of Ellison <OTH> and <CIT> et al '
'For example , Animal would be mapped to Aa , GM would again be mapped to AA The tagger was applied and trained in the same way as described in <CIT> '
'As a result , the empirical approach has been adopted by almost all contemporary part-of-speech programs : Bahl and Mercer <OTH> , Leech , Garside , and Atwell <OTH> , Jelinek <OTH> , Deroualt and Merialdo <OTH> , Garside , Leech , and Sampson <OTH> , Church <OTH> , DeRose <OTH> , Hindle <OTH> , Kupiec <OTH> , Ayuso et al '
'Typicality was measured using the log-likelihood ratio test <CIT> '
'The translation system is a factored phrasebased translation system that uses the Moses toolkit <OTH> for decoding and training , GIZA + + for word alignment <CIT> , and SRILM <OTH> for language models '
'In constrast with many previous approaches <CIT> , our model does not try to capture how Source sentences can be mapped into Target sentences , but rather how Source and Target sentences can be generated simultaneously '
'Table 2 shows results in lowercase BLEU <CIT> for both the baseline -LRB- B -RRB- and the improved baseline systems -LRB- B5 -RRB- on development and held151 out evaluation sets '
'This view is supported by <CIT> , who concludes that correlations to human judgments were increased by using multiple references but using single reference summary with enough number of samples was a valid alternative '
'To date , researchers have harvested , with varying success , several resources , including concept lists <OTH> , topic signatures <OTH> , facts <OTH> , and word similarity lists <CIT> '
'Most existing methods treat word tokens as basic alignment units <CIT> , however , many languages have no explicit word boundary markers , such as Chinese and Japanese '
'To model aspects of co-occurrence association that might be obscured by raw frequency , the log-likelihood ratio G2 <CIT> was also used to transform the feature space '
'In Table 1 , the MALINE row 3 shows that the English name has a palato-alveolar modification 2 As <CIT> point out , these insights are not easy to come by : These rules are based on first author Dr Andrew Freemans experience with reading and translating Arabic language texts for more than 16 years <CIT> '
'The ITG we apply in our experiments has more structural labels than the primitive bracketing grammar : it has a start symbol S , a single preterminal C , and two intermediate nonterminals A and B used to ensure that only one parse can generate any given word-level alignment , as discussed by <CIT> and Zens and Ney -LRB- 2003 -RRB- '
'They are a subset of the features used in <CIT> '
'Other corpus-based methods determine associations between words <CIT> , which yields a basis for computing thesauri , or dictionaries of terminological expressions and multiword lexemes <OTH> '
'The data set is same as in Section 51 , except that we also parsed the English-side using a variant of the <CIT> parser , and then extracted 247M tree-to-string rules using the algorithm of <OTH> '
'The triplet lexicon model presented in this work can also be interpreted as an extension of the standard IBM model 1 <CIT> with an additional trigger '
'Both systems rely on the OpenNlp maximum-entropy part-of-speech tagger and chunker <CIT> , but KNOWITALL applies them to pages downloaded from the Web based on the results of Google queries , whereas KNOWITNOW applies them once to crawled and indexed pages6 Overall , each of the above elements of KNOWITALL and KNOWITNOW are the same to allow for controlled experiments '
'However , if we are willing to accept that occasionally our model will be unable to distinguish between distinct n-grams , then it is possible to store each parameter in constant space independent of both n and the vocabulary size <OTH> , <CIT> '
'Models of that form include hidden Markov models <OTH> as well as discriminative tagging models based on maximum entropy classification <CIT> , conditional random fields <OTH> , and large-margin techniques <OTH> '
'We enrich the semantic information available to the classifier by using semantic similarity measures based on the WordNet taxonomy <CIT> '
'We can stipulate the time line to be linearly ordered -LRB- although it is not in approaches that build ignorance of relative times into the representation of time <CIT> nor in approaches using branching futures <OTH> -RRB- , and we can stipulate it to be dense -LRB- although it is not in the situation calculus -RRB- '
'Typical approaches to conversion of constituent structures into dependencies are based on handconstructed head percolation rules , an idea that has its roots in lexicalized constituent parsing <CIT> '
'We extracted 181,250 case frames from the WSJ -LRB- Wall Street Journal -RRB- bracketed corpus of the Penn Tree Bank <CIT> '
'c2008 Association for Computational Linguistics Refining Event Extraction through Cross-document Inference Heng Ji Ralph Grishman Computer Science Department New York University New York , NY 10003 , USA -LRB- hengji , grishman -RRB- @ csnyuedu Abstract We apply the hypothesis of One Sense Per Discourse <CIT> to information extraction -LRB- IE -RRB- , and extend the scope of discourse from one single document to a cluster of topically-related documents '
'Section 3 describes two standard lexicalized models <CIT> , as well as an unlexicalized baseline model '
'Many researchers build alignment links with bilingual corpora <CIT> '
'The simplest version , called Dependency Model with Valence -LRB- DMV -RRB- , has been used in isolation and in combination with other models <CIT> '
'The translation table is obtained as described in (Koehn et al. , 2003), i.e. the alignment tool GIZA++ is run over the training data in both translation directions, and the two alignTest Setting BLEU B1 standard phrase-based SMT 29.22 B2 (B1) + clause splitting 29.13 Table 2: Experiment Baseline Test Setting BLEU BLEU 2-ary 2,3-ary 1 rule 29.77 30.31 2 ME (phrase label) 29.93 30.49 3 ME (left,right) 30.10 30.53 4 ME ((3)+head) 30.24 30.71 5 ME ((3)+phrase label) 30.12 30.30 6 ME ((4)+context) 30.24 30.76 Table 3: Tests on Various Reordering Models The 3rd column comprises the BLEU scores obtained by reordering binary nodes only, the 4th column the scores by reordering both binary and 3-ary nodes.'
'6 Related Work The most relevant previous works include word sense translation and translation disambiguation <OTH> , frame semantic induction <CIT> , and bilingual semantic mapping <OTH> '
'We take the generator of <CIT> as our baseline generator '
'translation systems <CIT> and use Moses <CIT> to search for the best target sentence '
'Moreover , as P-DOP is formulated as an enrichment of the treebank Probabilistic Context-free Grammar -LRB- PCFG -RRB- , it allows for much easier comparison to alternative approaches to statistical parsing <CIT> '
'This model shares some similarities with the stochastic inversion transduction grammars -LRB- SITG -RRB- presented by Wu in <CIT> '
'For Czech , we created a prototype of the first step of this process - the part-of-speech -LRB- POS -RRB- tagger - using Rank Xerox tools <OTH> , <CIT> '
'The rules are then treated as events in a relative frequency estimate4 We used Giza + + Model 4 to obtain word alignments <CIT> , using the grow-diag-final-and heuristic to symmetrise the two directional predictions <OTH> '
'Mutual Informatio n <CIT> discussed the use of the mutual information statistics as a way to identify a variety of interesting linguistic phenomena , ranging from semanti c relations of the doctor\/nurse type -LRB- content word\/content word -RRB- to lexico-syntactic co-occurrence preferences between verbs and prepositions -LRB- content word\/function word -RRB- '
'A constituent-based system using Collins parser <CIT> '
'This task evaluated parsing performance on 10 languages : Arabic , Basque , Catalan , Chinese , Czech , English , Greek , Hungarian , Italian , and Turkish using data originating from a wide variety of dependency treebanks , and transformations of constituency-based treebanks <CIT> '
'Among the most widely studied is the Gibbs distribution <OTH> '
'Illustrative clusterings of this type can also be found in Pereira , Tishby , and Lee <OTH> , <CIT> , Kneser and Ney <OTH> , and Brill et al '
'Since then , supervised learning from sense-tagged corpora has since been used by several researchers : Zernik <OTH> , Hearst <OTH> , Leacock , Towell , and Voorhees <OTH> , Gale , Church , and Yarowsky <OTH> , Bruce and Wiebe <OTH> , Miller et al '
'We tuned Pharaohs four parameters using minimum error rate training <CIT> on DEV12 We obtained an increase of 08 9As in the POS features , we map each phrase pair to its majority constellation '
'The performance figures given below are based on training each method on the 1-million-word Brown corpus <OTH> and testing it on a 3\/4-million-word corpus of Wall Street Journal text <CIT> '
'Many researchers -LRB- <OTH> ; <OTH> -RRB- have suggested that the informationtheoretic notion of mutual information score -LRB- MIS -RRB- directly captures the idea of context '
'Word alignment and phrase extraction We used the GIZA + + word alignment software 3 to produce initial word alignments for our miniature bilingual corpus consisting of the source French file and the English reference file , and the refined word alignment strategy of <CIT> to obtain improved word and phrase alignments '
'For each training data size , we report the size of the resulting language model , the fraction of 5-grams from the test data that is present in the language model , and the BLEU score <CIT> obtained by the machine translation system '
'The approach is based on the hypothesis that positive words co-occur more than expected by chance , and so do negative words ; this hypothesis was validated , at least for strong positive\/negative words , in <CIT> '
'Following initial work by <OTH> and <OTH> , an early , online distributional thesaurus presented in <OTH> has been widely used and cited , and numerous authors since have explored thesaurus properties and parameters : see survey component of <CIT> '
'The tree is produced by a state-of-the-art dependency parser <OTH> trained on the Wall Street Journal Penn Treebank <CIT> '
'Part-of-speech tagging is an active area of research ; a great deal of work has been done in this area over the past few years <OTH> '
'However , following the work of <CIT> , Yarowsky -LRB- 1995 -RRB- , many supervised WSD systems use minimal information about syntactic structures , for the most part restricting the notion of context to topical and local features '
'Graphically speaking , parsing amounts to identifying rectangular crosslinguistic constituents by assembling smaller rectangles that will together cover the full string spans in both dimensions -LRB- compare <CIT> -RRB- '
'Liang <OTH> uses the discriminative perceptron algorithm <CIT> to score whole character tag sequences , finding the best candidate by the global score '
'The rationale for using Kappa is explained in <CIT> '
'The measures vary from simple edge-counting to attempt to factor in peculiarities of the network structure by considering link direction , relative path , and density , such as vector , lesk , hso , lch , wup , path , res , lin and jcn <CIT> '
'Variations of SCFGs go back to Aho and Ullman <OTH> s Syntax-Directed Translation Schemata , but also include the Inversion Transduction Grammars in <CIT> , which restrict grammar rules to be binary , the synchronous grammars in Chiang -LRB- 2005 -RRB- , which use only a single nonterminal symbol , and the Multitext Grammars in Melamed <OTH> , which allow independent rewriting , as well as other tree-based models such as Yamada and Knight <OTH> and Galley et al '
'The interest reader is referred to <OTH> , for a summary of ARIOSTO , an integrated tool for extensive acquisition of lexieal knowledge from corpora that we used to demonstrate and validate our approach '
'53 Baseline System We conducted experiments using different segmenters with a standard log-linear PB-SMT model : GIZA + + implementation of IBM word alignment model 4 <CIT> , the refinement and phrase-extraction heuristics described in <OTH> , minimum-errorrate training <CIT> , a 5-gram language model with Kneser-Ney smoothing trained with SRILM <OTH> on the English side of the training data , and Moses <OTH> to translate both single best segmentation and word lattices '
'The prior probability P0 is the prior distribution for the phrase probability which is estimated using the phrase normalized counts commonly used in conventional Phrasebased SMT systems , eg , <CIT> '
'Collocation map that is first suggested in <OTH> is a sigmoid belief network with words as probabilistic variables '
'The model of <CIT> incorporated a latent variable for named entity class '
'CIT -RRB- '
'We thus introduce a multiplier to form the actual objective function that we minimize with respect to :4 summationdisplay iL logp , i -LRB- yi -RRB- + Nsummationdisplay inegationslashL H -LRB- p , i -RRB- -LRB- 4 -RRB- One may regard as a Lagrange multiplier that is used to constrain the classifiers uncertainty H to be low , as presented in the work on entropy regularization <CIT> '
'Much work has gone into methods for measuring synset similarity ; early work in this direction includes <CIT> , which attempted to discover sense similarities between dictionary senses '
'<CIT> considered some location constrains in meeting summarization evaluation , which utilizes speaker information to some extent '
'In contrast to the semi-supervised LEAF alignment algorithm of <CIT> , which requires 1,5002,000 CPU days per iteration to align 84M ChineseEnglish sentences -LRB- anonymous , pc -RRB- , link deletion requires only 450 CPU hours to re-align such a corpus -LRB- after initial alignment by GIZA + + , which requires 20-24 CPU days -RRB- '
'There are more sophisticated surface generation packages , such as FUF\/SURGE <OTH> , KPML <OTH> , MUMBLE <OTH> , and RealPro <OTH> , which produce natural language text from an abstract semantic representation '
'Since so many concepts used in discourse are graindependent , a theory of granularity is also fundamental <CIT> '
'63 Comparison with re-ranking approach Finally , we compared our algorithm with the reranking approach <CIT> , where we rst generate the n-best candidates using a model with only local features -LRB- the rst model -RRB- and then re-rank the candidates using a model with non-local features -LRB- the second model -RRB- '
'For this paper , we used POS tags that were provided either by the Treebank itself -LRB- gold standard tags -RRB- or by the perceptron POS tagger3 presented in <CIT> '
'The approach presented here has some resemblance to the bracketing transduction grammars -LRB- BTG -RRB- of <CIT> , which have been applied to a phrase-based machine translation system in <OTH> '
'In <CIT> , an undirected graphical model for constituent parse reranking uses dependency relations to define the edges '
'For an HMM with a set of states T and a set of output symbols V : t T t Dir -LRB- 1 , T -RRB- -LRB- 1 -RRB- t T t Dir -LRB- 1 , V -RRB- -LRB- 2 -RRB- ti ti1 , ti1 Multi -LRB- ti1 -RRB- -LRB- 3 -RRB- wi ti , ti Multi -LRB- ti -RRB- -LRB- 4 -RRB- One advantage of the Bayesian approach is that the prior allows us to bias learning toward sparser structures , by setting the Dirichlet hyperparameters , to a value less than one <CIT> '
'The above observations can be stated formally from the perspective of <CIT> Model 2 '
'The progress in parsing technology are noteworthy , and in particular , various statistical dependency models have been proposed <CIT> , , <OTH> , <OTH> '
'Statistical and information theoretic approaches <OTH> , <OTH> , <OTH> , <OTH> Using lexical collocations to determine PPA with statistical techniques was first proposed by <OTH> '
'Furthermore , recent studies revealed that word clustering is useful for semi-supervised learning in NLP <CIT> '
'<OTH> , Johansson and Nugues <OTH> , Prokopidis et al '
'2.2 Corpus occurrence In order to get a feel for the relative frequency of VPCs in the corpus targeted for extraction, namely 0 5 10 15 20 25 30 35 40 0 10 20 30 40 50 60 70 VPC types (%) Corpus frequency Figure 1: Frequency distribution of VPCs in the WSJ Tagger correctextracted Prec Rec Ffl=1 Brill 135135 1.000 0.177 0.301 Penn 667800 0.834 0.565 0.673 Table 1: POS-based extraction results the WSJ section of the Penn Treebank, we took a random sample of 200 VPCs from the Alvey Natural Language Tools grammar (Grover et al. , 1993) and did a manual corpus search for each.'
'Online discriminative training has already been studied by Tillmann and Zhang <OTH> and <CIT> et al '
'Accordingly, in this section we describe a set of experiments which extends the work of (Way and Gough, 2005) by evaluating the Marker-based EBMT system of (Gough & Way, 2004b) against a phrase-based SMT system built using the following components:  Giza++, to extract the word-level correspondences;  The Giza++ word alignments are then refined and used to extract phrasal alignments ((Och & Ney, 2003); or (Koehn et al. , 2003) for a more recent implementation);  Probabilities of the extracted phrases are calculated from relative frequencies;  The resulting phrase translation table is passed to the Pharaoh phrase-based SMT decoder which along with SRI language modelling toolkit5 performs translation.'
'2 Related Work There has been a large and diverse body of research in opinion mining , with most research at the text <OTH> , sentence <OTH> or word <CIT> level '
'Most previous work with CRFs containing nonlocal dependencies used approximate probabilistic inference techniques , including TRP <OTH> and Gibbs sampling <OTH> '
'A more fine-grained distinction is made by <CIT> and Vieira and Poesio -LRB- 2000 -RRB- to distinguish restrictive from non-restrictive postmodification by ommitting those modifiers that occur between commas , which should not be classified as chain starting '
'32 Training Algorithm We adopt the perceptron training algorithm of <CIT> to learn a discriminative model mapping from inputs xX to outputs yY , where X is the set of sentences in the training corpus and Y is the set of corresponding labeled results '
'Since so many concepts used in discourse are graindependent , a theory of granularity is also fundamental <CIT> '
'22 Three Treebanks The Treebanks that we used in this paper are the English Penn Treebank II <CIT> , the Chinese Penn Treebank <OTH> , and the Korean Penn Treebank <OTH> '
'One of the theoretical problems with phrase based SMT models is that they can not effectively model the discontiguous translations and numerous attempts have been made on this issue <CIT> '
'Furthermore , we use averaged weights <CIT> in Algorithm 1 '
'Alternatively , one can train them with respect to the final translation quality measured by some error criterion <CIT> '
'(Blitzer et al., 2006; Jiang and Zhai, 2007).'
'They propose two modifications to f-measure: varying the precision/recall tradeoff, and fully-connecting the alignment links before computing f-measure.11 Weighted Fully-Connected F-Measure Given a hypothesized set of alignment links H and a goldstandard set of alignment links G, we define H+ = fullyConnect(H) and G+ = fullyConnect(G), and then compute: f-measure(H+) = 1 precision(H+) + 1 recall(H+) For phrase-based Chinese-English and ArabicEnglish translation tasks, (Fraser and Marcu, 2007a) obtain the closest correlation between weighted fully-connected alignment f-measure and BLEU score using =0.5 and =0.1, respectively.'
'One of the steps in the analysis of English is named entity recognition using Stanford Named Entity Recognizer <CIT> '
'<CIT> and <OTH> -RRB- '
'52 Maximum Entropy Model We use the Maximum Entropy -LRB- ME -RRB- Model <CIT> for our classification task '
'TheChinesesentencefromtheselected pair is used as the single reference to tune and evaluate the MT system with word-based BLEU-4 <CIT> '
'The reliability of the annotations was checked using the kappa statistic <CIT> '
'A major issue in MaxEnt training is how to select proper features and determine the feature targets <CIT> '
'For example , our system configuration for the shared task incorporates a wrapper around GIZA + + <OTH> for word alignment and a wrapper around Moses <CIT> for decoding '
'<CIT> and Chan and Ng -LRB- 2008 -RRB- use WordNet , and Zhou et al '
'The solution we employ here is the discriminative training procedure of <CIT> '
'<OTH> invented heuristic symmetriza57 FRENCH\/ENGLISH ARABIC\/ENGLISH SYSTEM F-MEASURE -LRB- = 04 -RRB- BLEU F-MEASURE -LRB- = 01 -RRB- BLEU GIZA + + 735 3063 758 5155 <OTH> 741 3140 791 5289 LEAF UNSUPERVISED 745 723 LEAF SEMI-SUPERVISED 763 3186 845 5434 Table 3 : Experimental Results tion of the output of a 1-to-N model and a M-to-1 model resulting in a M-to-N alignment , this was extended in <CIT> '
'Independently , in AI an effort arose to encode large amounts of commonsense knowledge <CIT> '
'Different methods have been proposed to reduce error propagation between pipelined tasks , both in general <CIT> and for specific problems such as language modeling and utterance classification <OTH> and labeling and chunking <OTH> '
'Both data were extracted from the Penn Treebank Wall Street Journal -LRB- WSJ -RRB- Corpus <CIT> '
'This preprocessing step can be accomplished by applying the GIZA + + toolkit <CIT> that provides Viterbi alignments based on IBM Model-4 '
'Research on the automatic classification of movie or product reviews as positive or negative -LRB- eg , <CIT> -RRB- is perhaps the most similar to our work '
'The de-facto answer came during the 1990s from the research community on Statistical Machine Translation , who made use of statistical tools based on a noisy channel model originally developed for speech recognition <CIT> '
'These problems formulations are similar to those studied in <CIT> and <OTH> , respectively '
'<CIT> reported that it was appropriate in 722 % of cases '
'<OTH> 019-048 Leacock & Chodrow <OTH> 036 <CIT> 036 Resnik -LRB- 1995 -RRB- 037 Proposed 0504 7 Conclusion We proposed a relational model to measure the semantic similarity between two words '
'More recently , the integration of information sources , and the modeling of more complex language processing tasks in the statistical framework has increased the interest in smoothing methods <CIT> '
'As a sanity check , we duplicated <CIT> baseline in which all unigrams that appear four or more times in the training documents are used as features '
'This was a difcult challenge as many participants in the task failed to obtain any meaningful gains from unlabeled data <CIT> '
'The work most similar in spirit to ours that of <CIT> '
'Day 1 Day 2 No ASR adaptation 2939 2741 Unsupervised ASR adaptation 3155 2766 Supervised ASR adaptation 3219 2765 Table 2 : Impact of ASR adaptation to SMT Table 2 shows the impact of ASR adaptation on the performance of the translation system in BLEU <CIT> '
'32 Learning Algorithm For learning coreference decisions , we used a Maximum Entropy <CIT> model '
'For example it has been used to measure centrality in hyperlinked web pages networks <OTH> , lexical networks <CIT> , and semantic networks <CIT> '
'Two LUs close in the space are likely to be in a paradigmatic relation , ie to be close in a is-a hierarchy <CIT> '
'The WSJNPVP set consists of part-of speech tagged Wall Street Journal material <CIT> , supplemented with syntactic tags indicating noun phrase and verb phrase boundaries <OTH> '
'For example , <CIT> developed the Sub-Tree Metric -LRB- STM -RRB- over constituent parse trees and the Head-Word Chain Metric -LRB- HWCM -RRB- over dependency parse trees '
'Because our algorithm does not consider the context given by the preceding sentences , we have conducted the following experiment to see to what extent the discourse context could improve the performance of the wordsense disambiguation : Using the semantic concordance files <OTH> , we have counted the occurrences of content words which previously appear in the same discourse file '
'In some cases , class -LRB- or part of speech -RRB- n-grams are used instead of word n-gram <CIT> '
'It is mentioned that the limitation is largely caused by inconsistencies in the corpus <CIT> '
'As our approach for incorporating unlabeled data , we basically follow the idea proposed in <CIT> '
'The training is performed by a single generalized perceptron <CIT> '
'Furthermore , as pointed out in <CIT> , the sense division in an MRD is frequently too fine-grained for the purpose of WSD '
'Generative word alignment models , initially developed at IBM <OTH> , and then augmented by an HMM-based model <OTH> , have provided powerful modeling capability for word alignment '
'<OTH> and <CIT> '
'We report results on the Boston University -LRB- BU -RRB- Radio Speech Corpus <OTH> and Boston Directions Corpus -LRB- BDC -RRB- <OTH> , two publicly available speech corpora with manual ToBI annotations intended for experiments in automatic prosody labeling '
'The co-occurrence relation can also be based on distance in a bitext space , which is a more general representations of bitext correspondence <OTH> , or it can be restricted to words pairs that satisfy some matching predicate , which can be extrinsic to the model <OTH> '
'One option is what <CIT> calls many-to-one -LRB- M-to-1 -RRB- accuracy , in which each induced tag is labeled with its most frequent gold tag '
'We used the Maximum Entropy approach5 <CIT> as a machine learner for this task '
'We report that our parsing framework achieved high accuracy -LRB- 886 % -RRB- in dependency analysis of Japanese with a combination of an underspecified HPSG-based Japanese grammar , SLUNG <OTH> and the maximum entropy method <CIT> '
'The model weights of the transducer are tuned based on the development set using a grid-based line search , and the translation results are evaluated based on a single Chinese reference6 using BLEU-4 <CIT> '
'<CIT> deserves the credit for bringing to the attention of computational linguists '
'Thus , we can compute the source dependency LM score in the same way we compute the target side score , using a procedure described in <CIT> '
'The typical problems like doctor-nurse <CIT> could be avoided by using such information '
'Kanayama and Nasukawa used both intraand inter-sentential co-occurrence to learn polarity of words and phrases <CIT> '
'For comparison purposes , we also computed the value of R 2 for adequacy using the BLEU score formula given in <CIT> , for the 7 systems using the same one reference , and we obtain a similar value , 8391 % ; computing the value of R 2 for adequacy using the BLEU scores computed with all 4 references available also yielded a lower value for R 2 , 6221 % '
'They first extract English collocations using the Xtract systetn <CIT> , and theu look for French coutlterparts '
'<OTH> , is to translate dependency parses into neo-Davidsonian-style quasilogical forms , and to perform weighted abductive theorem proving in the tradition of <CIT> '
'61 Evaluation of Translation Performance We use the BLEU score <CIT> to evaluate our systems '
'It is often argued that the ability to translate discontiguous phrases is important to modeling translation <CIT> , and it may be that this explains the results '
'Clustering-based approaches usually represent word contexts as vectors and cluster words based on similarities of the vectors <CIT> '
'However , <CIT> show that , in phrase-based translation , improvements in AER or f-measure do not necessarily correlate with improvements in BLEU score '
'<OTH> 916 % 916 % F\/3 = 1 9386 9326 928 9203 916 Table 3 : The overall pertbrmance of the majority voting combination of our best five systems -LRB- selected on tinting data perfbrnmnce -RRB- applied to the standard data set pnt tbrward by <CIT> together with an overview of earlier work '
'Given a sentence-pair -LRB- f , e -RRB- , the most likely -LRB- Viterbi -RRB- word alignment is found as <CIT> : a = argmaxa P -LRB- f , a e -RRB- '
'We directly model the conditional probability of the alignment a , given x and y , using the maximum entropy framework <CIT> , P -LRB- a x , y -RRB- = exp -LCB- F -LRB- a , x , y -RRB- -RCB- summationdisplay aC -LRB- x , y -RRB- exp -LCB- F -LRB- a , x , y -RRB- -RCB- '
'The first approaches are used for Penn Treebank <CIT> and the KAIST language resource <OTH> '
'<CIT> introduced a statistical measurement called mutual information for extracting strongly associated or collocated words '
'A variety of other measures of semantic relatedness have been proposed , including distributional similarity measures based on co-occurrence in a body of text see <CIT> for a survey '
'The Collins parser <CIT> does use dynamic programming in its search '
'On the other hand , both BLEU <CIT> and NIST <OTH> scores are higher for the baseline system -LRB- mteval-v11bpl -RRB- '
'The application of this algorithm to the basic problem using a parallel bilingual corpus aligned on the sentence level is described in <CIT> '
'We further note that our results are different from that of <CIT> as they use extensive feature engineering and weight tuning during the graph generation process that we have not been able to reproduce '
'Selectional preferences are estimated using grammatical collocation information from the British National Corpus -LRB- BNC -RRB- , obtained with the Word Sketch Engine -LRB- WSE -RRB- <OTH> '
'1 Introduction The probabilistic relation between verbs and their arguments plays an important role in modern statistical parsers and supertaggers <OTH> , and in psychological theories of language processing <OTH> '
'Hence , either the best translation hypothesis is directly extracted from the word graph and output , or an N-best list of translations is computed <OTH> '
'1 Introduction Parsing sentences using statistical information gathered from a treebank was first examined a decade ago in <OTH> and is by now a fairly well-studied problem -LRB- <OTH> , <CIT> , <OTH> -RRB- '
'Feature function weights in the loglinear model are set using Ochs minium error rate algorithm <CIT> '
'As in tile HMM we easily can extend the dependencies in the alignment model of Model 4 easily using the word class of the previous English word E = G -LRB- ci , -RRB- , or the word class of the French word F = G -LRB- Ij -RRB- <CIT> '
'Methods focussing on the use and generation of dictionaries capturing the sentiment of words have ranged from manual approaches of developing domain-dependent lexicons <OTH> to semi-automated approaches <OTH> , and even an almost fully automated approach <CIT> '
'Examples of such techniques are Markov Random Fields <OTH> , and boosting or perceptron approaches to reranking <CIT> '
'Inside\/Outside This representation was first introduced in <CIT> , and has been applied for base NP chunking '
'We use the n-best generation scheme interleaved with optimization as described in <CIT> '
'We also plan to employ this evaluation metric as feedback in building dialogue coherence models as is done in machine translation <CIT> '
'In related work <CIT> , both supervised and unsupervised approaches have been shown to have their pros and cons '
'A phrase-based translation model is one of the modern approaches which exploits a phrase , a contiguous sequence of words , as a unit of translation <CIT> '
'<CIT> predicates the sentiment orientation of a review by the average semantic orientation of the phrases in the review that contain adjectives or adverbs , which is denoted as the semantic oriented method '
'<CIT> for other approaches with an evaluation based on true mentions only -RRB- '
'For evaluation we have selected a set of 8 metric variants corresponding to seven different families : BLEU -LRB- n = 4 -RRB- <OTH> , NIST -LRB- n = 5 -RRB- <OTH> , GTM F1-measure -LRB- e = 1,2 -RRB- <OTH> , 1-WER <OTH> , 1-PER <OTH> , ROUGE -LRB- ROUGE-S \* -RRB- <OTH> and METEOR3 <CIT> '
'Previous approaches to processing lnetonymy have used hand-constructed ontologies or semantic networks -LRB- -RRB- ? ass , 1988 ; Iverson and Hehnreich , 1992 ; B <OTH> '
'The parameters for each phrase table were tuned separately using minimum error rate training <CIT> '
'Most of them were developed for exhaustive parsing , ie , producing all parse results that are given by the grammar <OTH> '
'<OTH> and <CIT> also report results for semi-supervised learning for these domains '
'All 8,907 articles were tagged by the Xerox Part-ofSpeech Tagger <CIT> 4 '
'Also , even the two-category version of the rating-inference problem for movie reviews has proven quite challenging for many automated classi cation techniques <CIT> '
'31 Data The starting corpus we use is formed by a mix of three different sources of data , namely the Penn Treebank corpus <CIT> , the Los Angeles Times collection , as provided during TREC conferences1 , and Open Mind Common Sense2 , a collection of about 400,000 commonsense assertions in English as contributed by volunteers over the Web '
'2 Related Work There has been extensive research in opinion mining at the document level , for example on product and movie reviews <CIT> '
'To contrast , <CIT> concentrated on analyzing human-written summaries in order to determine how professionals construct summaries '
'We build sentencespecific zero-cutoff stupid-backoff <CIT> 5-gram language models , estimated using 47B words of English newswire text , and apply them to rescore either 10000-best lists generated by HCP or word lattices generated by HiFST '
'The results we obtained on the CoNLL03 test set were consistent with what was reported in <CIT> '
'These problems include collocation discovery <OTH> , smoothing and estimation <CIT> and question answering <OTH> '
'K-best suffix arrays have been used in autocomplete applications <CIT> '
'Given the motivations for performing a linguistically-informedextraction whichwere also put forth , among others , by Church and Hanks <OTH> , <CIT> and Heid -LRB- 1994 -RRB- and given the recent developmentof linguisticanalysistools , itseemsplausiblethatthe linguisticstructurewill be more and more taken intoaccountbycollocationextractionsystems '
'To avoid this problem we use the concept of class proposed for a word n-gram model <CIT> '
'Much of the recent work in word alignment has focussed on improving the word alignment quality through better modeling <OTH> or alternative approaches to training <CIT> '
'<CIT> report 8715 % accuracy using a unigram-based SVM classifier combined with subjectivity detection '
'In acknowledgment of this fact , a series of conferences like Text Retrieval Conferences -LRB- TREC -RRB- <OTH> , Message Understanding Conferences -LRB- MUC -RRB- <OTH> , TIPSTER SUMMAC Text Summarization Evaluation <OTH> , Document Understanding Conference -LRB- DUC -RRB- <OTH> , and Text Summar </context> </contexts> <marker> Voorhees , Harman , 1999 </marker> <rawString> Voorhees , E M and Harman , D K , 1999 '
'This problem has been considered for instance in <CIT> for his inversion transduction grammars and has applications in the support of several tasks of automatic annotation of parallel corpora , as for instance segmentation , bracketing , phrasal and word alignment '
'Use of sententially aligned corpora for word alignment has already been recommended in <CIT> '
'summarization <OTH> , paraphrasing <OTH> , natural language generation <OTH> , and language modeling <CIT> '
'53 Translation Results For the translation experiments on the BTEC task , we report the two accuracy measures BLEU <CIT> and NIST <OTH> as well as the two error rates : word error rate -LRB- WER -RRB- and position-independent word error rate -LRB- PER -RRB- '
'For a detailed description for Model 4 the reader is referred to <CIT> '
'Obtained percent agreement of 0988 and coefficient <CIT> of 0975 suggest high convergence of both annotations '
'While early machine learning approaches for the task relied on local , discriminative classifiers <OTH> , more recent approaches use joint and\/or global models <CIT> '
'More recent papers <CIT> , Pereira and Tishby -LRB- 1992 -RRB- proposed to cluster nouns on the basis of a metric derived from the distribution of subject , verb and object in the texts '
'A word is considered to be known when it has an ambiguous tag -LRB- henceforth ambitag -RRB- attributed to it in the LEXICON , which is compiled in the same way as for the MBT-tagger <OTH> '
'Semantic classification programs <CIT> use statistical information based on cooccurrence with appropriate marker words to partition a set of words into semantic groups or classes '
'317 Citation Observed data Hidden data <CIT> Treebank tree with head child annotated on each nonterminal No hidden data '
'Word alignments traditionally are based on IBM Models 1-5 <CIT> or on HMMs <OTH> '
'32 This problem is also a central concern in the work by <CIT> '
'41 Part-of-speech tagging experiments We split the Penn Treebank corpus <OTH> into training , development and test sets as in <CIT> '
'BLEU For all translation tasks , we report caseinsensitive NIST BLEU scores <CIT> using 4 references per sentence '
'The success of statistical methods in particular has been quite evident in the area of syntactic parsing , most recently with the outstanding results of <OTH> and <OTH> on the now-standard English test set of the Penn Treebank <CIT> '
'Learning to Disambiguate Word Senses Several recent research projects have taken a corpus-based approach to lexical disambiguation <OTH> '
'Experimental Comparison 41 Experiments on the ATIS corpus For our first comparison , we used I0 splits from the Penn ATIS corpus <CIT> into training sets of 675 sentences and test sets of 75 sentences '
'Research in the field of unsupervised and weakly supervised parsing ranges from various forms of EM training <OTH> over bootstrapping approaches like selftraining <CIT> to feature-based enhancements of discriminative reranking models <OTH> and the application of semisupervised SVMs <OTH> '
'<CIT> used patterns representing part-of-speech sequences , <OTH> recognized adjectival phrases , and <OTH> learned N-grams '
'22 Inversion Transduction Grammar <CIT> s inversion transduction grammar -LRB- ITG -RRB- is a synchronous grammar formalism in which derivations of sentence pairs correspond to alignments '
'NER is typically viewed as a sequential prediction problem , the typical models include HMM <OTH> , CRF <OTH> , and sequential application of Perceptron or Winnow <CIT> '
'The published F score for voted perceptron is 9353 % with a different feature set <CIT> '
'Therefore , the base forms have been introduced manually and the POS tags have been provided partly manually and partly automatically using a statistical maximum-entropy based POS tagger similar to the one described in <CIT> '
'To solve this problem , we will adapt the idea of null generated words from machine translation <CIT> '
'4 Architecture of the SMT system The goal of statistical machine translation (SMT) is to produce a target sentence e from a source sentence f. It is today common practice to use phrases as translation units (Koehn et al., 2003; Och and Ney, 2003) and a log linear framework in order to introduce several models explaining the translation process: e = argmaxp(e|f) = argmaxe {exp(summationdisplay i ihi(e,f))} (1) The feature functions hi are the system models and the i weights are typically optimized to maximize a scoring function on a development set (Och and Ney, 2002).'
'In the training phase , bilingual parallel sentences are preprocessed and aligned using alignment algorithms or tools such as GIZA + + <CIT> '
'A natural fit to the existing statistical machine translation framework A metric that ranks a good translation high in an nbest list could be easily integrated in a minimal error rate statistical machine translation training framework <CIT> '
'The set of such ITG alignments , AITG , are a strict subset of A1-1 <CIT> '
'Also , we used Adwait Ratnaparkhis part-of-speech tagger <CIT> to tag unknown words in the test data '
'One of the earliest attempts at extracting interrupted collocations '' -LRB- ie non-contiguous collocations , including VPCs -RRB- , was that of <CIT> '
'For example , if the lexicon contains an adjective excellent , it matches every adjective phrase that includes excellent such as view-excellent etc As a baseline , we built lexicon similarly by using polarity value of <CIT> '
'Existing statistical NLG -LRB- i -RRB- uses corpus statistics to inform heuristic decisions in what is otherwise symbolic generation <OTH> ; -LRB- ii -RRB- applies n-gram models to select the overall most likely realisation after generation -LRB- HALOGEN family -RRB- ; or -LRB- iii -RRB- reuses an existing parsing grammar or treebank for surface realisation <CIT> '
'GIZA + + refined alignments have been used in state-of-the-art phrase-based statistical MT systems such as <CIT> ; variations on the refined heuristic have been used by <OTH> -LRB- diag and diag-and -RRB- and by the phrase-based system Moses -LRB- grow-diag-final -RRB- <OTH> '
'IC function is a derivative of Fano ''s mutual information formula recently used by <CIT> to compute word co-occurrence patterns in a 44 million word corpus of Associated Press news stories '
'This represents the translation probability of a phrase when it is decomposed into a series of independent word-for-word translation steps <CIT> , and has proven a very effective feature <OTH> '
'Collins head words finder rules have been modified to extract semantic head word <CIT> '
'For instance , <CIT> train an independent subjectivity classifier to identify and remove objective sentences from a review prior to polarity classification '
'Ordinary Prologstyle , backchaining deduction is augmented with the capability of making assumptions and of factoring two goal literals that are unifiable <CIT> '
'Pearsons correlation coefficient is a standard measure of the correlation strength between two distributions; it can be calculated as follows:  = E(XY ) E(X)E(Y )radicalbigE(X2)  [E(X)]2radicalbigE(Y 2)  [E(Y )]2 (1) where X = (x1,,xn) and Y = (y1,,yn) are vectors of numerical scores for each paraphrase provided by the humans and the competing systems, respectively, n is the number of paraphrases to score, and E(X) is the expectation of X. Cosine correlation coefficient is another popular alternative and was used by Nakov and Hearst (2008); it can be seen as an uncentered version of Pearsons correlation coefficient:  = X.YbardblXbardblbardblYbardbl (2) Spearmans rank correlation coefficient is suitable for comparing rankings of sets of items; it is a special case of Pearsons correlation, derived by considering rank indices (1,2,) as item scores . It is defined as follows:  = n summationtextx iyi  ( summationtextx i)( summationtexty i)radicalBig nsummationtextx2i  (summationtextxi)2 radicalBig nsummationtexty2i  (summationtextyi)2 (3) One problem with using Spearmans rank coefficient for the current task is the assumption that swapping any two ranks has the same effect.'
'3 Methodology Similar to <CIT> , we use comparison to human assocation datasets as a test bed for the scores produced by computational association measures '
'A summary of the differences between our proposed approach and that of <CIT> would include : The reliance of BLEU on the diversity of multiple reference translations in order to capture some of the acceptable alternatives in both word choice and word ordering that we have shown above '
'As a result , the problem of opinion mining has seen increasing attention over the last three years from <CIT> and many others '
'Our learning algorithm stems from Perceptron training in <CIT> '
'4 Relation to Previous Work There is a significant volume of work exploring the use of CRFs for a variety of chunking tasks , including named-entity recognition , gene prediction , shallow parsing and others <CIT> '
'Finally we trained model weights by maximizing BLEU <CIT> and set decoder optimization parameters -LRB- n-best list size , timeouts 14 etc -RRB- on a development test set of 200 held-out sentences each with a single reference translation '
'For subproblem -LRB- a -RRB- , we have devised a new method , based on LPR , which has some good properties not shared by the methods proposed so far <OTH> '
'Recent computational work either focuses on sentence subjectivity <OTH> , concentrates just on explicit statements of evaluation , such as of films <CIT> , or focuses on just one aspect of opinion , eg , <OTH> on adjectives '
'NP chunks in the shared task data are BaseNPs , which are non-recursive NPs , a definition first proposed by <CIT> '
'We adopted an N-best hypothesis approach <CIT> to train '
'Above the phrase level , some models perform no reordering <OTH> , some have a simple distortion model that reorders phrases independently of their content <CIT> , and some , for example , the Alignment Template System <CIT> , hereafter ATS , and the IBM phrase-based system <OTH> , have phrase-reordering models that add some lexical sensitivity '
'Comparison With Previous Work Most of the recent corpus-based POS taggers in the literature are either statistically based , and use Markov Model <OTH> or Statistical Decision Tree <OTH> -LRB- SDT -RRB- techniques , or are primarily rule based , such as Drill ''s Transformation Based Learner <OTH> -LRB- TBL -RRB- '
'Previous studies have shed light on the predictability of the next unix command that a user will enter <OTH> , the next keystrokes on a small input device such as a PDA <OTH> , and of the translation that a human translator will choose for a given foreign sentence <OTH> '
'5 Effectiveness Comparison 51 English-Chinese ATIS Models Both the transfer and transducer systems were trained and evaluated on English-to-Mandarin Chinese translation of transcribed utterances from the ATIS corpus <OTH> '
'Using techniques described in <CIT> , Church and Hanks -LRB- 1990 -RRB- , and Hindle and Rooth -LRB- 1991 -RRB- , below are some examples of the most frequent V-O pairs from the AP corpus '
'First , the graph-based models have better precision than the transition-based models when predicting long arcs , which is compatible with the results of <CIT> '
'Brown , <CIT> uses the same bigrams and by means of a greedy algorithm forms the hierarchical clusters of words '
'<OTH> ; <CIT> -RRB- '
'Lexical collocation functions , especially those determined statistically , have recently attracted considerable attention in computational linguistics <OTH> mainly , though not exclusively , for use in disambiguation '
'As a first step , SemEval2007 Task 4 offered many useful insights into the performance of different approaches to semantic relation classification ; it has also motivated followup research <CIT> '
'Our evaluation metric is BLEU <CIT> with caseinsensitive matching from unigram to four-gram '
'This is in contrast to work by researchers such as Schiitze and Pedersen <OTH> , <CIT> et al <OTH> and Futrelle and Gauch -LRB- 1995 -RRB- , where it is often the most frequent words in the lexicon which are clustered , predominantly with the purpose of determining their grammatical classes '
'Then , we used the refinement technique grow-diag-final-and <CIT> to all 50 50 bidirectional alignment pairs '
'The significance of G 2 based on the exact conditional distribution does not rely on an asymptotic approximation and is accurate for sparse and skewed data samples <OTH> 42 Information criteria The family of model evaluation criteria known as information criteria have the following expression : IC , ~ = G 2 ~ x dof -LRB- 3 -RRB- where G ~ and dof are defined above '
'2 Overview 21 The word segmentation problem As statistical machine translation systems basically rely on the notion of words through their lexicon models <OTH> , they are usually capable of outputting sentences already segmented into words when they translate into languages like Chinese or Japanese '
'WSD has received increasing attention in recent literature on computational linguistics <CIT> '
'35 Maximum Entropy Model In order to build a unified probabilistic query alteration model , we used the maximum entropy approach of <OTH> , which Li et al '
'21 Data representation We have compared four complete and three partial data representation formats for the baseNP recognition task presented in <CIT> '
'Importantly , this Bayesian approach facilitates the incorporation of sparse priors that result in a more practical distribution of tokens to lexical categories <CIT> '
'The last important fact is that it is possible to demonstrate that -LRB- Ei , j -RRB- = k P -LRB- Ri , jT ei , j -RRB- 1P -LRB- Ri , jT ei , j -RRB- = = kodds -LRB- Ri , j -RRB- where k is a constant -LRB- see <CIT> -RRB- that will be neglected in the maximization process '
'Reranking methods have also been proposed as a method for using syntactic information <CIT> '
'For the efficiency of minimum-errorrate training <CIT> , we built our development set -LRB- 580 sentences -RRB- using sentences not exceeding 50 characters from the NIST MT-02 evaluation test data '
'Therefore , having correct transliterations would give only small improvements in terms of BLEU <CIT> and NIST scores '
'Preparing an aligned abbreviation corpus , we obtain the optimal combination of the features by using the maximum entropy framework <CIT> '
'8This result is presented as 0053 with the official ROUGE scorer <CIT> '
'After line 17 , we can employ the one-sense-per-discourse heuristic to further classify unclassified data , as proposed in <CIT> '
'Many studies on collocation extraction are carried out based on co-occurring frequencies of the word pairs in texts <CIT> '
'1 Introduction Parsers have been developed for a variety of grammar formalisms , for example HPSG <OTH> , LFG <CIT> , TAG <OTH> , CCG <OTH> , and variants of phrase-structure grammar <OTH> , including the phrase-structure grammar implicit in the Penn Treebank <OTH> '
'Decoding weights are optimized using Ochs algorithm <CIT> to set weights for the four components of the log-linear model : language model , phrase translation model , distortion model , and word-length feature '
'The way a decoder constructs translation hypotheses is directly related to the weights for different model features in a SMT system , which are usually optimized for a given set of models with minimum error rate training -LRB- MERT -RRB- <CIT> to achieve better translation performance '
'Null productions are also a source of double counting, as there are many possible orders in 926 N I 2+ N IN N I } N IN I I I N N N (a) Normal Domain Rules } I squigglerightN 2+ I squigglerightNI I squigglerightNI I squigglerightN N N N I I I (b) Inverted Domain Rules N 11 ,fN 11 N 11 N 10 N 10 N 10 e, N 10 N 00 } N 11 ,f  N 10 } N 10 N 00 e,  } N 00 I 11 N NI 11 N NI 00 N 00 I + 11 I 00 N 00 N 10 N 10 N 11 N N I 11 I 11 I 00 N 00 N 11 (c) Normal Domain with Null Rules } } } I 11 squiggleright ,fI 11 I 11 squigglerightI 10 I 11 squiggleright ,f  I 10 I 10 squiggleright I 10 e,  I 10 squigglerightI 00 I 10 squiggleright I 00 e,   I 00 squigglerightN + 11 N 00 I I N 00 N 11 N 11 I 00 squigglerightN 11 I I squigglerightN 11 I I squigglerightN 00 I 00 I 00 I 10 I 10 I 11 I 11 (d) Inverted Domain with Null Rules Figure 2: Illustration of two unambiguous forms of ITG grammars: In (a) and (b), we illustrate the normal grammar without nulls (presented in Wu (1997) and Zens and Ney (2003)).'
'Models of this kind assume that an input word is generated by only one output word <CIT> '
'N-gram language models have also been used in Statistical Machine Translation -LRB- SMT -RRB- as proposed by <CIT> '
'In natural language processing , recent years have seen ME techniques used for sentence boundary detection , part of speech tagging , parse selection and ambiguity resolution , and stochastic attribute-value grammars , to name just a few applications <CIT> '
'Due to the parameter interdependencies introduced by the one-to-one assumption , we are unlikely to find a method for decomposing the assignments into parameters that can be estimated independently of each other as in <CIT> et al <OTH> -RRB- '
'This tolerant search uses the well known concept of Levenshtein distance in order to obtain the most similar string for the given prefix -LRB- see <CIT> for more details -RRB- '
'This is related to the wellstudied problem of identifying paraphrases <CIT> and the more general variant of recognizing textual entailment , which explores whether information expressed in a hypothesis can be inferred from a given premise '
'A systematic exploration of a set of such features for proteinprotein interaction extraction was recently provided by <CIT> , who also used features derived from the Collins parser '
'The feature weights i in the log-linear model are determined using a minimum error rate training method , typically Powells method <CIT> '
'We use the version extracted and preprocessed by <CIT> '
'This problem will be solved by incorporating other resources such as thesaurus or a dictionary , orcombiningourmethodwithothermethods using external wider contexts <CIT> '
'<OTH> , <CIT> , <OTH> '
'(Collins, 2002).'
'The first is to align the words using a standard word alignement technique , such as the Refined Method described in <CIT> -LRB- the intersection of two IBM Viterbi alignments , forward and reverse , enriched with alignments from the union -RRB- and then generate bi-phrases by combining together individual alignments that co-occur in the same pair of sentences '
'The simplest ` period-space-capital_letter '' approach works well for simple texts but is rather unreliable for texts with many proper names and abbreviations at the end of sentence as , for instance , the Wall Street Journal -LRB- WSJ -RRB- corpus -LRB- <CIT> -RRB- '
'In addition , since word senses are often associated with domains <CIT> , word senses can be consequently distinguished by way of determining the domain of each description '
'Smadja <OTH> , which is the classic work on collocation extraction , uses a two-stage filtering model in which , in the first step , n-gram statistics determine possible collocations and , in the second step , these candidates are submitted to a syntactic valida7Of course , lexical material is always at least partially dependent on the domain in question '
'Previously published approaches to reducing the rule set include : enforcing a minimum span of two words per non-terminal <CIT> , which would reduce our set to 115M rules ; or a minimum count -LRB- mincount -RRB- threshold <OTH> , which would reduce our set to 78M -LRB- mincount = 2 -RRB- or 57M -LRB- mincount = 3 -RRB- rules '
'We also tested other automatic methods : content-based evaluation , BLEU <OTH> and ROUGE-1 <CIT> , and compared their results with that of evaluation by revision as reference '
'From a theoretical point of view , it is difficult to find motivation for the parameter estimation methods used by <OTH> see <OTH> for discussion '
'3 Probability Model This paper takes a ` history-based '' approach <OTH> where each tree-building procedure uses a probability model p -LRB- alb -RRB- , derived from p -LRB- a , b -RRB- , to weight any action a based on the available context , or history , b First , we present a few simple categories of contextual predicates that capture any information in b that is useful for predicting a Next , the predicates are used to extract a set of features from a corpus of manually parsed sentences '
'The algorithms were trained and tested using version 3 of the Penn Treebank , using the training , development , and test split described in <CIT> and also employed by Toutanova et al '
'Other possibilities for the weighting include assigning constant one or the exponential of the final score etc One of the advantages of the proposed phrase training algorithm is that it is a parameterized procedure that can be optimized jointly with the trans82 lation engine to minimize the final translation errors measured by automatic metrics such as BLEU <CIT> '
'Indeed , the result of <CIT> that including low support features helps a voted perceptron model but harms a maximum entropy model is undone once the weights of the maximum entropy model are regularized '
'Of the 1600 IBM sentences that have been parsed <CIT> , only 67 overlapped with the IBM-manual treebank that was bracketed by University of Lancaster '
'The goal of integrating syntactic information into the translation model has prompted many researchers to pursue tree-based transfer models <CIT> , with increasingly encouraging results '
'The syntactic parameters are the same as in Section 51 and are smoothed as in <CIT> '
'The sets obtainedare then ranked usingthe loglikelihoodratiostes <CIT> '
'with parse action sequences for 40,000 Wall Street Journal sentences derived from the Penn Treebank <CIT> '
'Supertags Part-of-speech disambiguation techniques -LRB- POS taggers -RRB- <OTH> are often used prior to parsing to eliminate -LRB- or substantially reduce -RRB- the part-of-speech ambiguity '
'<CIT> and Liang et al '
'As association measure we apply log-likelihood ratio <CIT> to normalized frequency '
'item form : -LRB- i , j , ueve -RRB- goal : -LRB- I , j , ue -RRB- rules : -LRB- i , j , ue -RRB- R -LRB- fifiprime\/ejejprime -RRB- -LRB- iprime , j , ejejprime -RRB- -LRB- i , j , ueejve -RRB- -LRB- i , j + 1 , ueejve -RRB- ej +1 = rj +1 -LRB- Logic MONOTONE-ALIGN -RRB- Under the boolean semiring , this -LRB- minimal -RRB- logic decides if a training example is reachable by the model , which is required by some discriminative training regimens <CIT> '
'5 Related Work Although there have been many studies on collocation extraction and mining using only statistical approaches <CIT> , there has been much less work on collocation acquisition which takes into account the linguistic properties typically associated with collocations '
'<CIT> and Alshawi et al '
'The noun phrases in this data set are the same as in the Treebank and therefore the baseNPs in this data set are slightly different from the ones in the <CIT> data sets '
'This second expression is similar to that used in <OTH> '
'The result in <CIT> implies that for the special case of Bracketing ITGs , the time complexity of the algorithm is parenleftbigT3V 3parenrightbig where T and V are the lengths of the two sentences '
'<CIT> used a likelihood ratio to test word similarity under the assumption that the words in text have a binomial distribution '
'We use eight similarity measures implemented within the WordNet : : Similarity package5 , described in <CIT> ; these include three measures derived from the paths between the synsets in WordNet : HSO <OTH> , LCH <OTH> , and WUP <OTH> ; three measures based on information content : RES <OTH> , LIN <OTH> , and JCN <OTH> ; the gloss-based Extended Lesk Measure LESK , <CIT> , and finally the gloss vector similarity measure VECTOR <OTH> '
'Following this idea , there have been introduced a parameter estimation approach for non-generative approaches that can effectively incorporate unlabeled data <CIT> '
'First , hierarchical word clusters are derived from unlabeled data using the Brown et al clustering algorithm <CIT> '
'Some improvements on BOW are given by the use of dependency trees and syntactic parse trees <OTH> , <OTH> , <OTH> , but these , too are not adequate when dealing with complex questions whose answers are expressed by long and articulated sentences or even paragraphs '
'Following <CIT> , we only include features which occur 5 times or more in training data '
'1 Introduction The Inversion Transduction Grammar -LRB- ITG -RRB- of <CIT> is a syntactically motivated algorithm for producing word-level alignments of pairs of translationally equivalent sentences in two languages '
'Dependency models have recently gained considerable interest in many NLP applications , including machine translation <CIT> '
'The classical Bayes relation is used to introduce a target language model <CIT> : e = argmaxe Pr -LRB- e f -RRB- = argmaxe Pr -LRB- f e -RRB- Pr -LRB- e -RRB- where Pr -LRB- f e -RRB- is the translation model and Pr -LRB- e -RRB- is the target language model '
'Note that Row 3 of Table 3 corresponds to <CIT> s system which applies only word pair features '
'Since the advent of manually tagged corpora such as the Brown Corpus and the Penn Treebank -LRB- Francis <OTH> , Marcus <OTH> -RRB- , the efficacy of machine learning for training a tagger has been demonstrated using a wide array of techniques , including : Markov models , decision trees , connectionist machines , transformations , nearest-neighbor algorithms , and maximum entropy -LRB- Weischedel <OTH> , Black <OTH> , Schmid <OTH> , Brill <OTH> , Daelemans <OTH> , <CIT> -RRB- '
'With our best performing features , we get ROUGE-2 <CIT> scores of 011 and 00925 on 2007 and 2006 5This threshold was derived experimentally with previous data '
'For the future , the joint model would benefit from lexical weighting like that used in the standard model <CIT> '
'All model weights were trained on development sets via minimum-error rate training -LRB- MERT -RRB- <CIT> with 200 unique n-best lists and optimizing toward BLEU '
'Seen from Table 2 , our result about SCL is in accord with that in <CIT> on the whole '
'71 Interand Intra-annotator agreement We measured pairwise agreement among annotators usingthekappacoefficient -LRB- K -RRB- whichiswidelyused in computational linguistics for measuring agreement in category judgments <CIT> '
'Online baselines include Top-1 Perceptron <CIT> , Top-1 Passive-Aggressive -LRB- PA -RRB- , and k-best PA <OTH> '
'For the MUC6 data set , we extract noun phrases -LRB- mentions -RRB- automatically , but for MPQA , we assume mentions for coreference resolution are given as in <CIT> '
'MET <CIT> iterative parameter estimation under IBM BLEU is performed on the development set '
'The only requirement will be that a parallel corpus exist for the language under consideration and one or more other languages.2 Induction of grammars from parallel corpora is rarely viewed as a promising task in its own right; in work that has addressed the issue directly (Wu, 1997; Melamed, 2003; Melamed, 2004), the synchronous grammar is mainly viewed as instrumental in the process of improving the translation model in a noisy channel approach to statistical MT.3 In the present paper, we provide an important prerequisite for parallel corpus-based grammar induction work: an efficient algorithm for synchronous parsing of sentence pairs, given a word alignment.'
'We use the averaged perceptron algorithm , as presented in <CIT> , to train the parser '
'IBM constraints <OTH> , lexical word reordering model <OTH> , and inversion transduction grammar -LRB- ITG -RRB- constraints <CIT> belong to this type of approach '
'We evaluated annotation reliability by using the Kappa statistic <CIT> '
'Direct feedback loops that copy a predicted output label to the input representation of the next example have been used in symbolic machine-learning architectures such as the the maximum-entropy tagger described by <CIT> and the memory-based tagger -LRB- MBT -RRB- proposed by Daelemans et al '
'This is important when LARGE CUT-OFF 0 5 100 NAIVE 541,721 184,493 35,617 SASH 10,599 8,796 6,231 INDEX 5,844 13,187 32,663 Table 4 : Average number of comparisons per term considering that different tasks may require different weights and measures <CIT> '
'264-285.</rawString> </citation> <citation valid=''true''> <authors> <author>T Fukushima</author> <author>M Okumura</author> </authors> <title>Text summarization challenge: text summarization in Japan</title> <date>2001</date> <booktitle>in Proceedings of NAACL 2001 Workshop Automatic Summarization</booktitle> <pages>51--59</pages> <contexts> <context>Conferences (MUC) (Chinchor et al, 1993), TIPSTER SUMMAC Text Summarization Evaluation (Mani et al, 1998), Document Understanding Conference (DUC) (DUC, 2004), and Text Summarization Challenge (TSC) (Fukushima and Okumura, 2001), have attested the importance of this topic.'
'Thus , we are lead to an ` ontologically promiscuous '' semantics <CIT> '
'Movie-review dataset consists of positive and negative reviews from the Internet Movie Database -LRB- IMDb -RRB- archive <CIT> '
'The idea of topic signature terms was introduced by Lin and Hovy <CIT> in the context of single document summarization , and was later used in several multi-document summarization systems <OTH> '
'421 Teufel and Moens Summarizing Scientific Articles We use the kappa coefficient K <OTH> to measure stability and reproducibility , following <CIT> '
'Classes were identified using a POS tagger <CIT> trained on the tagged Switchboard corpus '
'303 Wiebe , Wilson , Bruce , Bell , and Martin Learning Subjective Language While it is common in studies of collocations to omit low-frequency words and expressions from analysis , because they give rise to invalid or unrealistic statistical measures <CIT> , we are able to identify higher-precision collocations by including placeholders for unique words -LRB- ie , the ugen-n-grams -RRB- '
'In showing how DLTAG and an interpretative process on its derivations operate , we must , of necessity , gloss over how inference triggered by adjacency or associated with a structural connective provides the intended relation between adjacent discourse 578 Computational Linguistics Volume 29 , Number 4 units : It may be a matter simply of statistical inference , as in <CIT> , or of more complex inference , as in Hobbs et al '
'An alternative would be using a vector space model for classi cation where calltypes and utterances are represented as vectors including word a2 - grams <CIT> '
'2 IBM Model 4 Various statistical alignment models of the form Pr -LRB- fJ1 ; aJ1jeI1 -RRB- have been introduced in <CIT> '
'1 Introduction Recent work in statistical machine translation -LRB- MT -RRB- has sought to overcome the limitations of phrasebased models <CIT> by making use of syntactic information '
'Several authors have used mutual information and similar statistics as an objective function for word clustering <CIT> , for automatic determination of phonemic baseforms <OTH> , and for language modeling for speech recognition <OTH> '
'2 The IBM Model 4 For the work described in this paper we used a modified version of the statistical machine translation tool developed in the context of the 1999 Johns HopkinsSummer Workshop <OTH> , which implements IBM translation model 4 <CIT> '
'For this we used two resources : CELEX a linguistically annotated dictionary of English , Dutch and German <OTH> , and the Dutch snowball stemmer implementing a suf x stripping algorithm based on the Porter stemmer '
'32 Word Order Differences Another problem that has been noticed as early as 1993 with the first research on word alignment <CIT> concerns the differences in word order between source and target language '
'<OTH> , <CIT> and Lee <OTH> , Wilson et al '
'As suggested in <CIT> , we use the averaged perceptron when applying the model to held-out or test data '
'Opinion forecasting differs from that of opinion analysis , such as extracting opinions , evaluating sentiment , and extracting predictions <CIT> '
', Models 2 and 3 of <CIT> '
'The approach is in the spirit of Smadja <OTH> on retrieving collocations from text corpora , but is more integrated with parsing '
'<CIT> indicated that their results can not directly compare to the results of Shi and Wang <OTH> due to different experimental settings '
'<OTH> found that direct annotation takes twice as long as automatic tagging plus correction , for partof-speech annotation -RRB- ; and the output quality reflects the difficulty of the task -LRB- inter-annotator disagreement is on the order of 10 % , as contrasted with the approximately 3 % error rate reported for part-of-speech annotation by <CIT> et al -RRB- '
'1 Introduction In the community of sentiment analysis <CIT> , transferring a sentiment classifier from one source domain to another target domain is still far from a trivial work , because sentiment expression often behaves with strong domain-specific nature '
'We view L2P as a tagging task that can be performed with a discriminative learning method , such as the Perceptron HMM <CIT> '
'Using a variant of the voted perceptron <CIT> , we discriminatively trained our parser in an on-line fashion '
'Perhaps the most widely accepted convention is that of ignoring punctuation for the purposes of assigning constituent span, under the perspective that, fun788 Phrase Evaluation Scenario System Type (a) (b) (c) Modified All 98.37 99.72 99.72 Truth VP 92.14 98.70 98.70 Li and Roth All 94.64 (2001) VP 95.28 Collins (1997) All 92.16 93.42 94.28 VP 88.15 94.31 94.42 Charniak All 93.88 95.15 95.32 (2000) VP 88.92 95.11 95.19 Table 1: F-measure shallow bracketing accuracy under three different evaluation scenarios: (a) baseline, used in Li and Roth (2001), with original chunklink script converting treebank trees and context-free parser output; (b) same as (a), except that empty subject NPs are inserted into every unary SVP production; and (c) same as (b), except that punctuation is ignored for setting constituent span.'
'32-39 Proceedings of HLT-NAACL 2003 similar distribution patterns <CIT> '
'31 System Tuning Minimum error training <CIT> under BLEU <OTH> was used to optimise the feature weights of the decoder with respect to the dev2006 development set '
'The interpolation weights a65 -LRB- Equation 2 -RRB- are trained using discriminative training <CIT> using ROUGEa129 as the objective function , on the development set '
'53 Evaluation Metric This paper focuses on the BLEU metric as presented in <CIT> '
'where mk is one mention in entity e , and the basic model building block PL -LRB- L = 1je , mk , m -RRB- is an exponential or maximum entropy model <CIT> '
'Word alignment models were first introduced in statistical machine translation <CIT> '
'Applying the projection WTx -LRB- where x is a training instance -RRB- would give us m new features , however , for both computational and statistical reasons <CIT> a low-dimensional approximation of the original feature space is computed by applying Singular Value Decomposition -LRB- SVD -RRB- on W -LRB- step 4 -RRB- '
'One of the main directions is sentiment classification , which classifies the whole opinion document -LRB- eg , a product review -RRB- as positive or negative <CIT> '
'<CIT> all use multiple context words as discriminating features '
'2Note that sentence extraction does not solve the problem of selecting and ordering summary sentences to form a coherent There are several approaches to modeling document content : simple word frequency-based methods <OTH> , graph-based approaches <OTH> , as well as more linguistically motivated techniques <OTH> '
'By introducing the hidden word alignment variable a, the following approximate optimization criterion can be applied for that purpose: e = argmaxe Pr(e | f) = argmaxe summationdisplay a Pr(e,a | f)  argmaxe,a Pr(e,a | f) Exploiting the maximum entropy (Berger et al. , 1996) framework, the conditional distribution Pr(e,a | f) can be determined through suitable real valued functions (called features) hr(e,f,a),r = 1R, and takes the parametric form: p(e,a | f)  exp Rsummationdisplay r=1 rhr(e,f,a)} The ITC-irst system (Chen et al. , 2005) is based on a log-linear model which extends the original IBM Model 4 (Brown et al. , 1993) to phrases (Koehn et al. , 2003; Federico and Bertoldi, 2005).'
'Formally , by distributional similarity -LRB- or co-occurrence similarity -RRB- of two words w 1 and w 2 , we mean that they tend to occur in similar contexts , for some definition of context ; or that the set of words that w 1 tends to co-occur with is similar to the set that w 2 tends to co-occur with ; or that if w 1 is substituted for w 2 in a context , its plausibility <CIT> is unchanged '
'(Note that conditioning on the rules parent is needed to disallow the structure [NP [NP PP] PP]; see Johnson [1997] for further discussion.)'
'Recent work has explored two-stage decoding , which explicitly decouples decoding into a source parsing stage and a target language model integration stage <CIT> '
'Machine translation has code-like characteristics , and indeed , the initial models of <CIT> took a word-substitution\/transposition approach , trained on a parallel text '
'Training Procedure Our algorithm is a modification of the perceptron ranking algorithm <OTH> , which allows for joint learning across several ranking problems <CIT> '
'We are currently investigating more challenging problems like multiple category classification using the Reuters-21578 data set <OTH> and subjective sentiment classification <CIT> '
'We used <CIT> statistical parser trained on examples from the Penn Treebank to generate parses of the same format for the sentences in our data '
'They constructed word clusters by using HMMs or Browns clustering algorithm <CIT> , which utilize only information from neighboring words '
'An additional translation set called the Maximum BLEU set is employed by the SMT system to train the weights associated with the components of its log-linear model <CIT> '
'In our SRL system , we select maximum entropy <CIT> as a classi er to implement the semantic role labeling system '
'5 Related Work There has not been much previous work on graphical models for full parsing , although recently several latent variable models for parsing have been proposed <CIT> '
'Although some work has been done on syllabifying orthographic forms <CIT> , syllables are , technically speaking , phonological entities that can only be composed of strings of phonemes '
'Also , on WS-353 , our hybrid sense-filtered variants and word-cos-ll obtained a correlation score higher than published results using WordNet-based measures <OTH> -LRB- 33 to 35 -RRB- and Wikipediabased methods <CIT> -LRB- 19 to 48 -RRB- ; and very close to the results obtained by thesaurus-based <OTH> -LRB- 55 -RRB- and LSA-based methods <OTH> -LRB- 56 -RRB- '
'Also in the Penn Treebank -LRB- <CIT> , <CIT> -RRB- a limited set of relations is placed over the constituencybased annotation in order to make explicit the -LRB- morpho-syntactic or semantic -RRB- roles that the constituents play '
'1 Introduction Syntax-based translation models <OTH> are usually built directly from Penn Treebank -LRB- PTB -RRB- <CIT> style parse trees by composing treebank grammar rules '
'As an alternative to linear interpolation , we also employ a weighted product for phrase-table combination : p -LRB- s t -RRB- productdisplay j pj -LRB- s t -RRB- j -LRB- 3 -RRB- This has the same form used for log-linear training of SMT decoders <CIT> , which allows us to treateachdistributionasafeature , andlearnthemixing weights automatically '
'This second point is emphasized by the second paper on self-training for adaptation <CIT> '
'2 Background Several graph-based learning techniques have recently been developed and applied to NLP problems : minimum cuts <CIT> , random walks <OTH> , graph matching <OTH> , and label propagation <OTH> '
'The main reason behind this lies in the difference between the two corpora used : Penn Treebank <CIT> and EDR corpus <OTH> '
'31 Model-based Phrase Pair Posterior In a statistical generative word alignment model <CIT> , it is assumed that -LRB- i -RRB- a random variable a specifies how each target word fj is generated by -LRB- therefore aligned to -RRB- a source 1 word eaj ; and -LRB- ii -RRB- the likelihood function f -LRB- f , a e -RRB- specifies a generativeprocedurefromthesourcesentencetothe target sentence '
'Unfortunately , as shown in <CIT> , with the represetation of sentences that we use , linear classifiers can not discriminate real sentences from sentences sampled from a trigram , which is the model we use as a baseline , so here we resort to a non-linear large-margin classifier -LRB- see section 3 for details -RRB- '
'Tuning is done for each experimental condition using Ochs Minimum Error Training <CIT> '
'Maximum entropy estimation for translation of individual words dates back to <CIT> , and the idea of using multi-class classifiers to sharpen predictions normally made through relative frequency estimates has been recently reintroducedundertherubricofwordsensedisambiguation and generalized to substrings <OTH> '
'Experimental results are reported in Table 2 : here cased BLEU results are reported on MT03 Arabic-English test set <CIT> '
'For this study, the Levenshtein edit-distance score (where a perfect match scores zero) is  Roman Chinese (Pinyin) Alignment Score LEV ashburton ashenbodu |   a   s   h   b   u   r   t   o   n   | |   a   s   h   e   n   b  o  d    u   | 0.67 MLEV ashburton ashenbodu |  a   s   h       b   u   r    t   o   n  | |  a   s   h   e   n   b   o     d   u    | 0.72 MALINE asVburton aseCnpotu |   a   sV    b   <   u   r   t   o   |   n |   a   s   eC  n   p   o     t   u   |   0.48 3 normalized to a similarity score as in (Freeman et al. 2006), where the score ranges from 0 to 1, with 1 being a perfect match.'
'To find the optimal coefficients for a loglinear combination of these experts , we use separate development data , using the following procedure due to <CIT> : 1 '
'Discriminatively trained parsers that score entire trees for a given sentence have only recently been investigated <CIT> '
'have been proposed <CIT> '
'1 Introduction NLP researchers have developed many algorithms for mining knowledge from text and the Web , including facts <OTH> , semantic lexicons <OTH> , concept lists <OTH> , and word similarity lists <CIT> '
'The later IBM models are formulated to prefer collocations <CIT> '
'Since the lexical translations and dependency paths are typically not labeled in the English corpus , a given pair must be counted fractionally according to its posterior probability of satisfying these conditions , given models of contextual translation and English parsing3 3Similarly , <CIT> imputes missing trees by using comparable corpora '
'While this heuristic estimator gives good empirical results , it does not seem to optimize any intuitively reasonable objective function of the -LRB- wordaligned -RRB- parallel corpus -LRB- see eg , <OTH> -RRB- The mounting number of efforts attacking this problem over the last few years <CIT> exhibits its difficulty '
'Other work aims to do truly unsupervised learning of taggers , such as Goldwater and Griffiths <OTH> and <CIT> <OTH> '
'The LFG annotation algorithm of <CIT> was used to produce the f-structures for development , test and training sets '
'Decoding is carried-out using the Moses decoder <CIT> '
'Statistical techniques , both supervised learning from tagged corpora <CIT> , <OTH> , and unsupervised learning <CIT> , <OTH> , have been investigated '
'A variety of approaches have been investigated for speech summarization , for example , maximum entropy , conditional random fields , latent semantic analysis , support vector machines , maximum marginal relevance <CIT> '
'<CIT> The heuristics in Section 6 are designed specifically to find the interesting features in that featureless desert '
'During the SRC stage, a Maximum entropy (Berger et al., 1996) classifier is used to predict the probabilities of a word in the sentence Language No-duplicated-roles Catalan arg0-agt, arg0-cau, arg1-pat, arg2-atr, arg2-loc Chinese A0, A1, A2, A3, A4, A5, Czech ACT, ADDR, CRIT, LOC, PAT, DIR3, COND English A0, A1, A2, A3, A4, A5, German A0, A1, A2, A3, A4, A5, Japanese DE, GA, TMP, WO Spanish arg0-agt, arg0-cau, arg1-pat, arg1-tem, arg2-atr, arg2-loc, arg2-null, arg4-des, argL-null, argMcau, argM-ext, argM-fin Table 1: No-duplicated-roles for different languages to be each semantic role.'
'<CIT> proposed a method to identify discourse relations between text segments using Nave Bayes classifiers trained on a huge corpus '
'Note that the predicate language representation utilized by Carmel-Tools is in the style of Davidsonian event based semantics <CIT> '
'Alignment performance is measured by the Alignment Error Rate -LRB- AER -RRB- <CIT> AER -LRB- B ; B -RRB- = 12 B B \/ -LRB- B + B -RRB- where B is a set reference word links , and B are the word links generated automatically '
'43 Corpora The evaluations of the different models were carried out on the Penn Wall Street Journal corpus <CIT> for English , and the Tiger treebank <OTH> for German '
'Open-domain opinion extraction is another trend of research on opinion extraction , which aims to extract a wider range of opinions from such texts as newspaper articles <CIT> '
'Alignment , whether for training a translation model using EM or for nding the Viterbi alignment of test data , is O -LRB- n6 -RRB- <CIT> , while translation -LRB- decoding -RRB- is O -LRB- n7 -RRB- using a bigram language model , and O -LRB- n11 -RRB- with trigrams '
'The data sets used are the standard data sets for this problem <CIT> taken from the Wall Street Journal corpus in the Penn Treebank <OTH> '
'2 Phrase-based SMT We use a phrase-based SMT system , Pharaoh , <CIT> , which is based on a log-linear formulation <OTH> '
'<CIT> described symmetrized training of a 1-toN log-linear model and a M-to-1 log-linear model '
'Model parameters are estimated using maximum entropy <CIT> '
'We use Viterbi training <OTH> but neighborhood estimation <CIT> or pegging <OTH> could also be used '
'It also contains tools for tuning these models using minimum error rate training <CIT> and evaluating the resulting translations using the BLEU score <OTH> '
'2 Problem Setting In the multi-class setting , instances from an input spaceX take labels from a finite setY , Y = K 496 We use a standard approach <CIT> for generalizing binary classification and assume a feature function f -LRB- x , y -RRB- Rd mapping instances xX and labels yY into a common space '
'First , the Wikipedia gazetteer improved the accuracy as expected , ie , it reproduced the result of <CIT> for Japanese NER '
'At this point , one can imagine estimating a linear matching model in multiple ways , including using conditional likelihood estimation , an averaged perceptron update -LRB- see which matchings are proposed and adjust the weights according to the dierence between the guessed and target structures <CIT> -RRB- , or in large-margin fashion '
'For example , both papers propose minimum-risk decoding , and McDonald and Satta <OTH> discuss unsupervised learning and language modeling , while <CIT> and <CIT> <OTH> define hiddenvariable models based on spanning trees '
'For example , <CIT> only requires sense number and a few seeds for each sense of an ambiguous word -LRB- hereafter called keyword -RRB- '
'2 Background : Overview of BLEU This section briefly describes the original BLEU <CIT> 1 , which was designed for English translation evaluation , so English sentences are used as examples in this section '
'Therefore , to make the phrase-based SMT system robust against data sparseness for the ranking task , we also make use of the IBM Model 4 <CIT> in both directions '
'Then the alignments are symmetrized using a refined heuristic as described in <CIT> '
'To measure the coherence of sentences , we use a statistical parser Toolkit <CIT> to assign each sentence a parsers score that is the related log probability of parsing '
'For comparison , we also implemented a different N-best phrase alignment method , where _ _ _ _ the_light_was_red _ _ _ the_light was_red _ _ the_light was red -LRB- 1 -RRB- -LRB- 2 -RRB- -LRB- 3 -RRB- Figure 4 : N-best phrase alignments phrase pairs are extracted using the standard phrase extraction method described in <CIT> '
'33 Unknown word features Most of the models presented here use a set of unknown word features basically inherited from <CIT> , which include using character n-gram prefixes and suffixes -LRB- for n up to 4 -RRB- , and detectors for a few other prominent features of words , such as capitalization , hyphens , and numbers '
'Concept similarity is often measured by vectors of co-occurrence with context words that are typed with dependency information <CIT> '
'Metrics based on word alignment between MT outputs and the references <CIT> '
'This approach to term clustering is closely related to others from the literature <CIT> 2 Recall that the mutual information between random variables a0 and a1 can be written : a2a4a3a6a5a8a7a10a9a11a13a12a15a14a17a16a19a18a21a20a23a22a25a24a27a26a29a28 a14a17a16a19a18a21a20a23a22a25a24 a14a17a16a19a18a30a24a31a14a17a16a19a22a32a24 -LRB- 1 -RRB- Here , a0 and a1 correspond to term and context clusters , respectively , each event a18 and a22 the observation of some term and contextual term in the corpus '
'<OTH> -RRB- , the tagger for grammatical functions works with lexical -LRB- 1 -RRB- Selbst besucht ADV VVPP himself visited hat Peter Sabine VAFIN NE NE has Peter Sabine ` Peter never visited Sabine himself '' l hie ADV never Figure 2 : Example sentence and contextual probability measures PO -LRB- '' -RRB- depending on the category of a mother node -LRB- Q -RRB- '
'<CIT> , Liang et al '
'First , we considered single sentences as documents , and tokens as sentences -LRB- we define a token as a sequence of characters delimited by 1In our case , the score we seek to globally maximize by dynamic programming is not only taking into account the length criteria described in <OTH> but also a cognate-based one similar to <OTH> '
'Then the initial precision is 1 <CIT> , citing <CIT> , actually uses a superficially different score that is , however , a monotone transform of precision , hence equivalent to precision , since it is used only for sorting '
'The use of Profile HMMs for multiple sequence alignment also presents applications to the acquisition of mapping dictionaries <CIT> and sentence-level paraphrasing <CIT> '
'Our approach is related to those of <CIT> and Taskar et al '
'The pipeline extracts a Hiero-style synchronous context-free grammar <OTH> , employs suffix-array based rule extraction <OTH> , and tunes model parameters with minimum error rate training <CIT> '
'Therefore , the Viterbi alignment is comlmted only approximately using the method described in <CIT> '
'The most important tree-bank transformation in the literature is lexicalization : Each node in a tree is labeled with its head word , the most important word of the constituent under the node -LRB- Magerman <OTH> , <CIT> , Charniak -LRB- 1997 -RRB- , Collins -LRB- 1997 -RRB- , Carroll and Rooth -LRB- 1998 -RRB- , etc -RRB- '
'Even a length limit of 3 , as proposed by <CIT> , would result in almost optimal translation quality '
'Each element of the resulting vector was replaced with its log-likelihood value -LRB- see Definition 10 in Section 23 -RRB- which can be considered as an estimate of how surprising or distinctive a co-occurrence pair is <CIT> '
'<CIT> argue that generic sentence fusion is an ill-defined task '
'MI is defined in general as follows : y -RRB- I ix y -RRB- = log2 P -LRB- x -RRB- P -LRB- y -RRB- We can use this definition to derive an estimate of the connectedness between words , in terms of collocations <OTH> , but also in terms of phrases and grammatical relations <CIT> '
'The original training set -LRB- before the addition of the feedback sets -RRB- consisted of a few dozen examples , in comparison to thousands of examples needed in other corpus-based methods <CIT> '
'Its size is compatible to <CIT> '
'2 Related Work There have been various efforts to integrate linguistic knowledge into SMT systems , either from the target side <OTH> , the source side <OTH> or both sides <CIT> , just to name a few '
'Recently , some kinds of learning techniques have been applied to cumulatively acquire exemplars form large corpora <CIT> '
'The second approach <OTH> takes triples -LRB- verb , prep , noun2 -RRB- and -LRB- nounl , prep , noun2 -RRB- , like those in Table 10 , as training data for acquiring semantic knowledge and performs PP-attachment disambiguation on quadruples '
'Unlexicalized parsers , on the other hand , achieved accuracies almost equivalent to those of lexicalized parsers <CIT> '
'22 Themaximumentropytagger The maximum entropy model used in POStagging is described in detail in <CIT> andthePOCtaggerhereusesthesame probability model '
'It is worth noting that we observed the same relation between subjectivity detection and polarity classification accuracy as described by <CIT> and Eriksson -LRB- 2006 -RRB- '
'<CIT> uses a conceptually similar technique for WSD that learns from a small set of seed examples and then increases recall by bootstrapping , evaluated on 12 idiosyncratically polysemous words '
'2 RelatedWork 21 Sentiment Classification Most previous work on the problem of categorizing opinionated texts has focused on the binary classification of positive and negative sentiment <CIT> '
'We generate POS tags using the MXPOST tagger <CIT> for English and Chinese , and Connexor for Spanish '
'Motivation There have been quite a number of recent papers on parallel text : <CIT> , Chen <OTH> , Church <OTH> , Church et al <OTH> , Dagan et al <OTH> , Gale and Church <OTH> , Isabelle <OTH> , Kay and Rgsenschein <OTH> , Klavans and Tzoukermann <OTH> , Kupiec <OTH> , Matsumoto <OTH> , Ogden and Gonzales <OTH> , Shemtov <OTH> , Simard et al <OTH> , WarwickArmstrong and Russell <OTH> , Wu -LRB- to appear -RRB- '
'In this paper we will describe extensions to tile Hidden-Markov alignment model froln <OTH> and compare tlmse to Models 1 4 of <CIT> '
'Unlike Church and Hanks <OTH> , <CIT> goes beyond the ` two-word '' limitation and deals with ` collocations of arbitrary length '' '
'Automatic text summarization approaches have offered reasonably well-performing approximations for identifiying important sentences <OTH> but , not surprisingly , text -LRB- re -RRB- generation has been a major challange despite some work on sub-sentential modification <CIT> '
'Tagging can also be done using maximum entropy modeling -LRB- see Section 24 -RRB- : a maximum entropy tagger , called MXPOST , was developed by <CIT> -LRB- we will refer to this tagger as MXP below -RRB- '
'Further , it has been shown <CIT> that performance of Lins distributional similarity score decreases more significantly than other measures for low frequency nouns '
'Zero derivation <CIT> pointed out that it is helpful to identify zero-derived noun\/verb pairs for such tasks as normalization of the semantics of expressions that are only superficially different '
'2 Automatic Annotation Schemes Using ROUGE Similarity Measures ROUGE -LRB- Recall-Oriented Understudy for Gisting Evaluation -RRB- is an automatic tool to determine the quality of a summary using a collection of measures ROUGE-N -LRB- N = 1,2,3,4 -RRB- , ROUGE-L , ROUGE-W and ROUGE-S which count the number of overlapping units such as n-gram , word-sequences , and word-pairs between the extract and the abstract summaries <CIT> '
'a176 Base NP standard data set -LRB- baseNP-S -RRB- This data set was first introduced by <CIT> , and taken as the standard data set for baseNP identification task2 '
'2 The Data Our experiments on joint syntactic and semantic parsing use data that is produced automatically by merging the Penn Treebank -LRB- PTB -RRB- with PropBank -LRB- PRBK -RRB- <CIT> , as shown in Figure 1 '
'In all the experiments , our source side language is English , and the Stanford Named Entity Recognizer <CIT> was used to extract NEs from the source side article '
'The Duluth Word Alignment System is a Perl implementation of IBM Model 2 <CIT> '
'Other systems <OTH> also look at Web product reviews but they do not extract 345 opinions about particular product features '
'Minimum error rate training was used to tune the model feature weights <CIT> '
'Some studies have been done for acquiring collocation translations using parallel corpora <OTH> '
'31 Maximum Entropy This section presents a brief description of ME A more detailed and informative description can be found in Berger <OTH> 4 , <CIT> , Manning and Shutze -LRB- 2000 -RRB- to name just a few '
'PairClass generates probability estimates , whereas <CIT> uses a cosine measure of similarity '
'For every class the weights of the active features are combined and the best scoring class is chosen <CIT> '
'The sentences were processed with the Collins parser <CIT> to generate automatic parse trees '
'7 Independently , <CIT> quote a performance of 800 words per second for their part-of-speech tagger based on hidden Markov models '
'The value of Dist -LRB- D -LRB- T -RRB- -RRB- can be defined in various ways , and they found that using log-likelihood ratio <CIT> worked best which is represented as follows : 0 # log -RRB- -LRB- # log D K k TD k k i M ii i i M ii i = = , where k i and K i are the frequency of a word w i in D -LRB- W -RRB- and D 0 respectively , and -LCB- w 1 , , w M -RCB- is the set of all words in D 0 As stated in introduction , Dist -LRB- D -LRB- T -RRB- -RRB- is normalized by the baseline function , which is referred as B Dist -LRB- -RRB- here '
'<OTH> , Ponzetto and Strube <OTH> -RRB- and the exploitation of advanced techniques that involve joint learning -LRB- eg , <CIT> and Marcu <OTH> -RRB- and joint inference -LRB- eg , Denis and Baldridge -LRB- 2007 -RRB- -RRB- for coreference resolution and a related extraction task '
'In this paper , we present Phramer , an open-source system that embeds a phrase-based decoder , a minimum error rate training <CIT> module and various tools related to Machine Translation -LRB- MT -RRB- '
'For example , the HMM aligner achieves an AER of 207 when using the competitive thresholding heuristic of <CIT> '
'We used GIZA + + package <CIT> to train IBM translation models '
'Others , such as <CIT> , Pang and Vaithyanathan -LRB- 2002 -RRB- , have examined the positive or negative polarity , rather than presence or absence , of affective content in text '
'We will employ the structural correspondence learning -LRB- SCL -RRB- domain adaption algorithm used in <CIT> for linking the translated text and the natural text '
'53 Systematic Sense Shift <CIT> contend that there is strong evidence to suggest that a large part of word sense ambiguity is not arbitrary but follows regular patterns '
'Among all the language modeling approaches , ngram models have been most widely used in speech recognition <CIT> and other applications '
'Inter-sentential contexts as in our approach were used as a clue also for subjectivity analysis <CIT> , which is two-fold classification into subjective and objective sentences '
'We also employ the voted perceptron algorithm <OTH> and the early update technique as in <CIT> '
'High quality word alignments can yield more accurate phrase-pairs which improve quality of a phrase-based SMT system <CIT> '
'Co-occurrence information between neighboring words and words in the same sentence has been used in phrase extraction <CIT> , phrasal translation <CIT> , target word selection <OTH> , domain word translation <OTH> , sense disambiguation <OTH> , and even recently for query translation in cross-language IR as well <OTH> '
'<CIT> , by contrast to the above , used Wikipedia primarily for Named Entity Disambiguation , following the path of Bunescu and Paca <OTH> '
'Recently there have been some works on using multiple treebanks for domain adaptation of parsers , where these treebanks have the same grammar formalism <CIT> '
'Given a collection of facts , ME chooses a model consistent with all the facts , but otherwise as uniform as possible <CIT> '
'The earliest work in this direction are those of <OTH> , <CIT> , <OTH> , <OTH> , <OTH> and <OTH> '
'Compared with their string-based counterparts , treebased systems offer some attractive features : they are much faster in decoding -LRB- linear time vs cubic time , see <OTH> -RRB- , do not require a binary-branching grammar as in string-based models <CIT> , and can have separate grammars for parsing and translation , say , a context-free grammar for the former and a tree substitution grammar for the latter <OTH> '
'The studies presented by Goldwater and Griffiths <OTH> and <CIT> <OTH> differed in the number of states that they used '
'BLEU score In order to measure the extent to which whole chunks of text from the prompt are reproduced in the student essays , we used the BLEU score , known from studies of machine translation <CIT> '
'09595 09590 09611 09085 09134 09152 Table 8 : Comparison of F1 results of our baseline model with Nakagawa and Uchimoto <OTH> and <CIT> on CTB 30 '
'<CIT> -RRB- , the tagger for grammatical functions works with lexical -LRB- 1 -RRB- Selbst besucht ADV VVPP himself visited hat Peter Sabine VAFIN NE NE has Peter Sabine ` Peter never visited Sabine himself '' l hie ADV never Figure 2 : Example sentence and contextual probability measures PO -LRB- '' -RRB- depending on the category of a mother node -LRB- Q -RRB- '
'This leads to 49 methods that use semi-supervised techniques on a treebank-infered grammar backbone , such as <CIT> '
'Smadja <CIT> proposed a statistical model by measuring the spread of the distribution of cooccurring pairs of words with higher strength '
'Existing automatic evaluation measures such as BLEU <CIT> and ROUGE -LRB- Lin 2The collections are available from http://wwwcsail '
'To generate the n-best lists , a phrase based SMT <CIT> was used '
'EnglishChinese <CIT> and EnglishSpanish <OTH> '
'CIT -RRB- '
'A structured perceptron <CIT> learns weights for our transliteration features , which are drawn from two broad classes : indicator and hybrid generative features '
'Four alternatives are proposed in these special issues : -LRB- 1 -RRB- Brent <OTH> , -LRB- 2 -RRB- Briscoe and Carroll -LRB- this issue -RRB- , -LRB- 3 -RRB- Hindle and Rooth -LRB- this issue -RRB- , and -LRB- 4 -RRB- Weischedel et al '
'Building upon the large body of research to improve tagging performance for various languages using various models -LRB- eg , <OTH> -RRB- and the recent work on PCFG grammars with latent annotations <CIT> , we will investigate the use of fine-grained latent annotations for Chinese POS tagging '
'The parser has been trained , developed and tested on a large collection of syntactically analyzed sentences , the Penn Treebank <CIT> '
'<OTH> discuss three approaches : hand-crafted rules ; grammatical inference of subsequential transducers ; and log-linear classifiers with bigram and trigram features used as taggers <CIT> '
'Both taggers used the Penn Treebank tagset and were trained on the Wall Street Journal corpus <CIT> '
'<OTH> , and <CIT> et al '
'Nevertheless , EM sometimes fails to find good parameter values2 The reason is that EM tries to assign roughly the same number of word tokens to each of the hidden states <CIT> '
'21 Global linear models We follow the framework outlined in <CIT> '
'When updating model parameters , we employ a memorizationvariant of a local updating strategy <CIT> in which parameters are optimized toward a set of good translations found in the k-best list across iterations '
'We tested several measures , such as ROUGE <CIT> and the cosine distance '
'Following extraction , O-CRF applies the RESOLVER algorithm <CIT> to find relation synonyms , the various ways in which a relation is expressed in text '
'Computational approaches to prosodic modeling of DAs have aimed to automatically extract various prosodic parameters -- such as duration , pitch , and energy patterns -- from the speech signal <OTH> '
'1 Introduction A recent theme in parsing research has been the application of statistical methods to linguistically motivated grammars , for example LFG <CIT> , HPSG <OTH> , TAG <OTH> and CCG <OTH> '
'Furthermore , they extended WSD to phrase sense disambiguation -LRB- PSD -RRB- <CIT> '
'As in <CIT> fstructures are generated from the -LRB- now altered -RRB- treebank and from this data , along with the treebank trees , the PCFG-based grammar , which is used for training the generation model , is extracted '
'We also test an MI model inspired by Erk <OTH> : MISIM -LRB- n , v -RRB- = log summationdisplay nSIMS -LRB- n -RRB- Sim -LRB- n , n -RRB- Pr -LRB- v , n -RRB- Pr -LRB- v -RRB- Pr -LRB- n -RRB- We gather similar words using <CIT> , mining similar verbs from a comparable-sized parsed corpus , and collecting similar nouns from a broader 10 GB corpus of English text4 We also use Keller and Lapata <OTH> s approach to obtaining web-counts '
'The search also uses a Tag Dictionary constructed from training data , described in <CIT> , that reduces the number of actions explored by the tagging model '
'~ gtPdl= |&.allm~WI.Lqlf IDW,t~lIO, r I~''1~~ ~ II, Mlmulm,  IP, il~,,lllb, l~ ~ I I I I I I I I I 0 200 400 600 800 1000 1200 1400 1600 1800 Article# 2000 Figure 1: Distribution of Tags for the word ''about'' vs. Article# Training Size(wrds)I Test571190 Size(wrds) I Baseline44478 97.04% Specialized 197.13% Table 10: Performance of Baseline ~ Specialized Model When Tested on Consistent Subset of Development Set 139 POS Tag 35 30 25 2O 15 10 5 0 1 I o. Oho m I I I B ~ m M I I I 2 3 4 Annotator Figure 2: Distribution of Tags for the word ''about'' vs. Annotator (Weischedel et al. , 1993) provide the results from a battery of ''tri-tag'' Markov Model experiments, in which the probability P(W,T) of observing a word sequence W = {wl,w2,,wn} together with a tag sequence T = {tl,t2,,tn} is given by: P(TIW)P(W) p(tl)p(t21tl)  H P(tilti-lti-2) p(wilti i=3 Furthermore, p(wilti) for unknown words is computed by the following heuristic, which uses a set of 35 pre-determined endings: p(wilti) p(unknownwordlti ) x p(capitalfeature[ti) x p(endings, hypenationlti ) This approximation works as well as the MaxEnt model, giving 85% unknown word accuracy(Weischedel et al. , 1993) on the Wall St. Journal, but cannot be generalized to handle more diverse information sources.'
'Phrase pairs are extracted up to a fixed maximum length , since very long phrases rarely have a tangible impact during translation <CIT> '
'The resulting model has an exponential form with free parameters a102 a91 a24a94a93 a8 a87 a24 a10a11a10a11a10 a24a46a95 The parameter values which maximize the likelihood for a given training corpus can be computed with the socalled GIS algorithm -LRB- general iterative scaling -RRB- or its improved version IIS <CIT> '
'0 005 01 015 02 025 03 035 04 45 50 55 60 65 70 75 80 85Correlation Coefficient with Human Judgement -LRB- R -RRB- Human-Likeness Classifier Accuracy -LRB- % -RRB- Figure 1 : This scatter plot compares classifiers accuracy with their corresponding metrics correlations with human assessments been previously observed by <CIT> '
'Two error rates : the sentence error rate -LRB- SER -RRB- and the word error rate -LRB- WER -RRB- that we seek to minimize , and BLEU <CIT> , that we seek to maximize '
'We report precision , recall and balanced F-measure <CIT> '
'51 Pharaoh The baseline system we used for comparison was Pharaoh <OTH> , a freely available decoder for phrase-based translation models : p -LRB- e f -RRB- = p -LRB- f e -RRB- pLM -LRB- e -RRB- LM pD -LRB- e , f -RRB- D length -LRB- e -RRB- W -LRB- e -RRB- -LRB- 10 -RRB- We ran GIZA + + <CIT> on the training corpus in both directions using its default setting , and then applied the refinement rule diagand described in <OTH> to obtain a single many-to-many word alignment for each sentence pair '
'The mapping of answer terms to question terms is modeled using Black et als <OTH> simplest model , called IBM Model 1 '
'Recent research <OTH> shows that using different clusters for predicted and conditional words can lead to cluster models that are superior to classical cluster models , which use the same clusters for both words <CIT> '
'1 Introduction Recent trends in machine translation illustrate that highly accurate word and phrase translations can be learned automatically given enough parallel training data <CIT> '
'In the SUMMAC experiments , the Kappa score <CIT> for interannotator agreement was reported to be 038 <OTH> '
'It has been shown by Shapiro and Stephens <OTH> and <CIT> -LRB- 1997 , Sec '
'Alternatively , one can train them with respect to the final translation quality measured by an error criterion <CIT> '
'First , we need to determine whether or not the positive effect of SVD feature selection is preserved in more complex feature spaces such as syntactic feature spaces as those used in <CIT> '
'Och showed thatsystemperformanceisbestwhenparametersare optimizedusingthesameobjectivefunctionthatwill be used for evaluation ; BLEU <OTH> remains common for both purposes and is often retained for parameter optimization even when alternative evaluation measures are used , eg , <CIT> '
'2 Phrasal Inversion Transduction Grammar We use a phrasal extension of Inversion Transduction Grammar <CIT> as the generative framework '
'6 Training Similar to most state-of-the-art phrase-based SMT systems , we use the SRI toolkit <OTH> for language model training and Giza + + toolkit <CIT> for word alignment '
'We chose to train maximum entropy models <CIT> '
'Because of this , <CIT> and Zens and Ney -LRB- 2003 -RRB- introduced a normal form ITG which avoids this over-counting '
'The second approximation proposed in <CIT> takes into consideration the fact that , after each decision is made , all the preceding latent variables should have their means i updated '
'It is also possible to train statistical models using unlabeled data with the expectation maximization algorithm <CIT> '
'44 Related Work <OTH> implemented an MEMM model for supertagging which is analogous to the POS tagging model of <CIT> '
'24 Maximum Entropy Classifier Maximum Entropy Models <CIT> seek to maximize the conditional probability of classes , given certain observations -LRB- features -RRB- '
'3 Maximum Entropy Taggers The taggers are based on Maximum Entropy tagging methods <CIT> , and can all be trained on new annotated data , using either GIS or BFGS training code '
'The fact that the error rate more than doubles when the seeds in <CIT> experiments are reduced from a sense ''s best collocations to just one word per sense suggests that the error rate would increase further if no seeds were provided '
'Any linguistic annotation required during the extraction process , therefore , is produced through automatic means , and it is only for reasons of accessibility and comparability with other research that we choose to work over the Wall Street Journal section of the Penn Treebank <CIT> '
'-LRB- General grammars with infinite numbers of nonterminals were studied by <CIT> -RRB- '
'Such tasks will require an extension of the current framework of <CIT> beyond evidence from the direct cooccurrence of target word pairs '
'51 Baseline System We trained Moses on all Spanish-English Europarl sentences up to length 20 -LRB- 177k sentences -RRB- using GIZA + + Model 4 word alignments and the growdiag-final-and combination heuristic <CIT> , which performed better than any alternative combination heuristic13 The baseline estimates -LRB- Heuristic -RRB- come fromextractingphrasesuptolength7fromtheword alignment '
'A Head Percolation Table has previously been used in several statistical parsers <CIT> to find heads of phrases '
'Numerous approaches have been explored for exploiting situations where some amount of annotated data is available and a much larger amount of data exists unannotated , eg Marialdo ''s HMM part-of-speech tagger training <OTH> , Charniak ''s parser retraining experiment <OTH> , <CIT> and Nigam et al ''s -LRB- 1998 -RRB- topic classifier learned in part from unlabelled documents '
'2 Inversion transduction grammars Inversion transduction grammars (ITGs) (Wu, 1997) are a notational variant of binary syntax-directed translation schemas (Aho and Ullman, 1972) and are usually presented with a normal form: A  [BC] A  BC A  e|f A  e| A  |f where A,B,C  N and e,f  T. The first production rule, intuitively, says that the subtree [[]B[]C]A in the source language translates into 62 a subtree [[]B[]C]A, whereas the second production rule inverts the order in the target language, i.e. [[]C[]B]A. The universal recognition problem of ITGs can be solved in time O(n6|G|) by a CYKstyle parsing algorithm with two charts.'
'Tuning -LRB- learning the values discussed in section 41 -RRB- was done using minimum error rate training <CIT> '
'In NLP community , it has been shown that having more data results in better performance <CIT> '
'<CIT> use a syntaxbased distance in an HMM word alignment model to favor syntax-friendly alignments '
'<OTH> , these models have non-uniform linguistically motivated structure , at present coded by hand '
'Additional evidence for this distinction is given in Pustejovsky and Anick <OTH> and Briscoe et al '
'22 Perceptron algorithm Our discriminative n-gram model training approach uses the perceptron algorithm , as presented in <OTH> , which follows the general approach presented in <CIT> '
'As expected , Malt and MST have very similar accuracy for short sentences but Malt degrades more rapidly with increasing sentence length because of error propagation <CIT> '
'<CIT> explores whether lexical reordering or the phrase discontiguity inherent in hierarchical rules explains improvements over phrase-based systems '
'Clearly the present research task is quite considerably harder than the parsing and tagging tasks undertaken in <OTH> , which would seem to be the closest work to ours , and any comparison between this work and ours must be approached with extreme caution '
'3 A Categorization of Block Styles In <CIT> , multi-word cepts -LRB- which are realized in our block concept -RRB- are discussed and the authors state that when a target sequence is sufficiently different from a word by word translation , only then should the target sequence should be promoted to a cept '
'5http : \/ \/ wwwstatmtorg\/wmt08 185 the BLEU score <CIT> , and tested on test2008 '
'6 Experiment 61 Setup The experiments we report were done on the Penn Treebank WSJ Corpus <CIT> '
'INTRODUCTION Word associations have been studied for some time in the fields of psycholinguistics -LRB- by testing human subjects on words -RRB- , linguistics -LRB- where meaning is often based on how words co-occur with each other -RRB- , and more recently , by researchers in natural language processing <CIT> using statistical measures to identify sets of associated words for use in various natural language processing tasks '
'In a next step , chunk information was added by a rule-based language-independent chunker <CIT> that contains distituency rules , which implies that chunk boundaries are added between two PoS codes that can not occur in the same constituent '
'A few exceptions are the hierarchical -LRB- possibly syntaxbased -RRB- transduction models <CIT> and the string transduction models <OTH> '
'Yet , the very nature of these alignments , as defined in the IBM modeling approach <CIT> , lead to descriptions of the correspondences between sourcelanguage -LRB- SL -RRB- and target-language -LRB- TL -RRB- words of a translation that are often unsatisfactory , at least from a human perspective '
'Previous attempts have used , for instance , the similarities between case frames <OTH> , anchor words <CIT> , and a web-based method <OTH> '
'In order to calculate a global score or probability for a transition sequence , two systems used a Markov chain approach <OTH> '
'We measured stability -LRB- the degree to which the same annotator will produce an annotation after 6 weeks -RRB- and reproducibility -LRB- the degree to which two unrelated annotators will produce the same annotation -RRB- , using the Kappa coefficient K <CIT> , which controls agreement P -LRB- A -RRB- for chance agreement P -LRB- E -RRB- : K = PA -RRB- - P -LRB- E -RRB- 1-P -LRB- Z -RRB- Kappa is 0 for if agreement is only as would be expected by chance annotation following the same distribution as the observed distribution , and 1 for perfect agreement '
'2.2.1 The evaluator The evaluator is a function p(t[t'', s) which assigns to each target-text unit t an estimate of its probability given a source text s and the tokens t'' which precede t in the current translation of s. Our approach to modeling this distribution is based to a large extent on that of the IBM group (Brown et al. , 1993), but it diflhrs in one significant aspect: whereas the IBM model involves a ''noisy channel'' decomposition, we use a linear combination of separate predictions from a language model p(t[t'') and a translation model p(t[s).'
'First , a non-anaphoric NP classifier identifies definite noun phrases that are existential , using both syntactic rules and our learned existential NP recognizer <CIT> , and removes them from the resolution process '
'The algorithm is slightly different from other online training algorithms <CIT> in that we keep and update oracle translations , which is a set of good translations reachable by a decoder according to a metric , ie BLEU <OTH> '
'Inter-annotator agreement was determined for six pairs of two annotators each , resulting in kappa values -LRB- <CIT> -RRB- ranging from 062 to 082 for the whole database -LRB- Carlson et al '
'Discriminative, context-specific training seems to yield a better set of similar predicates, e.g. the highest-ranked contexts for DSPcooc on the verb join,3 lead 1.42, rejoin 1.39, form 1.34, belong to 1.31, found 1.31, quit 1.29, guide 1.19, induct 1.19, launch (subj) 1.18, work at 1.14 give a better SIMS(join) for Equation (1) than the top similarities returned by (Lin, 1998a): participate 0.164, lead 0.150, return to 0.148, say 0.143, rejoin 0.142, sign 0.142, meet 0.142, include 0.141, leave 0.140, work 0.137 Other features are also weighted intuitively.'
'Then , some manual and automatic symbol splitting methods are presented , which get comparable performance with lexicalized parsers <CIT> '
'<OTH> , Johnson <OTH> -- that conditioning the probabilities of structures on the context within which they appear , for example on the lexical head of a constituent <CIT> , on the label of its parent nonterrninal <OTH> , or , ideally , on both and many other things besides , leads to a much better parsing model and results in higher parsing accuracies '
'The normalization is visualized as a translation problem where messages in the SMS language are to be translated to normal English using a similar phrase-based statistical MT method <CIT> '
'However there has recently been much work drawing connections between the two methods <CIT> ; in this section we review this work '
'To circumvent these computational limitations , various pruning techniques are usually needed , eg , <CIT> '
'In addition to raw inter-tagger agreement , the kappa statistic , which removes from the agreement rate the amount of agreement that is expected by chance <CIT> , was also determined '
'of the works of (Kuplec, Pedersen, and Chen, 1995) and (Brandow, Mltze, .and Ran, 1995), and advances summarmatlon technology by applynag corpus-based statistical NLP teehmques, robust information extraction, and readily avaalable on-hne resources Our prehxmnary experiments with combining different summarization features have been reported, and our current effort to learn to combine these features to produce the best summaries has been described The features derived by these robust NLP techmques were also utihzed m presentmg multiple summary.vtews to the user m a novel way References Advanced Research Projects Agency 1995 Proceed:rigs of S:zth Message Understanding Conference (MUC-6) Morgan Kanfmann Pubhshers Brandow, Ron, Karl Mltze, and Lisa Ran 1995 Automatic condensation of electromc pubhcatlous by sentence selection Information Processing and  Management, 31, forthcoming .Bull, Eric 1993 A Comps-based Approach to Language Learning Ph D thesm, Umverslty of Pennsylvania Church, Kenneth and Patrick Hanks 1990 Word  Aesoclatlon Norrns, Mutual Information, and Lexicography Computational Lmgmstscs, 16(1) Church, Kenneth W 1995 One term or two 9 In Proceedings of the 17th Annual International SIGIR Conference on Research and Development In Informatzon Retrzeral, pages 310-318 Edmundson, H P 1969 New methods m automatic abstracting Journal of the ACM, 16(2) 264-228 Fum, Dando, Glovanm Gmda, and Carlo Tasso 1985 Evalutatmg Importance A step towards text surnmarlzatlon In I3CAI85, pages 840-844IJCAi, AAAI Hahn, Udo 1990 Topic parsing Accounting for text macro structures m full-text analysm In format:on Processing and Management, 26(1)135170 Harman, Donna 1991 How effective is suttixang ~ Journal of the Amerlcan Sot:cry for Informatwn Sc:ence, 42(1) 7-15 Harman, Donna 1996 Overview of the fifth text retrieval conference (tree-5) In TREC-5 Conference Proceedings Jmg, Y and B Croft 1994 An Assoc:atwn Thesaurns for Informatzon Retrseval Umass Techmcal Report 94-I7 Center for Intelligent Information Retrieval, University of Massachusetts Johnson, F C, C D Prate, W J Black, and A P Neal 1993.'
'The dataset is available only in English and has been widely used in previous semantic relatedness evaluations -LRB- eg , <CIT> -RRB- '
'For practical reasons , the maximum size of a token was set at three for Chinese , andfor forKorean2 Minimum error rate training <OTH> was run on each system afterwardsand BLEU score <CIT> was calculated on the test sets '
'Assuming that the parameters P -LRB- etk fsk -RRB- are known , the most likely alignment is computed by a simple dynamic-programming algorithm1 Instead of using an Expectation-Maximization algorithm to estimate these parameters , as commonly done when performing word alignment <CIT> , we directly compute these parameters by relying on the information contained within the chunks '
'287 System Train + base Test + base 1 Baseline 8789 8789 2 Contrastive 8870 082 8845 056 -LRB- 5 trials\/fold -RRB- 3 Contrastive 8882 093 8855 066 -LRB- greedy selection -RRB- Table 1 : Average F1 of 7-way cross-validation To generate the alignments , we used Model 4 <OTH> , as implemented in GIZA + + <OTH> '
'While the need for annotation by multiple raters has been well established in NLP tasks <CIT> , most previous work in error detection has surprisingly relied on only one rater to either create an annotated corpus of learner errors , or to check the systems output '
'The model scaling factors M1 are optimized with respect to the BLEU score as described in <CIT> '
'We implemented an N-gram indexer\/estimator using MPI inspired by the MapReduce implementation of N-gram language model indexing\/estimation pipeline <CIT> '
'The idea of bidirectional parsing is related to the bidirectional sequential classification method described in <CIT> '
'There bas recently been work in the detection of semantically related nouns via , for example , shared argument structures <CIT> , and shared dictionary definition context <OTH> '
'It is believed that improvement can be achieved by training the generative model based on a discriminative optimization criteria <CIT> in which the training procedure is designed to maximize the conditional probability of the parses given the sentences in the training corpus '
'In this paper , we used CTB 50 <OTH> as our main corpus , defined the training , development and test sets according to <CIT> , and designed our experiments to explore the impact of the training corpus size on our approach '
'Expectation Evaluation is the soul of parameter estimation <CIT> , <OTH> '
'The samplers that Goldwater and Griffiths <OTH> and <CIT> <OTH> describe are pointwise collapsed Gibbs samplers '
'Recently , methods from nonparametric Bayesian statistics have been gaining popularity as a way to approach unsupervised learning for a variety of tasks , including language modeling , word and morpheme segmentation , parsing , and machine translation <CIT> '
'The bigram translation probability t2 -LRB- f f , e -RRB- specifies the likelihood that target word f is to follow f in a phrase generated by source word e 170 21 Properties of the Model and Prior Work The formulation of the WtoP alignment model was motivated by both the HMM word alignment model <OTH> and IBM Model-4 with the goal of building on the strengths of each '
'The automatic alignments were extracted by appending the manually aligned sentences on to the respective Europarl v3 corpora and aligning them using GIZA + + <OTH> and the growfinal-diag algorithm <CIT> '
'This scoring function has been successfully applied to resolve ambiguity problems in an English-to-Chinese machine translation system -LRB- BehaviorTran -RRB- <OTH> and a spoken language processing system <OTH> '
'This paper proposes a method for building a bilingual lexicon through a pivot language by using phrase-based statistical machine translation -LRB- SMT -RRB- <CIT> '
'Networks (Toutanova et al., 2003) 97.24 SVM (Gimenez and M`arquez, 2003) 97.05 ME based a bidirectional inference (Tsuruoka and Tsujii, 2005) 97.15 Guided learning for bidirectional sequence classification (Shen et al., 2007) 97.33 AdaBoost.SDF with candidate features (=2,=1,=100, W-dist) 97.32 AdaBoost.SDF with candidate features (=2,=10,=10, F-dist) 97.32 SVM with candidate features (C=0.1, d=2) 97.32 Text Chunking F=1 Regularized Winnow + full parser output (Zhang et al., 2001) 94.17 SVM-voting (Kudo and Matsumoto, 2001) 93.91 ASO + unlabeled data (Ando and Zhang, 2005) 94.39 CRF+Reranking(Kudo et al., 2005) 94.12 ME based a bidirectional inference (Tsuruoka and Tsujii, 2005) 93.70 LaSo (Approximate Large Margin Update) (Daume III and Marcu, 2005) 94.4 HySOL (Suzuki et al., 2007) 94.36 AdaBoost.SDF with candidate featuers (=2,=1,=, W-dist) 94.32 AdaBoost.SDF with candidate featuers (=2,=10,=10,W-dist) 94.30 SVM with candidate features (C=1, d=2) 94.31 One of the reasons that boosting-based classifiers realize faster classification speed is sparseness of rules.'
'The second uses the decoder to search for the highest-B translation <CIT> , which Arun and Koehn -LRB- 2007 -RRB- call max-B updating '
'<OTH> show that this model is a member of an exponential family with one parameter for each constraint , specifically a model of the form 1 ~ I ~ -LRB- x , ~ -RRB- p -LRB- yl -RRB- = E '' in which z -LRB- x -RRB- = eZ , Y The parameters A1 , , An are Lagrange multipliers that impose the constraints corresponding to the chosen features fl , - , fnThe term Z -LRB- x -RRB- normalizes the probabilities by summing over all possible outcomes y <CIT> et al '
'While this is certainly a daunting task , it is possible that for annotation studies that do not require expert annotators and extensive annotator training , the newly available access to a large pool of inexpensive annotators , such as the Amazon Mechanical Turk scheme <OTH> ,4 or embedding the task in an online game played by volunteers <CIT> could provide some solutions '
'Translation scores are reported using caseinsensitive BLEU <CIT> with a single reference translation '
'We used GIZA + + <CIT> to align approximately 751,000 sentences from the German-English portion of the Europarl corpus <OTH> , in both the German-to-English and English-to-German directions '
'A Greek model was trained on 440,082 aligned sentences of Europarl v3 , tuned with Minimum Error Training <CIT> '
'Therefore , <CIT> defined the translation candidate with the minimum word-error rate as pseudo reference translation '
'In the field of eomputationa1 linguistics , mutual information <OTH> , 2 <CIT> , or a likelihood ratio test -LRB- Dunning , 199a -RRB- are suggested '
'Previous authors have used numerous HMM-based models <OTH> and other types of networks including maximum entropy models <OTH> , conditional Markov models <CIT> , conditional random elds -LRB- CRF -RRB- <OTH> , and cyclic dependency networks <OTH> '
'On the other hand , other authors -LRB- eg , <CIT> -RRB- do use the expression phrase-based models '
'4 Methodology 41 Data In order to be able to compare our results with the results obtained by other researchers , we worked with the same data sets already used by <CIT> for NP and SV detection '
'Alternatively , one can view -LRB- 2 -RRB- as inducing an alignment between terms in the h to the terms in the t , somewhat similar to alignment models in statistical MT <CIT> '
'4 Semantic Class Induction from Wikipedia Wikipedia has recently been used as a knowledge source for various language processing tasks , including taxonomy construction <OTH> , coreference resolution <OTH> , and English NER -LRB- eg , Bunescu and Pasca <OTH> , <CIT> , Kazama and Torisawa -LRB- 2007 -RRB- , Watanabe et al '
'Such a lexicon can be used , eg , to classify individual sentences or phrases as subjective or not , and as bearing positive or negative sentiments <CIT> '
'accuracy Training data <CIT> 66 % unsupervised Pang & Lee -LRB- 2004 -RRB- 8715 % supervised Aue & Gamon -LRB- 2005 -RRB- 914 % supervised SO 7395 % unsupervised SM+SO to increase seed words , then SO 7485 % weakly supervised Table 7 : Classification accuracy on the movie review domain <CIT> achieves 66 % accuracy on the movie review domain using the PMI-IR algorithm to gather association scores from the web '
'In some recent grammar induction and MT work <CIT> it has been shown that even a small amount of knowledge about a language , in the form of grammar fragments , treelets or prototypes , can go a long way in helping with the induction of a grammar from raw text or with alignment of parallel corpora '
'The results have demonstrated the existence of priming effects in corpus data : they occur for specific syntactic constructions <OTH> , consistent with the experimental literature , but also generalize to syntactic rules across the board , which repeated more often than expected by chance <CIT> '
'Treebanks have been used within the field of natural language processing as a source of training data for statistical part og speech taggers <OTH> and for statistical parsers <CIT> '
'The next two methods are heuristic -LRB- H -RRB- in <CIT> and grow-diagonal -LRB- GD -RRB- proposed in <OTH> '
'number of words in target string These statistics are combined into a log-linear model whose parameters are adjusted by minimum error rate training <CIT> '
'It has also obtained competitive scores on general GR evaluation corpora <CIT> '
'The following four metrics were used speci cally in this study : BLEU <CIT> : A weighted geometric mean of the n-gram matches between test and reference sentences multiplied by a brevity penalty that penalizes short translation sentences '
'In addition , we use the measure from Resnik <OTH> , which is computed using an intrinsic information content measure relying on the hierarchical structure of the category tree <OTH> '
'<CIT> -RRB- , training on a corpus of one type and then applying the tagger to a corpus of a different type usually results in a tagger with low accuracy <OTH> '
'Rules have the form X e , f , where e and f are phrases containing terminal symbols -LRB- words -RRB- and possibly co-indexed instances of the nonterminal symbol X2 Associated with each rule is a set of translation model features , i -LRB- f , e -RRB- ; for example , one intuitively natural feature of a rule is the phrase translation -LRB- log - -RRB- probability -LRB- f , e -RRB- = log p -LRB- e f -RRB- , directly analogous to the corresponding feature in non-hierarchical phrase-based models like Pharaoh <CIT> '
'This paper continues a line of research on online discriminative training <CIT> , extending that of Watanabe et al '
'We used the preprocessed data to train the phrase-based translation model by using GIZA + + <CIT> and the Pharaoh tool kit <OTH> '
'We can find some other machine-learning approaches that use more sophisticated LMs, such as Decision Trees (Mhrquez and Rodrfguez, 1998)(Magerman, 1996), memory-based approaclms to learn special decision trees (Daelemans et al. , 1996), maximmn entropy approaches that combine statistical information from different sources (Ratnaparkhi, 1996), finite state autonmt2 inferred using Grammatical Inference (Pla and Prieto, 1998), etc. The comparison among different al)t)roaches is dif ficult due to the nmltiple factors that can be eonsid614 ered: tile languagK, tile mmfl)er and tyt)e of the tags, the size of tilt vocabulary, thK ambiguity, the diiticulty of the test ski, Kte.'
'The principle of our approach is more similar to <CIT> '
'We adopt the similarity score proposed by <CIT> as the distributional similarity score and use 50 nearest neighbors in line with McCarthy et al For the random baseline we select one word sense at random for each word token and average the precision over 100 trials '
'3 Implementation 31 Feature Structure To implement the twin model , we adopt the log linear or maximum entropy -LRB- MaxEnt -RRB- model <CIT> for its flexibility of combining diverse sources of information '
'In <CIT> non-terminals in a standard PCFG model are augmented with latent variables '
'The tag propagation\/elimination scheme is adopted from <CIT> '
'The implementation includes path-length <OTH> , information-content <OTH> and text-overlap <CIT> measures , as described in Strube & Ponzetto -LRB- 2006 -RRB- '
'But it makes obvious that <OTH> were tackling a problem different from <OTH> given the fact that their baseline was at 59 % guessing noun attachment -LRB- rather than 67 % in the Hindle and Rooth experiments -RRB- 3 Of course , the baseline is not a direct indicator of the difficulty of the disambiguation task '
'For a detailed introduction to IBM translation models , please see <CIT> '
'By using only the bidirectional word alignment links , one can implement a very robust such filter , as the bidirectional links are generally reliable , even though they have low recall for overall translational correspondences <CIT> '
'Agglomerative clustering <CIT> iteratively merges the most similar clusters into bigger clusters , which need to be labeled '
'After parsing the corpus <CIT> , we artificially introduced verb form errors into these sentences , and observed the resulting disturbances to the parse trees '
'3 Statistical Word Alignment According to the IBM models <CIT> , the statistical word alignment model can be generally represented as in equation -LRB- 1 -RRB- '
'1 Introduction In a classical statistical machine translation , a foreign language sentence f J1 = f1 , f2 , fJ is translated into another language , ie English , eI1 = e1 , e2 , , eI by seeking a maximum likely solution of : eI1 = argmax eI1 Pr -LRB- eI1 f J1 -RRB- -LRB- 1 -RRB- = argmax eI1 Pr -LRB- f J1 eI1 -RRB- Pr -LRB- eI1 -RRB- -LRB- 2 -RRB- The source channel approach in Equation 2 independently decomposes translation knowledge into a translation model and a language model , respectively <CIT> '
'This is an unsuitable measure for inferring reliability , and it was the use of this measure that prompted <CIT> to recommend chance-corrected measures '
'Once the set of features functions are selected , algorithm such as improved iterative scaling <CIT> or sequential conditional generalized iterative scaling <OTH> can be used to find the optimal parameter values of fkg and fig '
'In the similaritybased approaches <CIT> , rather than a class , each word is modelled by its own set of similar words derived from statistical data collected from corpora '
'Estimation of the parameters has been described elsewhere <CIT> '
'According to one account <OTH> the majority of errors arise because of the statistical filtering process , which is reported to be particularly unreliable for low frequency SCFs <OTH> '
'Sentiment summarization has been well studied in the past decade <CIT> '
'This is also true for reranking and discriminative training , where the k-best list of candidates serves as an approximation of the full set <CIT> '
'The tree-based reranker includes the features described in <CIT> as well as features based on non-projective edge attributes explored in <OTH> '
'The chunking classification was made by <CIT> based on the parsing information in the WSJ corpus '
'A similar approach was taken in <OTH> where an unknown word was guessed given the probabilities for an unknown word to be of a particular POS , its capitalization feature and its ending '
'In retrospect , however , there are perhaps even greater similarities to that of <CIT> '
'Decoding weights are optimized using Ochs algorithm <CIT> to set weights for the four components of the loglinear model : language model , phrase translation model , distortion model , and word-length feature '
'We use the GIZA + + implementation of IBM Model 4 <CIT> coupled with the phrase extraction heuristics of Koehn et al '
'While simple statistical alignment models like IBM-1 <CIT> and the symmetric alignment approach by Hiemstra -LRB- 1996 -RRB- treat sentences as unstructured bags of words , the more sophisticated IBM-models by Brown et al '
'Two block sets are derived for each of the training sets using a phrase-pair selection algorithm similar to <CIT> '
'The different approaches <OTH> vary largely according to the methods used and the number of SCFS being extracted '
'<OTH> and <CIT> -LRB- and see below for discussions -RRB- , so in this paper we focus on the less studied , but equally important problem of annotationstyle adaptation '
'Syntactic context information is used <CIT> to compute term similarities , based on which similar words to a particular word can directly be returned '
'To obtain their corresponding weights , we adapted the minimum-error-rate training algorithm <CIT> to train the outside-layer model '
'For a full description of the algorithm , see <CIT> '
'Following <CIT> , we adopt the view that the syntactic structure of sentences paraphrasing some sentence s should be inspired by the structure of s Because dependency syntax is still only a crude approximation to semantic structure , we augment the model with a lexical semantics component , based on WordNet <OTH> , that models how words are probabilistically altered in generating a paraphrase '
'We consider the outputs of the top 3 allwords WSD systems that participated in Senseval-3 : Gambl <OTH> , SenseLearner <OTH> , and KOC University <OTH> '
'33 System evaluation Since both the system translations and the reference translations are available for the tuning 43 set , we first compare each output to the reference translation using BLEU <OTH> and METEOR <CIT> and a combined scoring scheme provided by the ULC toolkit <OTH> '
'Specifically , we will consider a system which was developed for the ACE -LRB- Automatic Content Extraction -RRB- task 3 and includes the following stages : name structure parsing , coreference , semantic relation extraction and event extraction <CIT> '
'The preprocessed training data was filtered for length and aligned using the GIZA + + implementation of IBM Model 4 <CIT> in both directions and symmetrized using the grow-diag-final-and heuristic '
'This gives the translation model more information about the structure of the source language , and further constrains the reorderings to match not just a possible bracketing as in <CIT> , but the specific bracketing of the parse tree provided '
'To support a more rigorous analysis , however , wc have followed <CIT> of using the K coettMcnt <OTH> as a measure of coder agreement '
'Co-selection measures include precision and recall of co-selected sentences , relative utility <OTH> , and Kappa <CIT> '
'Goodman <OTH> and Johnson <OTH> both suggest this strategy '
'61 Hiero Results Using the MT 2002 test set , we ran the minimumerror rate training -LRB- MERT -RRB- <CIT> with the decoder to tune the weights for each feature '
'An alternative representation for baseNPs has been put forward by <CIT> '
'We compare those algorithms to generalized iterative scaling -LRB- GIS -RRB- <OTH> , non-preconditioned CG , and voted perceptron training <CIT> '
'The other main difference is the apparently nonlocal nature of the problem , which motivates our choice of a Maximum Entropy -LRB- ME -RRB- model for the tagging task <CIT> '
'1 Introduction Statistical machine translation <CIT> has seen many improvements in recent years , most notably the transition from wordto phrase-based models <OTH> '
'<CIT> To reduce the inference time , following <OTH> , we collapsed the 45 different POS labels contained in the original data '
'For example , in phrase-based SMT systems <CIT> , distortion model is used , in which reordering probabilities depend on relative positions of target side phrases between adjacent blocks '
'The word alignment models implemented in GIZA + + , the so-called IBM <CIT> and HMM alignment models <OTH> are typical implementation of the EM algorithm <OTH> '
'c2009 Association for Computational Linguistics Improving Mid-Range Reordering using Templates of Factors Hieu Hoang School of Informatics University of Edinburgh hhoang @ smsedacuk Philipp Koehn School of Informatics University of Edinburgh pkoehn @ infedacuk Abstract We extend the factored translation model <CIT> to allow translations of longer phrases composed of factors such as POS and morphological tags to act as templates for the selection and reordering of surface phrase translation '
'<CIT> s Inversion Transduction Grammar , as well as tree-transformation models of translation such as Yamada and Knight <OTH> , Galley et al '
'Finally , recent work has explored learning to map sentences to lambda-calculus meaning representations <CIT> '
'However , we do not rely on linguistic resources <OTH> or on search engines <CIT> to determine the semantic orientation , but rather rely on econometrics for this task '
'We perform term disambiguation on each document using an entity extractor <CIT> '
'In recent years , reranking techniques have been successfully applied to the so-called history-based models <OTH> , especially to parsing <OTH> '
'We measured inter-annotator agreement with the Kappa statistic <CIT> using the 1,391 items that two annotators scored in common '
'This setup provides an elegant solution to the fairly complex task of integrating multiple MT results that may differ in word order using only standard software modules , in particular GIZA + + <CIT> for the identification of building blocks and Moses for the recombination , but the authors were not able to observe improvements in 1see http://wwwstatmtorg/moses/ terms of BLEU score '
'Our baseline uses Giza + + alignments <OTH> symmetrized with the grow-diag-final-and heuristic <CIT> '
'2 Machine Translation using Inversion Transduction Grammar The Inversion Transduction Grammar -LRB- ITG -RRB- of <CIT> is a type of context-free grammar -LRB- CFG -RRB- for generating two languages synchronously '
'All of the features of the ATR\/Lancaster Treebank that are described below represent a radical departure from extant large-scale <CIT> treebanks '
'C0 , C , q 1 , q xq xq1 xq1 xq xr xr +1 Table 6 : Lexicalized Features for Joint Models aging of the weights suggested by <CIT> '
'31 Experiments The model described in section 2 has been tested on the Brown corpus <OTH> , tagged with the 45 tags of the Penn treebank tagset <CIT> , which constitute the initial tagset T0 '
'In order to avoid this problem we implemented a simple bootstrapping procedure in which a seed data set of 100 instances of each of the eight categories was hand tagged and used to generate a decision list classifier using the C45 algorithm <OTH> with the word frequency and topic signature features described below '
'A null Assuming that one SMS word is mapped exactly to one English word in the channel model under an alignment , we need to consider only two types of probabilities : the alignment probabilities denoted by Pm and the lexicon mapping probabilities denoted by <CIT> '
'Computational linguists have demonstrated that a words meaning is captured to some extent by the distribution of words and phrases with which it commonly co-occurs <CIT> '
'In our experiments using BLEU <CIT> as the metric , the interpolated synthetic model achieves a relative improvement of 117 % over the best RBMT system that is used to produce the synthetic bilingual corpora '
'We suggest two ways to do it : a version of -LRB- -LRB- obbs et al ''s <OTH> Generation as Abduction ; and the Interactive Defaults strategy introduced by aoshi et al <OTH> '
'of Linguistics University of Potsdam kuhn @ linguni-potsdamde Abstract The empirical adequacy of synchronous context-free grammars of rank two -LRB- 2-SCFGs -RRB- <OTH> , used in syntaxbased machine translation systems such as <CIT> , Zhang et al '
'42 Word alignment We have used IBM models proposed by Brown <OTH> for word aligning the parallel corpus '
'In Experiment 1 , we applied three standard parsing models from the literature to Negra : an unlexicalized PCFG model -LRB- the baseline -RRB- , Carroll and Rooths <OTH> head-lexicalized model , and <CIT> model based on head-head dependencies '
'The IBM models <CIT> benefit from a one-tomany constraint , where each target word has ex105 the tax causes unrest l '' impt cause le malaise Figure 1 : A cohesion constraint violation '
'The first step is to label each node as either a head , complement , or adjunct based on the approaches of Magerman <OTH> and <CIT> '
'<CIT> have build a chunker by applying transformation-based learning to sections of the Penn Treebank '
'Unfortunately , determining the optimal segmentation is challenging , typically requiring extensive experimentation <CIT> '
'Unlike a full blown machine translation task <CIT> , annotators and systems will not be required to translate the whole context but just the target word '
'aoifecahill @ imsuni-stuttgartde and van Genabith <OTH> , which do not rely on handcrafted grammars and thus can easily be ported to new languages '
'We adopted log-likelihood ratio <OTH> , which gave the best pertbrmance among crude non-iterative methods in our test experiments 6 '
'Translation quality is reported using case-insensitive BLEU <CIT> '
'These heuristics define a phrase pair to consist of a source and target ngrams of a word-aligned source-target sentence pair such that if one end of an alignment is in the one ngram , the other end is in the other ngram -LRB- and there is at least one such alignment -RRB- <CIT> '
'Zens and Ney <OTH> compute the viterbi alignments for German-English and French-English sentences pairs using IBM Model 5 , and then measure how many of the resulting alignments fall within the hard constraints of both <CIT> and Berger et al '
'There are many choices for modeling co-occurrence data <OTH> '
'ProAlign models P -LRB- A E , F -RRB- directly , using a different decomposition of terms than the model used by IBM <CIT> '
'This test set was tagged using MXPOST <CIT> which was itself trained on Switchboard '
'Experiments are presented in table 1 , using BLEU <OTH> and METEOR5 <CIT> , and we also show the length ratio -LRB- ratio of hypothesized tokens to reference tokens -RRB- '
'We use MXPOST tagger <OTH> for POS tagging , Charniak parser <OTH> for extracting syntactic relations , SVMlight1 for SVM classifier and David Bleis version of LDA2 for LDA training and inference '
'CIT -RRB- '
'Domain adaptation deals with these feature distribution changes <CIT> '
'<CIT> work on classiflcation of reviews is perhaps the closest to ours2 He applied a speciflc unsupervised learning technique based on the mutual information between document phrases and the words excellent '' and poor '' , where the mutual information is computed using statistics gathered by a search engine '
'These distributions are modeled using a maximum entropy formulation <CIT> , using training data which consists of human judgments of question answer pairs '
'SMT has evolved from the original word-based approach <OTH> into phrase-based approaches <OTH> and syntax-based approaches <CIT> '
'Given a wordq , its set of featuresFq and feature weightswq -LRB- f -RRB- for f Fq , a common symmetric similarity measure is Lin similarity <CIT> : Lin -LRB- u , v -RRB- = summationtext fFuFv -LRB- wu -LRB- f -RRB- + wv -LRB- f -RRB- -RRB- summationtext fFu wu -LRB- f -RRB- + summationtext fFv wv -LRB- f -RRB- where the weight of each feature is the pointwise mutual information -LRB- pmi -RRB- between the word and the feature : wq -LRB- f -RRB- = log -LRB- Pr -LRB- f q -RRB- Pr -LRB- f -RRB- -RRB- '
'This is seen in that each time we check for the nearest intersection to the current 1-best for some n-best list l, we Algorithm 1 Och (2003)s line search method to find the global minimum in the loss, lscript, when starting at the point w and searching along the direction d using the candidate translations given in the collection of n-best lists L. Input: L, w, d, lscript I {} for l L do for e  l do m{e} e.features d b{e} e.features w end for bestn argmaxel m{e}{b{e} breaks ties} loop bestn+1 = argminel max parenleftBig 0, b{bestn}b{e}m{e}m{bestn} parenrightBig intercept  max parenleftBig 0, b{bestn}b{bestn+1}m{bestn+1}m{bestn} parenrightBig if intercept > 0 then add(I, intercept) else break end if end loop end for add(I, max(I)+2epsilon1) ibest = argminiI evallscript(L,w+(iepsilon1)d) return w+(ibest epsilon1)d must calculate its intersection with all other candidate translations that have yet to be selected as the 1-best.'
'We generated for each phrase pair in the translation table 5 features : phrase translation probability -LRB- both directions -RRB- , lexical weighting <CIT> -LRB- both directions -RRB- and phrase penalty -LRB- constant value -RRB- '
'Formally , transformational rules ri presented in <CIT> are equivalent to 1-state xRs transducers mapping a given pattern -LRB- subtree to match in pi -RRB- to a right hand side string '
'Mincuts have been used 4As of this writing , WordNet is available for more than 40 world languages -LRB- http://wwwglobalwordnetorg -RRB- Figure 2 : Semi-supervised classification using mincuts in semi-supervised learning for various tasks , including document level sentiment analysis <CIT> '
'The original intention of assignment 2 was that students then use this maxent classifier as a building block of a maxent part-of-speech tagger like that of <CIT> '
'Finally, we show in Section 7.3 that our SCL PoS 124 (a) 100 500 1k 5k 40k75 80 85 90 Results for 561 MEDLINE Test Sentences Number of WSJ Training Sentences Accuracy supervised semiASO SCL (b) Accuracy on 561-sentence test set Words Model All Unknown Ratnaparkhi (1996) 87.2 65.2 supervised 87.9 68.4 semi-ASO 88.4 70.9 SCL 88.9 72.0 (c) Statistical Significance (McNemars) for all words Null Hypothesis p-value semi-ASO vs. super 0.0015 SCL vs. super 2.1 1012 SCL vs. semi-ASO 0.0003 Figure 5: PoS tagging results with no target labeled training data (a) 50 100 200 500 86 88 90 92 94 96 Number of MEDLINE Training Sentences Accuracy Results for 561 MEDLINE Test Sentences 40kSCL 40ksuper 1kSCL 1ksuper nosource (b) 500 target domain training sentences Model Testing Accuracy nosource 94.5 1k-super 94.5 1k-SCL 95.0 40k-super 95.6 40k-SCL 96.1 (c) McNemars Test (500 training sentences) Null Hypothesis p-value 1k-super vs. nosource 0.732 1k-SCL vs. 1k-super 0.0003 40k-super vs. nosource 1.9 1012 40k-SCL vs. 40k-super 6.5 107 Figure 6: PoS tagging results with no target labeled training data tagger improves the performance of a dependency parser on the target domain.'
'<OTH> , <CIT> et al '
'Many researchers <CIT> , have observed consistent gains by using more flexible matching criteria '
'Our approach is based on earlier work on LFG semantic form extraction <OTH> and recent progress in automatically annotating the Penn-II treebank with LFG f-structures <CIT> '
'Following <CIT> , the input to the NP chunker consists of the words in a sentence annotated automatically with part-of-speech -LRB- POS -RRB- tags '
'3 The Framework 31 The Algorithm Our transductive learning algorithm , Algorithm 1 , is inspired by the Yarowsky algorithm <CIT> '
'The features we used are as follows : word posterior probability <OTH> ; 3 , 4-gram target language model ; word length penalty ; Null word length penalty ; Also , we use MERT <CIT> to tune the weights of confusion network '
'The score combination weights are trained by a minimum error rate training procedure similar to <CIT> '
'We use the likelihood ratio for a binomial distribution <CIT> , which tests the hypothesis whether the term occurs independently in texts of biographical nature given a large corpus of biographical and non-biographical texts '
'41 The test environment For our experiments , we used a manually corrected version of the Air Travel Information System -LRB- ATIS -RRB- spoken language corpus <OTH> annotated in the Pennsylvania Treebank <CIT> '
'Following <OTH> , 765 Feature Sets Templates Error % A Ratnaparkhis 305 B A + -LRB- t0 , t1 -RRB- , -LRB- t0 , t1 , t1 -RRB- , -LRB- t0 , t1 , t2 -RRB- 292 C B + -LRB- t0 , t2 -RRB- , -LRB- t0 , t2 -RRB- , -LRB- t0 , t2 , w0 -RRB- , -LRB- t0 , t1 , w0 -RRB- , -LRB- t0 , t1 , w0 -RRB- , -LRB- t0 , t2 , w0 -RRB- , -LRB- t0 , t2 , t1 , w0 -RRB- , -LRB- t0 , t1 , t1 , w0 -RRB- , -LRB- t0 , t1 , t2 , w0 -RRB- 284 D C + -LRB- t0 , w1 , w0 -RRB- , -LRB- t0 , w1 , w0 -RRB- 278 E D + -LRB- t0 , X = prefix or suffix of w0 -RRB- ,4 -LRB- X 9 272 Table 2 : Experiments on the development data with beam width of 3 we cut the PTB into the training , development and test sets as shown in Table 1 '
'We estimated the probabilities P -LRB- c I Pi -RRB- and P -LRB- c -RRB- similarly to Resnik <OTH> by using relative frequencies from the BNC , together with WordNet <OTH> as a source of taxonomic semantic class information '
'Thus , GCNF is a more restrictive normal form than those used by <CIT> and Melamed -LRB- 2003 -RRB- '
'The CDR <OTH> is assigned with access to clinical and cognitive test information , independent of performance on the battery of neuropsychological tests used for this research study , and has been shown to have high expert inter-annotator reliability <OTH> '
'We evaluate its performance on the standard Penn English Treebank -LRB- PTB -RRB- dependency parsing task , ie , train on sections 02-21 and test on section 23 with automatically assigned POS tags -LRB- at 972 % accuracy -RRB- using a tagger similar to <CIT> , and using the headrules of Yamada and Matsumoto -LRB- 2003 -RRB- for conversion into dependency trees '
'The outcomes of CW resemble those of MinCut <OTH> : Dense regions in the graph are grouped into one cluster while sparsely connected regions are separated '
'Bilingual alignments have so far shown that they can play multiple roles in a wide range of linguistic applications , such as computer assisted translation <CIT> , terminology <OTH> lexicography <OTH> , and cross-language information retrieval -LRB- Nie et al , \* This research was funded by the Canadian Department of Foreign Affairs and International Trade -LRB- http://~dfait-maecigcca/ -RRB- , via the Agence de la francophonie -LRB- http : \/ \/ ~ '
'To identify these , we use a word-aligned corpus annotated with parse trees generated by statistical syntactic parsers <CIT> '
'Therefore , we also carried out evaluations using the NIST <OTH> , METEOR <CIT> , WER <OTH> , PER <OTH> and TER <OTH> machine translation evaluation techniques '
'Similar techniques are used in <OTH> for socalled direct translation models instead of those proposed in <OTH> '
'In this case it is possible to perform the correct selection if we used only statistics about the cooccurrences of ` corruption '' with either ` investigator '' or ` researcher '' , without looking for any syntactic relation -LRB- as in <CIT> -RRB- '
'In <CIT> and -LRB- Pereira et al , ! 993 -RRB- , clusters of similar words are evaluated by how well they are able to recover data items that are removed from the input corpus one at a time '
'7 Related Work Much work on sentiment analysis classifies documents by their overall sentiment , for example determining whether a review is positive or negative -LRB- eg , <CIT> -RRB- '
'5http : \/ \/ nlpcsberkeleyedu\/Mainhtml # Parsing 47 Figure 3 : Predicate argument structure timized automatically by assigning latent variables to each nonterminal node and estimating the parameters of the latent variables by the EM algorithm <CIT> '
'<OTH> , it is much higher than the 26 % unknown word rate in the test set for <CIT> English POS tagging experiments '
'Our appoach is based on Maximum Entropy -LRB- MaxEnt henceforth -RRB- technique <CIT> '
'Our approach is based on earlier work on LFG semantic form extraction <OTH> and recent progress in automatically annotating the Penn-II and Penn-III Treebanks with LFG f-structures <CIT> '
'The parse trees on the English side of the bitexts were generated using a parser <OTH> implementing the Collins parsing models <CIT> '
'In this paper , we make a direct comparison of a syntactically unsupervised alignment model , based on <CIT> , with a syntactically supervised model , based on Yamada and Knight -LRB- 2001 -RRB- '
'31 Data and Experimental Setup The data set by <CIT> consists of 2000 movie reviews -LRB- 1000-pos , 1000-neg -RRB- from the IMDb review archive '
'We measure translation performance by the BLEU score <CIT> and Translation Error Rate -LRB- TER -RRB- <OTH> with one reference for each hypothesis '
'<CIT> predict the results of an election by analyzing forums discussing the elections '
'5 Analysis Over the last few years , several automatic metrics for machine translation evaluation have been introduced , largely to reduce the human cost of iterative system evaluation during the development cycle <CIT> '
'Conditional probability , the log-likelihood ratio , and Resnik ''s <OTH> selectional association measure were also significantly correlated with plausibility ratings '
'1153 While much research <CIT> has explored how to reconcile pairwise decisions to form coherent clusters , we simply take the transitive closure of our pairwise decision -LRB- as in Ng and Cardie <OTH> and Bengston and Roth <OTH> -RRB- which can and does cause system errors '
'The problem itself has started to get attention only recently <CIT> '
'In this form , the distinction between our two models is sometimes referred to as joint versus conditional '' <CIT> rather than generative versus discriminative '' <OTH> '
'<CIT> demonstrates a discriminatively trained system for machine translation that has the following characteristics : 1 -RRB- requires a varying update strategy -LRB- local vs bold -RRB- depending on whether the reference sentence is reachable or not , 2 -RRB- uses sentence level BLEU as a criterion for selecting which output to update towards , and 3 -RRB- only trains on limited length -LRB- 5-15 words -RRB- sentences '
'Walker et al -LRB- forthcoming -RRB- and Boguraev and Briscoe <OTH> -RRB- '
'Introduction Verb subcategorizafion probabilities play an important role in both computational linguistic applications <CIT> and psycholinguisfic models of language processing <OTH> '
'Standard data sets for machine learning approaches to this task were put forward by <CIT> '
'Its distribution is asymptotic to a 2 distribution and can hence be used as a test statistic <CIT> '
'Ultinmtely , however , it seems that a more complex ai -RRB- t -RRB- roach incorporating back-off and smoothing is necessary ill order to achieve the parsing accuracy achieved by Charniak <OTH> and <CIT> <OTH> '
'Instead researchers condition parsing decisions on many other features , such as parent phrase-marker , and , famously , the lexical-head of the phrase <CIT> -LRB- and others -RRB- '
'Use of probability estimates is not a serious limitation of this approach because in practice candidates are normally provided by some probabilistic model and its probability estimates are used as additional features in the reranker <CIT> '
'Related Works Generally speaking , approaches to MWE extraction proposed so far can be divided into three categories : a -RRB- statistical approaches based on frequency and co-occurrence affinity , b -RRB- knowledgebased or symbolic approaches using parsers , lexicons and language filters , and c -RRB- hybrid approaches combining different methods <CIT> '
'In statistical computational linguistics , maximum conditional likelihood estimators have mostly been used with general exponential or maximum entropy models because standard maximum likelihood estimation is usually computationally intractable <CIT> '
'<CIT> observed that sense division in MRD is frequently too free for the purpose of WSD '
'Some researchers <CIT> classify terms by similarities based on their distributional syntactic patterns '
'Discriminative parsing has been investigated before , such as in Johnson <OTH> , Clark and Curran <OTH> , Henderson <OTH> , <CIT> , Turian et al '
'For example , <CIT> shows how to train a log-linear translation model not by maximizing the likelihood of training data , but maximizing the BLEU score -LRB- among other metrics -RRB- of the model on 53 the data '
'3 Related work Word collocation Various collocation metrics have been proposed , including mean and variance <OTH> , the t-test <OTH> , the chi-square test , pointwise mutual information -LRB- MI -RRB- <OTH> , and binomial loglikelihood ratio test -LRB- BLRT -RRB- <CIT> '
'This may stem from the differences between the two models '' feature templates , thresholds , and approximations of the expected values for the features , as discussed in the beginning of the section , or may just reflect differences in the choice of training and test sets -LRB- which are not precisely specified in <CIT> -RRB- '
'1 Introduction Most -LRB- if not all -RRB- statistical machine translation systems employ a word-based alignment model <CIT> , which treats words in a sentence as independent entities and ignores the structural relationship among them '
'len : median length of sequences of co-specifying referring expressions with Cohen ''s n <CIT> '
'While close attention has been paid to multi-document summarization technologies <CIT> , the inherent properties of humanwritten multi-document summaries have not yet been quantified '
'Similar ideas were explored in <CIT> '
'1 Introduction In the first SMT systems <CIT> , word alignment was introduced as a hidden variable of the translation model '
'SMT has evolved from the original word-based approach <OTH> into phrase-based approaches <CIT> and syntax-based approaches <OTH> '
'<CIT> cites the convention from the domain of content analysis indicating that 67 K K -LRB- 8 indicates marginal agreement , while K -RRB- 8 is an indication of good agreement '
'The synchronous grammar rules are extracted from word aligned sentence pairs where the target sentence is annotated with a syntactic parse <CIT> '
'Since <OTH> , numerous works have used patterns for discovery and identification of instances of semantic relationships -LRB- eg , <CIT> -RRB- '
'31 Selecting Coreference Systems A learning-based coreference system can be defined by four elements : the learning algorithm used to train the coreference classifier , the method of creating training instances for the learner , the feature set 2Examples of such scoring functions include the DempsterShafer rule -LRB- see Kehler <OTH> and <CIT> -RRB- and its variants -LRB- see Harabagiu et al '
'However , after several advances in tasks such as automatic tagging of text with high level semantics such as parts-of-speech <CIT> , named-entities <OTH> , sentence-parsing <OTH> , etc , there is increasing hope that one could leverage this information into IR techniques '
'Instead , we follow a simplified form of previous work on biography creation , where a classifier is trained to distinguish biographical text <CIT> '
'<CIT> , mention about substrings of collocations '
'In <CIT> , a maximum entropy tagger is presented '
'Identifying subjectivity helps separate opinions from fact , which may be useful in question answering , summarization , etc Sentiment detection is the task of determining positive or negative sentiment of words <OTH> , phrases and sentences <OTH> , or documents <CIT> '
'CIT -RRB- '
'We selected four binary NLP datasets for evaluation : 20 Newsgroups1 and Reuters <OTH> -LRB- used by Tong and Koller -RRB- and sentiment classification <CIT> and spam <OTH> '
'We set the feature weights by optimizing the Bleu score directly using minimum error rate training <CIT> on the development set '
'Such measures as mutual information <OTH> , latent semantic analysis <OTH> , log-likelihood ratio <CIT> have been proposed to evaluate word semantic similarity based on the co-occurrence information on a large corpus '
'33 Methods We parsed the English side of each bilingual bitext and both sides of each English\/English bitext using an off-the-shelf syntactic parser <OTH> , which was trained on sections 02-21 of the Penn English Treebank <CIT> '
'To implement this method , we rst use the Stanford Named Entity Recognizer4 <CIT> toidentifythesetofpersonandorganisation entities , E , from each article in the corpus '
'Then , by using evaluations similar to those described in <OTH> and by <CIT> , we show that the best distance-based measures correlate better overall with human association scores than do the best window based configurations -LRB- see Section 4 -RRB- , and that they also serve as better predictors of the strongest human associations -LRB- see Section 5 -RRB- '
'My guess is that the features used in eg , the Collins <OTH> or Charniak <OTH> parsers are probably close to optimal for English Penn Treebank parsing <CIT> , but that other features might improve parsing of other languages or even other English genres '
'Typically , a small set of seed polar phrases are prepared , and new polar phrases are detected based on the strength of co-occurrence with the seeds <CIT> '
'For example , aspects of a digital camera could include picture quality , battery life , size , color , value , etc Finding such aspects is a challenging research problem that has been addressed in a number of ways <CIT> '
'The approach made use of a maximum entropy model <CIT> formulated from frequency information for various combinations of the observed features '
'Instead , we opt to utilize the Stanford NER tagger <CIT> over the sentences in a document and annotate each NP with the NER label assigned to that mention head '
'The simple model 1 <CIT> for the translation of a SL sentence d = dldt in a TL sentence e = el em assumes that every TL word is generated independently as a mixture of the SL words : m l P -LRB- e -LRB- d -RRB- , , ~ H ~ t -LRB- ej -LRB- di -RRB- -LRB- 2 -RRB- j = l i = O In the equation above t -LRB- ej -LRB- di -RRB- stands for the probability that ej is generated by di '
'Moreover , as <CIT> mentions , some of the benefits of Model 2 are already captured by inclusion of the distance measure '
'The translation models they presented in various papers between 1988 and 1993 <CIT> are commonly referred to as IBM models 15 , based on the numbering in Brown , Della Pietra , Della Pietra , and Mercer <OTH> '
'To tune all lambda weights above , we perform minimum error rate training <CIT> on the development set described in Section 7 '
'<CIT> ` Ontological Promiscuity '' , Proceedings of the 23rd Annual Meeting of the Association for Computational Linguistics , Chicago , Illinois , pp '
'Examples of monolingual parallel corpora that have been used are multiple translations of classical French novels into English , and data created for machine translation evaluation methods such as Bleu <CIT> which use multiple reference translations '
'22 Motivation from previous work 221 Parsing In recent years , the success of statistical parsing techniques can be attributed to several factors , such as the increasing size of computing machinery to accommodate larger models , the availability of resources such as the Penn Treebank <CIT> and the success of machine learning techniques for lowerlevel NLP problems , such as part-of-speech tagging <OTH> , and PPattachment <OTH> '
'<OTH> -RRB- and view the POS tags and word identities as two separate sources of information '
'In all of the cited approaches , the Penn Wall Street Journal Treebank <CIT> is used , the availability of whichobviates the standard eort required for treebank traininghandannotating large corpora of specic domains of specic languages with specic parse types '
'Many corpus based methods have been proposed to deal with the sense disambiguation problem when given de nition for each possible sense of a target word or a tagged corpus with the instances of each possible sense , eg , supervised sense disambiguation <OTH> , and semi-supervised sense disambiguation <CIT> '
'Additionally , our approach makes it possible to do inference in just about twice the inference time with a single sequential CRF ; in contrast , approaches like Gibbs Sampling that model the dependencies directly can increase inference time by a factor of 30 <CIT> '
'Construct a parse chart with a CKY parser simultaneously constrained on the foreign string and English tree , similar to the bilingual parsing of <CIT> 1 '
'<CIT> used a different update style based on a convex loss function : = L -LRB- e , e ; et -RRB- max parenleftBig 0 , 1 parenleftBig si -LRB- f t , e -RRB- si -LRB- f t , e -RRB- parenrightBigparenrightBig 768 Table 1 : Experimental results obtained by varying normalized tokens used with surface form '
'This research has focused mostly on the development of statistical parsers trained on large annotated corpora , in particular the Penn Treebank WSJ corpus <CIT> '
'It also contains tools for tuning these models using minimum error rate training <OTH> and evaluating the resulting translations using the BLEU score <CIT> '
'Binarizing the grammars <CIT> further increases the size of these sets , due to the introduction of virtual nonterminals '
'We argue that linguistic knowledge could not only improve results <CIT> but is essential when extracting collocations from certain languages : this knowledge provides other applications -LRB- or a lexicon user , respectively -RRB- with a ne-grained description of how the extracted collocations are to be used in context '
'From this aligned training corpus , we extract the phrase pairs according to the heuristics in <CIT> '
'31 Conditional Random Field for Alignment Our conditional random field -LRB- CRF -RRB- for alignment has a graphical model structure that resembles that of IBM Model 1 <CIT> '
'As discussed in (Och, 2003), the direct translation model represents the probability of target sentence English e = e1eI being the translation for a source sentence French f = f1 fJ through an exponential, or log-linear model p(e|f) = exp( summationtextm k=1 k  hk(e,f))summationtext eprimeE exp( summationtextm k=1 k  hk(eprime,f)) (1) where e is a single candidate translation for f from the set of all English translations E,  is the parameter vector for the model, and each hk is a feature function of e and f. In practice, we restrict E to the set Gen(f) which is a set of highly likely translations discovered by a decoder (Vogel et al. , 2003).'
'Evaluation Metrics We evaluated the generated translations using three different evaluation metrics : BLEU score <CIT> , mWER -LRB- multi-reference word error rate -RRB- , and mPER -LRB- multi-reference positionindependent word error rate -RRB- <OTH> '
'The study is conducted on both a simple Air Travel Information System -LRB- ATIS -RRB- corpus <OTH> and the more complex Wall Street Journal -LRB- WSJ -RRB- corpus <CIT> '
'This sparse information , however , can be propagated across all data based on distributional similarity <CIT> '
'41 Overview In this work , factored models <CIT> are experimented with three factors : the surface form , the lemma and the part of speech -LRB- POS -RRB- '
'ROUGE-L <CIT> This measure evaluates summaries by longest common subsequence -LRB- LCS -RRB- defined by Equation 4 '
'We also report state-of-the-art results for Hebrew full mor1Another notable work , though within a slightly different framework , is the prototype-driven method proposed by <CIT> , in which the dictionary is replaced with a very small seed of prototypical examples '
'24 Factor Model Decomposition Factored translation models <CIT> extend the phrase-based model by integrating word level factors into the decoding process '
'Given that semantically similar words can be identified automatically on the basis of distributional properties and linguistic cues <CIT> , identifying the semantic orientation of words would allow a system to further refine the retrieved semantic similarity relationships , extracting antonyms '
'The sequential classi cation approach can handle many correlated features , as demonstrated in work on maximum-entropy <CIT> and a variety of other linear classi ers , including winnow <OTH> , AdaBoost <OTH> , and support-vector machines <OTH> '
'lscript1-regularized log-linear models -LRB- lscript1-LLMs -RRB- , on the other hand , provide sparse solutions , in which weights of irrelevant features are exactly zero , by assumingaLaplacianpriorontheweight <CIT> '
'Equation -LRB- 2 -RRB- is rewritten as : -RRB- -LRB- -RRB- -LRB- -RRB- -LRB- -RRB- -LRB- -RRB- -LRB- -RRB- -LRB- -RRB- -LRB- 2211 21 ce colecolcolcolcol rrpcepcep crpcepcepcep = = -LRB- 3 -RRB- It is equal to a word translation model if we take the relation type in the collocations as an element like a word , which is similar to Model 1 in <CIT> '
'One heuristic approach is to adapt the self-training algorithm <CIT> to our model '
'This is referred to as an IOB representation <CIT> '
'We participated in the multilingual track of the CoNLL 2007 shared task <OTH> , and evaluated the system on data sets of 10 languages <CIT> '
'By associating natural language with concepts as they are entered into a knowledge A Model Of Semantic Analysis All of the following discussion is based on a model of semantic analysis similar to that proposed in <CIT> '
'Table 3 compares precision , recall , and F scores for our system with CoNLL-2001 results training on sections 15-18 of the Penn Treebank and testing on section 21 <CIT> '
'Comparatively , <CIT> propose to use the N-gram Overlap metric to capture similarities between sentences and automatically create paraphrase corpora '
'We used the MXPOST tagger <CIT> for POS annotation '
'Techniques for weakening the independence assumptions made by the IBM models 1 and 2 have been proposed in recent work <CIT> '
'A first family of libraries was based on a word alignment A , produced using the Refined method described in <CIT> -LRB- combination of two IBM-Viterbi alignments -RRB- : we call these the A libraries '
'D <CIT> , Noun classification from predicate argument structures , in <OTH> '
'Unlike stochastic approaches to part-of-speech tagging <CIT> , up to now the knowledge found in finite-state taggers has been handcrafted and was not automatically acquired '
'Even if the idea of using Wikipedia links for disambiguation is not novel <CIT> , it is applied for the first time to FrameNet lexical units , considering a frame as a sense definition '
'A ~ value of 08 or greater indicates a high level of reliability among raters , with values between 067 and 08 indicating only moderate agreement <CIT> '
'To accommodate multiple overlapping features on observations , some other approaches view the sequence labeling problem as a sequence of classification problems , including support vector machines -LRB- SVMs -RRB- <OTH> and a variety of other classifiers <CIT> '
'Our POS tagger is essentially the maximum entropy tagger by <CIT> retrained on the CTB-I data '
'Most of this work has so far focused either on post-processing to recover non-local dependencies from context-free parse trees <OTH> , or on incorporating nonlocal dependency information in nonterminal categories in constituency representations <CIT> or in the categories used to label arcs in dependency representations <OTH> '
'These tables were computed from a small fragment of the Canadian Hansards that has been used in a number of other studies : Church <OTH> and Simard et al <OTH> '
'We use 3500 sentences from CoNLL <OTH> as the NER data and section 20-23 of the WSJ <CIT> as the POS\/chunk data -LRB- 8936 sentences -RRB- '
'Training Data Our source for syntactically annotated training data was the Penn Treebank <CIT> '
'<CIT> describes a method -LRB- Latent Relational Analysis -RRB- that extracts subsequence patterns for noun pairs from a large corpus , using query expansion to increase the recall of the search and feature selection and dimensionality reduction to reduce the complexity of the feature space '
'General purpose text annotations , such as part-of-speech tags and noun-phrase bracketing , are costly to obtain but have wide applicability and have been used successfully to develop statistical NLP systems <OTH> '
'To compare different clustering algorithms , results with the standard method of <CIT> -LRB- SRILMs ngram-class -RRB- are also reported '
'In this vein , the CoNLL 2008 shared task sets the challenge of learning jointly both syntactic dependencies -LRB- extracted from the Penn Treebank <CIT> -RRB- and semantic dependencies -LRB- extracted both from PropBank <OTH> c2008 '
'Both Agichtein and Ganti <OTH> and <CIT> train a language model for each database column '
'Related to this issue , we note that the head rules , which were nearly identical to those used in <CIT> , have not been tuned at all to this task '
'The first is a baseline of sorts , our own version of the ` chunking as tagging '' approach introduced by Ramshaw and Marcus <CIT> '
'In most cases , supervised learning methods can perform well <CIT> '
'following our previous work <CIT> '
'Then , we build a classier learned by training data , using a maximum entropy model <CIT> and the features related to spelling variations in Table 3 '
'1 Introduction Sentiment analysis have been widely conducted in several domains such as movie reviews , product reviews , news and blog reviews <CIT> '
'1 Introduction Several approaches including statistical techniques <CIT> , lexical techniques <OTH> and hybrid techniques <OTH> , have been pursued to design schemes for word alignment which aims at establishing links between words of a source language and a target language in a parallel corpus '
'Under a phrase based translation model <CIT> , this distinction is important and will be discussed in more detail '
'32 Mapping Mapping the identified units -LRB- tokens or sequences -RRB- to their equivalents in the other language was achieved by training a new translation model -LRB- IBM 2 -RRB- using the EM algorithm as described in <CIT> '
'Feature-based methods <CIT> use pre-defined feature sets to extract features to train classification models '
'Multiple translations of the same text <CIT> , corresponding articles from multiple news sources <CIT> , and bilingual corpus <OTH> have been utilized '
'In contrast , the C&C tagger , which is based on that of <CIT> , utilizes a wide range of features and a larger contextual window including the previous two tags and the two previous and two following words '
'Subjective phrases are used by <CIT> and others in order to classify reviews or sentences as positive or negative '
'42 String-Based Evaluation We evaluate the output of our generation system against the raw strings of Section 23 using the Simple String Accuracy and BLEU <CIT> evaluation metrics '
'Dagan , Church , and Gale <OTH> expanded on this idea by replacing <CIT> word alignment parameters , which were based on absolute word positions in aligned segments , with a much smaller set of relative offset parameters '
'Denote the global feature vector for segmented sentence y with (y)  Rd, where d is the total number of features in the model; then Score(y) is computed by the dot product of vector (y) and a parameter vector   Rd, where i is the weight for the ith feature: Score(y) = (y) 841 Inputs: training examples (xi,yi) Initialization: set  = 0 Algorithm: for t = 1T, i = 1N calculate zi = argmaxyGEN(xi) (y) if zi negationslash= yi  =  + (yi)(zi) Outputs:  Figure 1: the perceptron learning algorithm, adapted from Collins (2002) The perceptron training algorithm is used to determine the weight values .'
'Thus , an orthogonal line of research can involve inducing classes for words which are more general than single categories , ie , something akin to ambiguity classes <CIT> '
'These tools are important in that the strongest collocational associations often represent different word senses , and thus ` they provide a powerful set of suggestions to the lexicographer for what needs to be accounted for in choosing a set of semantic tags '' <CIT> '
'We see no good reason , however , why such text spans should necessarily be sentences , since the majority of tagging paradigms <CIT> do not attempt to parse an entire sentence and operate only in the local window of two to three tokens '
'The clusters were found automatically by attempting to minimize perplexity <CIT> '
'Many methods have been proposed to deal with this problem , including supervised learning algorithms <OTH> , semi-supervised learning algorithms <CIT> , and unsupervised learning algorithms <OTH> '
'For each language pair , we use two development sets : one for Minimum Error Rate Training <CIT> , and the other for tuning the scale factor for MBR decoding '
'The factored translation model combines features in a log-linear fashion <CIT> '
'The Maximum Entropy Markov Model used in POS-tagging is described in detail in <CIT> and the LMR tagger here uses the same probability model '
'3 Surface Realisation from f-Structures <CIT> present a probabilistic surface generation model for LFG <OTH> '
'We worked with an implementation of the log likelihood ratio -LRB- g-Score -RRB- as proposed by <CIT> and two variants of the t-score , one considering all values -LRB- t-score -RRB- and one where only positive values -LRB- t-score + -RRB- are kept following the results of Curran and Moens <OTH> '
'See <CIT> for details '
'To avoid this problem , we sample from a space of probable alignments , as is done in IBM models 3 and above <CIT> , and weight counts based on the likelihood of each alignment sampled under the current probability model '
'On the British National Corpus -LRB- BNC -RRB- , using <CIT> similarity method , we retrieve the following neighbors for the first and second sense , respectively : 1 '
'We use Viterbi training <CIT> but neighborhood estimation <OTH> or pegging <CIT> could also be used '
'<OTH> -RRB- , and distributional methods -LRB- eg , <CIT> et al '
'<OTH> , and <CIT> et al '
'The starting point is the log likelihood ratio <CIT> '
'Some o1 '' l ; his research has treated the sentenees as unstructured word sequences to be aligned ; this work has primarily involved the acquisition of bilingual lexical correspondences <OTH> , although there has also been a , n attempt to create a full MT system based on such trcat , ment <CIT> '
'There has been considerable use in the NLP community of both WordNet <OTH> and LDOCE <OTH> , but no one has merged the two in order to combine their strengths '
'Classi er Training Set Precision Recall F-Measure Linear 10K pairs 0837 0774 0804 Maximum Entropy 10K pairs 0881 0851 0866 Maximum Entropy 450K pairs 0902 0944 0922 Table 4 : Performance of Alignment Classi er 32 Paraphrase Acquisition Much recent work on automatic paraphrasing <CIT> has used relatively simple statistical techniques to identify text passages that contain the same information from parallel corpora '
'The window size may vary , <CIT> used windows of size 2 and 5 '
'We finally also include as alignment candidates those word pairs that are transliterations of each other to cover rare proper names <CIT> , which is important for language pairs that dont share the same alphabet such as Arabic and English '
'The phrase-based approach developed for statistical machine translation <CIT> is designed to overcome the restrictions on many-tomany mappings in word-based translation models '
'<CIT> used both supervised and unsupervised WSD for correct phonetizitation of words in speech synthesis '
'<CIT> state that AER is derived from F-Measure '
'Resources specifying the relations among lexical items such as WordNet <OTH> and HowNet <OTH> -LRB- among others -RRB- have inspired the work of many researchers in NLP <OTH> '
'Then , h -LRB- s -RRB- h -LRB- s -RRB- + Lmax , s S This epsilon1-admissible heuristic <OTH> bounds our search error by Lmax3 3 Bitext Parsing In bitext parsing , one jointly infers a synchronous phrase structure tree over a sentence ws and its translation wt <CIT> '
'Online discriminative training has already been studied by <CIT> and Liang et al '
'41 Experimental Set-up We used two different corpora : PropBank -LRB- wwwcisupennedu\/ace -RRB- along with PennTree bank 2 <CIT> and FrameNet '
'1 perform the following maximization : eI1 = argmax eI1 fPr -LRB- eI1 -RRB- Pr -LRB- fJ1 jeI1 -RRB- g -LRB- 2 -RRB- This approach is referred to as source-channel approach to statistical MT Sometimes , it is also referred to as the fundamental equation of statistical MT <CIT> '
'<CIT> develop a prototype-driven approach , which requires just a few prototype examples for each POS tag and exploits these labeled words to constrain the labels of their distributionally similar words '
'PairClass is most similar to the algorithm of <CIT> , but it differs in the following ways : PairClass does not use a lexicon to find synonyms for the input word pairs '
'scored with lowercased , tokenized NIST BLEU , and exact match METEOR <CIT> '
'This allows us to compute the conditional probability as follows <CIT> : P -LRB- flh -RRB- YIia -LRB- '' -LRB- n ` l -RRB- z ~ -LRB- h -RRB- -LRB- 2 -RRB- ~ , i -LRB- 3 -RRB- I i The maximum entropy estimation technique guarantees that for every feature gi , the expected value of gi according to the ME model will equal the empirical expectation of gi in the training corpus '
'It has been observed that words close to each other in the source language tend to remain close to each other in the translation <OTH> '
'More specifically , we use a class-based bigram model from <CIT> : -RRB- -LRB- -RRB- -LRB- -RRB- -LRB- 11 = iiiiii ccPcwPwwP -LRB- 3 -RRB- In Equation -LRB- 3 -RRB- , c i is the class of the word w i , which could be a syntactic class or a semantic class '
'2 Background 21 Hybrid Logic Dependency Semantics Hybrid Logic Dependency Semantics <OTH> is an ontologically promiscuous -LRB- <CIT> , 1985 -RRB- framework for representing the propositional content -LRB- or meaning -RRB- of an expression as an ontologically richly sorted , relational structure '
'The core technology of the proposed method , ie , the automatic evaluation of translations , was developed in research aiming at the efficient development of Machine Translation -LRB- MT -RRB- technology <CIT> '
'The weights of feature functions are optimized to maximize the scoring measure <CIT> '
'The problem is due to the assumption of normality in naive frequency based statistics according to <CIT> '
'(2003), Pang and Lee (2004, 2005).'
'They generally perform less well on low-frequency words <CIT> '
'The detailed algorithm can be found in <CIT> '
'Most work in the area of unknown words and tagging deals with predicting part-of-speech information based on word endings and affixation information , as shown by work in <OTH> , <OTH> , <OTH> , and <OTH> '
'Once this is accomplished , a variant of Powells algorithm is used to find weights that optimize BLEU score <CIT> over these hypotheses , compared to reference translations '
'Automatic methods for this often make use of lexicons of words tagged with positive and negative semantic orientation <CIT> '
'4 Semantic Class Induction from Wikipedia Wikipedia has recently been used as a knowledge source for various language processing tasks , including taxonomy construction <OTH> , coreference resolution <OTH> , and English NER -LRB- eg , Bunescu and Pasca <OTH> , Cucerzan <OTH> , <CIT> and Torisawa <OTH> , Watanabe et al '
'Citation texts have also been used to create summaries of single scientific articles in Qazvinian and Radev <OTH> and <CIT> and Zhai <OTH> '
'C c C, p(C]v,r) is just the probability of the disjunction of the concepts in C; that is, = Zp(clv, r) cEC In order to see how p(clv,r) relates to the input data, note that given a concept c, verb v and argument position r, a noun can be generated according to the distribution p(n[c, v, r), where p(nlc, v, r) = 1 nEsyn(c) Now we have a model for the input data: p(n, v, r) = p(v,r)p(niv,r) = p(v,r) p(clv, rlp(ntc, v,r) cecn(n) Note that for c  cn(n), p(nlc, v, r) = O. The association norm (and similar measures such as the mutual information score) have been criticised (Dunning, 1993) because these scores can be greatly over-estimated when frequency counts are low.'
'Our hierarchical system is Hiero <OTH> , modified to construct rules from a small sample of occurrences of each source phrase in training as described by <CIT> '
'So far , pivot features on the word level were used <CIT> , eg Does the bigram not buy occur in this document ? <CIT> '
'43 Experiments results Our evaluation metric is BLEU <CIT> , which are to perform case-insensitive matching of n-grams up to n = 4 '
'For example , alignments can be used to learn translation lexicons <OTH> , transfer rules <OTH> , and classifiers to find safe sentence segmentation points <CIT> '
'We were given around 15K sentences of labeled text from the Wall Street Journal -LRB- WSJ -RRB- <CIT> as well as 200K unlabeled sentences '
'Further details are in the original paper <CIT> '
'Pooling the sets to form two large CE and AE test sets , the AE system improvements are significant at a 95 % level <CIT> ; the CE systems are only equivalent '
'3 Related work Word collocation Various collocation metrics have been proposed , including mean and variance <CIT> , the t-test <OTH> , the chi-square test , pointwise mutual information -LRB- MI -RRB- <OTH> , and binomial loglikelihood ratio test -LRB- BLRT -RRB- <OTH> '
'Examples of the latter include providing suggestions from a machine labeler and using extremely cheap human labelers , eg with the Amazon Mechanical Turk <CIT> '
'2 Related Work WSD approaches can be classified as (a) knowledge-based approaches, which make use of linguistic knowledge, manually coded or extracted from lexical resources (Agirre and Rigau, 1996; Lesk 1986); (b) corpus-based approaches, which make use of shallow knowledge automatically acquired from corpus and statistical or machine learning algorithms to induce disambiguation models (Yarowsky, 1995; Schtze 1998); and (c) hybrid approaches, which mix characteristics from the two other approaches to automatically acquire disambiguation models from corpus supported by linguistic knowledge (Ng and Lee 1996; Stevenson and Wilks, 2001).'
'4 Experimental Set-up For the experiments , we use the WSJ portion of the Penn tree bank <CIT> , using the standard train\/development\/test splits , viz 39,832 sentences from 2-21 sections , 2416 sentences from section 23 for testing and 1,700 sentences from section 22 for development '
'Each model can represent an important feature for the translation , such as phrase-based , language , or lexical models <CIT> '
'models implement the intuition that the best model will be the one that is consistent with the set of constrains imposed by the evidence , but otherwise is as uniform as possible <CIT> '
'1 Introduction Much of statistical NLP research relies on some sort of manually annotated corpora to train their models , but these resources are extremely expensive to build , especially at a large scale , for example in treebanking <CIT> '
'33 Syntax based approach An alternative to the Window and Document-oriented approach is to use syntactical information <OTH> '
'Turney <CIT> reported that the NEAR operator outperformed simple page co-occurrence for his purposes ; our early experiments informally showed the same for this work '
'41 Training The training procedure is identical to the factored phrase-based training described in <CIT> '
'We concatenate the lists and we learn a new combination of weights that maximizes the Bleu score of the combined nbest list using the same development corpus we used for tuning the individual systems <CIT> '
'1 Introduction Recently , researchers have developed algorithms that learn to map natural language sentences to representations of their underlying meaning <CIT> '
'The group of collocations and compounds should be delimited using statistical approaches , such as Xtract <CIT> or LocalMax <OTH> , so that only the most relevantthose of higher frequency are included in the database '
'Expectation Maximization does surprisingly well on larger data sets and is competitive with the Bayesian estimators at least in terms of cross-validation accuracy , confirming the results reported by <CIT> '
'<CIT> describes experiments on the same named-entity dataset as in this paper , but using explicit features rather than kernels '
'Previous research has focused on classifying subjective-versus-objective expressions <OTH> , and also on accurate sentiment polarity assignment <CIT> '
'Corpora in various languages , such as the English Penn Treebank corpus <CIT> , the Swedish Stockholm-Ume corpus <OTH> , and the Icelandic Frequency Dictionary -LRB- IFD -RRB- corpus <OTH> , have been used to train -LRB- in the case of data-driven methods -RRB- and develop -LRB- in the case of linguistic rule-based methods -RRB- different taggers , and to evaluate their accuracy , eg '
'Adapting a vectorbased approach reported by <CIT> , the Task ID Frame Agent is domain-independent and automatically trained '
'5 Previous Work The LEAF model is inspired by the literature on generative modeling for statistical word alignment and particularly by Model 4 <CIT> '
'We scored systems and our own output using case-insensitive IBM-style BLEU 104 <CIT> , METEOR 06 <OTH> with all modules , and TER 5 <OTH> '
'3 Data The data consists of six sections of the Wall Street Journal part of the Penn Treebank <CIT> , and follows the setting of past editions of the CoNLL shared task : training set -LRB- sections 15-18 -RRB- , development set -LRB- section 20 -RRB- and test set -LRB- section 21 -RRB- '
'The rules are then treated as events in a relative frequency estimate4 We used Giza + + Model 4 to obtain word alignments <OTH> , using the grow-diag-final-and heuristic to symmetrise the two directional predictions <CIT> '
'It is an extension of Pharaoh <CIT> , and supports factor training and decoding '
'80 80 % Positive child education Positive cost Negative SUBJECT increase Figure 3 : An example of a word-polarity lattice Various methods have already been proposed for sentiment polarity classification , ranging from the use of co-occurrence with typical positive and negative words <CIT> to bag of words <OTH> and dependency structure <OTH> '
'Researchers have mostly looked at representing words by their surrounding words <OTH> and by their syntactical contexts <CIT> '
'Recently , a number of machine learning approaches have been proposed <CIT> '
'Another motivation to evaluate the performance of a phrase translation model that contains only syntactic phrases comes from recent efforts to built syntactic translation models <CIT> '
'Examples of formalisms using this approach include the work of Magerman <OTH> , Charniak <OTH> , <CIT> <OTH> , and Goodman <OTH> '
'<CIT> was an implicit or selforganizing syntax model as it did not use a Treebank '
'The other approach is to estimate a single score or likelihood of a translation with rich features , for example , with the maximum entropy -LRB- MaxEnt -RRB- method as in <CIT> '
'We tune using Ochs algorithm <CIT> to optimize weights for the distortion model , language model , phrase translation model and word penalty over the BLEU metric <OTH> '
'We obtained word alignments of the training data by first running GIZA + + <OTH> and then applying the refinement rule grow-diagfinal-and <CIT> '
'31 Results for English We used sections 0 to 12 of the WSJ part of the Penn Treebank <CIT> with a total of 24,618 sentences for our experiments '
'2 Details of the SO-PMI Algorithm The SO-PMI algorithm <CIT> is used to estimate the semantic orientation -LRB- SO -RRB- of a phrase by 1http : \/ \/ wwwepinionscom 189 References Peter D Turney '
'We are given a source -LRB- Chinese -RRB- sentence f = fJ1 = f1 , , fj , , fJ , which is to be translated into a target -LRB- English -RRB- sentence e = eI1 = e1 , , ei , , eI Among all possible target sentences , we will choose the sentence with the highest probability : eI1 = argmax eI1 -LCB- Pr -LRB- eI1 fJ1 -RRB- -RCB- -LRB- 1 -RRB- As an alternative to the often used source-channel approach <CIT> , we directly model the posterior probability Pr -LRB- eI1 fJ1 -RRB- <OTH> using a log-linear combination of feature functions '
'The skip-chain CRFs <CIT> model the long distance dependency between context and answer sentences and the 2D CRFs <OTH> model the dependency between contiguous questions '
'Recently , <CIT> combined an MRD and a corpus in a bootstrapping process '
'In parsing , the most relevant previous work is due to <CIT> , who considered three binary features of the intervening material : did it contain -LRB- a -RRB- any word tokens at all , -LRB- b -RRB- any verbs , -LRB- c -RRB- any commas or colons ? '
'Demonstrating the inadequacy of such approaches , Al-Onaizan and <CIT> showed that even given the words in the reference translation , and their alignment to the source words , a decoder of this sort charged with merely rearranging them into the correct target-language order could achieve a BLEU score <CIT> of at best 69 % and that only when restricted to keep most words very close to their source positions '
'3 The statistical model We use the Xerox part-of-speech tagger <CIT> , a statistical tagger made at the Xerox Palo Alto Research Center '
'The accuracy of the derived model depends heavily on the initial bias , but with a good choice results are comparable to those of method three <CIT> '
'Unlike MaxEnt training , the method <CIT> used for estimating the weight vector for BLEU maximization are not computationally scalable for a large number of feature functions '
'1 Introduction Sentence-aligned parallel bilingual corpora have been essential resources for statistical machine translation <CIT> , and many other multi-lingual natural language processing applications '
'The third baseline , COMP is the document compression system developed by <CIT> , which compresses documents by cutting out constituents in a combined syntax and discourse tree '
'During training each example is broken into elementary trees using head rules and argument\/adjunct rules similar to those of <CIT> '
'For example , the sets of tags and rule labels have been clustered by our team gr ~ : mm ~ trian , while a vocabulary of about 60,000 words has been clustered by machine <CIT> '
'Some of the early statistical terminology translation methods are <CIT> '
'Freund and Schapire <OTH> discuss how the theory for classification problems can be extended to deal with both of these questions ; <CIT> describes how these results apply to NLP problems '
'These joint counts are estimated using the phrase induction algorithm described in <CIT> , with symmetrized word alignments generated using IBM model 2 <OTH> '
'<CIT> argue that these results show a pattern where discriminative probability models are inferior to generative probability models , but that improvements can be achieved by keeping a generative probability model and training according to a discriminative optimization criteria '
'452 BLEU on NIST MT Test Sets We use MT02 as the development set4 for minimum error rate training -LRB- MERT -RRB- <CIT> '
'2 Data Sets for the Experiments 21 Coordination Annotation in the PENN TREEBANK For our experiments , we used the WSJ part of the PENN TREEBANK <CIT> '
'section 20 Majority voting (Mufioz et al. , 1999) (Tjong Kim Sang and Veenstra~ 1999) (Ramshaw and Marcus, 1995) (Argarnon et al. , 1998) accuracy precision O:98.10% C:98.29% 93.63% O:98.1% C:98.2% 93.1% 97.58% 92.50% 97.37% 91.80% 91.6% recall FZ=I 92.89% 93.26 92.4% 92.8 92.25% 92.37 92.27% 92.03 91.6% 91.6 section 00 accuracy precision Majority voting 0:98.59% C:98.65% 95.04% r (Tjong Kim Sang and Veenstra, 1999) 98.04% 93.71% (Ramshaw and Marcus, 1995) 97.8% 93.1% recall FB=I 94.75% 94.90 93.90% 93.81 93.5% 93.3 Table 3: The results of majority voting of different data representations applied to the two standard data sets put forward by (Ramshaw and Marcus, 1995) compared with earlier work.'
'PMI <CIT> between two phrases is de ned as : log2 prob -LRB- ph1 is near ph2 -RRB- prob -LRB- ph 1 -RRB- prob -LRB- ph2 -RRB- PMI is positive when two phrases tend to co-occur and negative when they tend to be in a complementary distribution '
'51 ExploringtheParameters Theparameterswhichhaveamajorinuenceonthe performance of a phrase-based SMT model are the alignment heuristics , the maximum phrase length -LRB- MPR -RRB- and the order of the language model <CIT> '
'Examples of such affinities include synonyms <OTH> , verb similarities <OTH> and word associations <CIT> '
'Although Phramer provides decoding functionality equivalent to Pharaohs , we preferred to use Pharaoh for this task because it is much faster than Phramer between 2 and 15 times faster , depending on the configuration and preliminary tests showed that there is no noticeable difference between the output of these two in terms of BLEU <CIT> score '
'In <CIT> , the authors proposed a method to integrate the IBM translation model 2 <CIT> with an ASR system '
'Other research has been conducted in analyzing sentiment at a sentence level using bootstrapping techniques <OTH> , finding strength of opinions <OTH> , summing up orientations of opinion words in a sentence <OTH> , and identifying opinion holders <CIT> '
'3 Experiments We tested our methods experimentally on the English Penn Treebank <CIT> and on the Czech Prague Dependency Treebank <OTH> '
'Much research is also being directed at acquiring affect lexica automatically <CIT> '
'We ran GIZA + + <OTH> on the training corpus in both directions using its default setting , and then applied the refinement rule diagand described in <CIT> to obtain a single many-to-many word alignment for each sentence pair '
'In this data set the 4-tuples of the test and training sets were extracted from Penn Treebank Wall Street Journal <CIT> '
'3 Previous Work on Subjectivity Tagging In previous work <OTH> , a corpus of sentences from the Wall Street Journal Treebank Corpus <CIT> was manually annotated with subjectivity classi cations bymultiplejudges '
'Our method is thus related to previous work based on Harris <OTH> s distributional hypothesis2 It has been used to determine both word and syntactic path similarity <CIT> '
'4 Training This section discusses how to extract our translation rules given a triple nullnull , null null , nullnull As we know , the traditional tree-to-string rules can be easily extracted from nullnull , null null , nullnull using the algorithm of Mi and Huang <OTH> 2 We would like 2 Mi and Huang <OTH> extend the tree-based rule extraction algorithm <OTH> to forest-based by introducing non-deterministic mechanism '
'Some other researchers also work on detecting negative cases , ie contradiction , instead of entailment <CIT> '
'The output of GIZA + + is then post-processed using the three symmetrization heuristics described in <CIT> '
'Another technique used was to filter sentences of the out-of-domain corpus based on their similarity to the target domain , as predicted by a classifier <CIT> '
'Evaluation We evaluate translation output using three automatic evaluation measures : BLEU <CIT> , NIST <OTH> , and METEOR <OTH> 5 All measures used were the case-sensitive , corpuslevel versions '
'This set of 800 sentences was used for Minimum Error Rate Training <CIT> to tune the weights of our system with respect to BLEU score '
'So far research in automatic opinion recognition has primarily addressed learning subjective language <OTH> , identifying opinionated documents <OTH> and sentences <OTH> , and discriminating between positive and negative language <CIT> '
'These findings are in line with <CIT> results with incremental parsing with perceptrons , where it is suggested that a generative baseline feature provides the perceptron algorithm with a much better starting point for learning '
'As two examples , <OTH> and <OTH> give good overviews of the techniques and equations used for Markov models and part-ofspeech tagging , but they are not very explicit in the details that are needed for their application '
'41 Data Sets Our results are based on syntactic data drawn from the Penn Treebank <CIT> , specifically the portion used by CoNLL 2000 shared task <OTH> '
'<CIT> Marcus , M , Santorini , B , and Malvinkiewicz , MA '
'According to the statistical machine translation formalism <CIT> , the translation process is to search for the best sentence bE such that bE = arg max E P -LRB- EjJ -RRB- = arg maxE P -LRB- JjE -RRB- P -LRB- E -RRB- where P -LRB- JjE -RRB- is a translation model characterizing the correspondence between E and J ; P -LRB- E -RRB- , the English language model probability '
'3 Results and Analysis <CIT> shows that the oracle parsing accuracy of a k-best edge-factored MST parser is considerably higher than the one-best score of the same parser , even when k is small '
'To examine the effects of including some known AMs on the performance , the following AMs had a 50 % chance of being included in the initial population : pointwise mutual information <CIT> , the Dice coefficient , and the heuristic measure defined in <OTH> : H -LRB- a , b , c -RRB- = 2log f -LRB- abc -RRB- f -LRB- a -RRB- f -LRB- c -RRB- if POS -LRB- b -RRB- = X , log f -LRB- abc -RRB- f -LRB- a -RRB- f -LRB- b -RRB- f -LRB- c -RRB- otherwise '
'41 Judging Rule Correctness Following the spirit of the fine-grained human evaluation in <CIT> , we randomly sampled 800 rules from our rule-base and presented them to an annotator who judged them for correctness , according to the lexical reference notion specified above '
'From this LFG annotated treebank , large-scale unification grammar resources were automatically extracted and used in parsing <CIT> and generation <CIT> '
'This approach is usually referred to as the noisy sourcechannel approach in SMT <CIT> '
'In many applications , it has been shown that sentences with subjective meanings are paid more attention than factual ones <CIT> <OTH> '
'Our approach was to identify a parallel corpus of manually and automatically transcribed documents , the TDT2 corpus , and then use a statistical approach <CIT> to identify tokens with significantly Table 5 : Impact of recall and precision enhancing devices '
'Most previous research in translation knowledge acquisition is based on parallel corpora <CIT> '
'This text was part-of-speech tagged using the Xerox HMM tagger <CIT> '
'GIZA + + toolkit <CIT> is used to perform word alignment in both directions with default settings , and the intersect-diag-grow method is used to generate symmetric word alignment refinement '
'Word-aligned corpora have been found to be an excellent source for translation-related knowledge , not only for phrase-based models <OTH> , but also for syntax-based models -LRB- eg , <CIT> -RRB- '
'As a solution , a given amount of labeled training data is divided into two distinct sets , ie , 4\/5 for estimating , and the 667 remaining 1\/5 for estimating <CIT> '
'In this paper it is shown that the synchronous grammars used in <CIT> , Zhang et al '
'Like the data used by <CIT> , this data was retagged by the Brill tagger in order to obtain realistic part-of-speech -LRB- POS -RRB- tags 3 '
'Purely syntactic categories lead to a smaller number of tags which also improves the accuracy of manual tagging 2 <CIT> '
'In our experiments , we used the Hidden Markov Model -LRB- HMM -RRB- tagging method described in <CIT> '
'However , at the short term , the incorporation of these type of features will force us to either build a new decoder or extend an existing one , or to move to a new MT architecture , for instance , in the fashion of the architectures suggested by Tillmann and Zhang <OTH> or <CIT> et al '
'The f are optimized by Minimum-Error Training -LRB- MERT -RRB- <CIT> '
'Recent work by <CIT> pro514 poses factored translation models that combine feature functions to handle syntactic , morphological , and other linguistic information in a log-linear model '
'Another application of hard clustering methods -LRB- in particular bottom-up variants -RRB- is that they can also produce a binary tree , which can be used for decision-tree based systems such as the SPATTER parser <OTH> or the ATR Decision-Tree Part-OfSpeech Tagger <OTH> '
'The parser is trained on dependencies extracted from the English Penn Treebank version 30 <CIT> by using the head-percolation rules of <OTH> '
'After each step the annotations were compared using the ~ statistic as reliability measure for all classification tasks <CIT> '
'Metrics in the Rouge family allow for skip n-grams <CIT> ; Kauchak and Barzilay -LRB- 2006 -RRB- take paraphrasing into account ; metrics such as METEOR <OTH> and GTM <OTH> calculate both recall and precision ; METEOR is also similar to SIA <OTH> in that word class information is used '
'To set the weights , m , we performed minimum error rate training <OTH> on the development set using Bleu <CIT> as the objective function '
'Parsing research has also begun to adopt discriminative methods from the Machine Learning literature , such as the perceptron <CIT> and the largemargin methods underlying Support Vector Machines <OTH> '
'Furthermore , it is not possible to apply the powerful ` one sense per discourse '' property <CIT> because there is no discourse in dictionaries '
'We might find better suited metrics , such as METEOR <CIT> , which is oriented towards word selection8 '
'Language models , such as N-gram class models <CIT> and Ergodic Hidden Markov Models <OTH> were proposed and used in applications such as syntactic class -LRB- POS -RRB- tagging for English <OTH> , clustering and scoring of recognizer sentence hypotheses '
'The data used for all our experiments is extracted from the PENN '' WSJ Treebank <CIT> by the program provided by Sabine Buchholz from Tilbug University '
'He then goes on to adapt the conventional noisy channel MT model of <CIT> to NLU , where extracting a semantic representation from an input text corresponds to finding : argmax -LRB- Sem -RRB- -LCB- p -LRB- Input Sem -RRB- p -LRB- Sem -RRB- -RCB- , where p -LRB- Sem -RRB- is a model for generating semantic representations , and p -LRB- Input Sem -RRB- is a model for the relation between semantic representations and corresponding texts '
'We use data from the CoNLL-2004 shared taskthe PropBank <OTH> annotations of the Penn Treebank <CIT> , with sections 1518 as the training set and section 20 as the development set '
'Here , we use the hidden Markov model -LRB- HMM -RRB- alignment model <OTH> and Model 4 of Brown et al '
'As reported previously , the standard left-corner grmninar embeds sufficient non-local infornlation in its productions to significantly improve the labeled precision and recall of its MLPs with respect to MLPs of the PCFG estimated from the untransfornmd trees <OTH> '
'Self-training <CIT> is a form of semi-supervised learning '
'A more recent bootstrapping approach is described in <CIT> '
'31 Collocation Features The collocation features were inspired by the one-sense-per-collocation heuristic proposed by <CIT> '
'1 Introduction In the past few years , there has been an increasing interest in mining opinions from product reviews <CIT> '
'For more detail , see <CIT> '
'62 Experiment 2 : Yarowskys Words We also conducted translation on seven of the twelve English words studied in <CIT> '
'One of the earliest attempts at extracting interrupted collocations '' -LRB- ie non-contiguous collocations , including VPCs -RRB- , was that of Smadja <OTH> '
'In this work , we employ a syntax-based model that applies a series of tree\/string -LRB- xRS -RRB- rules <CIT> to a source language string to produce a target language phrase structure tree '
'We first added sister-head dependencies for NPs -LRB- following <CIT> original proposal -RRB- and then for PPs , which are flat in Negra , and thus similar in structure to NPs -LRB- see Section 22 -RRB- '
'BABAR uses the log-likelihood statistic <CIT> to evaluate the strength of a co-occurrence relationship '
'While earlier approaches for text compression were based on symbolic reduction rules <OTH> , more recent approaches use an aligned corpus of documents and their human written summaries to determine which constituents can be reduced <CIT> '
'One kind is the Penn Treebank <CIT> '
'Table 2: Corpora and Modalities CORPUS MODALITY ACE asserted, or other TIMEML must, may, should, would, or could Prasad et al., 2006 assertion, belief, facts or eventualities Saur et al., 2007 certain, probable, possible, or other Inui et al., 2008 affirm, infer, doubt, hear, intend, ask, recommend, hypothesize, or other THIS STUDY S/O, necessity, hope, possible, recommend, intend   Table 3: Markup Scheme (Tags and Definitions) Tag Definition (Examples) R Remedy, Medical operation (e.g. radiotherapy) T Medical test, Medical examination (e.g., CT, MRI) D Deasese, Symptom (e.g., Endometrial cancer, headache) M Medication, administration of a drug (e.g., Levofloxacin, Flexeril) A patient action (e.g., admitted to a hospital) V Other verb (e.g., cancer spread to )   2 Related Works 2.1 Previous Markup Schemes In the NLP field, fact identification has not been studied well to date.'
'Including about 14 million sentence pairs extracted from the Gigaword data , we obtain a statistically significant improvement from 423 to 456 in BLEU <CIT> '
'For example , in IBM Model 1 the lexicon probability of source word f given target word e is calculated as <CIT> : p -LRB- f e -RRB- = summationtext k c -LRB- f e ; e k , fk -RRB- summationtext k , f c -LRB- f e ; e k , fk -RRB- -LRB- 1 -RRB- c -LRB- f e ; ek , fk -RRB- = summationdisplay ek , fk P -LRB- ek , fk -RRB- summationdisplay a P -LRB- a ek , fk -RRB- -LRB- 2 -RRB- summationdisplay j -LRB- f , fkj -RRB- -LRB- e , ekaj -RRB- Therefore , the distribution of P -LRB- ek , fk -RRB- will affect the alignment results '
'<CIT> proposed dealing with the sparseness problem by estimating the likelihood of unseen events from that of ` similar '' events that have been seen '
'In all experiments that follow , each system configuration was independently optimized on the NIST 2003 Chinese-English test set -LRB- 919 sentences -RRB- using minimum error rate training <CIT> and tested on the NIST 2005 Chinese-English task -LRB- 1082 sentences -RRB- '
'As a common strategy , POS guessers examine the endings of unknown words <CIT> along with their capitalization , or consider the distribution of unknown words over specific parts-of-speech <OTH> '
'Many statistical taggers and parsers have been trained on it , eg <CIT> , Srinivas -LRB- 1997 -RRB- and Alshawi and Carter -LRB- 1994 -RRB- '
'The XEROX tagger comes with a list of built-in ending guessing rules <CIT> '
'The translation output is measured using BLEU <CIT> '
'There exist many different string similarity measures : word overlap <OTH> , longest common subsequence <OTH> , Levenshteinedit distance <OTH> , word n-gramoverlap <CIT> etc Semantic similarity measures are obtained by first computing the semantic similarity of the words containedin the sentencesbeing compared '
'Minor variants support voted perceptron <CIT> and MEMMs <OTH> with the same ef cient feature encoding '
'In addition to the manual alignment supplied with these data , we create an automatic word alignment for them using GIZA + + <OTH> and the grow-diagfinal -LRB- GDF -RRB- symmetrization algorithm <CIT> '
'Adopting the SCF acquisition system of Briscoe and Carroll , we have experimented with an alternative hypothesis test , the binomial log-likelihood ratio -LRB- LLR -RRB- test <CIT> '
'Similarities are captured from different viewpoints : DP-HWC -LRB- i -RRB- - l This metric corresponds to the HWC metric presented by <CIT> '
'Model Overall Unknown Word Accuracy Accuracy Baseline , 9672 % 845 % J <CIT> Table 3 Baseline model performance This table also shows the results reported in Ratnaparkhi <OTH> for COnvenience '
'1142 We show that by using a variant of SVM Anchored SVM Learning <OTH> with a polynomial kernel , one can learn accurate models for English NP-chunking <CIT> , base-phrase chunking <OTH> , and Dutch Named Entity Recognition <OTH> , on a heavily pruned feature space '
'Our method is similar to <CIT> , <OTH> , and <OTH> in the use of dependency relationships as the word features '
'There are also research work on automatically classifying movie or product reviews as positive or negative <CIT> '
'As described in Section 3 we retrieved neighbors using <CIT> similarity measure on a RASP parsed <OTH> version of the BNC '
'This curve plots the average labeled attachment score over Basque , Chinese , English , and Turkish as a function of parsing time per token4 Accuracy of only 1 % below the maximum can be achieved with average processing time of 17 ms per token , or 60 tokens per second5 We also refer the reader to <CIT> for more detailed analysis of the ISBN dependency parser results , where , among other things , it was shown that the ISBN model is especially accurate at modeling long dependencies '
'<CIT> extracted hyponymyrelationsfromtherstsentences -LRB- ie , dening sentences -RRB- of Wikipedia articles and then used them as a gazetteer for NER '
'The feature functions are combined under a log-linear framework , andtheweights aretuned bytheminimum-error-rate training <CIT> using BLEU <OTH> as the optimization metric '
'We extracted tagged sentences from the parse trees5 We split the data into training , development , and test sets as in <CIT> '
'-LRB- RM95 -RRB- Lance A <CIT> '
'Statistical techniques developed for lexicalized grammars <CIT> , readily apply to CCG to improve the average parsing performance in large-scale practical applications <OTH> '
'4An adaptation of the averaged perceptron algorithm <CIT> is used to tune the model parameters '
'<OTH> , <CIT> -RRB- '
'In the SMT research community , the second step has been well studied and many methods have been proposed to speed up the decoding process , such as node-based or span-based beam search with different pruning strategies <CIT> and cube pruning <OTH> '
'In this method , each training sentence is decoded and weights are updated at every iteration <CIT> '
'Distance from a target word is used for this purpose and it is calculated by the assumption that the target words in the context window have the same sense <CIT> '
'5 Phrase Pair Induction A common approach to phrase-based translation is to extract an inventory of phrase pairs -LRB- PPI -RRB- from bitext <CIT> , For example , in the phraseextract algorithm <OTH> , a word alignment am1 is generated over the bitext , and all word subsequences ei2i1 and fj2j1 are found that satisfy : am1 : aj -LRB- i1 , i2 -RRB- iff j -LRB- j1 , j2 -RRB- '
'Most approaches <CIT> inherently extract semantic knowledge in the abstracted form of semantic clusters '
'Besides being linguistically motivated , the need for EDL is also supported by empirical findings in MT that one-level rules are often inadequate <CIT> '
'The class-based approaches <CIT> calculate co-occurrence data of words belonging to different classes , ~ rather than individual words , to enhance the co-occurrence data collected and to cover words which have low occurrence frequencies '
'The model scaling factors M1 are trained with respect to the final translation quality measured by an error criterion <CIT> '
'<CIT> first introduced the machine learning techniques to chunking problem '
'This is in contrast to standard summarization models that look to promote sentence diversity in order to cover as many important topics as possible <CIT> '
'The parameters of the NIST systems were tuned using Ochs algorithm to maximize BLEU on the MT02 test set <CIT> '
'The NP chunks in the shared task data are base-NP chunks which are non-recursive NPs , a definition first proposed by <CIT> '
'Heuristic approaches obtain word alignments by using various similarity functions between the types of the two languages <OTH> '
'Proceedings of EACL ''99 Determinants of Adjective-Noun Plausibility Maria Lapata and Scott McDonald and Frank Keller School of Cognitive Science Division of Informatics, University of Edinburgh 2 Buccleuch Place, Edinburgh EH8 9LW, UK {mlap, scottm, keller} @cogsci.ed.ac.uk Abstract This paper explores the determinants of adjective-noun plausibility by using correlation analysis to compare judgements elicited from human subjects with five corpus-based variables: co-occurrence frequency of the adjective-noun pair, noun frequency, conditional probability of the noun given the adjective, the log-likelihood ratio, and Resnik''s (1993) selectional association measure.'
'<CIT> et al 1993 -RRB- '
'We used minimum error rate training <OTH> and the A \* beam search decoder implemented by Koehn <CIT> '
'In cases like <CIT> , unsupervised methods offer accuracy results than rival supervised methods <CIT> while requiring only a fraction of the data preparation effort '
'The most popular non-data-splitting methods for predicting test set cross-entropy -LRB- or likelihood -RRB- are AIC and variants such as AICc , quasi-AIC -LRB- QAIC -RRB- , and QAICc <OTH> '
'We use as our English corpus the Wall Street Journal -LRB- WSJ -RRB- portion of the Penn Treebank <CIT> '
'Though inter-rater reliability using the kappa statistic <CIT> may be calculated for each group , the distribution of categories in the contribution group was highly skewed and warrants further discussion '
'Next , using our feature vector , we applied five different linear classifiers to extract PPI from AIMed : L2-SVM , 1-norm soft-margin SVM -LRB- L1-SVM -RRB- , logistic regression -LRB- LR -RRB- <OTH> , averaged perceptron -LRB- AP -RRB- <CIT> , and confidence weighted linear classification -LRB- CW -RRB- <OTH> '
'6 Coding reliability The reliability of the annotation was evaluated using the kappa statistic <CIT> '
'We implement this algorithm using the perceptron framework , as it can be easily modified for structured prediction while preserving convergence guarantees <CIT> '
'The word alignment used in GHKM is usually computed independent ofthesyntacticstructure,andasDeNeroandKlein (2007) and May and Knight (2007) have noted, Ch-En En-Ch Union Heuristic 28.6% 33.0% 45.9% 20.1% Table 1: Percentage of corpus used to generate big templates, based on different word alignments 9-12 13-20 21 Ch-En 18.2% 17.4% 64.4% En-Ch 15.9% 20.7% 63.4% Union 9.8% 15.1% 75.1% Heuristic 24.6% 27.9% 47.5% Table 2: In the selected big templates, the distribution of words in the templates of different sizes, which are measured based on the number of symbols in their RHSs is not the best for SSMT systems.'
'21 Likelihood Ratios in the Type-based Stage The log-likelihood ratio by <CIT> tests whether the probability of a word is dependent on the occurrence of the preceding word type '
'Word alignment models were first introduced in statistical machine translation <CIT> '
'Combining statistical and parsing methods has been done by <CIT> and <OTH> '
'The other is the self-training <CIT> which first parses and reranks the NANC corpus , and then use them as additional training data to retrain the model '
'3 The Effect of Training Corpus Size A number of past research work on WSD , such as <OTH> , were tested on a small number of words like ` line '' and ` interest '' '
'Among various language modeling approaches , ngram modeling has been widely used in many applications , such as speech recognition , machine translation <CIT> '
'Annotation was highly reliable with a kappa <CIT> of 3https : \/ \/ wwwciagov\/cia\/publications \/ factbook\/indexhtml 4Given that the task is not about standard Named Entity Recognition , we assume that the general semantic class of the name is already known '
'When we have a junction tree for each document , we can efficiently perform belief propagation in order to compute argmax in Equation -LRB- 1 -RRB- , or the marginal probabilities of cliques and labels , necessary for the parameter estimation of machine learning classifiers , including perceptrons <OTH> , and maximum entropy models <CIT> '
'Standard SMT alignment models <CIT> are used to align letter-pairs within named entity pairs for transliteration '
'There has been a large interest in recognizing non-overlapping noun phrases -LRB- <CIT> and follow-up papers -RRB- but relatively little has been written about identifying phrases of other syntactic categories '
'Several approaches for learning from both labeled and unlabeled data have been proposed <CIT> where the unlabeled data is utilised to boost the performance of the algorithm '
'96 Research on DA classification initially focused on two-party conversational speech <OTH> and , more recently , has extended to multi-party audio recordings like the ICSI corpus <OTH> '
'Charniak 1996 , 1997 -RRB- , while most current stochastic parsing models use a ` markov grammar '' <CIT> '
'The typical practice of preprocessing distributional data is to remove rare word co-occurrences , thus aiming to reduce noise from idiosyncratic word uses and linguistic processing errors and at the same time form more compact word representations <CIT> '
'4 Related Work The automatic extraction of English subcategorization frames has been considered in <OTH> , where a procedure is presented that takes untamed text as input and generates a list of verbal subcategorization frames '
'The algorithm is similar to the perceptron algorithm described in <CIT> '
'In our experiments we use standard methods in phrase-based systems <CIT> to define the set of phrase entries for each sentence in training data '
'In additioil , <CIT> , <CIT> point ou ; that there is a st , rent tenden -LRB- : y for words 1 ; O occur in -LRB- -RCB- Ile sense within any given dis : ourse -LRB- ` one sense pe , r dis : ourse '' -RRB- '
'This is a particularly exciting area in computational linguistics as evidenced by the large number of contributions in these special issues : Biber <OTH> , Brent <OTH> , Hindle and Rooth -LRB- this issue -RRB- , Pustejovsky et al '
'Treebank <CIT> , six of which are errors '
'Stochastic ITGs are parameterized like their PCFG counterparts <CIT> ; productions A X are assigned probability Pr -LRB- X A -RRB- '
'Introduction Word sense disambiguation has long been one of the major concerns in natural language processing area <CIT> , whose aim is to identify the correct sense of a word in a particular context , among all of its senses defined in a dictionary or a thesaurus '
'As a learning algorithm for our classification model , we used Maximum Entropy <CIT> '
'Their weights are calculated by deleted interpolation <CIT> '
'Many NLP systems use the output of supervised parsers -LRB- eg , <OTH> for QA , <OTH> for IE , <OTH> for SRL , <CIT> for Textual Inference and <OTH> for MT -RRB- '
'Our evaluation metric is BLEU-4 <CIT> , as calculated by the script mteval-v11bpl with its default setting except that we used case-sensitive matching of n-grams '
'Yarowsky proposed the unsupervised learning method for WS <CIT> '
'We also combine our basic algorithm -LRB- Section 42 -RRB- with <CIT> s algorithm in order to resolve the modifier-function traces '
'7An alternative framework that formally describes some dependency parsers is that of transition systems <CIT> '
'Words surrounding the current word have been occasionally used in taggers , such as <CIT> , Brills transformation based tagger <OTH> , and the HMM model of Lee et al '
'Recent work emphasizes a corpus-based unsupervised approach <CIT> that avoids the need for costly truthed training data '
'1 Introduction Word alignment is a critical component in training statistical machine translation systems and has received a significant amount of research , for example , <OTH> , including work leveraging syntactic parse trees , eg , <CIT> '
'Recently <CIT> have sought to acquire automatically some semantic patterns that can be used as contextual information to improve reference resolution , using techniques adapted from information extraction '
'A standard solution is to use a weighted linear mixture of N-gram models , 1 n N , <CIT> '
'Recently , confusion-network-based system combination algorithms have been developed to combine outputs of multiple machine translation -LRB- MT -RRB- systems to form a consensus output <CIT> '
'There are several basic methods for evaluating associations between words : based on frequency counts <OTH> , information theoretic <OTH> and statistical significance <CIT> '
'A possible solution to this problem is to directly estimate p -LRB- A w -RRB- by applying a maximum entropy model <CIT> '
'We used the preprocessed data to train the phrase-based translation model by using GIZA + + <OTH> and the Pharaoh tool kit <CIT> '
'We estimate loss gradients -LRB- Equation 13 -RRB- using a sample of the inference set , which gives a 100-fold increase in training speed <CIT> '
'To quickly -LRB- and approximately -RRB- evaluate this phenomenon , we trained the statistical IBM wordalignment model 4 <OTH> ,1 using the GIZA + + software <CIT> for the following language pairs : ChineseEnglish , Italian English , and DutchEnglish , using the IWSLT-2006 corpus <OTH> for the first two language pairs , and the Europarl corpus <OTH> for the last one '
'Perhaps the most related is 86 learning as search optimization -LRB- LASO -RRB- <CIT> '
'Experiments We have conducted a series of lexical acquisition experiments with the above algorithm on largescale English corpora , eg , the Brown corpus <OTH> and the PTB WSJ corpus <CIT> '
'To avoid this problem , generative models for NLP tasks have often been manually designed to achieve an appropriate representation of the joint distribution , such as in the parsing models of <CIT> '
'Although grammatical function and empty nodes annotation expressing long-distance dependencies are provided in Treebanks such as the Penn Treebank <CIT> , most statistical Treebank trained parsers fully or largely ignore them 1 , which entails two problems : first , the training can not profit from valuable annotation data '
'In the tagging domain , <CIT> compared log-linear and perceptron training for HMM-style tagging based on dynamic programming '
'413 Letter Lexical Transliteration Similar to IBM Model-1 <CIT> , we use a bag-of-letter generative model within a block to approximate the lexical transliteration equivalence : P -LRB- fj + lj ei + ki -RRB- = j + lproductdisplay jprime = j i + ksummationdisplay iprime = i P -LRB- fjprime eiprime -RRB- P -LRB- eiprime ei + ki -RRB- , -LRB- 10 -RRB- where P -LRB- eiprime ei + ki -RRB- similarequal 1 \/ -LRB- k +1 -RRB- is approximated by a bagof-word unigram '
'The weights for the various components of the model -LRB- phrase translation model , language model , distortion model etc -RRB- are set by minimum error rate training <CIT> '
'291 31 Level of Analysis Research on sentiment annotation is usually conducted at the text <CIT> or at the sentence levels <OTH> '
'4 Structural Correspondence Learning SCL -LRB- Structural Correspondence Learning -RRB- <CIT> is a recently proposed domain adaptation technique which uses unlabeled data from both source and target domain to learn correspondences between features from different domains '
'Uses for k-best lists include minimum Bayes risk decoding <OTH> , discriminative reranking <OTH> , and discriminative training <CIT> '
'The measure simHinate is the same as the similarity measure proposed in <CIT> , except that it does not use dependency triples with negative mutual information '
'Kappa is defined as K = P -LRB- A -RRB- P -LRB- E -RRB- 1P -LRB- E -RRB- <CIT> , where P -LRB- A -RRB- is the proportion of times that the labels agree , and P -LRB- E -RRB- is the proportion of times that they may agree by chance '
'Various clustering techniques have been proposed <CIT> which perform automatic word clustering optimizing a maximum-likelihood criterion with iterative clustering algorithms '
'<OTH> compare two tagging frameworks for tagging French , one that is statistical , built upon the Xerox tagger <CIT> , and another based on linguistic constraints only '
'So far , these techniques have focused on phrasebased models using contiguous phrases <CIT> '
'<CIT> propose a model that encodes how likely it is that different sized spans of text are skipped to reach words and phrases to recycle '
'<CIT> used collocation with excellent or poor to obtain positive and negative clues for document classification '
'These constituent matching\/violation counts are used as a feature in the decoders log-linear model and their weights are tuned via minimal error rate training -LRB- MERT -RRB- <CIT> '
'Some of the differences between our approach and those of <CIT> are mentioned below : ? ? objectives : <CIT> aims at binary text classification , while our objective is six class classification of one-liner headlines '
'Subjective phrases are used by <CIT> and others in order to classify reviews or sentences as positive or negative '
'For evaluation we use ROUGE <CIT> SU4 recall metric1 , which was among the official automatic evaluation metrics for DUC '
'Specifically , <CIT> identify which political candidate is predicted to win by an opinion posted on a message board and aggregate opinions to correctly predict an election result '
'Inversion Transduction Grammar -LRB- ITG -RRB- is the model of <CIT> , Tree-to-String is the model of Yamada and Knight -LRB- 2001 -RRB- , and Tree-to-String , Clone allows the node cloning operation described above '
'<CIT> also pointed out that due to the limited references for every MT output , using the overlapping ratio of n-grams longer than 2 did not improve sentence level evaluation performance of BLEU '
'2 Related Work The most commonly used similarity measures are based on the WordNet lexical database <CIT> and a number of such measures have been made publicly available <OTH> '
'decades like n-gram back-off word models <OTH> , class models <CIT> , structured language models <OTH> or maximum entropy language models <OTH> '
'For instance , instead of representing the polarity of a term using a binary value , Mullen and Collier <OTH> use <CIT> method to assign a real value to represent term polarity and introduce a variety of numerical features that are aggregate measures of the polarity values of terms selected from the document under consideration '
'Wed like to learn the number of paradigm classes from the data , but doing this would probably require extending adaptor grammars to incorporate the kind of adaptive statesplitting found in the iHMM and iPCFG <CIT> '
'This approach is similar to conventional techniques for automatic thesaurus construction <CIT> '
'51 CoNLL named entities presence feature We use Stanford named entity recognizer -LRB- NER -RRB- <CIT> to identify CoNLL style NEs7 as possible answer strings in a candidate sentence for a given type of question '
'Work on learning with hidden variables can be used for both CRFs <OTH> and for inference based learning algorithms like those used in this work <CIT> '
'Meanwhile , translation grammars have grown in complexity from simple inversion transduction grammars <CIT> to general tree-to-string transducers <OTH> and have increased in size by including more synchronous tree fragments <OTH> '
'Schtze , 1993 -RRB- is not suited to highly skewed distributions omni-present in natural language '
'Several other measures like Log-Likelihood <CIT> , Pearsons a2a4a3 <OTH> , Z-Score <OTH> , Cubic Association Ratio -LRB- MI3 -RRB- , etc , have been also proposed '
'The parser implementation in <OTH> was used in this experiment and it was run in a mode which emulated the <CIT> parser '
'Similarly to classical NLP tasks such as text chunking <CIT> and named entity recognition <OTH> , we formulate mention detection as a sequence classification problem , by assigning a label to each token in the text , indicating whether it starts a specific mention , is inside a specific mention , or is outside any mentions '
'Err:510'
'Recently , several solutions to the problem of tagging unknown words have been presented <OTH> '
'We also do not require a newly added feature to be either atomic or a collocation of an atomic feature with a feature already included into the model as it was proposed in <OTH> <CIT> '
'32 Domain Adaptation Track As mentioned previously , the source data is drawn from a corpus of news , specifically the Wall Street Journal section of the Penn Treebank <CIT> '
'Our technique of generating negative examples is similar to the approach of <CIT> '
'31 Binarizable segmentations -LRB- a -RRB- Following <CIT> , every sequence of phrase alignments can be viewed 1For example , if the cut-off on phrase pairs is ten words , all sentence pairs smaller than ten words in the training data will be included as phrase pairs as well '
'31 Phrase-Based Models According to the translation model presented in <CIT> , given a source sentence f , the best target translation can be obtained using the following model best e 288 -RRB- -LRB- -RRB- -LRB- -RRB- -LRB- maxarg -RRB- -LRB- maxarg e e e eef fee length LM best pp p = = -LRB- 1 -RRB- Where the translation model can be decomposed into -RRB- -LRB- efp = = I i i iii i i II aefpbadef efp 1 1 1 1 -RRB- , -LRB- -RRB- -LRB- -RRB- -LRB- -RRB- -LRB- w -LRB- 2 -RRB- Where -RRB- -LRB- i i ef is the phrase translation probability '
'The model is composed of three parts (Collins, 2002a): a set of candidate SAPTs GEN, which is the top n SAPTs of a sentence from SCISSOR; a function  that maps a sentence Inputs: A set of training examples (xi,yi ), i = 1n, where xi is a sentence, and yi is a candidate SAPT that has the highest similarity score with the gold-standard SAPT Initialization: Set W = 0 Algorithm: For t = 1T,i = 1n Calculate yi = argmaxyGEN(xi) (xi,y)  W If (yi negationslash= yi ) then W = W +(xi,yi )  (xi,yi) Output: The parameter vector W Figure 2: The perceptron training algorithm.'
'This is the same complexity as the ITG alignment algorithm used by <CIT> and others , meaning complete Viterbi decoding is possible without pruning for realistic-length sentences '
'One can also examine the distribution of character or word ngrams , eg Language Modeling <OTH> , phrases <CIT> , and so on '
'The key difference is that , instead of using the delta rule of Equation -LRB- 8 -RRB- -LRB- as shown in line 5 of Figure 4 -RRB- , <CIT> updates parameters using the rule : t +1 d t d + f d -LRB- w R i -RRB- f d -LRB- w i -RRB- '
'Statistics on co-occurrence of words in a local context were used recently for monolingual word sense disambiguation <OTH> <CIT> '
'That some model structures work better than others at real NLP tasks was discussed by Johnson <OTH> and <CIT> '
'This shows that hypothesis features are either not discriminative enough , or that the reranking model is too weak This performance gap can be mainly attributed to two problems : optimization error and modeling error -LRB- see Figure 1 -RRB- 1 Much work has focused on developing better algorithms to tackle the optimization problem -LRB- eg MERT <CIT> -RRB- , since MT evaluation metrics such as BLEU and PER are riddled with local minima and are difficult to differentiate with respect to re-ranker parameters '
'Figure 1 -LRB- b -RRB- shows several orders of the sentence which violate this constraint1 Previous studies have shown that if both the source and target dependency trees represent linguistic constituency , the alignment between subtrees in the two languages is very complex <CIT> '
'1 Introduction Word alignments were first introduced as an intermediate result of statistical machine translation systems <CIT> '
'2 Related work Our work is closest in spirit to the two papers that inspired us <CIT> and <OTH> '
'1 Introduction The importance of learning to manipulate monolingual paraphrase relationships for applications like summarization , search , and dialog has been highlighted by a number of recent efforts <CIT> '
'32 ROUGE Version 155 of the ROUGE scoring algorithm <CIT> is also used for evaluating results '
'Normally, one would eliminate the redundant structures produced by the grammar in (1) by replacing it with the canonical form grammar (Wu, 1997), which has the following form: S  A | B | C A  [AB] | [BB] | [CB] | [AC] | [BC] | [CC] B  AA |BA|CA| AC |BC|CC C  e/f (2) By design, this grammar allows only one struc147 a0 a1 a2 a0 a3 a4 a2 a5 a1 a6 a7 a8 a6 a8 a9 a8 a2 a8 a10 a8 a1 a2 a3 a6 a8 a4 a7 a8 a6 a8 a9 a8 a8 a11 a12 a11 a0 a1 a2 a0 a3 a4 a2 a5 a1 a6 a7 a8 a6 a8 a9 a8 a0 a1 a2 a0 a3 a4 a2 a5 a1 a6 a7 a8 a6 a8 a9 a8 a0 a1 a2 a0 a3 a4 a2 a5 a1 a6 a7 a8 a6 a8 a9 a8 a13 a11 Figure 3: An example of how dependency trees interact with ITGs.'
'The Bloomier filter LM <CIT> has a precomputed matching of keys shared between a constant number of cells in the filter array '
'From multilingual texts , translation lexica can be generated <OTH> '
'There are several basic methods for evaluating associations between words : based on frequency counts <OTH> , information theoretic <CIT> and statistical significance <OTH> '
'For -LRB- 1 -RRB- , the morphemes and labels for our task are : -LRB- 2 -RRB- kita NEG tINC inE1S chabe VT - j SC laj PREP inA1S yol S - j SC iin PRON We also consider POS-tagging for Danish , Dutch , English , and Swedish ; the English is from sections 00-05 -LRB- as training set -RRB- and 19-21 -LRB- as development set -RRB- of the Penn Treebank <CIT> , and the other languages are from the CoNLL-X dependency parsing shared task <OTH> 1 We split the original training data into training and development sets '
'Almost all recent work in developing automatically trained part-of-speech taggers has been on further exploring Markovmodel based tagging <OTH> '
'The evaluation metric is casesensitive BLEU-4 <CIT> '
'The first of these nonstructural problems with Model 1 , as standardly trained , is that rare words in the source language tend to act as garbage collectors <CIT> , aligning to too many words in the target language '
'42 Interpreting reliability results It has been argued elsewhere <CIT> that since the amount of agreement one would expect by chance depends on the number and relative frequencies of the categories under test , reliability for category classifications should be measured using the kappa coefficient '
'31 Exhaustive search by tree fragments This method generates all possible tree fragments rooted by each node in the source parse tree or forest , and then matches all the generated tree fragments against the source parts -LRB- left hand side -RRB- of translation rules to extract the useful rules <CIT> '
'The prime public domain examples of such implementations include the TrigramsnTags tagger <OTH> , Xerox tagger <CIT> and LT POS tagger <OTH> '
'The coreference resolution system employs a variety of lexical , semantic , distance and syntactic feature <CIT> '
'4 Experiments The Penn Treebank <CIT> is used as the testing corpus '
'24 Syntactic Similarity We have incorporated , with minor modifications , some of the syntactic metrics described by <CIT> and Amigo et al '
'CIT -RRB- '
'There have been many studies on POS guessing of unknown words <OTH> '
'-LRB- levelopment of cor1 -RRB- ora with morl -RRB- ho-synta -LRB- : ti -LRB- : and syntacti -LRB- : mmotation <CIT> , <OTH> '
'Reference-based metrics such as BLEU <CIT> have rephrased this subjective task as a somewhat more objective question : how closely does the translation resemble sentences that are known to be good translations for the same source ? '
'This method uses mutual information and loglikelihood , which <CIT> used to calculate the dependency value between words '
'In both cases there 1Alternatively , decisions from the sentence classifier can guide which input is seen by the document level classifier <CIT> '
'We use BLEU scores <CIT> to measure translation accuracy '
'To search for the most probable parse , we use the heuristic search algorithm described in <CIT> , which is a form of beam search '
'Maximum entropy -LRB- ME -RRB- models have been used in bilingual sense disambiguation , word reordering , and sentence segmentation <CIT> , parsing , POS tagging and PP attachment <OTH> , machine translation <OTH> , and FrameNet classification <OTH> '
'Furthermore , these systems have tackled the problem at different levels of granularity , from the document level <CIT> , sentence level <CIT> , phrase level <OTH> , as well as the speaker level in debates <OTH> '
'ca <OTH> and <CIT> , in mining relationships between named entities , or in extracting useful facet terms from news articles <OTH> '
'The translation quality is measured by three MT evaluation metrics : TER <OTH> , BLEU <CIT> , and METEOR <OTH> '
'Research have also been made into alternatives to the current log-linear scoring model such as discriminative models with millions of features <CIT> , or kernel based models <OTH> '
'It has been implemented in the TACITUS System <OTH> and has been applied to several varieties of text '
'<CIT> also includes a brief discussion of crossing constraints that can be derived from phrase structure correspondences '
'Various methods <CIT> have been proposed for synonym acquisition '
'Acknowledgments I want to thank my fellow organizers of the shared task , Johan Hall , Sandra Kubler , Ryan <CIT> , Jens Nilsson , Sebastian Riedel , and Deniz Yuret , whoarealsoco-authorsofthelongerpaperonwhich this paper is partly based <OTH> '
'We retrained the parser on lowercased Penn Treebank II <CIT> , to match the lowercased output of the MT decoder '
'The senses are : 1 material from cellulose 2 report 3 publication 4 medium for writing 5 scientific 6 publishing firm 7 physical object inventory is suitable for which application , other than cross-lingual applications where the inventory can be determined from parallel data <CIT> '
'Mutual information MI -LRB- x , y -RRB- is defined as following <CIT> : -RRB- -LRB- -RRB- -LRB- -RRB- , -LRB- log -RRB- -LRB- -RRB- -LRB- -RRB- , -LRB- log -RRB- , -LRB- 22 yfxf yxfN ypxp yxp yxMI = = -LRB- 4 -RRB- where f -LRB- x -RRB- and f -LRB- y -RRB- are frequency of term x and term y , respectively '
'Pr -LRB- pi , F , A -RRB- = summationdisplay i , c -LRB- -RRB- = -LRB- pi , F , A -RRB- productdisplay rji p -LRB- rj -RRB- -LRB- 4 -RRB- In order to acquire the rules specific to our model and to induce their probabilities , we parse the English side of our corpus with an in-house implementation <OTH> of Collins parsing models <OTH> and we word-align the parallel corpus with the Giza + +2 implementation of the IBM models <CIT> '
'The data sets used are the standard data sets for this problem <OTH> taken from the Wall Street Journal corpus in the Penn Treebank <CIT> '
'We achieve competitive performance in comparison to alternate model families , in particular generative models such as MRFs trained with EM <CIT> and HMMs trained with soft constraints <OTH> '
'23 ITG Constraints The Inversion Transduction Grammar -LRB- ITG -RRB- <CIT> , a derivative of the Syntax Directed Transduction Grammars <OTH> , constrains the possible permutations of the input string by defining rewrite rules that indicate permutations of the string '
'Following <CIT> , 765 Feature Sets Templates Error % A Ratnaparkhis 305 B A + -LRB- t0 , t1 -RRB- , -LRB- t0 , t1 , t1 -RRB- , -LRB- t0 , t1 , t2 -RRB- 292 C B + -LRB- t0 , t2 -RRB- , -LRB- t0 , t2 -RRB- , -LRB- t0 , t2 , w0 -RRB- , -LRB- t0 , t1 , w0 -RRB- , -LRB- t0 , t1 , w0 -RRB- , -LRB- t0 , t2 , w0 -RRB- , -LRB- t0 , t2 , t1 , w0 -RRB- , -LRB- t0 , t1 , t1 , w0 -RRB- , -LRB- t0 , t1 , t2 , w0 -RRB- 284 D C + -LRB- t0 , w1 , w0 -RRB- , -LRB- t0 , w1 , w0 -RRB- 278 E D + -LRB- t0 , X = prefix or suffix of w0 -RRB- ,4 -LRB- X 9 272 Table 2 : Experiments on the development data with beam width of 3 we cut the PTB into the training , development and test sets as shown in Table 1 '
'Another current topic of machine translation is automatic evaluation of MT quality <CIT> '
'This model can be seen as an extension of the standard Maximum Entropy Markov Model -LRB- MEMM , see <CIT> -RRB- with an extra dependency on the predicate label , we will henceforth refer to this model as MEMM + pred '
'Obviously , all these semantic resources have been acquiredusing a very differentset of processes <CIT> , tools and corpora '
'Following <OTH> and other work on general-purpose generators , we adopt BLEU score <CIT> , average simple string accuracy -LRB- SSA -RRB- and percentage of exactly matched sentences for accuracy evaluation6 For coverage evaluation , we measure the percentage of input fstructures that generate a sentence '
'Therefore , including a model based on surface forms , as suggested <CIT> , is also necessary '
'It has shown promise in improving the performance of many tasks such as name tagging <OTH> , semantic class extraction <OTH> , chunking <OTH> , coreference resolution <CIT> and text classification <OTH> '
'<CIT> argue for CL on grounds of accuracy , but see also Johnson -LRB- 2001 -RRB- '
'We evaluated the translation quality using the BLEU metric <CIT> , as calculated by mteval-v11bpl with its default setting except that we used case-sensitive matching of n-grams '
'We should note , however , that most other stochastic parsers do include counts of single nonheadwords : they appear in the backed-off statistics of these parsers <CIT> '
'Our method is based on the ones described in <CIT> , The objective of this paper is to dynamically rank speakers or participants in a discussion '
'2 Incremental Parsing This section gives a description of Collins and Roarks incremental parser <CIT> and discusses its problem '
'Hockenmaier et al <OTH> , although to some extent following the approach of Xia <OTH> where LTAGs are extracted , have pursued an alternative by extracting Combinatory Categorial Grammar -LRB- CCG -RRB- <OTH> lexicons from the Penn Treebank '
'The collocations have been calculated according to the method described in <CIT> by moving a window on the texts '
'<CIT> first introduced an iterative method for increasing a small set of seed data used to disambiguate dual word senses by exploiting the constraint that in a segment of discourse only one sense of a word is used '
'4 Using vector-based models of semantic representation to account for the systematic variances in neural activity 41 Lexical Semantic Representation Computational linguists have demonstrated that a words meaning is captured to some extent by the distribution of words and phrases with which it commonly co-occurs <CIT> '
'The Stanford parser is representative of a large number of PTB parsers , exemplified by <CIT> and Charniak -LRB- 2000 -RRB- '
'23 Previous Randomized LMs Recent work <CIT> has used lossy encodings based on Bloom filters <OTH> to represent logarithmically quantized corpus statistics for language modeling '
'The disambiguation model of Enju is based on a feature forest model <OTH> , which is a log-linear model <CIT> on packed forest structure '
'All formats 2The data described in (Ramshaw and Marcus, 1995) is available from ftp://ftp.cis.upenn.edu/pub/chunker/ 175 Proceedings of EACL ''99 word/POS context chunk tag context IOB1 L=2/R=I IOB2 L--2/R=I IOE1 L=I/R=2 IOE2 L=I/R=2 [ +] L=2/R=I + L=0/R=2 [ + IO L=2/R=0 + L=I/R=I IO +] L=I/R=I+L=0/R=2 F~=I 1/2 90.12 1/0 89.30 1/2 89.55 0/1 89.73 0/0 + 0/0 89.32 0/0 + I/I 89.78 1/1 + 0/0 89.86 Table 3: Results second experiment series: the best F~=I scores for different left (L) and right (R) chunk tag context sizes for the seven representation formats using 5-fold cross-validation on section 15 of the WSJ corpus.'
'Part-of-Speech -LRB- POS -RRB- annotation for example can be seen as the task of choosing the appropriate tag for a word from an ontology of word categories -LRB- compare for example the Penn Treebank POS tagset as described in <CIT> -RRB- '
'We have also implemented a Bloom Filter LM in Joshua , following <CIT> '
'pointwise mutual information <CIT> , 3 '
'72 Minimum-Risk Training Adjusting or changes the distribution p Minimum error rate training -LRB- MERT -RRB- <CIT> tries to tune to minimize the BLEU loss of a decoder that chooses the most probable output according to p '
'We use a hand-written competence grammar , combined with performance-driven disambiguation obtained from the Penn Treebank <CIT> '
'In Section 2 , we examine aggregate Markov models , or class-based bigram models <CIT> in which the mapping from words to classes 81 is probabilistic '
'For example , in the context of syntactic disambiguation , Black <OTH> and Magerman <OTH> proposed statistical parsing models based-on decisiontree learning techniques , which incorporated not only syntactic but also lexical\/semantic information in the decision-trees '
'We propose a method similar to <CIT> to generalize beyond the training set '
'SMT has evolved from the original word-based approach <CIT> into phrase-based approaches <OTH> and syntax-based approaches <OTH> '
'So we will engineer more such features , especially with lexicalization and soft alignments <CIT> , and study the impact of alignment quality on parsing improvement '
'<OTH> , the BBN parser builds augmented parse trees according to a process similar to that described in <CIT> '
'Starting with bilingualphrasepairsextractedfromautomatically aligned parallel text <CIT> , these PSCFG approaches augment each contiguous -LRB- in source and target words -RRB- phrase pair with a left-hand-side symbol -LRB- like the VP in the example above -RRB- , and perform a generalization procedure to form rules that include nonterminal symbols '
'The current approach does not use specialized probability features as in <CIT> in any stage during decoder parameter training '
'The simplest model of compound noun disambiguation compares the frequencies of the two competing analyses and opts for the most frequent one (Pustejovsky et al. , Model Alta BNC Baseline 63.93 63.93 f (n1;n2) : f (n2;n3) 77.86 66.39 f (n1;n2) : f (n1;n3) 78.68# 65.57 f (n1;n2)= f (n1) : f (n2;n3)= f (n2) 68.85 65.57 f (n1;n2)= f (n2) : f (n2;n3)= f (n3) 70.49 63.11 f (n1;n2)= f (n2) : f (n1;n3)= f (n3) 80.32 66.39 f (n1;n2) : f (n2;n3) (NEAR) 68.03 63.11 f (n1;n2) : f (n1;n3) (NEAR) 71.31 67.21 f (n1;n2)= f (n1) : f (n2;n3)= f (n2) (NEAR) 61.47 62.29 f (n1;n2)= f (n2) : f (n2;n3)= f (n3) (NEAR) 65.57 57.37 f (n1;n2)= f (n2) : f (n1;n3)= f (n3) (NEAR) 75.40 68.03# Table 8: Performance of Altavista counts and BNC counts for compound bracketing (data from Lauer 1995) Model Accuracy Baseline 63.93 Best BNC 68.036  Lauer (1995): adjacency 68.90 Lauer (1995): dependency 77.50 Best Altavista 78.686  Lauer (1995): tuned 80.70 Upper bound 81.50 Table 9: Performance comparison with the literature for compound bracketing 1993).'
'It was also included in the DUC 2004 evaluation plan where summary quality was automatically judged using a set of n-gram word overlap metrics called ROUGE <CIT> '
'In a test set containing 26 repairs Dowding et al 1993 , they obtained a detection recall rate of 42 % with a precision of 85 % , and a correction recall rate of 31 % with a precision of 62 % '
'A la <CIT> , they represent the words as a sequence of labeled words with IOB annotations , where the B marks a word at the beginning of a chunk , I marks a word inside a chunk , and O marks those words -LRB- and punctuation -RRB- that are outside chunks '
'Since we also adopt a linear scoring function in Equation -LRB- 3 -RRB- , the feature weights of our combination model can also be tuned on a development data set to optimize the specified evaluation metrics using the standard Minimum Error Rate Training -LRB- MERT -RRB- algorithm <CIT> '
'Based on annotation differences in the datasets <CIT> and a bug in their system <OTH> , their results are inconclusive '
'While this approach exploits only syntactic and lexical information , <CIT> also rely on cohesion information , derived from word distribution in a text : Phrases that are linked to a local context are retained , while phrases that have no such links are dropped '
'In the rest of the paper we use the following notation , adapted from <CIT> '
'The TRIPS structure generally has more levels of structure -LRB- roughly corresponding to levels in X-bar theory -RRB- than the Penn Treebank analyses <CIT> , in particular for base noun phrases '
'Independently , in artificial intelligence an effort arose to encode large amounts of commonsense knowledge <CIT> '
'We extracted all examples of each word from the 14-million-word English portion of the Hansards8 Note that this is considerably smaller than <CIT> corpus of 460 million words , so bootstrapping will not perform as well , and may be more sensitive to the choice of seed '
'Since one of these filters restricts the number of nonterminal symbols to two , our extracted grammar is equivalent to an inversion transduction grammar <CIT> '
'More recently , there have been many proposals to introduce syntactic knowledge into SMT models <CIT> '
'Sentiment classification at the document level investigates ways to classify each evaluative document -LRB- eg , product review -RRB- as positive or negative <CIT> '
'For example , the feature 1 On the ATR English Grammar , see below ; for a detailed description of a precursor to the Gr-r ~ raar , see <OTH> '
'The supervised training described in <CIT> uses manually annotated data for the estimation of the weight coefficients '
'4 Training This section discusses how to extract our translation rules given a triple nullnull , null null , nullnull As we know , the traditional tree-to-string rules can be easily extracted from nullnull , null null , nullnull using the algorithm of Mi and Huang <OTH> 2 We would like 2 Mi and Huang <OTH> extend the tree-based rule extraction algorithm <CIT> to forest-based by introducing non-deterministic mechanism '
'We use evaluations similar to those used before <CIT> '
'Syntactic Score -LRB- SC -RRB- Some erroneous sentences often contain words and concepts that are locally correct but can not form coherent sentences <CIT> '
'44 Corpora We ran the three syntactic preprocessors over a total of three corpora , of varying size : the Brown corpus -LRB- 460K tokens -RRB- and Wall Street Journal corpus -LRB- 12M tokens -RRB- , both derived from the Penn Treebank <CIT> , and the written component of the British National Corpus -LRB- 98M tokens : Burnard <OTH> -RRB- '
'We briefly describe the tagger -LRB- see <OTH> for more details -RRB- , a Hidden Markov Model trained with the perceptron algorithm introduced in <CIT> '
'3 Data The data consists of sections of the Wall Street Journal part of the Penn TreeBank <CIT> , with information on predicate-argument structures extracted from the PropBank corpus <OTH> '
'The Kappa statistic <CIT> is typically used to measure the human interrater agreement '
'Method Prec Rec F-measure GIZA + + Intersect 967 530 685 GIZA + + Union 825 690 751 GIZA + + GDF 840 682 752 Phrasal ITG 507 803 622 Phrasal ITG + NCC 754 780 767 Following the lead of <CIT> , we hand-aligned the first 100 sentence pairs of our training set according to the Blinker annotation guidelines <OTH> '
'Motivated by the fact that non-syntactic phrases make non-trivial contribution to phrase-based SMT , the tree sequencebased translation model is proposed <CIT> that uses tree sequence as the basic translation unit , rather than using single sub-tree as in the STSG '
'Recent work <CIT> explored the task of part-of-speech tagging -LRB- PoS -RRB- using unsupervised Hidden Markov Models -LRB- HMMs -RRB- with encouraging results '
'<CIT> trained their feature set using an online discriminative algorithm '
'We evaluated the translation quality using case-insensitive BLEU metric <CIT> '
'To generate phrase pairs from a parallel corpus , we use the ` diag-and '' phrase induction algorithm described in <CIT> , with symmetrized word alignments generated using IBM model 2 <OTH> '
'At one extreme are those , exemplified by that of <CIT> , that have no dependence on syntactic theory beyond the idea that natural language is hierarchical '
'We refer to a3a16a5a7 as the source language string and a10 a11a7 as the target language string in accordance with the noisy channel terminology used in the IBM models of <CIT> '
'<CIT> proposed Model 6 , a log-linear combination of IBM translation models and HMM model '
'For instance , we may find metrics which compute similarities over shallow syntactic structures\/sequences <OTH> , constituency trees <CIT> and dependency trees <CIT> '
'c2009 Association for Computational Linguistics Semi-supervised Training for the Averaged Perceptron POS Tagger Drahomra johanka Spoustova Jan Hajic Jan Raab Miroslav Spousta Institute of Formal and Applied Linguistics Faculty of Mathematics and Physics, Charles University Prague, Czech Republic {johanka,hajic,raab,spousta}@ ufal.mff.cuni.cz Abstract This paper describes POS tagging experiments with semi-supervised training as an extension to the (supervised) averaged perceptron algorithm, first introduced for this task by (Collins, 2002).'
'1 Introduction A recent development in data-driven parsing is the use of discriminative training methods <CIT> '
'<CIT> invented heuristic symmetriza57 FRENCH\/ENGLISH ARABIC\/ENGLISH SYSTEM F-MEASURE -LRB- = 04 -RRB- BLEU F-MEASURE -LRB- = 01 -RRB- BLEU GIZA + + 735 3063 758 5155 <OTH> 741 3140 791 5289 LEAF UNSUPERVISED 745 723 LEAF SEMI-SUPERVISED 763 3186 845 5434 Table 3 : Experimental Results tion of the output of a 1-to-N model and a M-to-1 model resulting in a M-to-N alignment , this was extended in <OTH> '
'Several algorithms have been proposed in the literature that try to find the best splits , see for instance <CIT> '
'4 Related Work <OTH> and <CIT> tackle the problem of segmenting Chinese while aligning it to English '
'Syntagmatic strategies for determining similarity have often been based on statistical analyses of large corpora that yield clusters of words occurring in similar bigram and trigram contexts <CIT> , as well as in similar predicateargument structure contexts <OTH> '
'<CIT> developed a training procedure that incorporates various MT evaluation criteria in the training procedure of log-linear MT models '
'Previous studies <OTH> defined a probabilistic model of unification-based grammars including HPSG as a log-linear model or maximum entropy model <CIT> '
'The elements of this set are pairs -LRB- x , y -RRB- where y is a possible translation for x 4 IBMs model 1 IBMs model 1 is the simplest of a hierarchy of five statistical models introduced in <CIT> '
'For this paper , we train the parameter vector using the perceptron algorithm <CIT> '
'In the supervised condition , we used just 2 additional task instances , plant and tank , each with 4000 handannotated instances drawn from a large balanced corpus <CIT> '
'MI is defined in general as follows : y -RRB- I ix y -RRB- = log2 P -LRB- x -RRB- P -LRB- y -RRB- We can use this definition to derive an estimate of the connectedness between words , in terms of collocations <CIT> , but also in terms of phrases and grammatical relations <OTH> '
'Future work will include : -LRB- i -RRB- applying the method to retrieve other types of collocations <CIT> , and -LRB- ii -RRB- evaluating the method using Internet directories '
'1 Introduction and Motivation Detecting contradictory statements is an important and challenging NLP task with a wide range of potential applications including analysis of political discourse , of scientific literature , and more <CIT> '
'Performance is also measured by the BLEU score <CIT> , which measures similarity to the reference translation taken from the English side of the parallel corpus '
'Some researchers <OTH> use manually designed rules to take into account the grammatical role of the antecedent candidates as well as the governing relations between the candidate and the pronoun , while others use features determined over the parse tree in a machine-learning approach <CIT> '
'41 Extraction from Definition Sentences Definition sentences in the Wikipedia article were used for acquiring hyponymy relations by <CIT> for named entity recognition '
'1 Introduction Word alignment was first proposed as an intermediate result of statistical machine translation <CIT> '
'As a model learning method , we adopt the maximum entropy model learning method <CIT> '
'1 Introduction Recent work in learning semantics has focused on mapping sentences to meaning representations -LRB- eg , some logical form -RRB- given aligned sentence\/meaning pairs as training data <CIT> '
'3 MaltParser MaltParser <OTH> is a languageindependent system for data-driven dependency parsing , based on a transition-based parsing model <CIT> '
'Following recent research about disambiguation models on linguistic grammars <OTH> , we apply a log-linear model or maximum entropy model <CIT> on HPSG derivations '
'Extensions to Hiero Several authors describe extensions to Hiero , to incorporate additional syntactic information <CIT> , or to combine it with discriminative latent models <OTH> '
'Daume III <CIT> divided features into three classes : domainindependent features , source-domain features and target-domain features '
'The algorithm we implemented is inspired by the work of <CIT> on word sense disambiguation '
'There have been considerable amount of efforts to improve the reordering model in SMT systems , ranging from the fundamental distance-based distortion model <CIT> , flat reordering model <OTH> , to lexicalized reordering model <OTH> , hierarchical phrase-based model <OTH> , and maximum entropy-based phrase reordering model <OTH> '
'MT output was evaluated using the standard evaluation metric BLEU <CIT> 2 The parameters of the MT System were optimized for BLEU metric on NIST MTEval2002 test sets using minimum error rate training <OTH> , and the systems were tested on NIST MTEval2003 test sets for both languages '
'We thus propose to adapt the statistical machine translation model <CIT> for SMS text normalization '
'These linguistically-motivated trimming rules <OTH> iteratively remove constituents until a desired sentence compression rate is reached '
'The extension of dynamic SBNs with incrementally specified model structure -LRB- ie Incremental Sigmoid Belief Networks , used in this paper -RRB- was proposed and applied to constituent parsing in <CIT> '
'-LRB- In our experiments , we use maximum entropy classification -LRB- MaxEnt -RRB- <CIT> to train this probability model -RRB- '
'<OTH> show that treating U + as a source for a new feature function in a loglinear model for SMT <CIT> allows us to maximally take advantage of unlabeled data by finding a weight for this feature using minimum error-rate training -LRB- MERT -RRB- <CIT> '
'The second voting model , a maximum entropy model <OTH> , was built as <CIT> found that it yielded higher accuracy than nave Bayes in a subsequent comparison of WSD performance '
'We can sum over all non-projective spanning trees by taking the determinant of the Kirchhoff matrix of the graph defined above , minus the row and column corresponding to the root node <CIT> '
'The weights are then averaged across all iterations of the perceptron , as in <CIT> '
'Note that it is straightforward to calculate these expected counts using a variant of the inside-outside algorithm <OTH> applied to the <OTH> dependency-parsing data structures <OTH> for projective dependency structures , or the matrix-tree theorem <CIT> for nonprojective dependency structures '
'We use minimum error rate training <CIT> to tune the feature weights for the log-linear model '
'For the first two tasks , all heuristics of the Pharaoh-Toolkit <CIT> as well as the refined heuristic <OTH> to combine both IBM4-alignments were tested and the best ones are shown in the tables '
'The decoder uses a binarized representation of the rules , which is obtained via a syncronous binarization procedure <CIT> '
'This kind of synchronizer stands in contrast to more ad-hoc approaches <OTH> '
'Sentiment classification is a well studied problem <CIT> and in many domains users explicitly 1We use the term aspect to denote properties of an object that can be rated by a user as in Snyder and Barzilay <OTH> '
'Every sentence was part-of-speech tagged using a maximum entropy tagger <CIT> and parsed using a state-of-the-art wide coverage phrase structure parser <OTH> '
'In each case the input to the network is a sequence of tag-word pairs5 5We used a publicly available tagger <CIT> to provide the tags '
'The target set is built using the 88-89 Wall Street Journal Corpus (WSJ) tagged using the (Ratnaparkhi, 1996) tagger and the (Bangalore & Joshi, 1999) SuperTagger; the feedback sets are built using WSJ sentences con330 Algorithm 1 KE-train: (Karov & Edelman, 1998) algorithm adapted to literal/nonliteral classification Require: S: the set of sentences containing the target word Require: L: the set of literal seed sentences Require: N: the set of nonliteral seed sentences Require: W: the set of words/features, w  s means w is in sentence s, s owner w means s contains w Require: epsilon1: threshold that determines the stopping condition 1: w-sim0(wx,wy) := 1 if wx = wy,0 otherwise 2: s-simI0(sx,sy) := 1, for all sx,sy  S S where sx = sy, 0 otherwise 3: i := 0 4: while (true) do 5: s-simLi+1(sx,sy) := summationtextwxsx p(wx,sx)maxwysy w-simi(wx,wy), for all sx,sy  S L 6: s-simNi+1(sx,sy) := summationtextwxsx p(wx,sx)maxwysy w-simi(wx,wy), for all sx,sy  S N 7: for wx,wy  W W do 8: w-simi+1(wx,wy) := braceleftBigg i = 0 summationtextsxownerwx p(wx,sx)maxsyownerwy s-simIi(sx,sy) else summationtextsxownerwx p(wx,sx)maxsyownerwys-simLi (sx,sy),s-simNi (sx,sy)} 9: end for 10: if wx,maxwyw-simi+1(wx,wy)w-simi(wx,wy)}  epsilon1 then 11: break # algorithm converges in 1epsilon1 steps.'
'Translation accuracy is measured in terms of the BLEU score <CIT> , which is computed here for translations generated by using the tuple n-gram model alone , in the case of Table 2 , and by using the tuple n-gram model along with the additional four feature functions described in Section 32 , in the case of Table 3 '
'Such an approach contrasts with the log-linear HMM\/Model -4 combination proposed by <CIT> '
'4 Dependency Parsing : Baseline 41 Learning Model and Features According to <CIT> , all data-driven models for dependency parsing that have been proposed in recent years can be described as either graph-based or transition-based '
'They may rely only on this information -LRB- eg , <OTH> -RRB- , or they may combine it with additional information as well -LRB- eg , <CIT> -RRB- '
'The learning methods using in discriminative parsing are Perceptron <CIT> and online large-margin learning -LRB- MIRA -RRB- <OTH> '
'As an alternative , <CIT> describes a forest-based reranking algorithm called cube growing , which also employs beam search , but focuses computation only where necessary in a top-down pass through a parse forest '
'<CIT> apply entropy regularization to dependency parsing '
'In information retrieval , word similarity can be used to identify terms for pseudo-relevance feedback <OTH> '
'One such relational reasoning task is the problem of compound noun interpretation , which has received a great deal of attention in recent years <CIT> '
'A large corpus is vahmble as a source of such nouns <CIT> '
'Because our system uses a synchronous CFG , it could be thought of as an example of syntax-based statistical machine translation -LRB- MT -RRB- , joining a line of research <CIT> that has been fruitful but has not previously produced systems that can compete with phrase-based systems in large-scale translation tasks such as the evaluations held by NIST '
'-LRB- ~ -RRB- 1995 Association for Computational Linguistics Computational Linguistics Volume 21 , Number 2 and Mancini 1991 ; Meteer , Schwartz , and Weischedel 1991 ; Merialdo 1991 ; Pelillo , Moro , and Refice 1992 ; Weischedel et al 1993 ; Wothke et al 1993 -RRB- '
'<OTH> propose a MaxEnt-based reordering model for BTG <CIT> while Setiawan et al '
'One major focus is sentiment classification and opinion mining <CIT> 2008 '
'It will also be relevant to apply advanced statistical models that can incorporate various useful information to this task , eg , the maximum entropy model <CIT> '
'Some methods parse two flat strings at once using a bitext grammar <CIT> '
'Table 2 shows the unknown word tags for chunking , which are known as the IOB2 model <CIT> '
'<OTH> and compare with results reported by HK06 <CIT> and CRR07 <OTH> '
'The frequency counts of dependency relationships are filtered with the loglikelihood ratio <CIT> '
'We propose a probabilistic quasi-synchronous grammar , inspired by one proposed for machine translation <CIT> , and parameterized by mixtures of a robust nonlexical syntax\/alignment model with a -LRB- n optional -RRB- lexical-semantics-drivenlog-linear model '
'Similarly , <CIT> used Wikipedia , particularly the first sentence of each article , to create lists of entities '
'Additionally , some research has explored cutting and pasting segments of text from the full document to generate a summary <CIT> '
'918 English For English we used the Wall Street Journal section of the Penn Treebank <CIT> '
'<OTH> that draws on a stochastic tagger -LRB- see <CIT> for details -RRB- as well as the SPECIALIST Lexicon5 , a large syntactic lexicon of both general and medical English that is distributed with the UMLS '
'8 Related Research Class-based LMs <CIT> or factored LMs <OTH> are very similar to our T+C scenario '
'This second source of evidence is sometimes referred to as distributional similarity <CIT> '
'Co-occurrence statistics is collected from either bilingual parallel and 334 non-parallel corpora <OTH> , or monolingual corpora <CIT> '
'<CIT> apply entropy regularization to dependency parsing '
'When evaluated against the state-of-the-art, phrase-based decoder Pharaoh (Koehn, 2004), using the same experimental conditions  translation table trained on the FBIS corpus (7.2M Chinese words and 9.2M English words of parallel text), trigram language model trained on 155M words of English newswire, interpolation weights a65 (Equation 2) trained using discriminative training (Och, 2003) (on the 2002 NIST MT evaluation set), probabilistic beam a90 set to 0.01, histogram beam a58 set to 10  and BLEU (Papineni et al. , 2002) as our metric, the WIDL-NGLM-Aa86 a129 algorithm produces translations that have a BLEU score of 0.2570, while Pharaoh translations have a BLEU score of 0.2635.'
'For scoring MT outputs , the proposed RSCM uses a score based on a translation model called IBM4 <CIT> -LRB- TM-score -RRB- and a score based on a language model for the translation target language -LRB- LM-score -RRB- '
'<CIT> -RRB- , in which translation and language models are trainable separately too '
'It is based on code and ideas from the system of <CIT> , but also includes some ideas from GUITAR <OTH> and other coreference systems <OTH> '
'However , union and rened alignments , which are many-to-many , are what are used to build competitive phrasal SMT systems , because intersection performs poorly , despite having been shown to have the best AER scores for the French\/English corpus we are using <CIT> '
'The Penn Wall Street Journal treebank <CIT> was used as training and test data '
'Maximum Entropy models have been used to express the interactions among multiple feature variables -LRB- eg , <CIT> -RRB- , but within this framework no systematic study of interactions has been proposed '
'The final SMT system performance is evaluated on a uncased test set of 3071 sentences using the BLEU <OTH> , NIST <OTH> and METEOR <CIT> scores '
'We also report on applying Factored Translation Models <CIT> for English-to-Arabic translation '
'We repeat Ramshaw and Marcus Transformation Based NP chunking <CIT> algorithm by substituting supertags for POS tags in the dataset '
'These techniques included unweighted FS morphology , conditional random fields <OTH> , synchronous parsers <CIT> , lexicalized parsers <OTH> ,22 partially supervised training ` a la <OTH> ,23 and grammar induction <OTH> '
'We ran each estimator with the eight different combinations of values for the hyperparameters and prime listed below , which include the optimal values for the hyperparameters found by <CIT> , and report results for the best combination for each estimator below 1 '
'3 Domain Adaptation Following <CIT> , we present an application of structural correspondence learning -LRB- SCL -RRB- to non-projective dependency parsing <OTH> '
'For the identification and labeling steps , we train a maximum entropy classifier <CIT> over sections 02-21 of a version of the CCGbank corpus <OTH> that has been augmented by projecting the Propbank semantic annotations <OTH> '
'Evaluation 81 Effects of Unpublished Details In this section we present the results of effectively doing a clean-room implementation of Collins parsing model , that is , using only information available in <CIT> , as shown in Table 4 '
'We contrast our work with <CIT> , highlight some severe limitations of probability estimates computed from single derivations , and demonstrate that it is critical to account for many derivations for each sentence pair '
'The model scaling factors are optimized using minimum error rate training <CIT> '
'We compare TERp with BLEU <CIT> , METEOR <OTH> , and TER <OTH> '
'<OTH> regarded MWE as connected collocations : a sequence of neighboring words whose exact meaning can not be derived from the meaning or connotation of its components , which means that MWEs also have low ST As some pioneers provide MWE identiflcation methods which are based on association metrics -LRB- AM -RRB- , such as likelihood ratio <CIT> '
'More recently , the problem has been tackled using unsupervised -LRB- eg , <CIT> -RRB- and supervised -LRB- eg , Evans -LRB- 2001 -RRB- , Ng and Cardie -LRB- 2002a -RRB- -RRB- approaches '
'a2 Maximum-entropy method The maximum-entropy method is useful with sparse data conditions and has been used by many researchers <CIT> '
'For this reason there is currently a great deal of interest in methods which incorporate syntactic information within statistical machine translation systems -LRB- eg , see <CIT> -RRB- '
'For that purpose , syntactical <OTH> , statistical <CIT> and hybrid syntaxicostatistical methodologies <OTH> have been proposed '
'But it is close to the paradigm described by Yarowsky <OTH> and <CIT> as it also employs self-training based on a relatively small seed data set which is incrementally enlarged with unlabelled samples '
'The corpus consists of sections 15-18 and section 20 of the Penn Treebank <CIT> , and is pre-divided into a 8936-sentence -LRB- 211727 tokens -RRB- training set and a 2012-sentence <OTH> test set '
'Here , we extract part-of-speech tags from the Collins parsers output <CIT> for section 23 instead of reinventing a tagger '
'46 Weakly-constrained algorithms In evaluation with ROUGE <CIT> , summaries are truncated to a target length K Yih et al -LRB- 2007 -RRB- usedastackdecodingwithaslightmodication , which allows the last sentence in a summary to be truncated to a target length '
'For English there are many POS taggers , employing machine learning techniques like transformation-based error-driven learning <OTH> , decision trees <OTH> , markov model <CIT> , maximum entropy methods <OTH> etc There are also taggers which are hybrid using both stochastic and rule-based approaches , such as CLAWS <OTH> '
'<CIT> used noun-verb syntactic relations , and Hatzivassiloglou and McKeown -LRB- 1993 -RRB- used coordinated adjective-adjective modifier pairs '
'Furthermore , training corpora for information extraction are typically annotated with domain-specific tags , in contrast to general-purpose annotations such as part-of-speech tags or noun-phrase bracketing <CIT> '
'However , in experiments in unsupervised POS tag learning using HMM structured models , <CIT> shows that VB is more effective than Gibbs sampling in approaching distributions that agree with the Zipfs law , which is prominent in natural languages '
'1 Introduction Maximum Entropy -LRB- ME -RRB- modeling has received a lot of attention in language modeling and natural language processing for the past few years <CIT> '
'32 Automatic ROUGE Evaluation ROUGE <CIT> measuresthen-grammatchbetween system generated summaries and human summaries '
'5To test the reliability of the annotation scheme , we had a subset of the data annotated by two annotators and found a satisfactory - agreement <CIT> of = 081 '
'We calculated the translation quality using Bleus modified n-gram precision metric <CIT> for n-grams of up to length four '
'22 Global Linear Models We follow the framework of <CIT> , recently applied to language modeling in Roark et al '
'In our experiments , we used the full parse output from Collins parser <CIT> , in which every non-terminal node is already annotated with head information '
'However , due to the computational issues with the voted perceptron , the averaged perceptron algorithm <CIT> is used instead '
'3 The data 31 The supervised data For English , we use the same data division of Penn Treebank -LRB- PTB -RRB- parsed section <OTH> as all of <OTH> , <OTH> , <OTH> and <CIT> do ; for details , see Table 1 '
'The translation quality is evaluated by BLEU metric <CIT> , as calculated by mteval-v11bpl 6 with case-sensitive matching of n-grams '
'52 Impact on translation quality As reported in Table 3 , small increases in METEOR <OTH> , BLEU <CIT> and NIST scores <OTH> suggest that SMT output matches the references better after postprocessing or decoding with the suggested lemma translations '
'138 2 Rule Generation We start with phrase translations on the parallel training data using the techniques and implementation described in <CIT> '
'Instead of interpolating the two language models , we explicitly used them in the decoder and optimized their weights via minimumerror-rate -LRB- MER -RRB- training <CIT> '
'We use the adaptation of this algorithm to structure prediction , first proposed by <CIT> '
'The flow using non-local features in two-stage architecture 24 Results We employ BIOE1 label scheme for the NER task because we found it performs better than IOB2 on Bakeoff 2006 <CIT> NER MSRA and CityU corpora '
'321 Jensen-Shannon divergence is defined as D -LRB- q , r -RRB- = 12 parenleftbigg D parenleftbigg q q + r2 parenrightbigg + D parenleftbigg r q + r2 parenrightbiggparenrightbigg These experiments are a kind of poor mans version of the deterministic annealing clustering algorithm <OTH> , which gradually increases the number of clusters during the clustering process '
'In fact, when the perceptron update rule of (Dekel et al. , 2004)  which modifies the weights of every divergent node along the predicted and true paths  is used in the ranking framework, it becomes virtually identical with the standard, flat, ranking perceptron of Collins (2002).5 In contrast, our approach shares the idea of (Cesa-Bianchi et al. , 2006a) that if a parent class has been predicted wrongly, then errors in the children should not be taken into account. We also view this as one of the key ideas of the incremental perceptron algorithm of (Collins and Roark, 2004), which searches through a complex decision space step-by-step and is immediately updated at the first wrong move.'
'The annotation can be considered reliable <OTH> with 95 % agreement and a kappa <CIT> of88 '
'Instead we report BLEU scores <CIT> of the machine translation system using different combinations of wordand classbased models for translation tasks from English to Arabic and Arabic to English '
'Collocations have been widely used for tasks such as word sense disambiguation -LRB- WSD -RRB- <CIT> , information extraction -LRB- IE -RRB- <OTH> , and named-entity recognition <OTH> '
'The POS tagger uses the same contextual predicates as <CIT> ; the supertagger adds contextual predicates corresponding to POS tags and bigram combinations of POS tags <OTH> '
'The principal training method is an adaptation of averaged perceptron learning as described by <CIT> '
'We optimize the model weights using a modified version of averaged perceptron learning as described by <CIT> '
'By habit , most systems for automatic role-semantic analysis have used Pennstyle constituents <OTH> produced by <CIT> or Charniaks -LRB- 2000 -RRB- parsers '
'We used the Berkeley Parser 2 to train such grammars on sections 2-21 of the Penn Treebank <CIT> '
'In <CIT> a standard phrase-based model is augmented with more than a million features whose weights are trained discriminatively by a variant of the perceptron algorithm '
'Despite the above differences , since the theorems of convergence and their proof <CIT> are only dependent on the feature vectors , and not on the source of the feature definitions , the perceptron algorithm is applicable to the training of our CWS model '
'It is appreciated that multi-sense words appearing in the same document tend to be tagged with the same word sense if they belong to the same common domain in the semantic hierarchy <CIT> '
'<CIT> describes a method of sentiment classification using two human-selected seed words -LRB- the words poor and excellent -RRB- in conjunction with a very large text corpus ; the semantic orientation of phrases is computed as their association with the seed words -LRB- as measured by pointwise mutual information -RRB- '
'24 GermanEnglish For GermanEnglish , we additionally incorporated rule-based reordering We parse the input using the Collins parser <CIT> and apply a set of reordering rules to re-arrange the German sentence so that it corresponds more closely English word order <CIT> '
'There is potential of developing Sense Definition Model to identify and represent semantic and stylistic differentiation reflected in the MRD glosses pointed out in DiMarco , Hirst and Stede <OTH> '
'On the same dataset as that of <OTH> , our new supertagger achieves an accuracy of a2a4a3a6a5a8a7a10a9a12a11 Compared with the supertaggers with the same decoding complexity <OTH> , our algorithm achieves an error reduction of a22a23a5a26a9a12a11 We repeat Ramshaw and Marcus Transformation Based NP chunking <CIT> test by substituting supertags for POS tags in the dataset '
'We shall take HMM-based word alignment model <OTH> as an example and follow the notation of <CIT> '
'The baseline score using all phrase pairs was 5911 <CIT> with a 95 % confidence interval of -LRB- 5713 , 6109 -RRB- '
'<CIT> et al 1996 presented a way of computing conditional maximum entropy models directly by modifying equation 6 as follows -LRB- now instead of w we will explicitly use -LRB- x , y -RRB- -RRB- : i ~ Cx ~ -RRB- = ~ f ~ -LRB- ~ , y -RRB- \* ~ -LRB- ~ , y -RRB- ~ ~ ~ -LRB- ~ , y -RRB- \* ~ -LRB- ~ -RRB- \* pCy I ~ -RRB- = p -LRB- xk -RRB- -LRB- 9 -RRB- x6X yEY xEX yEY where ~ -LRB- x , y -RRB- is an empirical probability of a joint configuration -LRB- w -RRB- of certain instantiated factor I variables with certain instantiated behavior variables '
'We also show that the domain adaptation work of <CIT> , which is presented as an ad-hoc preprocessing step , is actually equivalent to our formal model '
'Following the broad shift in the field from finite state transducers to grammar transducers <OTH> , recent approaches to phrase-based alignment have used synchronous grammar formalisms permitting polynomial time inference <CIT> '
'Within the NLP community , n-best list ranking has been looked at carefully in parsing , extractive summarization <CIT> , and machine translation <OTH> , to name a few '
'In the literature approaches to construction of taxonomies of concepts have been proposed <CIT> '
'-LRB- 1 -RRB- a I expected -LRB- nv the man who smoked NP -RRB- to eat ice-cream h I doubted -LRB- NP the man who liked to eat ice-cream NP -RRB- Current high-coverage parsers tend to use either custom , hand-generated lists of subcategorization frames <CIT> , or published , handgenerated lists like the Ozford Advanced Learner ''s Dictionary of Contemporary English , Hornby and Covey <OTH> <OTH> '
'The concept of mutual information , taken from information theory , was proposed as a measure of word association <OTH> '
'Here , we present experiments performed using two complex corpora , C1 and C2 , extracted from the Penn Treebank <CIT> '
'FollowingtheworkofKooetal <OTH> and <CIT> and <CIT> <OTH> , it is possible to compute all expectations in O -LRB- n3 + L n2 -RRB- through matrix inversion '
'To tune the decoder parameters , we conducted minimum error rate training <CIT> with respect to the word BLEU score <OTH> using 20K development sentence pairs '
'Agreement is sometimes measured as percentage of the cases on which the annotators agree , but more often expected agreement is taken into account in using the kappa statistic <CIT> , which is given by : = po pe1 p e -LRB- 1 -RRB- where po is the observed proportion of agreement and pe is the proportion of agreement expected by chance '
'In recent years , HMMs have enjoyed great success in many tagging applications , most notably part-of-speech -LRB- POS -RRB- tagging <OTH> and named entity recognition <OTH> '
'Inspired by the idea of graph based algorithms to collectively rank and select the best candidate , research efforts in the natural language community have applied graph-based approaches on keyword selection <CIT> , text summarization <CIT> , word sense disambiguation <CIT> , sentiment analysis <OTH> , and sentence retrieval for question answering <OTH> '
'The model can be seen as a bootstrapping learning process tbr disambiguation , where the information gained from one part -LRB- selectional preference -RRB- is used to improve tile other -LRB- disambiguation -RRB- and vice versa , reminiscent of the work by Riloff and Jones <OTH> and <CIT> '
'Rather than explicit annotation , we could use latent annotations to split the POS tags , similarly to the introduction of latent annotations to PCFG grammars <CIT> '
'Similarly , the sense disambiguation problem is typically attacked by comparing the distribution of the neighbors of a word ''s occurrence to prototypical distributions associated with each of the word ''s senses <OTH> '
'31 Generation using PHARAOH PHARAOH <CIT> is an SMT system that uses phrases as basic translation units '
'2 Related Work A number of researchers <CIT> have described approaches that preprocess the source language input in SMT systems '
'3 Stochastic Inversion Transduction Grammars Stochastic Inversion Transduction Grammars -LRB- SITGs -RRB- <CIT> can be viewed as a restricted subset of Stochastic Syntax-Directed Transduction Grammars '
'<OTH> argue that precise alignment can improve transliteration effectiveness , experimenting on English-Chinese data and comparing IBM models <CIT> with phonemebased alignments using direct probabilities '
'The proposed synchronous grammar is able to cover the previous proposed grammar based on tree <CIT> and tree sequence <CIT> alignment '
'2 The Problem of Coverage in SMT Statistical machine translation made considerable advances in translation quality with the introduction of phrase-based translation <CIT> '
'A similar soft projection of dependencies was used in supervised machine translation by <CIT> , who used a source sentences dependency paths to bias the generation of its translation '
'Recent computational work either focuses on sentence subjectivity <OTH> , concentrates just on explicit statements of evaluation , such as of films <CIT> , or focuses on just one aspect of opinion , eg , <OTH> on adjectives '
'First , a parsing-based approach attempts to recover partial parses from the parse chart when the input can not be parsed in its entirety due to noise , in order to construct a -LRB- partial -RRB- semantic representation <OTH> '
'For example , in our previous work <OTH> , we have used a statistical translation memory of phrases in conjunction with a statistical translation model <CIT> '
'<OTH> describe one application of MI to identify word collocations ; Kashioka et al '
'<OTH> and <CIT> , the specific technique we used by means of a context language model is rather different '
'1 Introduction Viewed at a very high level , statistical machine translationinvolvesfourphases : languageandtranslation model training , parameter tuning , decoding , and evaluation <CIT> '
'Tillmann and Zhang <OTH> , <CIT> et al '
'html -RRB- provided by Lynette Hirschman ; syntactic structures in the style of the Penn TreeBank <CIT> provided by Ann Taylor ; and an alternative annotation for the F0 aspects of prosody , known as Tilt <OTH> and provided by its inventor , Paul Taylor '
'DTM2, introduced in (Ittycheriah and Roukos, 2007), expresses the phrase-based translation task in a unified log-linear probabilistic framework consisting of three components: (i) a prior conditional distribution P0(.|S), (ii) a number of feature functions i() that capture the translation and language model effects, and (iii) the weights of the features i that are estimated under MaxEnt (Berger et al., 1996), as in (1): P(T|S) = P0(T,J|S)Z expsummationdisplay i ii(T,J,S) (1) Here J is the skip reordering factor for the phrase pair captured by i() and represents the jump from the previous source word, and Z is the per source sentence normalization term.'
'The only difference is that we 5See also work on partial parsing as a task in its own right : <CIT> inter alia '
'The ve part-ofspeech -LRB- POS -RRB- patterns from <CIT> were used for the extraction of indicators , all involving at least one adjective or adverb '
'<CIT> adapted the perceptron learning algorithm to tagging tasks , via sentence-based global feedback '
'Finally , it would be nice to merge some of the approaches by <OTH> and <CIT> with the ideas of semi-supervised learning introduced here , since they seem orthogonal in at least some aspects -LRB- eg , to replace the rudimentary lookahead features with full bidirectionality -RRB- '
'<OTH> and the English parser developed by <CIT> <OTH> '
'We implemented these models within an maximum entropy framework <CIT> '
'<OTH> -RRB- or Wikipedia <CIT> , and the contextual role played by an NP -LRB- see Bean and Riloff <OTH> -RRB- '
'SCISSOR is implemented by augmenting <CIT> head-driven parsing model II to incorporate the generation of semantic labels on internal nodes '
'We ran the trainer with its default settings -LRB- maximum phrase length 7 -RRB- , and then used Koehns implementation of minimumerror-rate training <CIT> to tune the feature weights to maximize the systems BLEU score on our development set , yielding the values shown in Table 2 '
'Err:510'
'240 2 Motivation Many approaches to identifying base noun phrases have been explored as part of chunking <CIT> , but determining sub-NP structure is rarely addressed '
'For colnparison ~ we refer here to <CIT> because this method and the proposed method have much in connnon '
'Many researchers have focused the related problem of predicting sentiment and opinion in text <CIT> , sometimes connected to extrinsic values like prediction markets <OTH> '
'While word and phrasal paraphrases can be assimilated to the well-studied notion of synonymy , sentencelevel paraphrasingis moredifficult to grasp and can not be equated with word-for-word or phrase-by-phrase substitution since it might entail changes in the structure of the sentence <CIT> '
'We used a feature set which included the current , next , and previous word ; the previous two tags ; various capitalization and other features of the word being tagged -LRB- the full feature set is described in <CIT> -RRB- '
'Clark and Curran <OTH> describe the supertagger , which uses log-linear models to define a distribution over the lexical category set for each local five-word context containing the target word <CIT> '
'1 Introduction Distributional Similarity has been an active research area for more than a decade <CIT> , <OTH> , <OTH> , <OTH> , <OTH> , <OTH> , <OTH> '
'Its applications range from sentence boundary disambiguation <OTH> to part-of-speech tagging <OTH> , parsing <OTH> and machine translation <CIT> '
'#Reference: If our player 2, 3, 7 or 5 has the ball and the ball is close to our goal line  PHARAOH++: If player 3 has the ball is in 2 5 the ball is in the area near our goal line  WASP1++: If players 2, 3, 7 and 5 has the ball and the ball is near our goal line  Figure 4: Sample partial system output in the ROBOCUP domain ROBOCUP GEOQUERY BLEU NIST BLEU NIST PHARAOH 0.3247 5.0263 0.2070 3.1478 WASP1 0.4357 5.4486 0.4582 5.9900 PHARAOH++ 0.4336 5.9185 0.5354 6.3637 WASP1++ 0.6022 6.8976 0.5370 6.4808 Table 1: Results of automatic evaluation; bold type indicates the best performing system (or systems) for a given domain-metric pair (p < 0.05) 5.1 Automatic Evaluation Weperformed4runsof10-foldcrossvalidation,and measured the performance of the learned generators using the BLEU score (Papineni et al. , 2002) and the NIST score (Doddington, 2002).'
'<CIT> prototype-driven approach requires just a few prototype examples for each POS tag , exploiting these labeled words to constrain the labels of their distributionally similar words when training a generative log-linear model for POS tagging '
'Within this class would fall the Lexical Implication Rules -LRB- LIRs -RRB- of <CIT> , the lexical rules of Copestake and Briscoe -LRB- 1991 -RRB- , the Generative Lexicon of Pustejovsky <OTH> , and the ellipsis recovery procedUres of Viegas and Nirenburg <OTH> '
'This model is similar in spirit to IBM model 1 <CIT> '
'We are also interested in examining the approach within a standard phrase-based decoder such as Moses <CIT> or a hierarchical phrase system <OTH> '
'Some papers <OTH> based on <CIT> learned an aided dictionary from a corpus to reduce the possibility of unknown words '
'Distributional cluster (Brown et al. , 1992): cost, expense, risk, profitability, deferral, earmarks, capstone, cardinality, mintage, reseller Word ''cost'' (2 alternatives) 0.5426 cost, price, terms, damage: the amount of money paid for something 0.4574 monetary value, price, cost: the amount of money it would bring if sold Word ''expense'' (2 alternatives) 1.0000 expense, expenditure, outlay, outgo, spending, disbursal, disbursement 0.0000 expense: a detriment or sacrifice; ''at the expense of'' Word ''risk'' (2 alternatives) 0.6267 hazard, jeopardy, peril risk: subconeept of danger 0.3733 risk, peril danger: subeonceptofventure Word ''profitability'' (1 alternatives) 1.0000 profitableness, profitability: subconcept of advantage, benefit, usefulness Word ''deferral'' (3 alternatives) 0.6267 abeyance, deferral, recess: subconcept of inaction, inactivity, inactiveness 0.3733 postponement, deferment, deferral, moratorium: an agreed suspension of activity 0.3733 deferral: subconeeptofpause, wait Word ''earmarks'' (2 alternatives) 0.2898 earmark: identification mark on the ear of a domestic animal 0.7102 hallma.k, trademark, earmark: a distinguishing characteristic or attribute Word ''capstone'' (1 alternatives) 1.0000 capstone, coping stone, stretcher: used at top of wall Word ''eardinality'' Not in WordNet Word ''mintage'' (1 alternatives) 62 1.0000 coinage, mintage, specie, metal money: subconcept of cash Word ''reseller'' Not in WordNet This cluster was one presented by Brown et al. as a randomly-selected class, rather than one hand-picked for its coherence.'
'The weights are trained using minimum error rate training <CIT> with BLEU score as the objective function '
'<CIT> present a procedure to directly optimize the global scoring function used by a phrasebased decoder on the accuracy of the translations '
'Sentiment summarization has been well studied in the past decade <CIT> '
'The Maximum Entropy model (Berger et al., 1996; Ratnaparkhi, 1997; Abney, 1997) is a conditional model that assigns a probability to every possible parse  for a given sentence s. The model consists of a set of m feature functions fj() that describe properties of parses, together with their associated weights j. The denominator is a normalization term where Y (s) is the set of parses with yield s: p(|s;) = exp( summationtextm j=1 jfj())summationtext yY (s) exp( summationtextm j=1 jfj(y))) (1) The parameters (weights) j can be estimated efficiently by maximizing the regularized conditional likelihood of a training corpus (Johnson et al., 1999; van Noord and Malouf, 2005):  = argmax  logL()  summationtextm j=1  2j 22 (2) where L() is the likelihood of the training data.'
'Our conception of the task is inspired by Ramshaw and Marcus representation of text chunking as a tagging problem <CIT> The information that can be used to train the system appears in columns 1 to 8 of Table 1 '
'Sentiment classification at the document level investigates ways to classify each evaluative document -LRB- eg , product review -RRB- as positive or negative <CIT> '
'If the alignments are not available , they can be automatically generated ; eg , using GIZA + + <CIT> '
'These include the bootstrapping approach <CIT> and the context clustering approach <OTH> '
'All features encountered in the training data are ranked in the DL -LRB- best evidence first -RRB- according to the following loglikelihood ratio <CIT> : Log Pr -LRB- reading i jfeature k -RRB- P j6 = i Pr -LRB- reading j jfeature k -RRB- We estimated probabilities via maximum likelihood , adopting a simple smoothing method <OTH> : 01 is added to both the denominator and numerator '
'<CIT> measure annotation quality in terms of precision and recall against manually constructed , gold-standard f-structures for 105 randomly selected trees from section 23 of the WSJ section of Penn-II '
'As other researchers pursued efficient default unification <OTH> , we also propose another definition of default unification , which we call lenient default unification '
'We examine Structural Correspondence Learning -LRB- SCL -RRB- <CIT> for this task , and compare it to several variants of Self-training <OTH> '
'The WordNet : : Similarity package <CIT> implements this distance measure and was used by the authors '
'The data consists of 2,544 main clauses from the Wall Street Journal Treebank corpus <CIT> '
'We train IBM Model-4 using GIZA + + toolkit <CIT> in two translation directions and perform different word alignment combination '
'<CIT> and Ros6 -LRB- 1995 -RRB- point out the importance of taking into account the expected chance agreement among judges when computing whether or not judges agree significantly '
'For example , the words corruption and abuse are similar because both of them can be subjects of verbs like arouse , become , betray , cause , continue , cost , exist , force , go on , grow , have , increase , lead to , and persist , etc , and both of them can modify nouns like accusation , act , allegation , appearance , and case , etc Many methods have been proposed to compute distributional similarity between words , eg , <CIT> , <OTH> , <OTH> and <OTH> '
'It consists of sections 15-18 of the Wall Street Journal part of the Penn Treebank II <CIT> as training data -LRB- 211727 tokens -RRB- and section 20 as test data -LRB- 47377 tokens -RRB- '
'sentence length : The longer the sentence is , the poorer the parser performs <CIT> '
'1 Introduction A recent development in data-driven parsing is the use of discriminative training methods <CIT> '
'This probability is computed using IBMs Model 1 <CIT> : P -LRB- Q A -RRB- = productdisplay qQ P -LRB- q A -RRB- -LRB- 3 -RRB- P -LRB- q A -RRB- = -LRB- 1 -RRB- Pml -LRB- q A -RRB- + Pml -LRB- q C -RRB- -LRB- 4 -RRB- Pml -LRB- q A -RRB- = summationdisplay aA -LRB- T -LRB- q a -RRB- Pml -LRB- a A -RRB- -RRB- -LRB- 5 -RRB- where the probability that the question term q is generated from answer A , P -LRB- q A -RRB- , is smoothed using the prior probability that the term q is generated from the entire collection of answers C , Pml -LRB- q C -RRB- '
'Lexical collocation functions , especially those determined statistically , have recently attracted considerable attention in computational linguistics <OTH> mainly , though not exclusively , for use in disambiguation '
'To optimize the system towards a maximal BLEU or NIST score , we use Minimum Error Rate -LRB- MER -RRB- Training as described in <CIT> '
'The word-based edit distance heuristic yields pairs that are relatively clean but offer relatively minor rewrites in generation , especially when compared to the MSA model of <CIT> '
'We based our design on the IBM models 1 and 2 (Brown et al. , 1993), but taking into account that our model must generate correct derivations in a given grammar, not any seBEGIN some END<animals> eat <animals> (a) ''some a88 animalsa89 eat a88 animalsa89 '' BEGIN some END<animals> eat are <animals> dangerous (b) ''some a88 animalsa89 are dangerous'' BEGIN <animals> some END eat are <animals> dangerous (c) ''a88 animalsa89 are dangerous'' BEGIN snakes rats people some END eat are snakes rats people dangerous (d) Expansion of a88 animalsa89 Figure 3: Using a category a86 animalsa87 for ''snakes'', ''rats'' and ''people'' in the example of Figure 1.'
'This generates tens of millions features , so we prune those features that occur fewer than 10 total times , as in <CIT> '
'This metric tests the hypothesis that the probability of phrase is the same whether phrase has been seen or not by calculating the likelihood of the observed data under a binomial distribution using probabilities derived using each hypothesis <CIT> '
'Thus , some research has been focused on deriving different word-sense groupings to overcome the finegrained distinctions of WN <OTH> , <OTH> , <OTH> , <OTH> , <CIT> and <OTH> '
'Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers <CIT> , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship <OTH> '
'We use Minimal Error Rate Training <CIT> to maximize BLEU on the complete development data '
'5http : \/ \/ clcsokayama-uacjp\/rsc \/ jacabit \/ a4a6a5 which gathers the set of co-occurrence units a7 associated with the number of times that a7 and a2 occur together a8a6a9a10a9 a5 a11 In order to identify speci c words in the lexical context and to reduce word-frequency effects , we normalize context vectors using an association score such as Mutual Information <OTH> or Log-likelihood <CIT> '
'One of the simplest models that can be seen in the context of lexical triggers is the IBM model 1 <CIT> which captures lexical dependencies between source and target words '
'The corpus was automatically derived from the Penn Treebank II corpus <CIT> , by means of the script chunklinkpl <OTH> that we modified to fit our purposes '
'There has been some previous work on accuracy-driven training techniques for SMT , such as MERT <CIT> and the Simplex Armijo Downhill method <OTH> , which tune the parameters in a linear combination of various phrase scores according to a held-out tuning set '
'<OTH> , <CIT> , and Karol & Edelman -LRB- 1996 -RRB- where strong reliance on statistical techniques for the calculation of word and context similarity commands large source corpora '
'5 Synchronous DIG 51 Definition <CIT> introduced synchronous binary trees and <OTH> introduced synchronous tree adjoining grammars , both of which view the translation process as a synchronous derivation process of parallel trees '
'This analysis depends on the SPECIALIST Lexicon and the Xerox part-of-speech tagger <CIT> and provides simple noun phrases that are mapped to concepts in the UMLS Metathesaurus using MetaMap <OTH> '
'<CIT> presented results suggesting that the additional parameters required to ensure that a model is not deficient result in inferior performance , but we plan to study whether this is the case for our generative model in future work '
'High quality word alignments can yield more accurate phrase-pairs which improve quality of a phrase-based SMT system <CIT> '
'The last row shows the results for the feature augmentation algorithm <CIT> '
'Another WSD approach incorporating context-dependent phrasal translation lexicons is given in <CIT> and has been evaluated on several translation tasks '
'In our research , 23 scores , namely BLEU <CIT> with maximum n-gram lengths of 1 , 2 , 3 , and 4 , NIST <OTH> with maximum n-gram lengths of 1 , 2 , 3 , 4 , and 5 , GTM <OTH> with exponents of 10 , 20 , and 30 , METEOR -LRB- exact -RRB- <OTH> , WER <OTH> , PER <OTH> , and ROUGE <OTH> with n-gram lengths of 1 , 2 , 3 , and 4 and 4 variants -LRB- LCS , S , SU , W-12 -RRB- , were used to calculate each similarity S i Therefore , the value of m in Eq '
'It has been shown that human knowledge , in the form of a small amount of manually annotated parallel data to be used to seed or guide model training , can significantly improve word alignment F-measure and translation performance <CIT> '
'Intuitively, if we allow any Source words to be aligned to any Target words, the best alignment that we can come up with is the one in Figure 1.c. Sentence pair (S2, T2) offers strong evidence that b c in language S means the same thing as x in language T. On the basis of this evidence, we expect the system to also learn from sentence pair (S1, T1) that a in language S means the same thing as y in language T. Unfortunately, if one works with translation models that do not allow Target words to be aligned to more than one Source word  as it is the case in the IBM models (Brown et al. , 1993)  it is impossible to learn that the phrase b c in language S means the same thing as word x in language T. The IBM Model 4 (Brown et al. , 1993), for example, converges to the word alignments shown in Figure 1.b and learns the translation probabilities shown in Figure 1.a.2 Since in the IBM model one cannot link a Target word to more than a Source word, the training procedure 2To train the IBM-4 model, we used Giza (Al-Onaizan et al. , 1999).'
'Our approach is data-driven : following the methodology in <CIT> , we automatically convert the English PennII treebank and the Chinese Penn Treebank <OTH> into f-structure banks '
'In this paper we present a novel PCFG-based architecture for probabilistic generation based on wide-coverage , robust Lexical Functional Grammar -LRB- LFG -RRB- approximations automatically extracted from treebanks <CIT> '
'BLEU <CIT> is a precision metric that assesses the quality of a translation in terms of the proportion of its word n-grams -LRB- n 4 has become standard -RRB- that it shares with several reference translations '
'These IBM models and more recent refinements <OTH> as well as algorithms that bootstrap from these models like the HMM algorithm described in <OTH> are unsupervised algorithms '
'This situation is very similar to that involved in training HMM text taggers , where joint probabilities are computed that a particular word corresponds to a particular part-ofspeech , and the rest of the words in the sentence are also generated <CIT> '
'Almost all of the work in the area of automatically trained taggers has explored Markov-model based part of speech tagging <CIT> '
'Recently , methods for training binary classifiers to maximize the F 1 - score have been proposed for SVM <OTH> and LRM <CIT> '
'PB , available at wwwcisupennedu\/ace , is used along with the Penn TreeBank 2 -LRB- wwwcisupennedu \/ treebank -RRB- <CIT> '
'Many traditional clustering techniques <CIT> attempt to maximize the average mutual information of adjacent clusters = 21 , 2 12 2121 -RRB- -LRB- -RRB- -LRB- log -RRB- -LRB- -RRB- , -LRB- WW WP WWP WWPWWI , -LRB- 2 -RRB- where the same clusters are used for both predicted and conditional words '
'1 Introduction Over the past five years progress in machine translation , and to a lesser extent progress in natural language generation tasks such as summarization , has been driven by optimizing against n-grambased evaluation metrics such as Bleu <CIT> '
'Previouswork , eg <CIT> , has focusedonimprovingtheperformanceofPowells algorithm '
'1984 -RRB- , written discourse <CIT> , and conversational data <OTH> '
'Itowever , Harris '' methodology implies also to simplify and transform each parse tree 2 , so as to obtain so-called ` elementary sentences '' exhibiting the main conceptual classes for the domain -LRB- Sager lIa ` or instance , Hindle <CIT> needs a six million word corpus in order to extract noun similarities from predicate-argunlent structures '
'Most statistical parsing research , such as <CIT> , has centered on training probabilistic context-free grammars using the Penn Treebank '
'Our aim is not only to determine the utility of citation texts for survey creation , but also to examine the quality distinctions between this form of input and others such as abstracts and full textscomparing the results to human-generated surveys using both automatic and nugget-based pyramid evaluation <CIT> '
'For better probability estimation , the model was extended to work with -LRB- hidden -RRB- word classes <CIT> '
'First , it has been noted that in many natural language applications it is sufficient to use shallow parsing information ; information such as noun phrases -LRB- NPs -RRB- and other syntactic sequences have been found useful in many large-scale language processing applications including information extraction and text summarization <OTH> '
'However , the best performing statistical approaches to lexical ambiguity resolution l ; lmmselves rely on complex infornmtion sources such as ` lemmas , inflected forms , parts of speech and arbitrary word classes If -RRB- local and distant collocations , trigram sequences , and predicate m ` gument association '' -LRB- <CIT> , p 190 -RRB- or large context-windows up to 1000 neighboring words <OTH> '
'One such relational reasoning task is the problem of compound noun interpretation , which has received a great deal of attention in recent years <CIT> '
'Table 1 shows the percentage of agreement in classifying words as compounds or non-compounds -LRB- Compound Classification Agreement , CCA -RRB- for each language and the Kappa score <CIT> obtained from it , and the percentage of words for which also the decomposition provided was identical -LRB- Decompounding Agreement , DA -RRB- '
'Thus , conventional methods had to introduce some kinds of restrictions such as the limitation of the kind of chains or the length of chains to be extracted <CIT> '
'CIT -RRB- '
'23 Probabilistic models for generation with HPSG Some existing studies on probabilistic models for HPSG parsing <OTH> adopted log-linear models <CIT> '
'Statistical machine translation views the translation process as a noisy-channel signal recovery process in which one tries to recover the input signal e , from the observed output signal f1 Early statistical machine translation systems used a purely word-based approach without taking into account any of the morphological or syntactic properties of the languages <CIT> '
'The difficulty of this task is that the standard method for converting NER to a sequence tagging problem with BIOencoding <CIT> , where each 1http : \/ \/ wwwnistgov\/speech\/tests \/ ace \/ indexhtm token is assigned a tag to indicate whether it is at the beginning -LRB- B -RRB- , inside -LRB- I -RRB- , or outside -LRB- O -RRB- of an entity , is not directly applicable when tokens belong to more than one entity '
'21418 examples of structures of the kind ` VB N1 PREP N2 '' were extracted from the Penn-TreeBank Wall Street Journal <CIT> '
'A tight integration of morphosyntactic information into the translation model was proposed by <CIT> where lemma and morphological information are translated separately , and this information is combined on the output side to generate the translation '
'34 Related work and issues for future research <CIT> and van der Eijk -LRB- 1993 -RRB- describe term translation methods that use bilingual texts that were aligned at the sentence level '
'Note in passing that the ratio 104-108\/997 % compares very favourably with other systems ; cf 30\/993 % by POST <OTH> and 104\/976 % or 109\/986 % by de Marcken <OTH> '
'Some statistical model to estimate the part of speech of unknown words from the case of the first letter and the prefix and suffix is proposed <CIT> '
'This corpus of 29 million words was provided to us by Michael <CIT> '
'Following our previous work <CIT> , we extract features from a sequence representation and a parse tree representation of each relation instance '
'Second , we follow <CIT> on taxonomy induction in incorporating transitive closure constraints in our probability calculations , as explained below '
'We then ranked the collected query pairs using loglikelihoodratio -LRB- LLR -RRB- <CIT> , whichmeasures the dependence between q1 and q2 within the context of web queries <OTH> '
'Prominent among these properties is the semi-free Language Size LR LP Source English 40,000 874 % 881 % <CIT> Chinese 3,484 690 % 748 % <OTH> Czech 19,000 800 % <CIT> Table 1 : Results for the Collins <OTH> model for various languages -LRB- dependency precision for Czech -RRB- wordorder , ie , German wordorder is fixed in some respects , but variable in others '
'An example set of tags can be found in the Penn Treebank project <CIT> '
'There are many research directions, e.g., sentiment classification (classifying an opinion document as positive or negative) (e.g., Pang, Lee and Vaithyanathan, 2002; Turney, 2002), subjectivity classification (determining whether a sentence is subjective or objective, and its associated opinion) (Wiebe and Wilson, 2002; Yu and Hatzivassiloglou, 2003; Wilson et al, 2004; Kim and Hovy, 2004; Riloff and Wiebe, 2005), feature/topic-based sentiment analysis (assigning positive or negative sentiments to topics or product features) (Hu and Liu 2004; Popescu and Etzioni, 2005; Carenini et al., 2005; Ku et al., 2006; Kobayashi, Inui and Matsumoto, 2007; Titov and McDonald.'
'It has been shown repeatedly -- eg , Briscoe and Carroll <OTH> , Charniak <OTH> , Collins <OTH> , Inui et al '
'For example , work which failed to detect improvements in translation quality with the integration of word sense disambiguation <OTH> , or work which attempted to integrate syntactic information but which failed to improve Bleu <CIT> may deserve a second look with a more targeted manual evaluation '
'As the baseline standard , we took the ending-guessing rule set supplied with the Xerox tagger <CIT> '
'The current release of PDTB20 contains the annotations of 1,808 Wall Street Journal articles -LRB- ~ 1 million words -RRB- from the Penn TreeBank <CIT> II distribution and a total of 40,600 discourse connective tokens <OTH> '
'This allows us to compute the conditional probability as follows <CIT> : ag ~ -LRB- h f -RRB- P -LRB- \/ Ih -RRB- 1L '' -LRB- 2 -RRB- Z -LRB- h -RRB- ct i '
'We believe that other kinds of translationunit such as n-gram <OTH> , factoredphrasaltranslation <CIT> , or treelet <OTH> can be used in this method '
'Taken together with cube pruning <OTH> , k-best tree extraction <CIT> , and cube growing <CIT> , these results provide evidence that lazy techniques may penetrate deeper yet into MT decoding and other NLP search problems '
'A class of training criteria that provides a tighter connection between the decision rule and the final error metric is known as Minimum Error Rate Training -LRB- MERT -RRB- and has been suggested for SMT in <CIT> '
'4 Experiments and Results We use the standard corpus for this task , the Penn Treebank <CIT> '
'Intuitively , if we are able to find good correspondences among features , then the augmented labeled source domain data should transfer better to a target domain -LRB- where no labeled data is available -RRB- <CIT> '
'The methodology used (Brown et al. , 1993) is based on the definition of a function Pr(tI1|sJ1) that returns the probability that tI1 is a 835 source Transferir documentos explorados a otro directorio interaction-0 Move documents scanned to other directory interaction-1 Move s canned documents to other directory interaction-2 Move scanned documents to a nother directory interaction-3 Move scanned documents to another f older acceptance Move scanned documents to another folder Figure 1: Example of CAT system interactions to translate the Spanish source sentence into English.'
'For tuning of decoder parameters , we conducted minimum error training <CIT> with respect to the BLEU score using 916 development sentence pairs '
'To this end , the translational correspondence is described within a translation rule , ie , <CIT> -LRB- or a synchronous production -RRB- , rather than a translational phrase pair ; and the training data will be derivation forests , instead of the phrase-aligned bilingual corpus '
'These methods are based on IBM statistical translation Model 2 <CIT> , but take advantage of certain characteristics of the segments of text that can typically be extracted from translation memories '
'While reranking has benefited many tagging and parsing tasks <CIT> including semantic role labeling <OTH> , it has not yet been applied to semantic parsing '
'1 Introduction The reranking approach is widely used in parsing <CIT> as well as in other structured classification problems '
'Unsupervised systems <CIT> are based on generative models trained with the EM algorithm '
'In our Machine % ` anslation system , transfer rules are generated automatically from parsed parallel text along the lines of <OTH> '
'These alignment models stem from the source-channel approach to statistical machine translation <CIT> '
'1 Motivation Question Answering has emerged as a key area in natural language processing -LRB- NLP -RRB- to apply question parsing , information extraction , summarization , and language generation techniques <CIT> '
'We use the beam search technique of <CIT> to search the space of all hypotheses '
'33 Tree Transducer Grammars Syntactic machine translation <CIT> uses tree transducer grammars to translate sentences '
'Previous work has shown that data collected through the Mechanical Turk service is reliable and comparable in quality with trusted sources <CIT> '
'41 Data Preparation NP chunking results have been reported on two slightly different data sets : the original RM data set of <CIT> , and the modi ed CoNLL-2000 version of Tjong Kim Sang and Buchholz -LRB- 2000 -RRB- '
'To determine the tree head-word we used a set of rules similar to that described by <OTH> <OTH> and also used by <OTH> , which we modified in the following way : The head of a prepositional phrase -LRB- PP-IN NP -RRB- was substituted by a function the name of which corresponds to the preposition , and its sole argument corresponds to the head of the noun phrase NP '
'The first work in SMT , done at IBM <CIT> , developed a noisy-channel model , factoring the translation process into two portions : the translation model and the language model '
'Recent work shows that k-best maximum spanning tree -LRB- MST -RRB- parsing and reranking is also viable <CIT> '
'(Pang & Lee, 2004; Aue & Gamon, 2005).'
'Our hierarchical training method yields significant improvement when compared to a similar nonhierarchical model which instead uses the standard 2Data and code used in this paper are available at http://peoplecsailmitedu/edc/emnlp07/ perceptron update of <CIT> '
'It would be necessary to apply either semiautomatic or automatic methods such as those in <CIT> to extend FrameNet coverage for final application to machine translation tasks '
'In the iNeast system <OTH> , the identification of relevant terms is oriented towards multi-document summarization , and they use a likelihood ratio <CIT> which favors terms which are representative of the set of documents as opposed to the full collection '
'For evaluation , we used the BLEU metrics , which calculates the geometric mean of n-gram precision for the MT outputs found in reference translations <CIT> '
'In Section 3 we then describe the probabilistic taxonomy learning model introduced by <CIT> '
'<CIT> use a graph-based technique to identify and analyze only subjective parts of texts '
'Given the estimated 3 % error rate of the WSJ tagging <CIT> , they argue that the difference in performance is not sufficient to establish which of the two taggers is actually better '
'In this paper , translation quality is evaluated according to -LRB- 1 -RRB- the BLEU metrics which calculates the geometric mean of ngram precision by the system output with respect to reference translations <CIT> , and -LRB- 2 -RRB- the METEOR metrics that calculates unigram overlaps between translations <OTH> '
'The WSJ corpus is based on the WSJ part of the PENN TREEBANK <CIT> ; we used the first 10,000 sentences of section 2-21 as the pool set , and section 00 as evaluation set -LRB- 1,921 sentences -RRB- '
'This upper bound is consistent with the upper limit of 50 % found by <CIT> which takes into account stemming differences '
'<OTH> grow the set of word links by appending neighboring points , while <CIT> and Hey <OTH> try to avoid both horizontal and vertical neighbors '
'52 Impact on translation quality As reported in Table 3 , small increases in METEOR <CIT> , BLEU <OTH> and NIST scores <OTH> suggest that SMT output matches the references better after postprocessing or decoding with the suggested lemma translations '
'Almost all of the work in the area of automatically trained taggers has explored Markov-model based part of speech tagging <OTH> '
'33 Accuracy Results <OTH> describe a model for unknown words that uses four features , but treats the features ms independent '
'In previous work <OTH> , we have reported some preliminary success in aligning the English and Japanese versions of the AWK manual -LRB- Aho , Kernighan , Weinberger <OTH> -RRB- , using charalign <OTH> , a method that looks for character sequences that are the same in both the source and target '
'<CIT> ` Noun Classification from Predicate-Argument Structures , '' Proceedings of the 28th Annual Meeting of the ACL , pp '
'Differences in behavior of WSD systems when applied to lexical-sample and all-words datasets have been observed on previous Senseval and Semeval competitions <OTH> : supervised systems attain results on the high 80s and beat the most frequent baseline by a large margin for lexical-sample datasets , but results on the all-words datasets were much more modest , on the low 70s , and a few points above the most frequent baseline '
'Slrs Parse Base <OTH> is 176 '
'Movie-domainSubjectivityDataSet -LRB- Movie -RRB- : <CIT> used a collection of labeled subjective and objective sentences in their work on review classification5 The data set contains 5000 subjective sentences , extracted from movie reviews collected from the Rotten Tomatoes web formed best '
'The features used in this study are : the length of t ; a single-parameter distortion penalty on phrase reordering in a , as described in <CIT> ; phrase translation model probabilities ; and trigram language model probabilities logp -LRB- t -RRB- , using Kneser-Ney smoothing as implemented in the SRILM toolkit <OTH> '
'<OTH> use the Learning as Search Optimization framework to take into account the non-locality behavior of the coreference features '
'An alternative representation for baseNPs has been put tbrward by <CIT> '
'Let us now compare our results to those obtained using shallow parsing , as previously done by <CIT> '
'<CIT> used bootstrapping to train decision list classifiers to disambiguate between two senses of a word , achieving impressive classification accuracy '
'Until now , we have defined BestLossk , a to be the minimum of the loss given that the kth feature is updated an optimal amount : BestLossk , amin d LogLossUpda , k , d In this section we sketch a different approach , based on results from <CIT> , which leads to an algorithm very similar to that for ExpLoss in Figures 3 and 4 '
'On the other hand , works done by <CIT> have proposed methodologies to automatically acquire these patterns mostly based on supervised learning to leverage manual work '
'212 Research on Syntax-Based SMT A number of researchers <CIT> have proposed models where the translation process involves syntactic representations of the source and\/or target languages '
'As described in Section 4 , we define the problem of term variation identifica1484 tion as a binary classification task , and build two types of classifiers according to the maximum entropy model <CIT> and the MART algorithm <OTH> , where all term similarity metrics are incorporated as features and are jointly optimized '
'The resulting intercoder reliability , measured with the Kappa statistic <CIT> , is considered excellent -LRB- = 080 -RRB- '
'For the classifier , we used the OpenNLP MaxEnt implementation -LRB- maxentsourceforgenet -RRB- of the maximum entropy classification algorithm <CIT> '
'In contrast , approaches to WSD attempt to take advantage of many different sources of information -LRB- eg see <OTH> -RRB- ; it seems possible to obtain benefit from sources ranging from local collocational clues <OTH> to membership in semantically or topically related word classes <OTH> to consistency of word usages within a discourse <OTH> ; and disambignation seems highly lexically sensitive , in effect requiring specialized disamhignators for each polysemous word '
'l lhmsetsu ideni , illcation is a ln ` oblem similar to ohm , king <OTH> in other l ; mguages '
'For example , given that each semantic class exhibits a particular syntactic behavior , information on the semantic class should improve POStagging for adjective-noun and adjective-participle ambiguities , probably the most difficult distinctions both for humans and computers <CIT> '
'so they conform to the Penn Treebank corpus <CIT> annotation style , and then do experiments using models built with Treebank data '
'ROUGE <CIT> is a set of recall-based criteria that is mainly used for evaluating summarization tasks '
'Pivots are features occurring frequently and behaving similarly in both domains <CIT> '
'<CIT> introduced one of those similarity schemes , ? two-level SoftTFIDF ? ? '
'1 Introduction : Defining SCMs The work presented here was done in the context of phrase-based MT <CIT> '
'Four teams had approaches that relied -LRB- to varying degrees -RRB- on an IBM model of statistical machine translation <CIT> '
'For English , we used the Penn Treebank <CIT> in our experiments and the tool Penn2Malt7 to convert the data into dependency structures using a standard set of head rules <OTH> '
'Non-anaphoric definite descriptions have been detected using heuristics -LRB- eg , Vieira and Poesio <OTH> -RRB- and unsupervised methods -LRB- eg , <CIT> -RRB- '
'The model presented above is based on our previous work <OTH> , which bears the same spirit of some other recent work on multitask learning <CIT> '
'The L1 or L2 norm is commonly used in statistical natural language processing <CIT> '
'Whereas language generation has benefited from syntax <OTH> , the performance of statistical phrase-based machine translation when relying solely on syntactic phrases has been reported to be poor <CIT> '
'Besides continued research on improving MT techniques , one line of research is dedicated to better exploitation of existing methods for the combination of their respective advantages <CIT> '
'RECALL F-SCORE Brackets 8917 8750 8833 Dependencies 9640 9640 9640 Brackets , revised 9756 9803 9779 Dependencies , revised 9927 9927 9927 Table 1 : Agreement between annotators few weeks , and increased to about 1000 words per hour after gaining more experience <CIT> '
'Following <CIT> , we do not distinguish rare words '
'4 The Dependency Labeler 41 Classifier We used a maximum entropy classifier <CIT> to assign labels to the unlabeled dependencies produced by the Bayes Point Machine '
'Carletta mentions this problem , asking what the difference would be if the kappa statistic were computed across ` clause boundaries , transcribed word boundaries , and transcribed phoneme boundaries '' <CIT> rather than the sentence boundaries she suggested '
'Previous uses of this model include language modeling <OTH> , machine translation <CIT> , prepositional phrase attachment <OTH> , and word morphology <OTH> '
'1999 -RRB- , OpenCCG <OTH> and XLE <OTH> , or created semi-automatically <OTH> , or fully automatically extracted from annotated corpora , like the HPSG <OTH> , LFG <CIT> and CCG <OTH> resources derived from the Penn-II Treebank -LRB- PTB -RRB- <OTH> '
'To optimize the parameters of the decoder , we performed minimum error rate training on IWSLT04 optimizing for the IBM-BLEU metric <CIT> '
'2 Discriminative Reordering Model Basic reordering models in phrase-based systems use linear distance as the cost for phrase movements <CIT> '
'Translation performance is measured using the automatic BLEU <CIT> metric , on one reference translation '
'<CIT> also uses wide context , but incorporates the one-senseper-discourse and one-sense-per-collocation constraints , using an unsupervised learning technique '
'When an S alignment exists , there will always also exist a P alignment such that P a65 S The English sentences were parsed using a state-of-the-art statistical parser <OTH> trained on the University of Pennsylvania Treebank <CIT> '
'<CIT> and Taskar et al '
'<OTH> and <CIT> and <CIT> <OTH> show how to employ the matrix-tree theorem '
'Determining the sense of an ambiguous word , using bootstrapping and texts from a different language was done by <CIT> , Hearst -LRB- 1991 -RRB- , Diab -LRB- 2002 -RRB- , and Li and Li -LRB- 2004 -RRB- '
'<CIT> proposed a method for extracting opinion holders , topics and opinion words , in which they use semantic role labeling as an intermediate step to label opinion holders and topics '
'Following previous work <CIT> , we assume that the tag of a word is independent of the tags of all preceding words given the tags of the previous two words -LRB- ie , = 2 in the equation above -RRB- '
'We then examined the inter-annotator reliability of the annotation by calculating the score <CIT> '
'Such methods stand in sharp contrast to partially supervised techniques that have recently been proposed to induce hidden grammatical representations that are finer-grained than those that can be read off the parsed sentences in treebanks <CIT> '
'The translation model is estimated via the EM algorithm or approximations that are bootstrapped from the previous model in the sequence as introduced in <CIT> '
'This clustering was created automatically with the aid of a methodology described in <CIT> '
'A similar approach is used here , including a collapsed version of the Treebank POS tag set <CIT> , with additions for specific words -LRB- eg personal pronouns and filled pause markers -RRB- , compound punctuation -LRB- eg multiple exclamation marks -RRB- , and a general emoticon tag , resulting in a total of 41 tags '
'It was first cast as a classification problem by <CIT> , as a problem of NP chunking '
'Standard CI Model 1 training , initialised with a uniform translation table so that t -LRB- ejf -RRB- is constant for all source\/target word pairs -LRB- f , e -RRB- , was run on untagged data for 10 iterations in each direction <OTH> '
'For the efficiency of minimum-error-rate training <CIT> , we built our development set -LRB- 580 sentences -RRB- using sentences not exceeding 50 characters from the NIST MT-02 evaluation test data '
'This differs from typical generative settings for IR and MT <CIT> , where all conditioned events are disjoint by construction '
'The parser induction algorithm used in all of the experiments in this paper was a distribution of Collins ''s model 2 parser <CIT> '
'4 Experiments Our experiments were conducted on CoNLL-2007 shared task domain adaptation track <OTH> using treebanks <CIT> '
'<CIT> state that a baseNP aims to identify essentially the initial portions of nonrecursive noun phrases up to the head , including determiners but not including postmodifying prepositional phrases or clauses However , work on baseNPs has essentially always proceeded via algorithmic extraction from fully parsed corpora such as the Penn Treebank '
'We used the same 58 feature types as <CIT> '
'context-free rules Charniak <OTH> <CIT> <OTH> , Eisner <OTH> context-free rules , headwords Charniak -LRB- 1997 -RRB- context-free rules , headwords , grandparent nodes <CIT> <OTH> context-free rules , headwords , grandparent nodes\/rules , bigrams , two-level rules , two-level bigrams , nonheadwords Bod <OTH> all fragments within parse trees Scope of Statistical Dependencies Model Figure 4 '
'I have made a preliminary analysis of the inventory of syntactic categories used in the tagging for labeling trees in the 18 Penn Treebank <CIT> , comparing them to the categories used in CGEL '
'For all performance metrics , we show the 70 % confidence interval with respect to the MAP baseline computed using bootstrap resampling <CIT> '
',(Brown et al. , 1992)).'
'They use a conditional model , based on Collins <OTH> , which , as the authors acknowledge , has a number of theoretical deficiencies ; thus the results of Clark et al provide a useful baseline for the new models presented here '
'We report BLEU scores <CIT> on untokenized , recapitalized output '
'This implementation is exactly the one proposed in <CIT> , and we will denote it as MB-D hereafter '
'We have also used TPTs to encode n-gram count databases such as the Google 1T web n-gram database (Brants and Franz, 2006), but are not able to provide detailed results within the space limitations of this paper.4 5.1 Perplexity computation with 5-gram language models We compared the performance of TPT-encoded language models against three other language model implementations: the SRI language modeling toolkit (Stolcke, 2002), IRSTLM (Federico and Cettolo, 2007), and the language model implementation currently used in the Portage SMT system (Badr et al., 2007), which uses a pointer-based implementation but is able to perform fast LM filtering at load time.'
'For example , Och reported that the quality of MT results was improved by using automatic MT evaluation measures for the parameter tuning of an MT system <CIT> '
'The user can select characters by their frequencies -LRB- ie - f and - g options -RRB- , the top or bottom N % -LRB- ie - m and - n options -RRB- , their ranks -LRB- ie - r and - s options -RRB- and by their frequencies above two standard deviations phlS the mean <CIT> -LRB- ie - z option -RRB- '
'In this paper , we propose an alignment algorithm between English and Korean conceptual units -LRB- or between English and Korean term constituents -RRB- in English-Korean technical term pairs based on IBM Model <CIT> '
'222 ENGLISH TRAINING DATA For training in the English experiments , we used WSJ <CIT> '
'The Decision List -LRB- DL -RRB- algorithm is described in <CIT> '
'Much work has been performed on learning to identify and classify polarity terms -LRB- ie , terms expressing a positive sentiment -LRB- eg , happy -RRB- or a negative sentiment -LRB- eg , terrible -RRB- -RRB- and exploiting them to do polarity classification -LRB- eg , Hatzivassiloglou and McKeown <OTH> , <CIT> , Kim and Hovy -LRB- 2004 -RRB- , Whitelaw et al '
'It is potentially useful in other natural language processing tasks , such as the problem of estimating n-gram models <CIT> or the problem of semantic tagging <OTH> '
'However , the maximum entropy <OTH> was found to yield higher accuracy than nave Bayes in a subsequent comparison by <CIT> , who used a different subset of either Senseval-1 or Senseval-2 English lexical sample data '
'Automatic approaches to creating a semantic orientation lexicon and , more generally , approaches for word-level sentiment annotation can be grouped into two kinds : -LRB- 1 -RRB- those that rely on manually created lexical resourcesmost of which use WordNet <CIT> ; and -LRB- 2 -RRB- those that rely on text corpora <OTH> '
'Recent work has applied Bayesian non-parametric models to anaphora resolution <OTH> , lexical acquisition <OTH> and language modeling <CIT> with good results '
': there is : want to : need not : in front of : as soon as : look at Figure 2 : Examples of entries from the manually developed dictionary 4 Experimental Setting 41 Evaluation The intrinsic quality of word alignment can be assessed using the Alignment Error Rate -LRB- AER -RRB- metric <CIT> , that compares a systems alignment output to a set of gold-standard alignment '
'If distributional similarity is conceived of as substitutability , as <CIT> and Lee -LRB- 1999 -RRB- emphasize , then asymmetries arise when one word appears in a subset of the contexts in which the other appears ; for example , the adjectives that typically modify apple are a subset of those that modify fruit , sofruit substitutes for apple better than apple substitutes for fruit '
'So unlike some other studies <CIT> , we used manually annotated alignments instead of automatically generated ones '
'The HWC metrics compare dependency and constituency trees for both reference and machine translations <CIT> '
'The inclusion of phrases longer than three words in translation resources has been avoided , as it has been shown not to have a strong impact on translation performance <CIT> '
'2 Related work Our approach for emotion classification is based on the idea of <OTH> and is similar to those of <CIT> and <CIT> '
'As expected , we see that MST does better than Malt for all categories except nouns and pronouns <CIT> '
'1 Introduction The probabilistic relation between verbs and their arguments plays an important role in modern statistical parsers and supertaggers <CIT> , and in psychological theories of language processing <OTH> '
'Moreover , the overall BLEU <CIT> and METEOR <OTH> scores , as well as numbers of exact string matches -LRB- as measured against to the original sentences in the CCGbank -RRB- are higher for the hypertagger-seeded realizer than for the preexisting realizer '
'ald , 2008 -RRB- , and is also similar to the Pred baseline for domain adaptation in <CIT> '
'21 Conditional Maximum Entropy Model The goal of CME is to find the most uniform conditional distribution of y given observation x , -LRB- -RRB- xyp , subject to constraints specified by a set of features -LRB- -RRB- yxf i , , where features typically take the value of either 0 or 1 <CIT> '
'To tackle this problem , we defined 2The best results of <CIT> -LRB- LR = 884 % , LP = 891 % and F = 888 % -RRB- are achieved when the parser utilizes the information about the final punctuation and the look-ahead '
'47 Fertility-Based Transducer In <CIT> , three alignment models are described that include fertility models , these are IBM Models 3 , 4 , and 5 '
'Similarly to <CIT> , the tree-to-string alignment templates discussed in this paper are actually transformation rules '
'The hierarchical translation operations introduced in these methods call for extensions to the traditional beam decoder <CIT> '
'1http : \/ \/ wwwnistgov\/speech\/tests \/ ace \/ 49 Bootstrapping techniques have been used for such diverse NLP problems as : word sense disambiguation <CIT> , named entity classification <OTH> , IE pattern acquisition <OTH> , document classification <OTH> , fact extraction from the web <OTH> and hyponymy relation extraction <OTH> '
'<CIT> used the one sense per collocation property as an essential ingredient for an unsupervised Word-SenseDisambiguationalgorithm '
'In order to extract the linguistic features necessary for the model , all sentences were first automatically part-of-speech-tagged using a maximum entropy tagger <OTH> and parsed using the Collins parser <CIT> '
'2 Previous Work Other researchers have investigated the topic of automatic generation of abstracts , but the focus has been different , eg , sentence extraction <OTH> , processing of structured templates <OTH> , sentence compression <OTH> , and generation of abstracts from multiple sources <OTH> '
'The learning algorithm for level-0 dependency is similar to the guided learning algorithm for labeling as described in <CIT> '
'Semantic -LRB- 1 -RRB- : The named entity -LRB- NE -RRB- tag of wi obtained using the Stanford CRF-based NE recognizer <CIT> '
'To make sense tagging more precise , it is advisable to place constraint on the translation counterpart c of w SWAT considers only those translations c that has been linked with w based the Competitive Linking Algorithm <OTH> and logarithmic likelihood ratio <CIT> '
'In summary , the strength of our approach is to exploit extremely precise structural clues , and to use 5 Semantic Orientation in <CIT> '
'Various learning models have been studied such as Hidden Markov models -LRB- HMMs -RRB- <OTH> , decision trees <OTH> and maximum entropy models <CIT> '
'Phrase-pairs are then extracted from the word alignments <CIT> '
'Since this transform takes a probabilistic grammar as input , it can also easily accommodate horizontal and vertical Markovisation -LRB- annotating grammar symbols with parent and sibling categories -RRB- as described by <CIT> and subsequently '
'Aware of this problem , Resnik and <CIT> suggest creating the sense distance matrix based on results in experimental psychology such as Miller and Charles <OTH> or Resnik <OTH> '
'<OTH> observe that their predominant sense method is not performing as well for 3We use the Lesk -LRB- overlap -RRB- similarity as implemented by the WordNet : : similarity package <CIT> '
'Using thesaurus categories directly as a coarse sense division may seem to be a viable alternative <CIT> '
'Similar to BLEU score , we also use the similar Brevity Penalty BP <CIT> to penalize the short translations in computing RAcc '
'Collins et al <OTH> proposed two algorithms for NER by modifying Yarowskys method <CIT> and the framework suggested by <OTH> '
'As far as we know , language modeling always improves with additional training data , so we add data from the North American News Text Corpus -LRB- NANC -RRB- <OTH> automatically parsed with the Charniak parser <CIT> to train our language model on up to 20 million additional words '
'The problem itself has started to get attention only recently <CIT> '
'42 Models with Prior Distributions Minimum discrimination information models <OTH> are exponential models with a prior distribution q -LRB- y x -RRB- : p -LRB- y x -RRB- = q -LRB- y x -RRB- exp -LRB- summationtextF i = 1 ifi -LRB- x , y -RRB- -RRB- Z -LRB- x -RRB- -LRB- 14 -RRB- The central issue in performance prediction for MDI models is whether q -LRB- y x -RRB- needs to be accounted for '
'The most commonly used MT evaluation metric in recent years has been IBM ? s Bleu metric <CIT> '
'<OTH> propose a new metric that extends n-gram matching to include synonyms and paraphrases ; and Lavie ? s METEOR metric <CIT> can be used with additionalknowledgesuchasWordNetinordertosupport inexact lexical matches '
'We follow the work of <CIT> and choose the hypothesis that best agrees with other hypotheses on average as the backbone by applying Minimum Bayes Risk -LRB- MBR -RRB- decoding <OTH> '
'Starting from the list of 12 ambiguous words provided by <CIT> which is shown in table 2 , we created a concordance for each word , with the lines in the concordances each relating to a context window of 20 words '
'It has been shown repeatedly -- eg , Briscoe and Carroll <OTH> , Charniak <OTH> , <CIT> <OTH> , Inui et al '
'When efficient techniques have been proposed <OTH> , they have been mostly evaluated on safe pairs of languages where the notion of word is rather clear '
'Three K-means algorithms using different distributional similarity or dissimilarity measures : cosine , - skew divergence <OTH> 4 , and Lins similarity <CIT> '
'Given the following SCFG rule : VP VB NP JJR , VB NP will be JJR we can obtain a set of equivalent binary rules using the synchronous binarization method <CIT> as follows : VP V1 JJR , V1 JJR V1 VB V2 , VB V2 V2 NP , NP will be This binarization is shown with the solid lines as binarization -LRB- a -RRB- in Figure 1 '
'Weights on the components were assigned using the <CIT> method for max-BLEU training on the development set '
'Inter-annotator agreement was assessed mainly using f-score and percentage agreement as well as 11 Table 1: Annotation examples of superlative adjectives example sup span det num car mod comp set The third-largest thrift institution in Puerto Rico also [] 22 def sg no ord 37 The Agriculture Department reported that feedlots in the 13 biggest ranch states held [] 910 def pl yes no 1112 The failed takeover would have given UAL employees 75 % voting control of the nation s second-largest airline [] 1717 pos sg no ord 1418 the kappa statistics (K), where applicable (Carletta, 1996).'
'We evaluate the summaries using the automatic evaluation tool ROUGE <CIT> -LRB- described in Section 6 -RRB- and the ROUGE value works as the feedback to our learning loop '
'The models were originally introduced in <CIT> ; the current article 1 gives considerably more detail about the models and discusses them in greater depth '
'We applied the union , intersection and refined symmetrization metrics <CIT> to the final alignments output from training , as well as evaluating the two final alignments directly '
'Feature weights were set with minimum error rate training <CIT> on a development set using BLEU <OTH> as the objective function '
'The judges had an acceptable 074 mean agreement <CIT> for the assignment of the primary class , but a meaningless 021 for the secondary class -LRB- they did not even agree on which lemmata were polysemous -RRB- '
'Pereira et al <OTH> , Curran and Moens <OTH> and <CIT> use syntactic features in the vector definition '
'3 Perceptron Training The parsing problem is to find a mapping from a set of sentences x ? ? X to a set of parses y ? ? Y We assume that the mapping F is represented through a feature vector -LRB- x , y -RRB- ? ? Rd and a parameter vector ? ? Rd in the following way <CIT> : F -LRB- x -RRB- = argmax y ? GEN -LRB- x -RRB- -LRB- x , y -RRB- -LRB- 1 -RRB- where GEN -LRB- x -RRB- denotes the set of possible parses for sentence x and -LRB- x , y -RRB- = summationtexti ii -LRB- x , y -RRB- is the inner product '
'Unknown words were not identified in <CIT> as a useful predictor for the benefit of self-training '
'Unsupervised approaches are attractive due to the the availability of large quantities of unlabeled text , and unsupervised morphological segmentation has been extensively studied for a number of languages <CIT> '
'The weights 1 , , M are typically learned to directly minimize a standard evaluation criterion on development data -LRB- eg , the BLEU score ; <CIT> -RRB- using numerical search <OTH> '
'A growing body of recent research has focused on the problems of identifying and generating paraphrases , eg , <CIT> , Lin & Pantel -LRB- 2002 -RRB- , Shinyama et al , -LRB- 2002 -RRB- , Barzilay & Lee -LRB- 2003 -RRB- , and Pang et al '
'So far , most previous work on domain adaptation for parsing has focused on data-driven systems <OTH> , ie systems employing -LRB- constituent or dependency based -RRB- treebank grammars <OTH> '
'<CIT> 1996 -RRB- '
'Several researchers also studied feature\/topicbased sentiment analysis <CIT> '
'The boosting approach to ranking has been applied to named entity segmentation <CIT> and natural language generation <OTH> '
'1 minority report 2 box office 3 scooby doo 4 sixth sense 5 national guard 6 bourne identity 7 air national guard 8 united states 9 phantom menace 10 special effects 11 hotel room 12 comic book 13 blair witch project 14 short story 15 real life 16 jude law 17 iron giant 18 bin laden 19 black people 20 opening weekend 21 bad guy 22 country bears 23 mans man 24 long time 25 spoiler space 26 empire strikes back 27 top ten 28 politically correct 29 white people 30 tv show 31 bad guys 32 freddie prinze jr 33 monsters ball 34 good thing 35 evil minions 36 big screen 37 political correctness 38 martial arts 39 supreme court 40 beautiful mind Figure 7: Result of re-ranking output from the phrase extension module 6.4 Revisiting unigram informativeness An alternative approach to calculate informativeness from the foreground LM and the background LM is just to take the ratio of likelihood scores, a11 fga9a54a86 a15 a23 a11 bga9a54a86 a15 . This is a smoothed version of relative frequency ratio which is commonly used to find subject-specific terms (Damerau, 1993).'
'Finally , the parameters i of the log-linear model -LRB- 18 -RRB- are learned by minimumerror-rate training <CIT> , which tries to set the parameters so as to maximize the BLEU score <OTH> of a development set '
'Given a manually compiled lexicon containing words and their relative frequencies Ps(fprimej), the best segmentationfJ1 is the one that maximizes the joint probability of all words in the sentence, with the assumption that words are independent of each other1: fJ1 = argmax fprimeJprime1 Pr(fprimeJprime1 |cK1 )  argmax fprimeJprime1 Jprimeproductdisplay j=1 Ps(fprimej), where the maximization is taken over Chinese word sequences whose character sequence is cK1 . 2.2 Translation system Once we have segmented the Chinese sentences into words, we train standard alignment models in both directions with GIZA++ (Och and Ney, 2002) using models of IBM-1 (Brown et al., 1993), HMM (Vogel et al., 1996) and IBM-4 (Brown et al., 1993).'
'Second , the significance of the K-S distance in case of the null hypothesis -LRB- data sets are drawn from same distribution -RRB- can be calculated <OTH> '
'<CIT> ask the user to suggest a few prototypes -LRB- examples -RRB- for each class and use those as features '
'For these experiments , we have implemented an alignment package for IBM Model 4 using a hillclimbing search and Viterbi training as described in <CIT> , and extended this to use new submodels '
'The forest concept is also used in machine translation decoding , for example to characterize the search space of decoding with integrated language models <CIT> '
'POS tagging and phrase chunking in English were done using the trained systems provided with the fnTBL Toolkit <OTH> ; both were trained from the annotated Penn Treebank corpus <CIT> '
'In the experiment , only the first 500 sentences were used to train the log-linear model weight vector , where minimum error rate -LRB- MER -RRB- training was used <CIT> '
'and Semantic Knowledge Sources for Coreference Resolution <CIT> and Strube & Ponzetto -LRB- 2006 -RRB- aimed at showing that the encyclopedia that anyone can edit can be indeed used as a semantic resource for research in NLP '
'the remarks on the a3 a4 measure in <CIT> -RRB- '
'First , manyto-many word alignments are induced by running a one-to-many word alignment model , such as GIZA + + <CIT> , in both directions and by combining the results based on a heuristic <CIT> '
'1 Introduction Word alignment was first proposed as an intermediate result of statistical machine translation <CIT> '
'22 Creation of a Coarse-Grained Sense Inventory To tackle the granularity issue , we produced a coarser-grained version of the WordNet sense inventory3 based on the procedure described by <CIT> '
'To tune the decoder parameters , we conducted minimum error rate training <OTH> with respect to the word BLEU score <CIT> using 20K development sentence pairs '
'History-based models for predicting the next parser action <CIT> 3 '
'Pharaoh also includes lexical weighting parameters that are derived from the alignments used to induce its phrase pairs <CIT> '
'32 Questions and Corpus To get a clear picture of the impact of using different information extraction methods for the offline construction of knowledge bases , similarly to <CIT> , we focused only on questions about persons , taken from the TREC8 through TREC 2003 question sets '
'Moreover , in order to determine whether the performances of the predictive criteria are consistent across different learning models within the same domain , we have performed the study on two parsing models : one based on a context-free variant of tree-adjoining grammars <OTH> , the Probabilistic Lexicalized Tree Insertion Grammar -LRB- PLTIG -RRB- formalism <OTH> , and <CIT> '
'1 Introduction Robust statistical syntactic parsers , made possible by new statistical techniques <OTH> and by the availability of large , hand-annotated training corpora such as WSJ <CIT> and Switchboard <OTH> , have had a major impact on the field of natural language processing '
'data set <OTH> '
'Reported and direct speech are certainly important in discourse <CIT> ; we do not believe , however , that they enter discourse relations of the type that RST attempts to capture '
'In this paper , we follow this line of research and try to solve the problem by extending Collins perceptron algorithm <CIT> '
'A total of 216 collocations were extracted , shown in Appendix A We compared the collocations in Appendix A with the entries for the above 10 words in the NTC ''s English Idioms Dictionary -LRB- henceforth NTC-EID -RRB- <OTH> , which contains approximately 6000 definitions of idioms '
'Our chunks and functions are based on the annotations in the third release of the Penn Treebank <CIT> '
'Others proposed distributional similarity measures between words <CIT> '
'Eisner <OTH> , Charniak <OTH> , <CIT> <OTH> , and many subsequent researchers1 annotated every node with lexical features passed up from its head child , in order to more precisely reflect the nodes inside contents '
'Model 1 is the word-pair translation model used in simple machine translation and understanding models <CIT> '
'43 Scoring All-N Rules We observed that the likelihood of nouns mentioned in a definition to be referred by the concept title depends greatly on the syntactic path connecting them -LRB- which was exploited also in <CIT> -RRB- '
'Kupiec <OTH> has proposed an estimation method for the N-gram language model using the Baum-Welch reestimation algorithm <OTH> from an untagged corpus and <CIT> et al '
'Empirical evaluations using two standard summarization metricsthe Pyramid method <OTH> and ROUGE <CIT> show that the best performing system is a CRF incorporating both order-2 Markov dependencies and skip-chain dependencies , which achieves 913 % of human performance in Pyramid score , and outperforms our best-performing non-sequential model by 39 % '
'The features we used are as follows : Direct and inverse IBM model ; 3 , 4-gram target language model ; 3 , 4 , 5-gram POS language model <CIT> ; 96 Sentence length posterior probability <OTH> ; N-gram posterior probabilities within the NBest list <OTH> ; Minimum Bayes Risk probability ; Length ratio between source and target sentence ; The weights are optimized via MERT algorithm '
'corpus <OTH> , the Penn Treebank <CIT> , the SUSANNE corpus <OTH> , the Spoken English Corpus <OTH> , the Oxford Psycholinguistic Database <OTH> , and the ` Computer-Usable '' version of the Oxford Advanced Learner ''s Dictionary of Current English <OTH> '
'Following <CIT> , we call the first the source domain , and the second the target domain '
'Generally , two edges can be re-combined if they satisfy the following two constraints : 1 -RRB- the LHS -LRB- left-hand side -RRB- nonterminals are identical and the sub-alignments are the same <CIT> ; and 2 -RRB- the boundary words 1 on both sides of the partial translations are equal between the two edges <OTH> '
'For the first set of experiments , we divide all inputs based on the mean value of the average system scores as in <CIT> '
'The first , Powells method , was advocated by <CIT> when MERT was first introduced for statistical machine translation '
'The true segmentation can now be compared with the N-best list in order to train an averaged perceptron algorithm <CIT> '
'It is available in several formats , and in this paper , we use the Penn Treebank <CIT> format of NEGRA '
'In previous research on splitting sentences , many methods have been based on word-sequence characteristics like N-gram <CIT> '
'For example , sentence alignment of bilingual texts are performed just by measuring sentence lengths in words or in characters <CIT> , or by statistically estimating word level correspondences <OTH> '
'2 Statistical Machine Translation We use a log-linear approach <CIT> in which a foreign language sentence f is translated into another language , for example English , e , by seeking a maximum solution : e = argmax e wT h -LRB- f , e -RRB- -LRB- 1 -RRB- where h -LRB- f , e -RRB- is a large-dimension feature vector '
'The quality of the translation output is evaluated using BLEU <CIT> '
'On one hand , as <CIT> evidence , clusters of paraphrases can lead to better learning of text-totext rewriting rules compared to just pairs of paraphrases '
'We then apply Brills rule-based tagger <OTH> and BaseNP noun phrase chunker <CIT> to extract noun phrases from these sentences '
'Except where noted , each system was trained on 27 million words of newswire data , aligned with GIZA + + <OTH> and symmetrized with the grow-diag-final-and heuristic <CIT> '
'When an S alignment exists , there will always also exist a P alignment such that P a65 S The English sentences were parsed using a state-of-the-art statistical parser <OTH> trained on the University of Pennsylvania Treebank <OTH> '
'For each word pair from the antonym set , we calculated the distributional distance between each of their senses using Mohammad and Hirsts <OTH> method of concept distance along with the modified form of <CIT> distributional measure -LRB- equation 2 -RRB- '
'We use the same feature processing as <CIT> , with the addition of context features in a window of3 '
'1 Introduction The goal of this study has been to automatically extract a large set of hyponymy relations , which play a critical role in many NLP applications , such as Q&A systems <CIT> '
'Distributional approaches , on the other hand , rely on text corpora , and model relatedness by comparing the contexts in which two words occur , assuming that related words occur in similar context -LRB- eg , <CIT> , Lin -LRB- 1998 -RRB- , Mohammad and Hirst -LRB- 2006 -RRB- -RRB- '
'Related Work The recent availability of large amounts of bilingual data has attracted interest in several areas , including sentence alignment <OTH> , word alignment <CIT> , alignment of groups of words <OTH> , and statistical translation <OTH> '
'The most commonly used MT evaluation metric in recent years has been IBMs Bleu metric <CIT> '
'On the other hand , <CIT> extracted hyponymy relations , which are independent of the NE categories , from Wikipedia and utilized it as a gazetteer '
'For instance , word alignment models are often trained using the GIZA + + toolkit <OTH> ; error minimizing training criteria such as the Minimum Error Rate Training <OTH> are employed in order to learn feature function weights for log-linear models ; and translation candidates are produced using phrase-based decoders <OTH> in combination with n-gram language models <CIT> '
'machine translation <CIT> but also in other applications such as word sense disanabiguation <CIT> and bilingnal lexicography <OTH> '
'More recently , <CIT> & Marcus -LRB- In press -RRB- apply transformation-based learning <OTH> to the problem '
'In comparison , <CIT> achieved 48 Table 1 : A summary of the experimental results on four polysemous words '
'3 Synchronous Binarization Optimization by Cost Reduction As discussed in Section 1 , binarizing an SCFG in a fixed -LRB- left-heavy -RRB- way <CIT> may lead to a large number of competing edges and consequently high risk of making search errors '
'However , CHECK moves are almost always about some information which the speaker has been told <CIT> - a description that models the backward looking functionality of a dialogue act '
'No artificial glue-rules or rule span limits were employed7 The parameters of the translation system were trained to maximize BLEU on the MT02 test set <CIT> '
'23 Forest minimum error training To tune the feature weights of our system , we used a variant of the minimum error training algorithm <CIT> that computes the error statistics from the target sentences from the translation search space -LRB- represented by a packed forest -RRB- that are exactly those that are minimally discriminable by changing the feature weights along a single vector in the dimensions of the feature space <OTH> '
'The production weights are estimated either by heuristic counting <CIT> or using the EM algorithm '
'Consequently , considerable effort has gone into devising and improving automatic word alignment algorithms , and into evaluating their performance <CIT> '
'Baron and Hirst <OTH> extracted collocations with Xtract <CIT> and classified the collocations using the orientations of the words in the neighboring sentences '
'<CIT> proposed a symmetrical measure : Par Lin -LRB- s t -RRB- = summationtext fF s F t -LRB- w -LRB- s , f -RRB- + w -LRB- t , f -RRB- -RRB- summationtext fF s w -LRB- s , f -RRB- + summationtext fF t w -LRB- t , f -RRB- , where F s and F t denote sets of features with positive weights for words s and t , respectively '
'159 21 Baseline System The baseline system is a phrase-based SMT system <CIT> , built almost entirely using freely available components '
'Many reordering constraints have been used for word reorderings , such as ITG constraints <OTH> , IBM constraints <CIT> and local constraints <OTH> '
'For transfer-learning baseline , we implement traditional SCL model -LRB- T-SCL -RRB- <CIT> '
'We rescore the ASR N-best lists with the standard HMM <OTH> and IBM <CIT> MT models '
'While choosing an optimum window size for an application is often subject to trial and error , there are some generally recognized trade-offs between small versus large windows , such as the impact of data-sparseness , and the nature of the associations retrieved <CIT> Measures based on distance between words in the text '
'<CIT> , and the third type is a mixture of the first and second type , employing n-gram and grammarbased features , eg '
'The task originally emerged as an intermediate result of training the IBM translation models <CIT> '
'<CIT> also states that in the behavioral sciences , K -RRB- 8 signals good replicability , and 67 -LRB- K -LRB- 8 allows tentative conclusions to be drawn '
'Furthermore , these systems have tackled the problem at different levels of granularity , from the document level <OTH> , sentence level <OTH> , phrase level <OTH> , as well as the speaker level in debates <CIT> '
'Aspect-based sentiment analysis summarizes sentiments with diverse attributes , so that customers may have to look more closely into analyzed sentiments <CIT> '
'The final model V uses the weight vector w = summationtextk j = 1 -LRB- cjwj -RRB- Tn <CIT> '
'Appendix B gives a sketch of one such approach , which is based on results from <CIT> '
'As the third test set we selected all tokens of the Brown corpus part of the Penn Treebank <CIT> , a selected portion of the original one-million word Brown corpus <OTH> , a collection of samples of American English in many different genres , from sources printed in 1961 ; we refer to this test set as BROWN '
'2 Architecture of the system The goal of statistical machine translation (SMT) is to produce a target sentence e from a source sentence f. It is today common practice to use phrases as translation units (Koehn et al., 2003; Och and Ney, 2003) and a log linear framework in order to introduce several models explaining the translation process: e = argmaxp(e|f) = argmaxe {exp(summationdisplay i ihi(e,f))} (1) The feature functions hi are the system models and the i weights are typically optimized to maximize a scoring function on a development set (Och and Ney, 2002).'
'Some studies have been done for acquiring collocation translations using parallel corpora <CIT> '
'In this paper , translation quality is evaluated according to -LRB- 1 -RRB- the BLEU metrics which calculates the geometric mean of ngram precision by the system output with respect to reference translations <OTH> , and -LRB- 2 -RRB- the METEOR metrics that calculates unigram overlaps between translations <CIT> '
'We used the Penn Treebank WSJ corpus <CIT> to perform the empirical evaluation of the considered approaches '
'We compare our methods with both the averaged perceptron <CIT> and conditional random fields <OTH> using identical predicate sets '
'We report case-insensitive scores on version 06 of METEOR <OTH> with all modules enabled , version 104 of IBM-style BLEU <CIT> , and version 5 of TER <OTH> '
'For more information on these models , please refer to <CIT> '
'For example , <OTH> developed a system to identify inflammatory texts and <CIT> developed methods for classifying reviews as positive or negative '
'For Japanese sentences , instead of using full parse trees , existing sentence compression methods trim dependency trees by the discriminative model <CIT> through the use of simple linear combined features <OTH> '
'Maximum Entropy models implement the intuition that the best model is the one that is consistent with the set of constraints imposed by the evidence but otherwise is as uniform as possible <CIT> '
'Although there is a modest cost associated with annotating data , we show that a reduction of 40 % relative in alignment error -LRB- AER -RRB- is possible over the GIZA + + aligner <CIT> '
'1113 : Recursive DP equations for summing over t and a alignments are treated as a hidden variable to be marginalized out10 Optimization problems of this form are by now widely known in NLP <CIT> , and have recently been used for machinetranslationaswell <OTH> '
'BLEU Score : BLEU is an automatic metric designed by IBM , which uses several references <CIT> '
'There have been many approaches to compute the similarity between words based on their distribution in a corpus <CIT> '
'For example , since the Collins parser depends on a prior part-of-speech tagger <CIT> , we included the time for POS tagging in our Collins measurements '
'In all experiments , word alignment was obtained using the grow-diag-final heuristic for symmetrizing GIZA + + <CIT> alignments '
'In this paper , we implement the SDB model in a state-of-the-art phrase-based system which adapts a binary bracketing transduction grammar -LRB- BTG -RRB- <CIT> to phrase translation and reordering , described in <OTH> '
'Many studies focus on rare words <CIT> ; butterflies are more interesting than moths '
'In this paper we use a non-projective dependency tree CRF <CIT> '
'For a full description of the algorithm , see <CIT> '
'This direction has been forming the mainstream of research on opinion-sensitive text processing <CIT> '
'The term global feature vector is used by <CIT> to distinguish between feature count vectors for whole sequences and the local feature vectors in ME tagging models , which are Boolean valued vectors containing the indicator features for one element in the sequence '
'edu Abstract This paper reports on our experience hand tagging the senses of 25 of the most frequent verbs in 12,925 sentences of the Wall Street Journal Treebank corpus <CIT> '
'Additionally , we present results of the tagger on the NEGRA corpus <OTH> and the Penn Treebank <CIT> '
'In this paper we will compare and evaluate several aspects of these techniques , focusing on Minimum Error Rate -LRB- MER -RRB- training <CIT> and Minimum Bayes Risk -LRB- MBR -RRB- decision rules , within a novel training environment that isolates the impact of each component of these methods '
'This situation is very similar to the training process of translation models in statistical machine translation <CIT> , where parallel corpus is used to find the mappings between words from different languages by exploiting their co-occurrence patterns '
'We also implemented an averaged perceptron system <CIT> -LRB- another online learning algorithm -RRB- for comparison '
'A variety of methods have been applied , ranging from simple frequency <OTH> , modified frequency measures such as c-values <OTH> and standard statistical significance tests such as the t-test , the chi-squared test , and loglikelihood <CIT> , and information-based methods , eg pointwise mutual information <CIT> '
'We show that our semi-supervised approach yields improvements for fixed datasets by performing parsing experiments on the Penn Treebank <CIT> and Prague Dependency Treebank <OTH> -LRB- see Sections 41 and 43 -RRB- '
'Deterministic Annealing : In this system , instead of using the regular MERT <CIT> whose training objective is to minimize the onebest error , we use the deterministic annealing training procedure described in Smith and Eisner <OTH> , whose objective is to minimize the expected error -LRB- together with the entropy regularization technique -RRB- '
'Since many concepts are expressed by idiomatic multiword expressions instead of single words , and different languages may realize the same concept using different numbers of words <CIT> , word alignment based methods , which are highly dependent on the probability information at the lexical level , are not well suited for this type of translation '
'In the following , ROUGE-SN denotes ROUGE-S with maximum skip distance N ROUGE-SU <CIT> This measure is an extension of ROUGE-S ; it adds a unigram as a counting unit '
'The corpus lines retained are part-of-speech tagged <CIT> '
'<CIT> used a quasisynchronous grammar to discover the correspondence between words implied by the correspondence between the trees '
'<OTH> , which is based on that of <CIT> '
'BLEU <CIT> is one of the methods for automatic evaluation of translation quality '
',2004 -RRB- appliedextractiontechniquessimilarto Xtractsystem <CIT> ; Japanese : -LRB- Ikeharaetal '
'A number of researches which utilized distributional similarity have been conducted , including <CIT> and many others '
'Alignment spaces can emerge from generative stories <OTH> , from syntactic notions <CIT> , or they can be imposed to create competition between links <OTH> '
'Given the parallel corpus , we tagged the English words with a publicly available maximum entropy tagger <CIT> , and we used an implementation of the IBM translation model <OTH> to align the words '
'53 Related works and discussion Our two-step model essentially belongs to the same category as the works of <OTH> and <CIT> '
'It seems nevertheless that all 2Church and Hanks <OTH> , <CIT> use statistics in their algorithms to extract collocations from texts '
'Before parsing , POS tags are assigned to the input sentence using our reimplementation of the POStagger from <CIT> '
'1 Introduction For statistical machine translation -LRB- SMT -RRB- , phrasebased methods <CIT> and syntax-based methods <OTH> outperform word-based methods <OTH> '
'The lexical acquisition phase uses the GIZA + + word-alignment tool , an implementation <OTH> of IBM Model 5 <CIT> to construct an alignment of MRs with NL strings '
'In the absence of an annotated corpus , dependencies can be derived by other means , eg part413 of-speech probabilities can be approximated from a raw corpus as in <CIT> , word-sense dependencies can be derived as definition-based similarities , etc Label dependencies are set as weights on the arcs drawn between corresponding labels '
'The description of the minimum cut framework in Section 41 was inspired by <CIT> '
'As we noted in Section 5 , we are able to significantly outperform basic structural correspondence learning <CIT> '
'<CIT> demonstrates that manual mappings can be created for a small number of words with relative ease , but for a very large number of words the e ort involved in mapping would approach presented involves no be considerable '
'The output of a contextfree parser , such as that of <CIT> or Charniak -LRB- 2000 -RRB- , can be transformed into a sequence of shallow constituents for comparison with the output of a shallow parser '
'The sequence Ws is thought as a noisy version of WT and the best guess I -RRB- d ~ is then computed as ^ W ~ = argmax P -LRB- WTWs -RRB- wT = argmax P -LRB- WslWT -RRB- P -LRB- WT -RRB- -LRB- 1 -RRB- wT In <CIT> they propose a method for maximizing P -LRB- WTIWs -RRB- by estimating P -LRB- WT -RRB- and P -LRB- WsIWT -RRB- and solving the problem in equation 1 '
'21 The Standard Machine Learning Approach We use maximum entropy -LRB- MaxEnt -RRB- classification <CIT> in conjunction with the 33 features described in Ng -LRB- 2007 -RRB- to acquire a model , PC , for determining the probability that two mentions , mi and mj , are coreferent '
'The resulting Kappa statistics <CIT> over the annotated data yields a0a2a1 a3a5a4a7a6 , which seems to indicate that human annotators can reliably distinguish between coherent samples -LRB- as in Example -LRB- 1a -RRB- -RRB- and incoherent ones -LRB- as in Example -LRB- 1b -RRB- -RRB- '
'There has been an increased interest recently in employing Bayesian modeling for probabilistic grammars in different settings , ranging from putting priors over grammar probabilities <OTH> to putting non-parametric priors over derivations <OTH> to learning the set of states in a grammar <CIT> '
'When the data has distinct sub-structures , models that exploit hidden state variables are advantageous in learning <CIT> '
'To model p -LRB- fJle ~ ; 8 , T -RRB- we assume the existence of an alignment a J We assume that every word fj is produced by the word e ~ j at position aj in the training corpus with the probability P -LRB- f ~ le , ~ i -RRB- : J p -LRB- f lc '' -RRB- = 1 -RRB- p -LRB- L Icon -RRB- j = l -LRB- 7 -RRB- The word alignment a J is trained automatically using statistical translation models as described in <CIT> '
'54 IBM-3 Word Alignment Models Since the true distribution over alignments is not known , we used the IBM-3 statistical translation model <CIT> to approximate This model is specified through four components : Fertility probabilities for words ; Fertility probabilities for NULL ; Word Translation probabilities ; and Distortion probabilities '
'The models are based on a maximum entropy framework <CIT> '
'A number of researchers have explored learning words and phrases with prior positive or negative polarity -LRB- another term is semantic orientation -RRB- -LRB- eg , <CIT> -RRB- '
'In the meantime , synchronous parsing methods efficiently process the same bitext phrases while building their bilingual constituents , but continue to be employed primarily for word-to-word analysis <CIT> '
'What , therefore , has to be explored are various similarity metrics , defining similarity in a concrete way and evaluate the results against human annotations <CIT> '
'One other work that investigates the use of a limited lexicon is <CIT> , which develops a prototype-drive approach to propagate the categorical property using distributional similarity features ; using only three exemplars of each tag , they achieve a tagging accuracy of 805 % using a somewhat larger dataset but also the full Penn tagset , which is much larger '
'Although the above statement was made about translation problems faced by human translators , recent research <CIT> suggests that it also applies to problems in machine translation '
'Originally introduced as a byproduct of training statistical translation models in <CIT> , word alignment has become the first step in training most statistical translation systems , and alignments are useful to a host of other tasks '
'IBM constraints <CIT> , the lexical word reordering model <OTH> , and inversion transduction grammar -LRB- ITG -RRB- constraints <OTH> belong to this type of approach '
'Finally , <CIT> achieve an SF of 9590 % and a TF of 9134 % by 10-fold cross validation using CTB data '
'First , for each verb occurrence subjects and objects were extracted from a parsed corpus <CIT> '
'Like the work of <CIT> and Mani et al '
'The huge increase in computational and storage cost of including longer phrases does not provide a signi cant improvement in quality <CIT> as the probability of reappearance of larger phrases decreases '
'In their presentation of the factored SMT models , <CIT> describe experiments for translating from English to German , Spanish and Czech , using morphology tags added on the morphologically rich side , along with POS tags '
'1 Introduction Word alignmentdetection of corresponding words between two sentences that are translations of each otheris usually an intermediate step of statistical machine translation -LRB- MT -RRB- <CIT> , but also has been shown useful for other applications such as construction of bilingual lexicons , word-sense disambiguation , projection of resources , and crosslanguage information retrieval '
'Our goal is to come up with a mechanism that , given an input string , identifies the phrases in this string , this is a fundamental task with applications in natural language <CIT> '
'<OTH> note that the bootstrapping algorithm works well but its performance can deteriorate rapidly when non-coreferring data enter as candidate heuristics '
'Letter successor variety -LRB- LSV -RRB- models <CIT> use the hypothesis that there is less certainty when predicting the next character at morpheme boundaries '
'This can be seen as a simplified version of <CIT> '
'The MSLR parser <OTH> performs syntactic analysis of the sentence '
'<CIT> The syntactic annotation task consists of marking constituent boundaries , inserting empty categories -LRB- traces of movement , PRO , pro -RRB- , showing the relationships between constituents -LRB- argument\/adjunct structures -RRB- , and specifying a particular subset of adverbial roles '
'766 System Beam Error % <OTH> 5 337 <OTH> 1 290 <OTH> 289 Guided Learning , feature B 3 285 <OTH> all 285 <OTH> 284 <OTH> 276 Guided Learning , feature E 1 273 Guided Learning , feature E 3 267 Table 4 : Comparison with the previous works According to the experiments shown above , we build our best system by using feature set E with beam width B = 3 '
'We will briefly review the perceptron algorithm , and its convergence properties see <CIT> for a full description '
'1 Introduction Word alignment was first proposed as an intermediate result of statistical machine translation <CIT> '
'The tree is produced by a state-of-the-art dependency parser <OTH> trained on the Wall Street Journal Penn Treebank <OTH> '
'For our out-of-domain training condition , the parser was trained on sections 2-21 of the Wall Street Journal -LRB- WSJ -RRB- corpus <CIT> '
'Close to the problem studied here is Jing and McKeowns <CIT> cut-and-paste method founded on EndresNiggemeyers observations '
'We analyzed a set of articles and identified six major operations that can be used for editing the extracted sentences , including removing extraneous phrases from an extracted sentence , combining a reduced sentence with other sentences , syntactic transformation , substituting phrases in an extracted sentence with their paraphrases , substituting phrases with more general or specific descriptions , and reordering the extracted sentences <CIT> '
'Using techniques described in Church and <CIT> , Church and Hanks -LRB- 1990 -RRB- , and Hindle and Rooth -LRB- 1991 -RRB- , Figure 4 shows some examples of the most frequent V-O pairs from the AP corpus '
'We used the procedure described in <CIT> , with the only modification being the multiplication of the loglikelihood values with a triangular function that depends on the logarithm of a words frequency '
'3 Schone & Jurafsky ''s results indicate similar results for log-likelihood & T-score , and strong parallelism among information-theoretic measures such as ChiSquared , Selectional Association <OTH> , Symmetric Conditional Probability <OTH> and the Z-Score <CIT> '
'(Barzilay and McKeown, 2001; Shinyama et al. , 2002; Barzilay and Lee, 2003).'
'32 Results In line with previous work <CIT> , we first compare Naive Bayes and Logistic regression on the two NLP tasks '
'Note that it is straightforward to calculate these expected counts using a variant of the inside-outside algorithm <OTH> applied to the <OTH> dependency-parsing data structures <OTH> for projective dependency structures , or the matrix-tree theorem <CIT> for nonprojective dependency structures '
'5 Related work The methodology which is closest to our framework is ORANGE <CIT> , which evaluates a similarity metric using the average ranks obtained by reference items within a baseline set '
'However , these unsupervised methodologies show a major drawback by extracting quasi-exact2 or even exact match pairs of sentences as they rely on classical string similarity measures such as the Edit Distance in the case of <OTH> and word N-gram overlap for <CIT> '
'These constraints tie words in such a way that the space of alignments can not be enumerated as in IBM models 1 and 2 <CIT> '
'Our approach differs in important ways from the use of hidden Markov models -LRB- HMMs -RRB- for classbased language modeling <OTH> '
'4 Sub Translation Combining For sub translation combining , we mainly use the best-first expansion idea from cube pruning <CIT> to combine subtranslations and generate the whole k-best translations '
'1 Introduction Recent approaches to statistical machine translation -LRB- SMT -RRB- piggyback on the central concepts of phrasebased SMT <CIT> and at the same time attempt to improve some of its shortcomings by incorporating syntactic knowledge in the translation process '
'Both calculate the precision of a translation by comparing it to a reference translation and incorporating a length penalty <CIT> -RRB- '
'Version of the System P R F Baseline 50.8 100 67.4 Discourse-new detection only 69 72 70 Hand-coded DT: partial 62 85 71.7 Hand-coded DT: total 77 77 77 ID3 75 75 75 Table 1: Overall results by Vieira and Poesio 2.2 Bean and Riloff Bean and Riloff (1999) developed a system for identifying discourse-new DDs1 that incorporates, in addition to syntax-based heuristics aimed at recognizing predicative and established DDs using postmodification heuristics similar to those used by Vieira and Poesio, additional techniques for mining from corpora unfamiliar DDs including proper names, larger situation, and semantically functional.'
'Phrase-based MT systems are straightforward to train from parallel corpora <OTH> and , like the original IBM models <OTH> , benefit from standard language models built on large monolingual , target-language corpora <CIT> '
'Set Test Set ENGLISH-WSJ Sections Section 22 Section 23 <CIT> 2-21 ENGLISH-BROWN see 10 % of 10 % of the <OTH> ENGLISH-WSJ the data6 the data6 FRENCH7 Sentences Sentences Sentences <OTH> 1-18 ,609 18,610-19 ,609 19,609-20 ,610 GERMAN Sentences Sentences Sentences <OTH> 1-18 ,602 18,603-19 ,602 19,603-20 ,602 Table 1 : Corpora and standard experimental setups '
'Similarly to MERT , Tillmann and Zhang estimate the parameters of a weight vector on a linear combination of -LRB- binary -RRB- features using a global objective function correlated with BLEU <CIT> '
'Lacking an automatic method , recent WSD works <CIT> still resort to human intervention to identify and group closely related senses in an MRD '
'It is easy to see that the main difference between the PA algorithms and the Perceptron algorithm -LRB- PC -RRB- <CIT> as well as the MIRA algorithm <OTH> is in line 9 '
'While we can only compare class models with word models on the largest training set , for this training set model M outperforms the baseline Katzsmoothed word trigram model by 19 % absolute6 4 Domain Adaptation In this section , we introduce another heuristic for improving exponential models and show how this heuristic can be used to motivate a regularized version of minimum discrimination information -LRB- MDI -RRB- models <OTH> '
'We use a statistical POS tagging system built on Arabic Treebank data with MaxEnt framework <CIT> '
'Though our motivation is similar to that of <CIT> , we chose to build an independent component for inflection prediction in isolation rather than folding morphological information into the main translation model '
'Each dataset consisted of a collection of flat rules such as Sput ! NP put NP PP extracted from the Penn Treebank <CIT> '
'However , since we are interested in the word counts that correlate to w , we adopt the concept of the translation model proposed by <CIT> '
'By no means an exhaustive list , the most commonly cited ranking and scoring algorithms are HITS <OTH> and PageRank <OTH> , which rank hyperlinked documents using the concepts of hubs and authorities '
'BLEU : Automatic evaluation by BLEU score <CIT> '
'The modified version of the Roark parser , trained on the Brown Corpus section of the Penn Treebank <CIT> , was used to parse the different narratives and produce the word by word measures '
'With respect to already available POS tagsets , the scheme allows corresponding extensions of the supertype POSTag to , eg , PennPOSTag -LRB- for the Penn Tag Set <CIT> -RRB- or GeniaPOSTag -LRB- for the GENIA Tag Set <OTH> -RRB- '
'of Words Person names 803 1749 Organization names 312 867 Location names 345 614 The BLEU score <CIT> with a single reference translation was deployed for evaluation '
'7 For the most frequent 184 expressions , on the average , the agreement rate between two human annotators is 093 and the Kappa value is 073 , which means allowing tentative conclusions to be drawn <CIT> '
'Except where noted , each system was trained on 27 million words of newswire data , aligned with GIZA + + <CIT> and symmetrized with the grow-diag-final-and heuristic <OTH> '
'It uses a log-linear model to define a distribution over the lexical category set for each word and the previous two categories <CIT> and the forward backward algorithm efficiently sums over all histories to give a distibution for each word '
'There has been a large number of studies in tagging and morphological disambiguation using various techniques such as statistical techniques , eg , <CIT> , constraint-based techniques <OTH> and transformation-based techniques <OTH> '
'Because their joint distributions have such closed-form expressions , the parameters can be estimated directly from the training data without the need for an iterative fitting procedure -LRB- as is required , for example , to estimate the parameters of maximum entropy models ; <CIT> -RRB- '
'For example , smoothing methods have played a central role in probabilistic approaches <CIT> , and yet they are not being used in current large margin training algorithms '
'Running words 1,864 14,437 Vocabulary size 569 1,081 Table 2 : ChineseEnglish corpus statistics <CIT> using Phramer <OTH> , a 3-gram language model with Kneser-Ney smoothing trained with SRILM <OTH> on the English side of the training data and Pharaoh <OTH> with default settings to decode '
'This task is quite common in corpus linguistics and provides the starting point to many other algorithms , eg , for computing statistics such as pointwise mutual information <CIT> , for unsupervised sense clustering <OTH> , and more generally , a large body of work in lexical semantics based on distributional profiles , dating back to Firth <OTH> and Harris <OTH> '
'The score for a given candidate a9 is given by a modified IBM Model 1 probability (Brown et al. , 1993) as follows: a2a4a3a6a9a21a10a13a12a15a7a14a2 a15 a24a26a17a16 a2a4a3a6a9a19a18 a14a15a10a12 a7 (4) a2 a15 a20 a24a16a22a21a24a23a26a25a1a27a28a27a28a27 a20 a24a16a30a29a1a23a26a25 a31 a32 a33 a23a35a34a37a36 a3a38a12 a33 a10a12a9 a16a8a39 a7 (5) where a40 is the length of a9, a41 is the length of a12, a15 is a scaling factor based on the number of matches of a9 found, and a14 a33 is the index of the English word aligned with a12 a33 according to alignment a14 . The probability a36 a3a6a9 a16a8a39 a10a12 a33 a7 is a linear combination of the transliteration and translation score, where the translation score is a uniform probability over all dictionary entries for a12 a33 . The scored matches form the list of translation candidates.'
'The heuristic estimator employs word-alignment -LRB- Giza + + -RRB- <CIT> and a few thumb rules for defining phrase pairs , and then extracts a multi-set of phrase pairs and estimates their conditional probabilities based on the counts in the multi-set '
'We say that wv and nq are semantically related if w ~ i and nq are semantically related and -LRB- wp , nq -RRB- and -LRB- w ~ i , nq -RRB- are semantically similar <CIT> '
'These rules are learned using a word alignment model , which finds an optimal mapping from words to MR predicates given a set of training sentences and their correct MRs Word alignment models have been widely used for lexical acquisition in SMT <CIT> '
'In contrast to the opinion extracts produced by <CIT> , our summaries are not text extracts , but rather explicitly identify and 337 characterize the relations between opinions and their sources '
'<OTH> , <OTH> , <OTH> , <OTH> , <CIT> '
'The hallucination process is motivated by the use of NULL alignments into Markov alignment models as done by <CIT> '
'Some of the data comes from the parsed files 2-21 of the Wall Street Journal Penn Treebank corpus <CIT> , and additional parsed text was obtained by parsing the 1987 Wall Street Journal text using the parser described in Charniak et al '
'and Gildea , 2007 ; <CIT> et al , 2006 ; Gildea , Satta , and <CIT> , 2006 -RRB- '
'Work in <CIT> modeled the limited information available at phrase-boundaries '
'Incremental Sigmoid Belief Networks <CIT> differ from simple dynamic SBNs in that they allow the model structure to depend on the output variable values '
'Second , the automatic approach , in which the model is automatically obtained from corpora -LRB- either raw or annotated -RRB- 1 , and consists of n-grams <CIT> , rules <OTH> or neural nets <OTH> '
'This approach is usually referred to as the noisy source-channel approach in statistical machine translation <CIT> '
'<CIT> use a BLEU oracle decoder for discriminative training of a local reordering model '
'Templates for local features are similar to the ones employed by <CIT> for POS-tagging -LRB- Table 3 -RRB- , though as our input already includes POStags , we can make use of part-of-speech information as well '
'From <CIT> 9 Combined metric BY BP B4AC BE B7BDB5C8CABPB4AC BE C8 B7 CAB5 , from <OTH> , AC BPBD '
'Generally , WSD methods use the context of a word for its sense disambiguation , and the context information can come from either annotated\/unannotated text or other knowledge resources , such as WordNet <OTH> , SemCor <OTH> , Open Mind Word Expert <CIT> , eXtended WordNet <OTH> , Wikipedia <CIT> , parallel corpora <OTH> '
'Giving the increasing sophistication of probabilistic linguistic models -LRB- for example , <CIT> has a statistical approach to learning gap-threading rules -RRB- a probabilistic extension of our work is attractive -- it will be interesting to see how far an integration of ` logical '' and statistical can go '
'The subset was the neighboring alignments <CIT> of the Viterbi alignments discovered by Model 1 and Model 2 '
'To support this claim , first , we used the coefficient <CIT> to assess the agreement between the classification made by FLSA and the classification from the corpora see Table 8 '
'<CIT> share the goal underlying our own research : improving , rather than replacing , Ochs MERT procedure '
'For example , the topics Sport and Education are important cues for differentiating mentions of Michael Jordan , which may refer to a basketball player , a computer science professor , etc Second , as noted in the top WePS run <CIT> , feature development is important in achieving good coreference performance '
'2 Background Default unification has been investigated by many researchers <OTH> in the context of developing lexical semantics '
'The decoder is capable of producing nbest derivations and nbest lists <OTH> , which are used for Maximum Bleu training <CIT> '
'61 Distributional cluster (Brown et al. , 1992): tie, jacket, suit Word ''tie'' (7 alternatives) 0.0000 0.0000 0.0000 1.0000 0.0000 0.0000 0.0000 draw, standoff, tie, stalemate affiliation, association, tie, tie-up: a social or business relationship tie, crosstie, sleeper: subconcept of brace, bracing necktie, tie link, linkup, tie, tie-in: something that serves to join or link drawstring, string, tie: cord used as a fastener tie, tie beam: used to prevent two rafters, e.g., from spreading apart Word ''jacket'' (4 alternatives) 0.0000 book jacket, dust cover: subeoncept of promotional material 0.0000 jacket crown, jacket: artificial crown fitted over a broken or decayed tooth 0.0000 jacket: subconceptofwrapping, wrap, wrapper 1.0000 jacket: a short coat Word ''suit'' (4 alternatives) 0.0000 suit, suing: subconcept of entreaty, prayer, appeal 1.0000 suit, suit of clothes: subconcept of garment 0.0000 suit: any of four sets of13'' cards in a paek 0.0000 legal action, action, case, lawsuit, suit: a judicial proceeding This cluster was derived by Brown et al. using a modification of their algorithm, designed to uncover ''semantically sticky'' clusters.'
'Others use sentence cohesion <CIT> , agreement\/disagreement between speakers <OTH> , or structural adjacency '
'2 Word Alignment algorithm We use IBM Model 4 <CIT> as a basis for our word alignment system '
'52 Translation experiments with a bigram language model In this section we consider two real translation tasks , namely , translation from English to French , trained on Europarl <CIT> and translation from German to Spanish training on the NewsCommentary corpus '
'Example of such algorithms are <OTH> and <CIT> that use syntactic features in the vector definition '
'More specifically , two recent works have suggested to use statistical data on lexical relations for resolving ambiguity cases of PP-attachment <CIT> and pronoun references <OTH> '
'The table also shows the - score , which is another commonly used measure for inter-annotator agreement <CIT> '
'For getting the syntax trees , the latest version of Collins parser <CIT> was used '
'c2009 Association for Computational Linguistics Automatic Treebank-Based Acquisition of Arabic LFG Dependency Structures Lamia Tounsi Mohammed Attia NCLT, School of Computing, Dublin City University, Ireland {lamia.tounsi, mattia, josef}@computing.dcu.ie Josef van Genabith Abstract A number of papers have reported on methods for the automatic acquisition of large-scale, probabilistic LFG-based grammatical resources from treebanks for English (Cahill and al., 2002), (Cahill and al., 2004), German (Cahill and al., 2003), Chinese (Burke, 2004), (Guo and al., 2007), Spanish (ODonovan, 2004), (Chrupala and van Genabith, 2006) and French (Schluter and van Genabith, 2008).'
'Recently , specic probabilistic tree-based models have been proposed not only for machine translation <OTH> , but also for summarization <OTH> , paraphrasing <OTH> , natural language generation <OTH> , parsing , and language modeling -LRB- Baker 1979 ; Lari and Young 1990 ; <CIT> 1997 ; Chelba and Jelinek 2000 ; Charniak 2001 ; Klein Information Sciences Institute , 4676 Admiralty Way , Marina del Rey , CA 90292 '
'Monotone Nonmonotone Target B A Positions C D Source Positions Figure 1 : Two Types of Alignment The IBM model 1 -LRB- IBM-1 -RRB- <CIT> assumes that all alignments have the same probability by using a uniform distribution : p -LRB- fJ1 eI1 -RRB- = 1IJ Jproductdisplay j = 1 Isummationdisplay i = 1 p -LRB- fj ei -RRB- -LRB- 2 -RRB- We use the IBM-1 to train the lexicon parameters p -LRB- f e -RRB- , the training software is GIZA + + <OTH> '
'The system is tested on base noun-phrase -LRB- NP -RRB- chunking using the Wall Street Journal corpus <CIT> '
'1 Introduction Most empirical work in translation analyzes models and algorithms using BLEU <CIT> and related metrics '
'This fact is being seriously challenged by current research -LRB- -RRB- , and might not be true in the near future <CIT> '
'To perform minimum error rate training <CIT> to tune the feature weights to maximize the systems BLEU score on development set , we used the script optimizeV5IBMBLEUm <OTH> '
'These tags are drawn from a tagset which is constructed by extending each argument label by three additional symbols a11 a24 a35 a24a4a12 , following <CIT> '
'The named-entity features are generated by the freely available Stanford NER tagger <CIT> '
'Some of them use human reference translations , eg , the BLEU method <CIT> , which is based on comparison of N-gram models in MT output and in a set of human reference translations '
'<OTH> -RRB- , or by using linguistic evidence , mostly lexical similarity -LRB- METEOR , <CIT> ; MaxSim , Chan and Ng -LRB- 2008 -RRB- -RRB- , or syntactic overlap -LRB- Owczarzak et al '
'Instead of using the NP bracketing information present in the tagged Treebank data , <CIT> and Marcus modified the data so as to include bracketing information related only to the non-recursive , base NPs present in each sentence while the subject verb phrases were taken as is The data sets include POS tag information generated by <CIT> and Marcus using Brill ''s transformational part-of-speech tagger <OTH> '
'Recently , many phrase reordering methods have been proposed , ranging from simple distancebased distortion model <CIT> , flat reordering model <OTH> , lexicalized reordering model <OTH> , to hierarchical phrase-based model <OTH> and classifier-based reordering model with linear features <OTH> '
'This was done for supervised parsing in different ways by <CIT> , Klein and Manning -LRB- 2003 -RRB- , and McDonald et al '
'While Schiitze and Pedersen <OTH> , <CIT> and Futrelle and Gauch <OTH> all demonstrate the ability of their systems to identify word similarity using clustering on the most frequently occurring words in their corpus , only Grefenstette <OTH> demonstrates his system by generating word similarities with respect to a set of target words '
'Intercoder reliability was assessed using Cohen ''s Kappa statistic -LRB- ~ -RRB- <CIT> '
'This technique is called system combination <CIT> '
'The agreement on identifying the boundaries of units , using the AK statistic discussed in <CIT> , was AK BP BMBL -LRB- for two annotators and 500 units -RRB- ; the agreement on features -LRB- 2 annotators and at least 200 units -RRB- was follows : Attribute AK Value utype 76 verbed 9 finite 81 subject 86 NPs Our instructions for identifying NP markables derive from those proposed in the MATE project scheme for annotating anaphoric relations <OTH> '
'As machine learners we used SVM-light1 <OTH> and the MaxEnt decider from the Stanford Classifier2 <CIT> '
'In order to estimate the entropy of English , <CIT> approximated P -LRB- kI <UNK> -RRB- by a Poisson distribution whose parameter is the average word length A in the training corpus , and P -LRB- cz cklk , <UNK> -RRB- by the product of character zerogram probabilities '
'32 Conversion to Dependencies 321 Syntactic Dependencies There exists no large-scale dependency treebank for English , and we thus had to construct a dependency-annotated corpus automatically from the Penn Treebank <CIT> '
'Some researchers have concentrated on producing WSD systems that base results on a limited number of words , for example <CIT> and Schtitze -LRB- 1992 -RRB- who quoted results for 12 words , and a second group , including Leacock , Towell , and Voorhees <OTH> and Bruce and Wiebe <OTH> , who gave results for just one , namely interest '
'They recover additional latent variables so-called nuisance variablesthat are not of interest to the user1 For example , though machine translation -LRB- MT -RRB- seeks to output a string , typical MT systems <CIT> 1These nuisance variables may be annotated in training data , but it is more common for them to be latent even there , ie , there is no supervision as to their correct values '
'It has been further observed that simply compressing sentences individually and concatenating the results leads to suboptimal summaries <CIT> '
'Another way to look the algorithm is from the self-training perspective <CIT> '
'Stochastic taggers use both contextual and morphological information , and the model parameters are usually defined or updated automatically from tagged texts -LRB- Cerf-Danon and E1-Beze 1991 ; Church 1988 ; Cutting et al 1992 ; Dermatas and Kokkinakis 1988 , 1990 , 1993 , 1994 ; Garside , Leech , and Sampson 1987 ; Kupiec 1992 ; Maltese \* Department of Electrical Engineering , Wire Communications Laboratory -LRB- WCL -RRB- , University of Patras , 265 00 Patras , Greece '
'In addition , we also made a word alignment available , which was derived using a variant of the current default method for word alignment <CIT> s refined method '
'In contrast , more recent research has focused on stochastic approaches that model discourse coherence at the local lexical <OTH> and global levels <OTH> , while preserving regularities recognized by classic discourse theories <OTH> '
'In Smadja ''s collocation algorithm Xtract , the lowest-frequency words are effectively discarded as well <CIT> '
'We are encoding the knowledge as axioms in what is for the most part a first-order logic , described by <CIT> , although quantification over predicates is sometimes convenient '
'-LRB- 2 -RRB- X1\/X2 Y1 : r1\/Y2 : r2 , -LRB- i1 , j1 , i2 , j2 -RRB- , Y1\/Y2 , -LRB- j1 , k1 , j2 , k2 -RRB- X1\/X2 Y1 : r1\/Y2 : r2 , -LRB- i1 , k1 , i2 , k2 -RRB- -LRB- 3 -RRB- X1\/X2 Y1 : r1\/Y2 : r2 , -LRB- i1 , j1 , j2 , k2 -RRB- , Y1\/Y2 , -LRB- j1 , k1 , i2 , j2 -RRB- X1\/X2 Y1 : r1\/Y2 : r2 , -LRB- i1 , k1 , i2 , k2 -RRB- Since each inference rule contains six free variables over string positions -LRB- i1 , j1 , k1 , i2 , j2 , k2 -RRB- , we get a parsing complexity of order O -LRB- n6 -RRB- for unlexicalized grammars -LRB- where n is the number of words in the longer of the two strings from language L1 and L2 -RRB- <CIT> '
'It ewduatos the pairwise agreement mnong a set ; of coders making categoryiudgment , correcting tbr expected chance agreement <CIT> '
'Consequently , the mainstream research in the literature has been focused on the modeling and utilization of local and sentential contexts , either linguistically in a rule-based framework or statistically in a searching and optimization set-up <CIT> '
'2 Background The natural language generator used in our experiments is the WSJ-trained system described in <CIT> and Hogan et al '
'Nevertheless , the generated rules are strictly required to be derived from the contiguous translational equivalences <CIT> '
'Previous approaches to the problem <CIT> have all been learning-based ; the primary difference between the present algorithm and earlier ones is that it is not learned , but explicitly incorporates principles of GovernmentBinding theory <OTH> , since that theory underlies the annotation '
'This does not seem to be the case , however , for common feature weighting functions , such as Point-wise Mutual Information <CIT> '
'Many statistical translation models <CIT> try to model word-toword correspondences between source and target words '
'<CIT> approached chucking by using Transformation Based Learning -LRB- TBL -RRB- '
'We use only the words that are content words -LRB- nouns , verbs , or adjectives -RRB- and not in the stopword list used in ROUGE <CIT> '
'However , certain properties of the BLEU metric can be exploited to speed up search , as described in detail by <CIT> '
'To solve the problem , <CIT> apply an automatic generation grammar transformation to their training data : they automatically label CFG nodes with additional case information and the model now learns the new improved generation rules of Tables 4 and 5 '
'This obviously does not preclude using the audio-based system together with other features such as utterance position , length , speakers roles , and most others used in the literature <CIT> '
'We used the implementation of MaxEnt classifier described in <CIT> '
'Averaged Perceptron Algorithm 5 Experiments We evaluate our method on both Chinese and English syntactic parsing task with the standard division on Chinese Penn Treebank Version 50 and WSJ English Treebank 30 <CIT> as shown in Table 1 '
'1 Introduction In this paper , we study the use of so-called word trigger pairs -LRB- for short : word triggers -RRB- <OTH> to improve an existing language model , which is typically a trigram model in combination with a cache component <OTH> '
'The latter approach has become increasingly popular <OTH> '
'The trigger-based lexicon model used in this work follows the training procedure introduced in <CIT> and is integrated directly in the decoder instead of being applied in n-best list reranking '
'The problem itself has started to get attention only recently <CIT> '
'alpha 0 0.1 0.2 0.3 0.4 0.5 Freq=2 13555 13093 12235 11061 10803 10458 Freq=3 4203 3953 3616 3118 2753 2384 Freq=4 1952 1839 1649 1350 1166 960 Freq=5 1091 1019 917 743 608 511 Freq>2 2869 2699 2488 2070 1666 1307 TOTAL 23670 22603 20905 18342 16996 15620 alpha 0.6 0.7 0.8 0.9 1.0 Freq=2 10011 9631 9596 9554 9031 Freq=3 2088 1858 1730 1685 1678 Freq=4 766 617 524 485 468 Freq=5 392 276 232 202 189 Freq>2 1000 796 627 517 439 TOTAL 14257 13178 12709 12443 11805 Table 7: Number of extracted MWUs by frequency 6.2 Qualitative Analysis As many authors assess (Frank Smadja, 1993; John Justeson and Slava Katz, 1995), deciding whether a sequence of words is a multiword unit or not is a tricky problem.'
'A sinfilar approach has been chosen by <OTH> '
'5 Discussion As stated above , we aim to build an unsupervised generative model for named entity clustering , since such a model could be integrated with unsupervised coreference models like <CIT> for joint inference '
'However , there is little agreement on what types of knowledge are helpful : Some suggestions concentrate on lexical information , eg , by the integration of word similarity information as in Meteor <CIT> or MaxSim <OTH> '
'We have explained elsewhere <OTH> how suitable features can be defined in terms of the a18 word , pos-tag a20 pairs in the context , and how maximum entropy techniques can be used to estimate the probabilities , following <CIT> '
'This statistic is given by -2 log A = 2 -LRB- log L -LRB- p1 , kl , hi -RRB- log L -LRB- p2 , k2 , n2 -RRB- - log L -LRB- p , kl , R1 -RRB- -- log L -LRB- p , k2 , n2 -RRB- -RRB- , where log LCo , k , n -RRB- = k logp + -LRB- n k -RRB- log -LRB- 1 - p -RRB- , and Pl = ~ , P2 = ~ , P = , ~ '' , ~ ; -LRB- For a detailed description of the statistic used , see <CIT> -RRB- '
'High-performance taggers typically also include joint three-tag counts in some way , either as tag trigrams <OTH> or tag-triple features <CIT> '
'Initial estimates of lexical translation probabilities came from the IBM Model 4 translation tables produced by GIZA + + <CIT> '
'1 Introduction In the field of statistical parsing , various probabilistic evaluation models have been proposed where different models use different feature types <OTH> <OTH> <OTH> <OTH> <CIT> <CIT> <OTH> <OTH> <OTH> <OTH> '
'For the evaluation of translation quality , we applied standard automatic evaluation metrics , ie , BLEU <CIT> and METEOR <OTH> '
'Since we need knowledge-poor Daille , 1996 -RRB- induction , we can not use human-suggested filtering Chi-squared -LRB- G24 -RRB- 2 <OTH> Z-Score <CIT> Students t-Score <OTH> n-gram list in accordance to each probabilistic algorithm '
'As we remarked earlier , however , the input data required by our method -LRB- triples -RRB- could be generated automatically from unparsed corpora making use of existing heuristic rules <CIT> , although for the experiments we report here we used a parsed corpus '
'Second , in keeping with ontological promiscuity <CIT> , we represent the importance of attributes by the salience of events and states in the discourse model -- these states and events now have the same status in the discourse model as any other entities '
'It has been claimed that content analysis researchers usually regard a -RRB- 8 to demonstrate good reliability and 67 -LRB- ~ -LRB- 8 alf16 lows tentative conclusions to be drawn -LRB- see <CIT> -RRB- '
'In recent years , many researchers have employed statistical models <CIT> or association measures <OTH> to build alignment links '
'Breidt <OTH> alsopointedouta coupleof problemsthatmakes extractionfor Germanmoredifficultthanfor English : the stronginflectionfor verbs , the variable word-order , andthepositionalambiguityoftheargumentsSheshowsthatevendistinguishingsubjectsfromobjectsisverydifficultwithoutparsing '
'TB TBR JJ , JJR , JJS JJ RB , RBR , RBS RB CD , LS CD CC CC DT , WDT , PDT DT FW FW MD , VB , VBD , VBG , VBN , VBP , VBZ , VH , VHD , VHG , VHN , VHP , VHZ MD NN , NNS , NP , NPS NN PP , WP , PP$ , WP$ , EX , WRB PP IN , TO IN POS PO RP RP SYM SY UH UH VV , VVD , VVG , VVN , VVP , VVZ VB <CIT> '
'The applications range from simple classification tasks such as text classification and history-based tagging <CIT> to more complex structured prediction tasks such as partof-speech -LRB- POS -RRB- tagging <OTH> , syntactic parsing <OTH> and semantic role labeling <OTH> '
'22 Automatic metrics Similarly to the Pyramid method , ROUGE <CIT> and Basic Elements <OTH> require multiple topics and model summaries to produce optimal results '
'Previous work on building hybrid systems includes , among others , approaches using reranking , regeneration with an SMT decoder <OTH> , and confusion networks <CIT> '
'The probabilities from these back-off levels are interpolated using the techniques in <CIT> '
'The idea of word class <CIT> gives a general solution to this problem '
'<OTH> and Nivre and <CIT> can be seen as methods to combine separately defined models '
'To be able identify that adjacent blocks -LRB- eg , the development and and progress -RRB- can be merged into larger blocks , our model infers binary -LRB- non-linguistic -RRB- trees reminiscent of <CIT> '
'To train models , we used projectivized versions of the training dependency trees2 1We are grateful to the providers of the treebanks that constituted the data for the shared task <CIT> '
'Some approaches have used syntax at the core <CIT> while others have integrated syntax into existing phrase-based frameworks <OTH> '
'Based on IBM Model 1 lexical parameters <CIT> , providing a complementary probability for each tuple in the translation table '
'In addition , a number of approaches have focused on developing discriminative approaches for unsupervised and semi-supervised tagging <CIT> '
'However for remedy , many of the current word alignment methods combine the results of both alignment directions , via intersection or 249 grow-diag-final heuristic , to improve the alignment reliability <CIT> '
'Correspondences between MALTUS and other tagsets <OTH> were also provided <OTH> '
'The extraction procedure consists of three steps : First , the bracketing of the trees in the Penn Treebank is corrected and extended based on the approaches of Magerman <OTH> and <CIT> '
'1 Introduction Word alignment , which can be defined as an object for indicating the corresponding words in a parallel text , was first introduced as an intermediate result of statistical translation models <CIT> '
'1 Introduction Most recent approaches in SMT , eg <CIT> , use a log-linear model to combine probabilistic features '
'Thispaperfocusesontheframeworkintroduced in Figure 2 for two reasons : -LRB- a -RRB- cautious al50 gorithms were shown to perform best for several NLP problems -LRB- including acquisition of IE patterns -RRB- , and -LRB- b -RRB- it has nice theoretical properties : <CIT> showed that , regardless of the selection procedure , sequential bootstrapping algorithms converge to a local minimum of K , where K is an upper bound of the negative log likelihood of the data '
'The information content of this set is defined as mutual information I -LRB- F -LRB- w -RRB- -RRB- <CIT> '
'The MT systems of <CIT> learn to generate text in the target language straight from the source language , without the aid of an explicit semantic representation '
'Many of the current approaches of domain modeling collapse together different instances and make the decision on what information is important for a domain based on this generalized corpus <CIT> '
'The benefits of using grammatical information for automatic WSD were first explored by <CIT> and Resnik -LRB- 1996 -RRB- in unsupervised approaches to disambiguating single words in context '
'determining document orientation -LRB- or polarity -RRB- , as in deciding if a given Subjective text expresses a Positive or a Negative opinion on its subject matter <CIT> ; 3 '
'A variety of algorithms -LRB- eg , bootstrapping <CIT> , co-training <OTH> , alternating structure optimization <OTH> , etc -RRB- '
'Since Chinese text is not orthographically separated into words , the standard methodology is to first preproce ~ input texts through a segmentation module <OTH> '
'Several authors have used mutual information and similar statistics as an objective function for word clustering <CIT> , for automatic determination of phonemic baseforms <OTH> , and for language modeling for speech recognition <OTH> '
'Uses Maximum Entropy <CIT> classification , trained on JNLPBA <OTH> -LRB- NER -RRB- '
'Word-aligned corpora have been found to be an excellent source for translation-related knowledge , not only for phrase-based models <CIT> , but also for syntax-based models -LRB- eg , <OTH> -RRB- '
'One judge annotated allarticles in four datasets of the Wall Street Journal Treebank corpus <CIT> -LRB- W9-4 , W9-10 , W9-22 , and W933 , each approximately 160K words -RRB- as well as thecorpusofWall Street Journal articles used in <OTH> -LRB- called WSJ-SE below -RRB- '
'We set all feature weights by optimizing Bleu <CIT> directly using minimum error rate training -LRB- MERT -RRB- <OTH> on the tuning part of the development set <OTH> '
'We use the Penn Treebank Wall Street Journal corpus as the large corpus and individual sections of the Brown corpus as the target corpora <CIT> '
'The most common answer is component testing , where the component is compared against a standard of goodness , usually the Penn Treebank for English <CIT> , allowing a numerical score of precision and recall <OTH> '
'In the present work , the approach taken by <CIT> is used to derive such values for selected phrases in the text '
'Mutual infornaation involves a problem in that it is overestimated for low-frequency terms -LRB- I -RRB- unning 1993 -RRB- '
'<OTH> uses the mutual information clustering algorithm described in <CIT> '
'The second attempts to instill knowledge of collocations in the data ; we use the technique described by <CIT> to compute multi-word expressions and then mark words that are commonly used as such with a feature that expresses this fact '
'Several sentiment information retrieval models were proposed in the framework of probabilistic language models by <CIT> '
'Word alignments were produced by GIZA + + <CIT> with a standard training regimen of five iterations of Model 1 , five iterations of the HMM Model , and five iterations of Model 4 , in both directions '
'32 Details To learn alignments , translation probabilities , etc in the first method we used work that has been done in statistical machine translation <CIT> , where the translation process is considered to be equivalent to a corruption of the source language text to the target language text due to a noisy channel '
'We apply the log likelihood principle <CIT> to compute this score '
'For automatic evaluation , we employed BLEU <CIT> by following <OTH> '
'The MBT <OTH> 180 Tagger Type Standard Trigram <OTH> MBT <OTH> Rule-based <OTH> Maximum-Entropy <OTH> Full Second-Order HMM SNOW <OTH> Voting Constraints <OTH> Full Second-Order HMM Known Unknown Overall Open\/Closed Lexicon ? '
'1 Introduction The Inversion Transduction Grammar or ITG formalism , which historically was developed in the context of translation and alignment , hypothesizes strong expressiveness restrictions that constrain paraphrases to vary word order only in certain allowable nested permutations of arguments <CIT> '
'Table 6 contrasts our results with those from <CIT> '
'Because of this , it is generally accepted that some kind of postprocessing should be performed to improve the final result , by shortening , fusing , or otherwise revising the material <CIT> '
'The other approach selected was <CIT> '
'The tagger from <CIT> first annotates sentences of raw text with a sequence of partof-speech tags '
'<CIT> used both supervised and unsupervised WSD for correct phonetizitation of words in speech synthesis '
'The former is a task of identifying positive and negative sentiments from a text which can be a passage , a sentence , a phrase and even a word <CIT> '
'2 Statistical Word Alignment According to the IBM models <OTH> , the statistical word alignment model can be generally represented as in Equation -LRB- 1 -RRB- '
'Further work will look at how to integrate probabilities such as p -LRB- clv , r -RRB- into a model of dependency structure , similar to that of <CIT> and Collins -LRB- 1997 -RRB- , which can be used -LRB- ` or parse selection '
'If POS denotes the POS of the English word , we can define the word-to-word distance measure -LRB- Equation 4 -RRB- as POS POS -LRB- 15 -RRB- Ratnaparkhis POS tagger <CIT> was used to obtain POS tags for each word in the English sentence '
'43 Baseline We use a standard log-linear phrase-based statistical machine translation system as a baseline : GIZA + + implementation of IBM word alignment model 4 <OTH> ,8 the refinement and phrase-extraction heuristics described in <CIT> , minimum-error-rate training 7More specifically , we choose the first English reference from the 7 references and the Chinese sentence to construct new sentence pairs '
'Surprisingly , though , rather little work has been devoted to learning local syntactic patterns , mostly noun phrases <CIT> '
'1 Introduction Finite-state parsing -LRB- also called chunking or shallow parsing -RRB- has typically been motivated as a fast firstpass for or approximation to more expensive context-free parsing <CIT> '
'This approach gave an improvement of 27 in BLEU <CIT> score on the IWSLT05 Japanese to English evaluation corpus -LRB- improving the score from 524 to 551 -RRB- '
'For instance , the Penn Treebank policy <CIT> is to annotate the lowest node that is unfinished with an - UNF tag as in Figure 4 -LRB- a -RRB- '
'This definition is similar to that of minimal translation units as described in <CIT> , although they allow null words on either side '
'122 SPECIFIC SYNTACTIC AND SEMANTIC ASSUMPTIONS The basic scheme , or some not too distant relative , is the one used in many large-scale implemented systems ; as examples , we can quote TEAM <OTH> , PUNDIT <OTH> , TACITUS <CIT> , MODL <OTH> , CLE <OTH> , and SNACK-85 <OTH> '
'Of particular relevance is other work on parsing the Penn WSJ Treebank <CIT> '
'In practice , we used MMR in our experiments , since the original MEAD considers also sentence positions 3 , which can always been added later as in <CIT> '
'The loglinear model weights are learned using Chiangs implementation of the maximum BLEU training algorithm <CIT> , both for the baseline , and the WSD-augmented system '
'On the other hand , structural annotation such as that used in syntactic treebanks <CIT> assigns a syntactic category to a contiguous sequence of corpus positions '
'Evaluations are typically carried out on newspaper texts , ie on section 23 of the Penn Treebank -LRB- PTB -RRB- <CIT> '
'We use the finite-state parses of FaSTU $ <OTH> for recognizing these entities , but the method extends to any basic phrasal parser 4 '
'The last line shows the results of <CIT> -LRB- recognizing NP ''s -RRB- with the same train\/test data '
'Prior to running the parsers , we trained the POS tagger described in <CIT> '
'9 The definition of BLEU used in this training was the original IBM definition <CIT> , which defines the effective reference length as the reference length that is closest to the test sentence length '
'287 System Train + base Test + base 1 Baseline 8789 8789 2 Contrastive 8870 082 8845 056 -LRB- 5 trials\/fold -RRB- 3 Contrastive 8882 093 8855 066 -LRB- greedy selection -RRB- Table 1 : Average F1 of 7-way cross-validation To generate the alignments , we used Model 4 <CIT> , as implemented in GIZA + + <OTH> '
'More rare words rather than common words are found even in standard dictionaries <CIT> '
'Several researchers also studied feature\/topicbased sentiment analysis <CIT> '
'The noun phrase chunking -LRB- NP chunking -RRB- module uses the basic NP chunker software from 483 <CIT> to recognize the noun phrases in the question '
'221 BLEU Evaluation The BLEU score <CIT> was defined to measure overlap between a hypothesized translation and a set of human references '
'We tune all feature weights automatically <CIT> to maximize the BLEU <OTH> score on the dev set '
'Other linear time algorithms for rank reduction are found in the literature <CIT> , but they are restricted to the case of synchronous context-free grammars , a strict subclass of the LCFRS with f = 2 '
'In the above equation , P -LRB- ti -RRB- and P -LRB- wi ; t -RRB- are estimated by the maximum-likelihood method , and the probability of a POC tag ti , given a character wi -LRB- P -LRB- tijwi ; ti 2 TPOC -RRB- -RRB- is estimated using ME models <CIT> '
'Hyperparameter is automatically selected from 2Although <CIT> that for their dataset similar to ours was 083 , this value can not be directly compared with our value because their dataset includes both individual words and pairs of words '
'-LRB- This example is adapted from Resnik <OTH> -RRB- '
'as follows : p -LRB- synI1 trgI1 -RRB- = -LRB- Iproductdisplay i = 1 p -LRB- syni trgi -RRB- -LRB- 4 -RRB- pprime -LRB- trgi syni -RRB- prime pw -LRB- syni trgi -RRB- w pwprime -LRB- trgi syni -RRB- wprime pd -LRB- syni , trgi -RRB- d -RRB- lw -LRB- synI1 -RRB- l c -LRB- synI1 -RRB- c pLM -LRB- synI1 -RRB- LM For estimation of the feature weights vector defined in equation -LRB- 4 -RRB- we employed minimum error rate -LRB- MER -RRB- training under the BLEU measure <CIT> '
'<CIT> also used windows of size 2 , which corresponds to word bigrams '
'Ralph Weischedel et al 1993 '
'Unlexicalized methods refine the grammar in a more conservative fashion , splitting each non-terminal or pre-terminal symbol into a much smaller number of subsymbols <CIT> '
'<CIT> describe the voted perceptron applied to the named-entity data in this paper , but using kernel-based features rather than the explicit features described in this paper '
'The algorithm is slightly different from other online training algorithms <OTH> in that we keep and update oracle translations , which is a set of good translations reachable by a decoder according to a metric , ie BLEU <CIT> '
'For instance , the resulting word graph can be used in the prediction engine of a CAT system <CIT> '
'2 '' -RRB- ` he WSJ corpus <CIT> '
'291 31 Level of Analysis Research on sentiment annotation is usually conducted at the text <CIT> or at the sentence levels <OTH> '
'Automatic measures like BLEU <OTH> or NIST <OTH> do so by counting sequences of words in such paraphrases '
'CIT -RRB- '
'The basic model uses the following features , analogous to Pharaohs default feature set : P -LRB- -RRB- and P -LRB- -RRB- the lexical weights Pw -LRB- -RRB- and Pw -LRB- -RRB- <CIT> ; 1 a phrase penalty exp -LRB- 1 -RRB- ; a word penalty exp -LRB- l -RRB- , where l is the number of terminals in '
'Other researchers <OTH> , <CIT> use clustering techniques coupled with syntactic dependency features to identify IS-A relations in large text collections '
'INTRODUCTION Class-based language models <CIT> have been proposed for dealing with two problems confronted by the well-known word n-gram language models -LRB- 1 -RRB- data sparseness : the amount of training data is insufficient for estimating the huge number of parameters ; and -LRB- 2 -RRB- domain robustness : the model is not adaptable to new application domains '
'The construction is defined in Fillmore ''s <OTH> Construction Grammar as '' a pairing of a syntactic pattern with a meaning structure '' ; they are similar to signs in HPSG <OTH> and pattern-concept pairs <OTH> '
'7Following <CIT> , we measure agreement in Kappa , which follows the formula K = P -LRB- A -RRB- P -LRB- E -RRB- 1P -LRB- E -RRB- where P -LRB- A -RRB- is observed , and P -LRB- E -RRB- expected agreement '
'In <CIT> , automatically extracted collocations are judged by a lexicographer '
'The mutual information of a pair of words is defined in terms of their co-occurrence frequency and respective occurrence frequencies <CIT> '
'We carefully implemented the original Grammar Association system described in (Vidal et al. , 1993), tuned empirically a couple of smoothing parameters, trained the models and, finally, obtained an a119a21a120 a100 a104a122a121 of correct translations.9 Then, we studied the impact of: (1) sorting, as proposed in Section 3, the set of sentences presented to ECGI; (2) making language models deterministic and minimum; (3) constraining the best translation search to those sentences whose lengths have been seen, in the training set, related to the length of the input sentence.'
'In order to improve translation quality , this tuning can be effectively performed by minimizing translation error over a development corpus for which manually translated references are available <CIT> '
'Recently , there have been several discriminative approaches at training large parameter sets including <CIT> and <OTH> '
'5 Datasets and Evaluation We train our models with verb instances extracted from three parsed corpora : -LRB- 1 -RRB- the Wall Street Journal section of the Penn Treebank -LRB- PTB -RRB- , which was parsed by human annotators <CIT> , -LRB- 2 -RRB- the Brown Laboratory for Linguistic Information Processing corpus of Wall Street Journal text -LRB- BLLIP -RRB- , which was parsed automatically by the Charniak parser <OTH> , and -LRB- 3 -RRB- the Gigaword corpus of raw newswire text -LRB- GW -RRB- , which we parsed ourselves with the Stanford parser '
'The original formulation of statistical machine translation <CIT> was defined as a word-based operation '
'This operation can be used in applications like Minimum Error Rate Training <CIT> , or optimizing system combination as described by Hillard et al '
'For evaluation we use a state-of-the-art baseline system -LRB- Moses -RRB- <OTH> which works with a log-linear interpolation of feature functions optimized by MERT <CIT> '
'Others try to accommodate both syntactic and lexical differences between the candidate translation and the reference , like CDER <OTH> , which employs a version of edit distance for word substitution and reordering ; or METEOR <CIT> , which uses stemming and WordNet synonymy '
'Generative methods <CIT> treat word alignment as a hidden process and maximize the likelihood of bilingual training corpus using the expectation maximization -LRB- EM -RRB- algorithm '
'42 Experiments on SRL dataset We used two different corpora : PropBank -LRB- wwwcisupennedu\/ace -RRB- along with Penn Treebank 2 <CIT> and FrameNet '
'These constraints tie words in such a way that the space of alignments can not be enumerated as in IBM models 1 and 2 <OTH> '
'In the literature on the kappa statistic , most authors address only category data ; some can handle more general data , such as data in interval scales or ratio scales <CIT> '
'Movie and product reviews have been the main focus of many of the recent studies in this area <CIT> '
'The number of weights wi is 3 plus the number of source languages , and they are trained using minimum error-rate training -LRB- MERT -RRB- to maximize the BLEU score <CIT> on a development set '
'Part-of-speech tags are assigned by the MXPOST maximum-entropy based part-of-speech tagger <CIT> '
'3.1 The traditional IBM alignment model IBM Model 4 (Brown et al. , 1993) learns a set of 4 probability tables to compute p(f|e) given a foreign sentence f and its target translation e via the following (greatly simplified) generative story: 361 NP-C NPB NPB NNP taiwan POS s NN surplus PP IN in NP-C NPB NN trade PP IN between NP-C NPB DT the CD two NNS shores FTD0 GR G4E7 DYBG EL DIDV TAIWAN IN TWO-SHORES TRADE MIDDLE SURPLUS R1: NP-C NPB x0:NPB x1:NN x2:PP x0 x2EL x1 R10: NP-C NPB x0:NPB x1:NN x2:PP x0 x2 x1 R10: NP-C NPB x0:NPB x1:NN x2:PP x0 x2 x1 R2: NPB NNP taiwan POS s FTD0 R11: NPB x0:NNP POS s x0 R17: NPB NNP taiwan x0:POS x0 R12: NNP taiwan FTD0 R18: POS s FTD0 R3: PP x0:IN x1:NP-C x0 x1 R13: PP IN in x0:NP-C GR x0EL R19: PP IN in x0:NP-C x0 R4: IN in  GR R5: NP-C x0:NPB x1:PP  x1 x0 R5: NP-C x0:NPB x1:PP x1 x0 R20: NP-C x0:NPB PP x1:IN x2:NP-C x2 x0 x1 R6: PP IN between NP-C NPB DT the CD two NNS shores G4E7 R14: PP IN between x0:NP-C x0 R21: IN between EL R15: NP-C x0:NPB x0 R15: NP-C x0:NPB x0 R16: NPB DT the CD two NNS shores G4E7 R22: NPB x0:DT CD two x1:NNS x0 x1 R23: NNS shores G4E7 R24: DT the GR R7: NPB x0:NN x0 R7: NPB x0:NN x0 R7: NPB x0:NN x0 R8: NN trade DYBG R9: NN surplus DIDV R8: NN trade DYBG R9: NN surplus DIDV R8: NN trade DYBG R9: NN surplus DIDV Figure 2: A (English tree, Chinese string) pair and three different sets of multilevel tree-to-string rules that can explain it; the first set is obtained from bootstrap alignments, the second from this papers re-alignment procedure, and the third is a viable, if poor quality, alternative that is not learned.'
'<OTH> -RRB- simplify these probability distributions , as given in Equations 9 and 10 '
'By using 8-bit floating point quantization 1 , N-gram language models are compressed into 10 GB , which is comparable to a lossy representation <CIT> '
'3 Previous Work on Subjectivity Tagging In previous work <OTH> , a corpus of sentences from the Wall Street Journal Treebank Corpus <CIT> was manually anno - tated with subjectivity classifications by multiple judges '
'In this paper , we bring forward the first idea by studying the issue of how to utilize structured syntactic features for phrase reordering in a phrase-based SMT system with BTG -LRB- Bracketing Transduction Grammar -RRB- constraints <CIT> '
'While <CIT> showed that this technique was effective when testing on WSJ , the true distribution was closer to WSJ so it made sense to emphasize it '
'4This was a straightforward task ; two annotators annotated independently , with very high agreementkappa score of over 095 <CIT> '
'There have been a lot of prol -RRB- OS ~ fls for statistical analysis , in ninny languages , in particular in English and Japanese <OTH> <OTH> <CIT> -LRB- I\/atnal -RRB- arkhi , 1997 -RRB- <OTH> <OTH> <OTH> <OTH> '
'1 Introduction Word associations -LRB- co-occurrences -RRB- have a wide range of applications including : Speech Recognition , Optical Character Recognition and Information Retrieval -LRB- IR -RRB- <CIT> '
'1 Full Morphological Tagging English Part of Speech -LRB- POS -RRB- tagging has been widely described in the recent past , starting with the <OTH> paper , followed by numerous others using various methods : neural networks <OTH> , HMM tagging <OTH> , decision trees <OTH> , transformation-based error-driven learning <OTH> , and maximum entropy <CIT> , to select just a few '
'1LDC2002E18 -LRB- 4,000 sentences -RRB- , LDC2002T01 , LDC2003E07 , LDC2003E14 , LDC2004T07 , LDC2005T10 , LDC2004T08 HK Hansards -LRB- 500,000 sentences -RRB- 2http : \/ \/ wwwstatmtorg\/wmt07\/shared-taskhtml For both the tasks , the word alignment were trained by GIZA + + in two translation directions and refined by grow-diag-final method <CIT> '
'input pegging a ?transfer correct partially correct b incorrect 1 raw no M4 decoding c 7 4 4 2 stemmed yes M4 decoding 8 3 4 3 stemmed no M4 decoding 13 2 0 4 raw no gloss 13 1 1 5a stemmed yes gloss 8 3 4 5b stemmed yes gloss 12 2 1 6 stemmed no gloss 11 2 2 a pegging causes the training algorithm to consider a larger search space b correct top level category but incorrect sub-category c translation by maximizing the IBM Model 4 probability of the source/translation pair (Brown et al. , 1993; Brown et al. , 1995) classification might be performed by automatic procedures rather than humans.'
'In addition , explicitly using the left context symbols allows easy use of smoothing techniques , such as deleted interpolation <OTH> , clustering techniques <CIT> , and model refinement techniques <OTH> to estimate the probabilities more reliably by changing the window sizes of the context and weighting the various estimates dynamically '
'If we assign a probability a15a17a16a19a18 a12 a13a7a21a20a4a6a5a7a23a22 to each pair of strings a18 a12a14a13a7a25a24 a4 a5a7 a22, then according to Bayes decision rule, we have to choose the target string that maximizes the product of the target language model a15a17a16a19a18 a12a14a13a7 a22 and the string translation model a15a17a16a19a18a26a4a6a5 a7 a20 a12 a13 a7 a22 . Many existing systems for statistical machine translation (Berger et al. , 1994; Wang and Waibel, 1997; Tillmann et al. , 1997; Nieen et al. , 1998) make use of a special way of structuring the string translation model like proposed by (Brown et al. , 1993): The correspondence between the words in the source and the target string is described by alignments that assign one target word position to each source word position.'
'We employ the phrase-based SMT framework <OTH> , and use the Moses toolkit <OTH> , and the SRILM language modelling toolkit <OTH> , and evaluate our decoded translations using the BLEU measure <CIT> , using a single reference translation '
'Some researchers <CIT> classify terms by similarities based on their distributional syntactic patterns '
'Other representative collocation research can be found in <CIT> and Smadja -LRB- 1993 -RRB- '
'Previous studies <OTH> defined a probabilistic model of unification-based grammars as a log-linear model or maximum entropy model <CIT> '
'These results confirm the observed figures in the previous subsection and reinforce the sight that clustering is a worthless effort for automatic paraphrase corpora construction , contrarily to what <CIT> suggest '
'Evaluating the algorithm on the output of Charniaks parser <OTH> and the Penn treebank <CIT> shows that the patternmatching algorithm does surprisingly well on the most frequently occuring types of empty nodes given its simplicity '
'52 Evaluation Metrics The commonly used criteria to evaluate the translation results in the machine translation community are : WER -LRB- word error rate -RRB- , PER -LRB- positionindependent word error rate -RRB- , BLEU <CIT> , and NIST <OTH> '
'2 Related Work A large amount of previous research on clustering has been focused on how to find the best clusters <CIT> '
'Thus , we propose a bootstrapping approach <CIT> to train the stochastic transducer iteratively as it extracts transliterations from a bitext '
'Nevertheless , in the problem described in this article , the source and the target sentences are given , and we are focusing on the optimization of the aligment a The translation probability Pr -LRB- f , a e -RRB- can be rewritten as follows : Pr -LRB- f , a e -RRB- = Jproductdisplay j = 1 Pr -LRB- fj , aj fj11 , aj11 , eI1 -RRB- = Jproductdisplay j = 1 Pr -LRB- aj fj11 , aj11 , eI1 -RRB- Pr -LRB- fj fj11 , aj1 , eI1 -RRB- -LRB- 2 -RRB- The probability Pr -LRB- f , a e -RRB- can be estimated by using the word-based IBM statistical alignment models <CIT> '
'To closely reproduce the experiment with the best performance carried out in <CIT> using SVM , we use unigram with the presence feature '
'The literature on relational similarity , on the other hand , has focused on pairs of words , devising various methods to compare how similar the contexts in which target pairs appear are to the contexts of other pairs that instantiate a relation of interest <CIT> '
'For process -LRB- 2 -RRB- , existing methods aim to distinguish between subjective and objective descriptions in texts <CIT> '
'25 Model Training We adapt the Minimum Error Rate Training -LRB- MERT -RRB- <CIT> algorithm to estimate parameters for each member model in co-decoding '
'In Step 3 , a simple perceptron update <CIT> is performed '
'Identifying subjectivity helps separate opinions from fact , which may be useful in question answering , summarization , etc Sentiment detection is the task of determining positive or negative sentiment of words <CIT> , phrases and sentences <OTH> , or documents <CIT> '
'ROUGE-S ROUGE-S is an extension of ROUGE-2 defined as follows (Lin, 2004b): ROUGE-Sa59a61a146a31a62a98a147a49a65a68a67 a59a68a101a161a128a104a162 a2 a65a161a163 a157 a134a61a135a93a245a246 a2 a59a61a146a31a62a98a147a49a65a161a163 a145 a134a61a135a89a245a246 a2 a59a61a146a31a62a164a147a49a65 a157 a134a136a135a93a245a246 a2 a59a61a146a31a62a90a147a49a65a51a128a104a162 a2 a145 a134a61a135a89a245a246 a2 a59a61a146a31a62a98a147a49a65 (11) Where a166a168a169a78a170a248a247a250a249 a26 and a171a138a169a90a170a158a247a250a249 a26 are defined as follows: a251 a134a61a135a89a245a246 a2 a59a61a146a31a62a90a147a49a65a68a67 a252a248a253a85a254a255 a1 a59a61a146a31a62a90a147a49a65 # of skip bigram a2a23a147 (12) a3 a134a136a135a93a245a246 a2 a59a61a146a31a62a90a147a49a65a68a67 a252a83a253a118a254a255 a1 a59a61a146a31a62a90a147a49a65 # of skip bigram a2 a146 (13) Here, function Skip2 returns the number of skipbi-grams that are common to a141 and a139 . ROUGE-SU ROUGE-SU is an extension of ROUGE-S, which includes unigrams as a feature defined as follows (Lin, 2004b): ROUGE-SUa59a61a146a31a62a90a147a49a65a68a67 a59a68a101a161a128a49a162 a2 a65a117a163 a157 a134a5a4 a59a61a146a31a62a98a147a49a65a71a163 a145 a134a6a4 a59a61a146a31a62a98a147a49a65 a157 a134a5a4 a59a61a146a31a62a90a147a49a65a47a128a49a162 a2 a145 a134a5a4 a59a61a146a31a62a164a147a49a65 (14) Where a166 a169a8a7 and a171 a169a8a7 are defined as follows: a251 a134a5a4 a59a61a146a31a62a98a147a49a65a68a67 a252 a9 a59a61a146a31a62a90a147a49a65 (# of skip bigrams + # of unigrams) a2 a147 (15) a3 a134a5a4 a59a61a146a31a62a90a147a49a65a68a67 a252 a9 a59a61a146a31a62a90a147a49a65 (# of skip bigrams + # of unigrams) a2 a146 (16) Here, function SU returns the number of skip-bigrams and unigrams that are common to a141 and a139 . ROUGE-L ROUGE-L is an LCS-based evaluation measure defined as follows (Lin, 2004b): ROUGE-La59a61a146a31a62a90a147a49a65a68a67 a59a68a101a161a128a49a162 a2 a65a161a163 a157a11a10 a225a90a134 a59a61a146a31a62a90a147a49a65a161a163 a145a12a10 a225a90a134 a59a61a146a31a62a98a147a49a65 a157a11a10 a225a90a134 a59a61a146a31a62a90a147a49a65a47a128a49a162 a2 a145a12a10 a225a98a134 a59a61a146a31a62a90a147a49a65 (17) where a166a14a13a250a241a132a169 and a171a15a13a250a241a130a169 are defined as follows: a157a11a10 a225a98a134 a59a61a146a31a62a98a147a49a65a68a67 a101 a91 a16 a75 a77a29a216 LCSa17a244a59a61a156 a88 a62a90a146a21a65 (18) a145a18a10 a225a98a134 a59a61a146a31a62a98a147a49a65a68a67 a101 a95 a16 a75a78a77a83a216 LCSa17 a59a61a156a34a88a78a62a98a146a21a65 (19) Here, LCSa19a244a28a78a144a183a114a93a32a93a139a102a36 is the LCS score of the union longest common subsequence between reference sentences a144a25a114 and a139 . a115 and a122 are the number of words contained in a141, and a139, respectively.'
'145 2 The Latent Variable Architecture In this section we will begin by briefly introducing the class of graphical models we will be using , Incremental Sigmoid Belief Networks <CIT> '
'The segmentation is based on the guidelines , given in the Chinese national standard GB13715 , <OTH> and the POS tagging specification was developed according to the Grammatical Knowledge-base of contemporary Chinese '
'For example , in this work we use loglikelihood ratio <CIT> to determine the SoA between a word sense and co-occurring words , and cosine to determine the distance between two DPWSs log likelihood vectors <OTH> '
'For example , the words test and exam are similar because both of them can follow verbs such as administer , cancel , cheat on , conduct , etc Many methods have been proposed to compute distributional similarity between words , eg , <CIT> '
'25 Evaluation Minnen and Carroll -LRB- Under review -RRB- report an evaluation of the accuracy of the morphological generator with respect to the CELEX lexical database <OTH> '
'We use two state-of-the-art POS taggersa maximum entropy based English POS tagger <CIT> , and an HMM based Chinese POS tagger '
'We used a publicly available tagger <CIT> to provide the part-of-speech tags for each word in the sentence '
'1 Introduction The use of various synchronous grammar based formalisms has been a trend for statistical machine translation -LRB- SMT -RRB- <CIT> '
'The closest work is that of Jing and McKeown <OTH> and <CIT> , in which multiple sentences are processed , with fragments within them being recycled to generate the novel generated text '
'2 Literature Survey The task of sentiment analysis has evolved from document level analysis -LRB- eg , <CIT> ; <OTH> -RRB- to sentence level analysis -LRB- eg , <OTH> ; <OTH> ; <OTH> -RRB- '
'Thus , a lot of alignment techniques have been suggested at ; the sentence <OTH> , phrase <OTH> , nomt t -RRB- hrase <OTH> , word <CIT> , collocation <OTH> and terminology level '
'The class based disambiguation operator is the Mutual Conditioned Plausibility -LRB- MCPI -RRB- <OTH> '
'Parsing has been also used after extraction <CIT> for filtering out invalid results '
'The significance values are obtained using the loglikelihood measure assuming a binomial distribution for the unrelatedness hypothesis <CIT> '
'Ramshaw and Marcus <CIT> views chunking as a tagging problem '
'This approach will generally take advantage of language-specific -LRB- eg in <CIT> -RRB- and domain-specific knowledge , of any external resources -LRB- eg database , names dictionaries , etc -RRB- , and of any information about the entities to process , eg their type -LRB- person name , organization , etc -RRB- , or internal structure -LRB- eg in <OTH> -RRB- '
'6 Bracketing of Compound Nouns The first analysis task we consider is the syntactic disambiguation of compound nouns , which has received a fair amount of attention in the NLP literature <OTH> '
'7 Related Work There has been a recent interest in training methods that enable the use of first-order features <CIT> '
'Ontologies are formal specifications of a conceptualization <OTH> so that it seems straightforward to formalize annotation schemes as ontologies and make use of semantic annotation tools such as OntoMat <OTH> for the purpose of linguistic annotation '
'Attempts to alleviate this tagbottleneck i ~ lude tmotstr ~ ias <OTH> and unsupervised algorith ~ -LRB- <CIT> , 199s -RRB- Dictionary-based approaches rely on linguistic knowledge sources such as ma ~ l ~ i , ~ e-readable dictionaries <OTH> and WordNet <OTH> and e0 -LRB- ploit these for word sense disaznbiguation '
'<CIT> and Chan et al '
'Feature selection methods have been proposed in the maximum-entropy literature by several authors <CIT> '
'Prototype-drive learning <CIT> specifies prior knowledge by providing a few prototypes -LRB- ie , canonical example words -RRB- for each label '
'Probabilities based on relative frequencies , or derived fl ` om the measure defined in <CIT> , for example , allow to take this fact into account '
'There have been considerable amount of efforts to improve the reordering model in SMT systems , ranging from the fundamental distance-based distortion model <CIT> , flat reordering model <OTH> , to lexicalized reordering model <CIT> , hierarchical phrase-based model <OTH> , and maximum entropy-based phrase reordering model <OTH> '
'ISBNs , originally proposed for constituent parsing in <CIT> , use vectors of binary latent variables to encode information about the parse history '
'The same Powells method has been used to estimate feature weights of a standard feature-based phrasal MT decoder in <CIT> '
'Parameter tuning is done with Minimum Error Rate Training -LRB- MERT -RRB- <CIT> '
'The lexical scores are computed as the -LRB- unnormalized -RRB- log probability of the Viterbi alignment for a phrase pair under IBM word-translation Model 1 <CIT> '
'Previous work <CIT> has shown it to be appropriate to large-scale language modeling '
'To perform minimum error rate training <CIT> to tune the feature weights to maximize the systems BLEU score on development set , we used optimizeV5IBMBLEUm <OTH> '
'These lists are rescored with the different models described above , a character penalty , and three different features based on IBM Models 1 and 2 <CIT> calculated in both translation directions '
'what does student want to write your Figure 3 : A derivation tree of lexicalized parse trees , such as the distinction of arguments\/modifiers and unbounded dependencies <CIT> , are elegantly represented in derivation trees '
'Within the machine learning paradigm , IL has been incorporated as a technique for bootstrapping an extensional learning algorithm , as in <CIT> '
'Algorithm 1 The RRM Decoding Algorithm foreacha26a29a27a67a42 foreacha68 a1a20a23a69a10a11a10a12a10a45 a60 a48a22a70a26a22a71 a1a73a72a2a25 a57a38a50 a7 a56 a48a54a57 a64a74a30 a57 a31a33a26a17a34 a5a11a75 a60a77a76a74a76 a31a78a26a35a34a66a79a81a80a83a82a38a84a69a85a86a80a24a87a88a48 a60 a48 a70a26a61a71 Somewhat similarly, the MaxEnt algorithm has an associated set of weights a31a33a89 a48a54a57 a34a48a90a50 a7a53a52a54a52a54a52a15 a57a38a50 a7a58a52a54a52a54a52 a25, which are estimated during the training phase so as to maximize the likelihood of the data (Berger et al. , 1996).'
'By treating a letter\/character as a word and a group of letters\/characters as a phrase or token unit in SMT , one can easily apply the traditional SMT models , such as the IBM generative model <CIT> or the phrase-based translation model <OTH> to transliteration '
'lscript1-regularized log-linear models -LRB- lscript1-LLMs -RRB- , on the other hand , provide sparse solutions , in which weights of irrelevant features are exactly zero , by assumingaLaplacianpriorontheweight <CIT> '
'Two major research topics in this field are Named Entity Recognition -LRB- NER -RRB- <CIT> and Word Sense Disambiguation -LRB- WSD -RRB- <CIT> '
'Finally , other approaches rely on reviews with numeric ratings from websites <CIT> and train -LRB- semi - -RRB- supervised learning algorithms to classify reviews as positive or negative , or in more fine-grained scales <CIT> '
'We have : -RRB- -LRB- -RRB- , -LRB- -RRB- , -LRB- -RRB- , , -LRB- -RRB- -LRB- 21 21 trictrictric trictritri erpercpercp ecrcpecp = = -LRB- 6 -RRB- Assumption 2 : For an English triple tri e , assume that i c only depends on -LCB- 1,2 -RCB- -RRB- -LRB- i i e , and c r only depends on e r Equation -LRB- 6 -RRB- is rewritten as : -RRB- -LRB- -RRB- -LRB- -RRB- -LRB- -RRB- -LRB- -RRB- , -LRB- -RRB- , -LRB- -RRB- -LRB- 2211 21 ec trietrictrictritri rrpecpecp erpercpercpecp = = -LRB- 7 -RRB- Notice that -RRB- -LRB- 11 ecp and -RRB- -LRB- 22 ecp are translation probabilities within triples , they are different from the unrestricted probabilities such as the ones in IBM models <CIT> '
'Towards a Meaning-Full Comparison of Lexieal Resources Kenneth C Lltkowska CL Research 9208 Gue Road Damascus, MD 20872 ken@clres corn http//www tires tom Abstract The mapping from WordNet to Hector senses m Senseval provides a ''gold standard'' against wluch to judge our ability to compare lexlcal resources The ''gold standard'' is provided through a word overlap analysis (with and without a stop list) for flus mapping, achieving at most a 36 percent correct mapping (inflated by 9 percent from ''empty'' assignments) An alternaUve componenttal analysis of the defimtaons, using syntacUc, collocatmnal, and semantac component and relation identification (through the use ofdefimng patterns integrated seamlessly mto the parsing thclaonary), provides an almost 41 percent correct mapping, with an additaonal 4 percent by recogmzmg semantic components not used in the Senseval mapping Defimtion sets of the Senseval words from three pubhshed thclaonanes and Dorr''s lextcal knowledge base were added to WordNet and the Hector database to exanune the nature of the mapping process between defimtton sets of more and less sco[~e The tecbauques described here consUtute only an maaal implementation of the componenUal analysis approach and suggests that considerable further improvements can be aclueved Introduction The difficulty of companng lemcal resources, long a s~gnfficant challenge in computauonal hnguistlcs (Atlans, 1991), came to the fore in the recent Senseval competatton (IOlgarnff, 1998), when some systems that relied heavily on the WordNet (Miller, et al, 1990) sense inventory were faced with the necessity of using another sense inventory (Hecto0 A hasty solutaon to the problem was the '' development of a map between the two inventories, but some part~cipants expressed concerns that use of flus map may have degraded their performance to an unknown degree Although there were disclaimers about the WordNet-Hector map, it nonetheless stands as a usable gold standard for efforts to compare lexical resources Moreover, we have a usable baseline (a word overlap method suggested m (Lesk, 1986)) against which to compare whether we are able to make improvements m the mapping (since flus method has been shown to perform not as well as expected (Krovetz, 1992)) We first describe the lextcal resources used m the study (Hector, WordNet, other dicUonanes, and a lex~cal knowledge base), first characterizing them in terms ofpolysemy and the types of leracal mformaUon each contmns (syntacUc properties and features, semantac components and relaUons, and collocaUonal properties) We then present results of perfornung the word overlap analysis of the 18 verbs used m Senseval, analyzing the definitions m WordNet and Hector We then expand our analysis to include other dictionaries We describe our methods of analysis, particularly the methods of parsing defimtaons and identff)qng semantic relations (semrels) based on defimng patterns, essentially takang first steps m Implementing the program described by Atkms and focusmg on the use of''meamng'' full mformataon rather than statistical mformaUon We identify the results that have been achieved thus far and outline further steps that may add more ''meanmg'' to the analysis IAll analyses described m this paper were performed automatically using functlonahty incorporated m DIMAP (Dictionary Maintenance Programs) (available for immediate download at (CL Research, 1999a)) This includes automatac extracuon of WordNet reformation for the selected words (mtegrated m DIMAP) Hector defimtlons were uploaded into DIMAP dicUonanes after use of a conversmn program Defimtlons for other 30 The Lexical Resources Tlus analysis focuses on the mmn verb senses used In Senseval (not ichoms and phrases), specifically the followmg AMAZE, BAND, BET, BOTHER, BURY, CALCULATE, CONSUME, DERIVE, FLOAT, HURDLE, INVADE, PROMISE, SACK, SANCTION, SCRAP, SEIZE, SHAKE, SLIGHT The Hector database used In Senseval consists of a tree of senses, each of which contains defimttons, syntactic properties, example usages, and ''clues'' (collocational information about the syntactic and semantic enwronment in wluch a word appears in the spectfic sense) The WordNet database contmns synonyms (synsets), perhaps a defimtton or example usages (gloss), some syntactic mformaUon (verb frames), hypernyms, hyponyms, and some other semrels (ENTAILS, CAUSES) To extend our analysis In order to look at other issues of lexacal resource comparison, we have included the defirauons or leracal information from the following additional sources  Webster''s 3 ra New International Dictionary (W3)  Oxford Advanced l.earners D~ctlonary (OALD)  American Hentage DlcUonary (AI-ID)  Dorr''s Lexacal Knowledge Base (Dorr) We used only the defimuons from W3, OALD, and AHD (which also contmn sample usages and some collocattonal information m the form of usage notes, not used at the present tame) Dorr''s database contains thematic grids wluch characterize the thematic roles of obligatory and optional semanuc components, frequently identifying accompanying preposmons (Olsen, et al, 1998) The following table identities the number of senses and average overall polysemy for each of these resources dictionaries were entered by hand Word amaze band bet bother bury calculate consume denve float hurdle invade pronuse sack sanction scrap seize shake shght Average Polysemy o o o 1 2 4 2 3 1 II 4 4 2 5 5 7 6 9 7 12 6 14 5 5 5 10 9 6 6 8 8 6 5 15 5 16 4 41 14 2 1 4 3 6 2 10 5 5 4 7 4 4 4 6 3 2 2 5 2 3 1 3 3 11 6 21 13 8 8 37 17 1 1 6 3 O 1 2 2 4 1 3 4 4 8 1 3 1 3 1 3 2 10 5 1 0 3 1 3 2 2 0 1 1 1 0 7 1 7 12 I 0 57 37 120 62 34 22 Word Overlap Analysis We first estabhsh a baseline for automatic replication of the lexicographer''s mappmg from WordNet 1 6 to Hector, using a s~mple word overlap analysis smular to (Lesk, 1986) The lextcographer mapped the 66 WordNet senses (each synset m which a test occurred) Into 102 Hector senses A total of 86 assignments were made, 9 WordNet senses were gwen no assignments, 40 recewed exactly one, and 17 senses received 2 or 3 asssgnments The WordNet senses contained 348 words (about half of wluch were common words appeanng on our stop list, which contained 165 words, mostly preposmons, pronouns, and conjunctions) The Hector senses selected m the word overlap analysis contained about 960 words (all Hector senses contained 1878 words) We performed a strict word overlap analysts (with and wsthout a stop hst) between tile definlUons in WordNet and the Hector senses, that is, we did not attempt to ldenttfy root forms of Inflected words We took each word m a WordNet sense and determined whether ~t appeared in a Hector sense, we selected a Hector sense based on the highest percentage of words over all Hector senses An 31 empty selection was made ff all the words in the WordNet sense did not appear in any Hector sense, only content words were considered when the stop hst was used For example, for bet, WordNet sense 2 (stake (money) on the outcome of an issue) mapped into Hector sense 4 ((of a person) to risk (a sum of money or property) m thts way) In this case, there was an overlap on two words (money, 039 in the Hector defimtlon (0 13 of its 15 words) without the stop list When the stop list was invoked, there was an overlap of only one word (money, 0 07 of the Hector defimtion) In this case, the lexicographer had made three assignments (Hector senses 2, 3, and 4), our scoring method treated flus as only 1 out of 3 correct (not using the relaxed method employed in Senseval of treating flus as completely correct) Without the stop hst, our selections matched the lexicographer''s in 28 of 86 cases (32 6%), using the stop list, we were successful in 31 of 86 cases (36 1%) The improvement arising when the stop list was used is deceptive, where 8 cases were due to empty assignments (so that only 23 cases, 26 7%, were due to matching content words) Overall, only 41 content words were involved in these 23 successes when the stop list was used, an average of I 8 content words To summanze the word overlap analysis (1) despite a ncher set of defimtions in Hector, 9 of 66 WordNet senses (13 6%) could not be assigned, (2) despite the greater detail in Hector senses compared to WordNet senses (2 8 times as many words), only 1 8 content words participated in the assignments, and (3) therefore, the defimng vocabulary between these two definition sets seems to be somewhat divergent Although it might appear as if the word overlap analysis does not perform well, this is not the case The analysis provides a broad overview of the defimuon companson process between two definmon sets and frames a deeper analysis of the differences Moreover, it appears that the accuracy of a ''gold standard'' mapping is not crucially important The quality of the mapping may help frame the subsequent analysis more precisely, but it seems sufficient that any reasonable mapping will suffice This will be discussed further after presenting the results of the componentlal analysis of the defimtlons 32 Meaning-Full Analysis of Definitions The deeper analysis of the mapping between two defimtion sets relies primarily on two major steps (1) parsing definitions and using defimng patterns to identify semrels present m the definitions and (2) relaxing values to these relations by allowing ''synonymic'' substitution (using WordNet) Thus, for example, ffwe identify hypernyms or instruments from parsing a defimtion, we would say that the defimtions are ''equal'' not just ffthe hypernym or instrument is the same word, but also Lf the hypernyms or instruments are members of the same synset This approach is based on the finding (Litkowski, 1978) that a dictionary induces a semantic network where nodes represent ''concepts'' that may be lexicahzed and verbalized in more than one way This finding implies, in general, the absence of true synonyms, and instead the kind of ''concept'' embodied in WordNet synsets (with several lexical items and phraseologles) A slmdar approach, parsing defimtlons and relaxing semrel values, was followed in (Dolan, 1994) for clnstenng related senses w~thin a single dictionary The ideal toward which this approach strives is a complete identification of the meamng components included in a defimtion The meaning components can include syntactic features and charactenstlcs (including subcategonzation patterns), semantm components (realized through identification of semrels), selectional restrictions, and coUocational specifications The first stage of the analysis parses the definitions (CL Research, 1999b, Litkowski, to appear) and uses the parse results to extract (via defining patterns) semrels Since definitions have many idiosyncrasies (that do not follow ordinary text), an important first step in this stage is preprocessmg the definition text to put it into a sentence frame that facilitates the extraction of semrels 2 2Note that the stop hst is not applicable to the definition parsing The parser is a full-scale sentence parser, where prepositmns and other words on the stop list are necessary for successful parsing Moreover, inclusion of the prepositions is cmcml to the method, since they are the bearers of much semrel information The extractmn of semrels examines the parse results, a e, a tree whose mtermedaate nodes represent non-ternunals and whose leaves represent the lextcal atems that compnse the defimuons, where any node may also include annotations such as characterizations of number and tense For all noun or verb defimttons, flus includes Identification of the head noun (with recogmtton of''empty'' heads) or verb, for verbs, we signal whether the defimtaon contmned any selecttonal restnctmus (that as, pamcular parenthesazed expressaons) for the subject and object We then exanune preposattonal phrases In the defimUon and deterrmne whether we have a ''defining pattern'' for the preposaUon whach we can use as mdacaUve of a partacular semrel We also identify adverbs m the parse tree and look these up in WordNet to adentffy an adjecuve synset from wluch they are derived (if one is gwen) The defimng pattems are actually part of the dictionary used by the parser That is, we do not have to develop specafic routines to look for specLfic patterns A defimng pattern ~s a regular expressaon that arlaculates a syntactac pattern to be matched Thus, to recograze a ''manner'' semrel, we have the foUowmg entry for ''m'' m(dpat((~ rep0 l(det(0)) adj manner(0) st(manner)))) This allows us to recognize ''m'' as possibly gwmg rise to a ''manner'' component, where we recogmze ''m'' (the tdde, which allows us to specify partacular elements before the ''m'' as well), vath a noun phrase that consasts of 0 or 1 determiner, an adjectwe, and the lateral ''manner'' The ''0  after the detenmner and the hteral mdacate that these words are not copied into the value for a ''manner'' role, so that the value to the ''manner'' semrel becomes only the adjectwe that as recogmzed The second stage of the analysis uses the populated lexacal database to compare senses and make the selectaons This process follows the general methodology used m Senseval (Lltkowska, to appear) Specifically, m the defimtaon comparison, we first exanune exclusaon cntena to rule out specific mappings These criteria include syntacUc properUes (e g, a verb sense that Is only transluve cannot map into one that Is only mtransRave) and collocataonal propertaes (e g, a sense that is used with a parUcle cannot map into one that uses a different particle) At the present tune, these are used only rmmmally 33 We next score each viable sense based on rots semrels We increment the score ff the senses have a common hypernym or If a sense''s hypernyms belong  to the same synset as the other sense''s hypernyms If a parUcular sense con~ns a large number of synonyms (that as, no differentiae on the hypernym) and they overlap consaderably m the synsets they evoke, the score can be increased substanUally Currently, we add 5 points for each match 3 We increment the score based on common semrels In tins amtml tmplementaUon, we have defimng patterns (usually qmte nummal) for recogmzmg Instrument, means, location, purpose, source, manner, has-constituents, has-members, is-part-of, locale, and goal 4 We Increment the score by 2 points when we have a common semrel and then by another 5 points when the value Is ~dentacal or m the same synset After all possable increments to the scores have been made, we then select the sense(s) w~th the lughest score Finally, we compare our selecuon with that of the gold standard to assess our mapping over all senses Another way an wluch our methodology follows the Senseval process as that at proceeds incrementally Thus, ~t ms not necessary to have a ''final'' perfect parse and mapping rouUne We can make conUnual refinements at any stage of the process and exarmne the overall effect As m Senseval, we may make changes to deal wath a particular phenomenon with the result that overall performance dechnes, but w~th a sounder basis for making subsequent amprovements Results of Componential Analysis The ''gold standard'' analysis Involves mapping 66 WordNet senses with 348 words into 102 Hector senses with 1878 words Using the method described above, we obtained 35 out of 86 correct 3At the present tame, we use WordNet to adentffy semreis We envaslon usmg the full semanlac network created by parsing all a dlcUonary''s defimtaons Thas would include a richer set of semrels than currently included m WordNet 4The defimng patterns are developed by hand We have onlyJust begun this effort, so the current set ms somewhat Impoverished mappmgs (407%), a shght improvement over the 31 correct assignments usmg the stop-last word overlap techmque However, as mentioned above, the stophst techmque had aclueved 8 of its successes by matclung null assignments Consadered on tlus basins, ~t seems that the componentaal analysis techmque provides substantial ~mprovement In addition, our technique ''erred'' on 4 cases by malang assagnments where none were made by the leracographer We suggest that these cases do con~n some common elements of meaning and may conceivably not be construed as errors The mapping from WordNet to Hector had relatavely few empty mappings, senses for wtuch It was not possable to make an assignment These are the cases where at appears that the chetmnanes do not overlap and thus prowde a tentative mdacataon of where two dictionaries may have different coverage The cases of multiple assignments mchcate the degree ofamblgmty m the mapping The average m both darecUons between Hector and WordNet were donunated by the mabdaty to obtain good dascnnunatton for the word ''semze'' Thus, tlus method identifies individual words where the &scnnunatwe ablhty needs to be further refined  Perhaps more importantly, the componentml analysis method exploits consaderably more WordNet Hector  mformauon than the word overlap methods Whereas the stop-hst word overlap mapping was  based on only 41 content words, the componenual ~ approach (In the selected mappings) had 228 hits in ~.~  developing ats scores, with only a small number of ~ .~ ~ defining patterns Comparison of Dictionaries tel O ~3 0''3 We next exanuned the nature of the mterrelalaons between parrs of chctaonanes w~thout use of a ''gold standard'' to assess the process of mapping For t/us purpose, we mapped m both &recttons between the paars {WordNet, Hector}, {W3, OALD}, and {W3, AHD We exanune Dorr''s lexacal knowledge base for the amphcatlons It may have m the mapping process Neither WordNet nor Hector are properly v~ewed as chcuonanes, since there was no mtenuon to pubhsh them as such WordNet ''glosses'' are generally smaller (53 words per sense) compared to Hector (184 words per sense), whach contains many words specff3nng selectmnal restnct~ons on the subject and object of the verbs Hector was used primarily for a large-scale sense tagging project The three formal d~ctmnanes were subject to rigorous pubhslung and style standards The average number of words per sense were 87 (OALD), 7 1 (AHD), and 9 9 (W3), w~th an average of 3 4, 62, and 120 senses per word Each table shows the average number of senses being mapped, the average number of assignments m the target dlCtmnary, the average number of senses for which no assagnment could be made, the average number of mulUple assignments per word, and the average score of the assignments that were made WN-Hector 37 47 06 17 119 Hector-WN 57 64 14 22 113 These points are further emphasized m the mapping between W3 and OALD, where the disparity between the empty and mulUple assagnments indicate that we are mapping between dictionaries qmte disparate This tends to be the case not only for the enUre set of words, but also is evident for individual words where there is a considerable d~spanty m the number of senses, wtuch then dominate the overall dlspanty Thus, for example, W3 has 41 defimUons for ''float'', while OALD has 10 We tend to be unable to find the specific sense m going from W3 to OALD, because at is likely that we have many more specific defimtlons that are not present In the other direction, we are hkely to have considerable ambiguity and multiple assignments W3-OALD OALD-W3 W3 OALD 120 78 60 18 99 34 60 07 32 86 34 A Between W3 and AHD, there ss less overall daspanty between the defimtaon sets, although since W3 Is tmabndged, we stall have a relatavely lugh number of senses m W3 that do not appear to be present m AHD Finally, It should be noted that the scores for the published dictaonanes tend to be a little lower than for WordNet and Hector Tlus reflects the hkehhood that we have not extracted as much mformataon as we dad m parsing and analyzmg the defimtaon sets used m Senseval W3 AHD oJ  ''q O W3-AHD 120 115 40 36 90 AHD-W3 6 2 9 1 1 2 4 1 9 1 We next considered Dorr''s lexacal database We first transformed her theta grids to syntactic spectflcataons (transttave or lntransmttve) and identtficataon of semreis (e g, where she Identified an instr component, we added such a semrel to the DIMAP sense) We were able to identify a mappmg from WordNet to her senses for two words (''float'' and ''shake'') for wluch Dorr has several entries However, smce she has considerably more semanuc components than we are currently able to recogmze, we dad not pursue this avenue any further at flus time More important than just mappmg between two words, Dorr''s data mdacates the posstbday of further exploitation of a richer set of semanUc components Spectfically, as reported m (Olsen, et al, 1998), m descnbmg procedures for automatically acqumng thematic grids for Mandann Chinese, ~t was noted that ''verbs that incorporate themaUc elements m their meamng would not allow that element to appear m the complement structure'' Thus, by usmg Dorr''s thematic grids when verb are parsed m defimtaons, it ~s possible to ~dentffy where partacular semantac components are lexicahzed and which others are transnutted through to the themaUc grid (complement or subcategonzataon pattern) for the defimendum The transmiss~on of semantic components to the thematic gnd ~s also reflected overtly m many defimtlons For example, shake has one definition, ''to bnng to a specified condatton by or as ffby repeated qmck jerky movements'' We would thus expect that the thematac grid for this defimtaon should include a ''goal'' And, deed, Dorr''s database has two senses whch reqmre a ''goal'' as part of their thematic grid Smularly, for many defimtaons m the sample set, we ~dentLfied a source defimng pattern based on the word ''from,'' frequently, the object of the preposmon was the word ''source'' ttseff, mdacatmg that the subcategonzaUon, properties of the defimendum should elude a source component Discussion Wlule the improvement m mapping by using the componentaal analysis techmque (over the word overlap methods) is modest, we consider these results qmte slgmficant m wew of the very small number of defimng patterns we have Implemented Most of the improvement stems from the word substatuUon pnnclple described earlier (as ewdenced by the preponderance of 5 point scores) This techmque also provides a mechamsm for bnngmg back the stop words, wz, the preposmons, wluch are the careers of mformatmn about semrels (the 2 point scores) The more general conclusion (from the word subsutuuon) is that the success arises from no longer considenng a defimtmn m ~solation The proper context for a word and its defimtions consists not .lUSt of the words that make up the definition, but also the total semantac network represented by the dictaonary We have aclueved our results by explomng only a small part of that network We have moved only a few steps to that network beyond the mdawdual words and their definitions We would expect that further expansmn, first by the addon of further and ~mproved semrel defining patterns, and second, through the identaficataon of more pnmmve semanuc components, will add considerably to our abflay to map between lexacal resources We also expect ~mprovements from consideration of other techniques, such as attempts at ontology ahgnment (Hovy, 1998) Although tile definition analysis provlded here was performed on definmons with a stogie language, the vanous meamng components m m m m m m m m 35 correspond to those used in an Interhngua The use of the exUncuon method (developed m order to charactenze verbs m another language, Clunese) can frmtfully be applied here as well Two further observaUons about tlus process can be made The first is that rchance on a wellestablished semantic network such as WordNet,s not necessary The componenUal analysis method rehes on the local neighborhood of words m the defimUons, not on the completeness of the network Indeed, the network ~tsel can be bootstrapped based on the parsing results The method can work vath any semanUc network or ontology and may be used to refine or flesh out the network or ontology The second observation is that it is not necessary to have a well-estabhshed ''gold standard'' Any mapping vail do All that Is necessary is for any mvesugator (lemcographer or not) to create a judgmental mappmg The methods employed here can then quanufy ttus mapping based on a word overlap analysis and then further examine tt based on the componenaal analysis The componenUal analysis method can then be used to exanune underlying subtleUes and nuances tn the defimUous, wluch a lemcographer or analyst can then examine m further detail to assess the mapping Future Work Tlus work has marked the first ume that all the necessary mfrastructure has been combmed tn a rudimentary form Because of its rudimentary status, the opportumUes for improvement are quite extensive In addlUon, there are many opportumUes for using the techmques descnbed here m further NLP apphcatlons First, the techmques described here have immediate apphcabtllty as part of a lexicographer''s workstaUon When defimUons are parsed and semrels are zdenttfied, the resulUng data structures can be apphed against a corpus of instances for parUcular words (as m Senseval) for improving word-sense disamblguaUon The techmques will also permit comparing an entry vath Itself to deternune the mterrelattonshtps among ~ts defimUons and of companng the defimUons of two ''synonyms'' to deternune the amount of overlap between them on a defimtlon by defimUon bas~s Although the analys,s here has focused on the parsing of defimUous, the development of defimng patterns clearly extends to generalized text parsing since the defimng patterns have been incorporated mto the same chcttonary used for parsing free text, the patterns can be used threctly to identify the presence of parUcular semrels among sentenual consUtuents We are working to integrate th~s funcUonahty into our word-sense &sambiguaUon techruques (both the defimng patterns and the semrels) Even further, mt seems that matclung defimng patterns in free text can be used for lextcal acquisition Textual matenal that contains these patterns could concewably be flagged as providing defimUonal matenal which can then be compared to emstmg defimUons to assess whether their use ts cous,stent vath these defimUons, and ff not, at least to flag the inconsistency The tecluuques descnbed here can be apphed directly to the fields of ontology development and analysis of ternunologlcal databases For ontoiogles, vath or w~thout defimuons, the methods employed can be used to compare entries m dai''erent ontologles based pnmanly on the relattous m the ontology, both luerarclucal and other For ternunologlcal databases, the methods descnbed here can be used to exanune the set of conceptual relaUons lmphed by the defimtmus The defimuon parsing wall facd~tate the development of the termmolog~ca I network tn the pamcular field covered by the database The componenUal analysts methods result m a richer semantic network that can be used m other apphcattous Thus, for example, ~t ts possible to extend the leracal chatmng methods described m (Green, 1997), which are based on the semrels used m WordNet The semrels developed with the componenttal analysis method would provide additional detad available for apphcauon of lexlcal cohesion methods In particular, addtUonal relattous would penmt some structunng wmthm the individual leracal chams, rather than just consldenng each cham as an amorphous set (Green, 1999) Finally, we are currently investigating the use of the componenUal analysts techmque for mformauon extracUon The techmque identifies (from defimtlous) slots that can be used as slots or fields m template generataon Once these slots are identified, we wall be attemptmg to extract slot values from Items m large catalog databases (mdhons of items) 36 In conclusion, it would seem that, instead of a paucity of tnformation allovang us to compare lexmal resources, by bnngmg m the full semantic network of the lexicon, we are overwhelmed with a plethora of data Acknowledgments I would like to thank Bonnie Dorr, Chnstiane Fellbaum, Steve Green, Ed Hovy, Ramesh Knshnamurthy, Bob Krovetz, Thomas Potter, Lucy Vanderwende, and an anonymous reviewer for their comments on an earlier draft of this paper References Atlans, B T S (1991) Bmldmga lexicon The contribution of lexicography lnternattonal Journal of Lextcography, 4(3), 167-204 CL Research (1999a) CL Research Demos http//www clres com/Demo html CL Research (1999b) Dmtlonary Parsing Project http//www clres com/dpp html Dolan, W B (1994, 5-9 Aug) Word Sense Amblguation Chistenng Related Senses COLING-94, The 15th International Conference on Computational Linguistics Kyoto, Japan Green, S J (1997) Automatically generating hypertext by computing semantic smulanty [Dlss], Toronto, Canada Umverstty of Toronto Green, S J (Sjgreen@mn mq edu au) (1999, 1 June) (Rich semantic networks) Hovy, E (1998, May) Combining and Standardizing Large-Scale, Practical Ontologms for Machine Translation and Other Uses Language Resources and Evaluation Conference Granada, Spam Kalgarnff, A (1998) SENSEVAL Home Page http//www itn bton ac uk/events/senseval/ Krovetz, R (1992, June) Sense-Linking m a Machine Readable Dictionary 30th Annual Meeting of the Association for Computational Lmgu~stics Newark, Delaware Association for Computational Lmgtustics Lesk, M (1986) Automatic Sense Dlsamblguation Using Machine Readable Dmttonanes How to Tell a Pine Cone from an Ice Cream Cone Proceechngs of SIGDOC Lttkowski, K C (1978) Models of the semantic structure of dictionaries American Journal of Computattonal Lmgutsttcs, Atf 81, 25-74 Lttkowskl, K C (to appear) SENSEVAL The CL Research Expenence Computers and the Humamttes Mtller, G A, Beckwlth, R, Fellbaum, C, Gross, D, & Miller, K J (1990) Introduction to WordNet An on-hne lexical database lnternatwnal Journal of Lexicography, 3(4), 235-244 Olsen, M B, Dorr, B J, & Thomas, S C (1998, 28-31 October) Enhancmg Automatic Acqulsmon of Thematic Structure in a Large-Scale Lexacon for Mandann Chinese Tlurd Conference of the Association for Machine Translation m the Americas, AMTA-98 Langhorne, PA'
'1 Introduction In phrase-based statistical machine translation <CIT> phrases extracted from word-aligned parallel data are the fundamental unit of translation '
'Some of these methods make use of prior knowledge in the form of an existing thesaurus <OTH> , while others do not rely on any prior knowledge <OTH> '
'(Snow et al., 2006; Nakov & Hearst, 2008).'
'There is a vast literature on language modeling ; see , eg , <CIT> '
'2 Related Work The Yarowsky algorithm <CIT> , originally proposed for word sense disambiguation , makes the assumption that it is very unlikely for two occurrences of a word in the same discourse to have different senses '
'The annotation consists of four parts : 1 -RRB- a context-free structure augmented with traces to mark movement and discontinuous constituents , 2 -RRB- phrasal categories that are annotated as node labels , 3 -RRB- a small set of grammatical functions that are annotated as extensions to the node labels , and 4 -RRB- part-of-speech tags <CIT> '
'We report on ROUGE-1 -LRB- unigrams -RRB- , ROUGE-2 -LRB- bigrams -RRB- , ROUGE W-12 -LRB- weighted LCS -RRB- , and ROUGE-S \* -LRB- skip bigrams -RRB- as they appear to correlate well with human judgments for longer multi-document summaries , particularly ROUGE-1 <CIT> '
'The word alignments were created with Giza + + <CIT> applied to a parallel corpus containing the complete Europarl training data , plus sets of 4,051 sentence pairs created by pairing the test sentences with the reference translations , and the test sentences paired with each of the system translations '
'6 Evaluation 61 Data The data used for our comparison experiments were developed as part of the OntoNotes project <OTH> , which uses the WSJ part of the Penn Treebank <CIT> '
'<OTH> and <CIT> et al '
'The system was trained in a standard manner , using a minimum error-rate training -LRB- MERT -RRB- procedure <CIT> with respect to the BLEU score <OTH> on held-out development data to optimize the loglinear model weights '
'Probabilistic generative models like IBM 1-5 (Brown et al., 1993), HMM (Vogel et al., 1996), ITG (Wu, 1997), and LEAF (Fraser and Marcu, 2007) define formulas for P(f | e) or P(e, f), with ok-voon ororok sprok at-voon bichat dat erok sprok izok hihok ghirok totat dat arrat vat hilat ok-drubel ok-voon anok plok sprok at-drubel at-voon pippat rrat dat ok-voon anok drok brok jok at-voon krat pippat sat lat wiwok farok izok stok totat jjat quat cat lalok sprok izok jok stok wat dat krat quat cat lalok farok ororok lalok sprok izok enemok wat jjat bichat wat dat vat eneat lalok brok anok plok nok iat lat pippat rrat nnat wiwok nok izok kantok ok-yurp totat nnat quat oloat at-yurp lalok mok nok yorok ghirok clok wat nnat gat mat bat hilat lalok nok crrrok hihok yorok zanzanok wat nnat arrat mat zanzanat lalok rarok nok izok hihok mok wat nnat forat arrat vat gat Figure 1: Word alignment exercise (Knight, 1997).'
'For comparison to previous results , table 2 lists the results on the testing set for our best model -LRB- TOP-Efficient-Freq20 -RRB- and several other statistical parsers <CIT> '
'2 Background 21 Phrase Table Extraction Phrasal decoders require a phrase table <CIT> , which contains bilingual phrase pairs and 17 scores indicating their utility '
'This set of words -LRB- rooted primarily in the verbs of the set -RRB- corresponds to the <OTH> Characterize -LRB- class 292 -RRB- , Declare -LRB- 294 -RRB- , Admire -LRB- 312 -RRB- , and Judgment verbs -LRB- 33 -RRB- and hence may have particular syntactic and semantic patterning '
'212 Research on Syntax-Based SMT A number of researchers <CIT> have proposed models where the translation process involves syntactic representations of the source and\/or target languages '
'The reported results for the full parse tree -LRB- on section 23 -RRB- are recall\/precision of 881\/875 <CIT> '
'1 Introduction Estimating the degree of semantic relatedness between words in a text is deemed important in numerous applications : word-sense disambiguation <CIT> , story segmentation <OTH> , error correction <OTH> , summarization <OTH> '
'Many studies and improvements have been conducted for Presently with Service Media Laboratory , Corporate ResearchandDevelopmentCenter , OkiElectricIndustry Co , Ltd POS tagging , and major methods of POS tagging achieve an accuracy of 9697 % on the Penn Treebank WSJ corpus , but obtaining higher accuracies is difficult <CIT> '
'4 Pattern switching The compositional translation presents problems which have been reported by <OTH> : Fertility SWTs and MWTs are not translated by a term of a same length '
'We attribute the difference in M3\/4 scores to the fact we use a Viterbi-like training procedure -LRB- ie , we consider a single configuration of the hidden variables in EM training -RRB- while GIZA uses pegging <CIT> to sum over a set of likely hidden variable configurations in EM '
'In particular , ROUGE-2 is the recall in bigrams with a set of human-written abstractive summaries <CIT> '
'There have been a number of methods proposed in the literature to address the word clustering problem -LRB- eg , <CIT> -RRB- '
'Supervised methods include hidden Markov model -LRB- HMM -RRB- , maximum entropy , conditional random fields -LRB- CRF -RRB- , and support vector machines -LRB- SVM -RRB- <CIT> '
'As a final note , following <CIT> , we used the averaged parameters from the training algorithm in decoding test examples in our experiments '
'SO can be used to classify reviews -LRB- eg , movie reviews -RRB- as positive or negative <CIT> , and applied to subjectivity analysis such as recognizing hostile messages , classifying emails , mining reviews <OTH> '
'A boundary-based model of co-occurrence assumes that both halves of the bitext have been segmented into s segments, so that segment Ui in one half of the bitext and segment Vi in the other half are mutual translations, 1 < i < s. Under the boundary-based model of co-occurrence, there are several ways to compute co-occurrence counts cooc(u, v) between word types u and v. In the models of Brown, Della Pietra, Della Pietra, and Mercer (1993), reviewed in Section 4.3, s COOC(R, V) = ~ ei(u) .j~(V), (12) i=1 where ei and j5 are the unigram frequencies of u and v, respectively, in each aligned text segment i. For most translation models, this method produces suboptimal results, however, when ei(u) > 1 and )~(v) > 1.'
'We guess it is an acronym for the authors of <CIT> : Michel Galley , Mark Hopkins , Kevin Knight and Daniel Marcu '
'1 Introduction The rapid and steady progress in corpus-based machine translation <CIT> has been supported by large parallel corpora such as the Arabic-English and Chinese-English parallel corpora distributed by the Linguistic Data Consortium and the Europarl corpus <OTH> , which consists of 11 European languages '
'A few exceptions are the hierarchical -LRB- possibly syntax-based -RRB- transduction models <CIT> and the string transduction models <OTH> '
'<CIT> also detailed techniques for collocation extraction and developed a program called XTRACT , which is capable of computing flexible collocations based on elaborated statistical calculation '
'Our observation is that this situation is ideal for so-called bootstrapping , co-training , or minimally supervised learning methods <CIT> '
'This is contrastive to the one dimensional models used by Collinss perceptronbased sequence method <CIT> which our algorithms are based upon , and by the linear-chain CRFs '
'The phrase translation table is learned in the following manner : The parallel corpus is word-aligned bidirectionally , and using various heuristics -LRB- see <CIT> for details -RRB- phrase correspondences are established '
'For example , the words test and exam are similar because both of them follow verbs such as administer , cancel , cheat on , conduct , and both of them can be preceded by adjectives such as academic , comprehensive , diagnostic , difficult , Many methods have been proposed to compute distributional similarity between words <CIT> '
'In the future , we plan to explore our discriminative framework on a full distortion model <CIT> or even a hierarchical model <OTH> '
'The MBT <OTH> 180 Tagger Type Standard Trigram <OTH> MBT <OTH> Rule-based <OTH> Maximum-Entropy <CIT> Full Second-Order HMM SNOW <OTH> Voting Constraints <OTH> Full Second-Order HMM Known Unknown Overall Open\/Closed Lexicon ? '
'Step 2 involves extracting minimal xRS rules <CIT> from the set of string\/tree\/alignments triplets '
'? ? queries : The queries of <CIT> are made up of a pair of adjectives , and in our approach the query contains the content words of the headline and an emotion '
'First , it recognizes non-recursive Base Noun Phrase -LRB- BNP -RRB- <CIT> '
'Statistical or probabilistic methods are often used to extract semantic clusters from corpora in order to build lexical resources for ANLP tools <CIT> , <OTH> , <OTH> , or for automatic thesaurus generation <OTH> '
'Examples of such early work include <CIT> '
'CRP-based samplers have served the communitywellinrelatedlanguagetasks , suchaswordsegmentation and coreference resolution <CIT> '
'Apart from the fact that we present an alternative model , our work differs from <CIT> in two important ways '
'2 Baseline DP Decoder The translation model used in this paper is a phrasebased model <CIT> , where the translation units are so-called blocks : a block b is a pair consisting of a source phrase s and a target phrase t which are translations of each other '
'2 Related Work Starting with the IBM models <CIT> , researchers have developed various statistical word alignment systems based on different models , such as hidden Markov models -LRB- HMM -RRB- <OTH> , log-linear models <OTH> , and similarity-based heuristic methods <OTH> '
'Text similarity has been also used for relevance feedback and text classification <OTH> , word sense disambiguation <OTH> , and more recently for extractive summarization <OTH> , and methods for automatic evaluation of machine translation <CIT> or text summarization <OTH> '
'Models describing these types of dependencies are referred to as alignrnent models <CIT> , <OTH> '
'Another way of doing the parameter estimation for this matching task would have been to use an averaged perceptron method , as in <CIT> '
'Phrases of up to 10 in length on the French side were extracted from the parallel text , and minimum-error-rate training <CIT> was 8 We can train on the full training data shown if tighter constraints are placed on rule extraction for the United Nations data '
'We have investigated this and our results are in line with <CIT> showing that the translation quality does not improve if we utilize phrases beyond a certain length '
'3 Related work Word collocation Various collocation metrics have been proposed , including mean and variance <OTH> , the t-test <CIT> , the chi-square test , pointwise mutual information -LRB- MI -RRB- <CIT> , and binomial loglikelihood ratio test -LRB- BLRT -RRB- <OTH> '
'(Papineni et al. , 2002).'
'By introducing the hidden word alignment variable a <OTH> , the optimal translation can be searched for based on the following criterion : \* 1 , arg max -LRB- -LRB- , , -RRB- -RRB- M mm m ea eh = = efa -LRB- 1 -RRB- where is a string of phrases in the target language , e f fa is the source language string of phrases , he are feature functions , weights -LRB- , , -RRB- m m are typically optimized to maximize the scoring function <CIT> '
'To do this , we first identify initial phrase pairs using the same criterion as previous systems <CIT> : Definition 1 '
'We use the minimum-error rate training procedure by <CIT> as implemented in the Moses toolkit to set the weights of the various translation and language models , optimizing for BLEU '
'Word association norms , mutual information , and lexicography , Computational Linguistics , 16 -LRB- 1 -RRB- : 22-29 <CIT> , M et al 1993 '
'These forest rescoring algorithms have potential applications to other computationally intensive tasks involving combinations of different models , for example , head-lexicalized parsing <CIT> ; joint parsing and semantic role labeling <OTH> ; or tagging and parsing with nonlocal features '
'For each word in the LDV , we consulted three existing thesauri : Rogets Thesaurus <OTH> , Collins COBUILD Thesaurus <OTH> , and WordNet <OTH> '
'Such methods were presented in <OTH> and ~ flensky , 1978 -RRB- '
'This direction has been forming the mainstream of research on opinion-sensitive text processing <CIT> '
'The up-arrows and down-arrows are shorthand for -LRB- M -LRB- ni -RRB- -RRB- = -LRB- ni -RRB- where ni is the c-structure node annotated with the equation2 Treebest : = argmaxTreeP -LRB- Tree F-Str -RRB- -LRB- 1 -RRB- P -LRB- Tree F-Str -RRB- : = productdisplay X Y in Tree Feats = -LCB- ai vj -LRB- -LRB- X -RRB- -RRB- ai = vj -RCB- P -LRB- X Y X , Feats -RRB- -LRB- 2 -RRB- The generation model of <CIT> maximises the probability of a tree given an f-structure -LRB- Eqn '
'where they are expected to be maximally discriminative <CIT> '
'In our own work on document compression models <CIT> , both of which extend the sentence compression model of Knight and Marcu <OTH> , we assume that sentences and documents can be summarized exclusively through deletion of contiguous text segments '
'4 Optimizing Metric Parameters The original version of Meteor <CIT> has instantiated values for three parameters in the metric : one for controlling the relative weight of precision and recall in computing the Fmean score -LRB- -RRB- ; one governing the shape of the penalty as a function of fragmentation -LRB- -RRB- and one for the relative weight assigned to the fragmentation penalty -LRB- -RRB- '
'It has been lately incorporated into computational lexicography in <OTH> , <CIT> , <OTH> , <OTH> , <OTH> -RRB- '
'5 Related Work We already discussed the relation of our work to <CIT> in Section 24 '
'In the proposed method , the statistical machine translation -LRB- SMT -RRB- <CIT> is deeply incorporated into the question answering process , instead of using the SMT as the preprocessing before the mono-lingual QA process as in the previous work '
'For the factored language models , a feature-based word representation was obtained by tagging the text with Rathnaparkis maximum-entropy tagger <CIT> and by stemming words using the Porter stemmer <OTH> '
'Due to the positive results in Ando <OTH> , <CIT> et al '
'Many-to-many alignments can be created by combining two GIZA + + alignments , one where English generates Foreign and another with those roles reversed <CIT> '
'Be-Comp Following the general idea in <CIT> , we identify the ISA pattern in the definition sentence by extracting nominal complements of the verb be , taking 451 No '
'2 Data and annotation Yahoo ! s image query API was used to obtain a corpus of pairs of semantically ambiguous images , in thumbnail and true size , and their corresponding web sites for three ambiguous keywords inspired by <CIT> : BASS , CRANE , and SQUASH '
'32 Wall Street Journal Our out-of-domain data is the Wall Street Journal -LRB- WSJ -RRB- portion of the Penn Treebank <CIT> which consists of about 40,000 sentences -LRB- one million words -RRB- annotated with syntactic information '
'production rules are typically learned from alignment structures <CIT> or from alignment structures and derivation trees for the source string <OTH> '
'A token can be a word or a punctuation symbol , and each of these neighboring tokens must be in the same sentence as a2 We use a sentence segmentation program <CIT> and a POS tagger <CIT> to segment the tokens surrounding a2 into sentences and assign POS tags to these tokens '
'POS disambiguation has usually been performed by statistical approaches , mainly using the hidden Markov model -LRB- HMM -RRB- in English research communities <CIT> '
'Optimization and measurement were done with the NIST implementation of case-insensitive BLEU 4n4r <CIT> 4 41 Baseline We compared translation by pattern matching with a conventional exact model representation using external prefix trees <OTH> '
'2 Bilingual Bracketing In <CIT> , the Bilingual Bracketing PCFG was introduced , which can be simplified as the following production rules : A ! -LRB- AA -RRB- -LRB- 1 -RRB- A ! -LRB- AA -RRB- -LRB- 2 -RRB- A ! f = e -LRB- 3 -RRB- A ! f = null -LRB- 4 -RRB- A ! null = e -LRB- 5 -RRB- Where f and e are words in the target vocabulary Vf and source vocabulary Ve respectively '
'<CIT> also used re-decoding to do system combination by extracting sentence-specific phrase translation tables from the outputs of different MT systems and running a phrase-based decoding with this new translation table '
'82 2 Aggregate Markov models In this section we consider how to construct classbased bigram models <CIT> '
'The average senior high school student achieves 57 % correct <CIT> '
'Secondly , we used the Kappa coefficient <CIT> , which has become the standard evaluation metric and the score obtained was 0905 '
'<OTH> and Chiang <OTH> , in terms of what alignments they induce , has been discussed in <CIT> and Wellington et al '
'The basic engine used to perform the tagging in these experiments is a direct descendent of the maximum entropy -LRB- ME -RRB- tagger of <CIT> which in turn is related to the taggers of <OTH> and <OTH> '
'In recent years , many researchers have tried to automatically evaluate the quality of MT and improve the performance of automatic MT evaluations <CIT> because improving the performance of automatic MT evaluation is expected to enable us to use and improve MT systems efficiently '
'52 Results on the Newsblaster data We measured how well the models trained on DUC data perform with current news labeled using human 4http : \/ \/ newsblastercscolumbiaedu 5a20 -LRB- kappa -RRB- is a measure of inter-annotator agreement over and above what might be expected by pure chance -LRB- See <CIT> for discussion of its use in NLP -RRB- a20a22a21a24a23 if there is perfect agreement between annotators anda20a25a21a27a26 if the annotators agree only as much as you would expect by chance '
'After unioning the Viterbi alignments , the stems were replaced with their original words , and phrase-pairs of up to five foreign words in length were extracted in the usual fashion <CIT> '
'Past work <CIT> has examined the use of monolingual parallel corpora for paraphrase extraction '
'Instead of analyzing sentences directly , AUCONTRAIRE relies on the TEXTRUNNER Open Information Extraction system <CIT> to map each sentence to one or more tuples that represent the entities in the sentences and the relationships between them -LRB- eg , was born in -LRB- Mozart , Salzburg -RRB- -RRB- '
'The training data for the French\/English data set is taken from the LDC Canadian Hansard data set , from which the word aligned data <CIT> was also taken '
'The feature weights are learned by maximizing the BLEU score <OTH> on held-out data,usingminimum-error-ratetraining <OTH> as implemented by <CIT> '
'We base our work partly on previous work done by Bagga and Baldwin <OTH> , which has also been used in later work <CIT> '
'We have developed a set of extensions to a probabilistic translation model <CIT> that enable us to successfully merge oversegmented regions into coherent objects '
'Evaluation metrics such as BLEU <CIT> have a built-in preference for shorter translations '
'4 POS Tagger and Named Entity Recognizer For the POS tagging task , the tagger is built based on the work of <CIT> which was applied for English POS tagging '
'Early experiments with syntactically-informed phrases <OTH> , and syntactic reranking of K-best lists <CIT> produced mostly negative results '
'1 Specifically , MIMIC uses an n-dimensional call router front-end <CIT> , which is a generalization of the vector-based call-routing paradigm of semantic interpretation <OTH> ; that is , instead of detecting one concept per utterance , MIMIC ''s semantic interpretation engine detects multiple -LRB- n -RRB- concepts or classes conveyed by a single utterance , by using n call touters in parallel '
'42 Cast3LB Function Tagging For the task of Cast3LB function tag assignment we experimented with three generic machine learning algorithms : a memory-based learner <OTH> , a maximum entropy classifier <CIT> and a Support Vector Machine classifier <OTH> '
'Specifically , we explore the statistical term weighting features of the word generation model with Support Vector machine -LRB- SVM -RRB- , faithfully reproducing previous work as closely as possible <CIT> '
'5 Evaluation 51 Datasets We used two datasets , customer reviews 1 <OTH> and movie reviews 2 <CIT> to evaluate sentiment classification of sentences '
'Semantic DSN : The construction of this network is inspired by <CIT> '
'distance -LRB- MSD -RRB- and the maximum swap segment size -LRB- MSSS -RRB- ranging from 0 to 10 and evaluated the translations with the BLEU7 metric <CIT> '
'An application of the idea of alternative targets can be seen in <CIT> work on election prediction '
'<OTH> from Sections 2-21 of the Wall Street Journal -LRB- WSJ -RRB- in the Penn Treebank <CIT> and its subsets3 We then converted them into strongly equivalent HPSG-style grammars using the grammar conversion described in Section 21 '
'As referring dataset , we used the PropBank corpora available at wwwcisupennedu\/ace , along with the Penn TreeBank 2 -LRB- wwwcisupennedu\/treebank -RRB- <CIT> '
'See <CIT> and Collins and Duffy -LRB- 2001 , 2002 -RRB- for applications of the perceptron algorithm '
'There has been a growing interest in corpus-based approaches which retrieve collocations from large corpora <OTH> , <OTH> <OTH> , <OTH> , <OTH> , <CIT> , <CIT> , <OTH> '
'42 Smoothing : Gaussian Priors Since NLP maximum entropy models usually have lots of features and lots of sparseness -LRB- eg features seen in testing not occurring in training -RRB- , smoothing is essential as a way to optimize the feature weights <CIT> '
'It is possible that there is a better automated method for finding such phrases , such as the methods in <CIT> '
'We then train word alignment models <CIT> using 6 Model-1 iterations and 6 HMM iterations '
'Sang used the IOB tagging method proposed by Ramshow <CIT> and memory-based learning for each level of chunking and achieved an f-score of 8049 on the Penn Treebank corpus '
'<CIT> present Factored Translation Models as an extension to phrase-based statistical machine translation models '
'We discriminatively trained our parser in an on-line fashion using a variant of the voted perceptron <CIT> '
'This criticism leads us to automatic approaches for building thesauri from large corpora <CIT> '
'In this paper , we use IBM model 1 <CIT> in order to get the probability P -LRB- Q DA -RRB- as follows '
'MXPOST <CIT> , and in order to discover more general patterns , we map the tag set down after tagging , eg NN , NNP , NNPS and NNS all map to NN '
'For instance , the to-PP frame is poorly '' represented in the syntactically annotated version of the Penn Treebank <OTH> '
'It also shows that DOP ''s frontier lexicalization is a viable alternative to constituent lexicalization <CIT> '
'They have used the <CIT> representation as well -LRB- IOB1 -RRB- '
'This sort of problem can be solved in principle by conditional variants of the Expectation-Maximization algorithm <OTH> '
'They are most commonly used for parsing and linguistic analysis <CIT> , but are now commonly seen in applications like machine translation <OTH> and question answering <OTH> '
'We run Maximum BLEU <CIT> for 25 iterations individually for each system '
'GIZA + + refined alignments have been used in state-of-the-art phrase-based statistical MT systems such as <OTH> ; variations on the refined heuristic have been used by <CIT> -LRB- diag and diag-and -RRB- and by the phrase-based system Moses -LRB- grow-diag-final -RRB- <CIT> '
'By introducing the hidden word alignment variable a, the following approximate optimization criterion can be applied for that purpose: e = argmaxe Pr(e | f) = argmaxe summationdisplay a Pr(e,a | f)  argmaxe,a Pr(e,a | f) Exploiting the maximum entropy (Berger et al. , 1996) framework, the conditional distribution Pr(e,a | f) can be determined through suitable real valued functions (called features) hr(e,f,a),r = 1R, and takes the parametric form: p(e,a | f)  exp Rsummationdisplay r=1 rhr(e,f,a)} The ITC-irst system (Chen et al. , 2005) is based on a log-linear model which extends the original IBM Model 4 (Brown et al. , 1993) to phrases (Koehn et al. , 2003; Federico and Bertoldi, 2005).'
'We consider three class models , models S , M , and L , defined as pS -LRB- cj c1cj1 , w1wj1 -RRB- = png -LRB- cj cj2cj1 -RRB- pS -LRB- wj c1cj , w1wj1 -RRB- = png -LRB- wj cj -RRB- pM -LRB- cj c1cj1 , w1wj1 -RRB- = png -LRB- cj cj2cj1 , wj2wj1 -RRB- pM -LRB- wj c1cj , w1wj1 -RRB- = png -LRB- wj wj2wj1cj -RRB- pL -LRB- cj c1cj1 , w1wj1 -RRB- = png -LRB- cj wj2cj2wj1cj1 -RRB- pL -LRB- wj c1cj , w1wj1 -RRB- = png -LRB- wj wj2cj2wj1cj1cj -RRB- Model S is an exponential version of the class-based n-gram model from <CIT> ; model M is a novel model introduced in <OTH> ; and model L is an exponential version of the model indexpredict from <OTH> '
'Errors from the sentence boundary detector in GATE <OTH> were especially problematic because they caused the Collins parser to fail , resulting in no dependency tree information '
'The system combination weights one for each system , LM weight , and word and NULL insertion penalties were tuned to maximize the BLEU <CIT> score on the tuning set -LRB- newssyscomb2009 -RRB- '
'For instance , the frequency collected from the data can be used to bias initial transition and emission probabilities in an HMM model ; the tagged words in IGT can be used to label the resulting clusters produced by the word clustering approach ; the frequent and unambiguous words in the target lines can serve as prototype examples in the prototype-driven approach <CIT> '
'The learning algorithm follows the global strategy introduced in <CIT> and adapted in <OTH> for partial parsing tasks '
'In a different work , <CIT> argued that the measured reliability of metrics can be due to averaging effects but might not be robust across translations '
'We automatically measure performance by comparing the produced headlines against one reference headline produced by a human using ROUGEa129 <CIT> '
'<OTH> effectively utilized unlabeled data to improve parsing accuracy on the standard WSJ training set , but they used a two-stage parser comprised of Charniaks lexicalized probabilistic parser with n-best parsing and a discriminative reranking parser <OTH> , and thus it would be better categorized as co-training <CIT> '
'713 Similarity via pagerank Pagerank <OTH> is the celebrated citation ranking algorithm that has been applied to several natural language problems from summarization <OTH> to opinion mining <OTH> to our task of lexical relatedness <CIT> '
'Furthermore , the underlying decoding strategies are too time consuming for our application We therefore use a translation model based on the simple linear interpolation given in equation 2 which combines predictions of two translation models - Ms and M ~ - both based on IBM-like model <CIT> '
'We use the following features in our induced English-to-English grammar :3 3Hiero also uses lexical weights <CIT> in both 122 ? ? The joint probability of the two English hierarchical paraphrases , conditioned on the nonterminal symbol , as defined by this formula : p -LRB- e1 , e2 x -RRB- = c -LRB- X ? ? ? e1 , e2 ? ? summationtext e1prime , e2prime c -LRB- X ? ? ? e1prime , e2prime ? ? '
'<OTH> 866 867 119 Klein and Manning <OTH> 869 857 863 309 110 Charniak <OTH> 874 875 100 <CIT> <OTH> 886 881 091 Table 3 : Comparison with other parsers -LRB- sentences of length 40 -RRB- as head information '
'The superiority of discriminative models has been shown on many tasks when the discriminative and generative models use exactly the same model structure <CIT> '
'ENGLISH GERMAN CHINESE <CIT> <OTH> <OTH> TrainSet Section 2-21 Sentences 1-18 ,602 Articles 26-270 DevSet Section 22 18,603-19 ,602 Articles 1-25 TestSet Section 23 19,603-20 ,602 Articles 271-300 Table 3 : Experimental setup '
'In the following experiments , we run two machine learning classifiers : Bayes Point Machines -LRB- BPM -RRB- <OTH> , and the maximum entropy model -LRB- ME -RRB- <CIT> '
'Semi-supervised conditional random fields -LRB- CRFs -RRB- based on a minimum entropy regularizer -LRB- SS-CRF-MER -RRB- have been proposed in <CIT> '
'<CIT> and Gao & Johnson -LRB- 2008 -RRB- assume that words are generated by a hidden Markov model and find that the resulting states strongly correlate with POS tags '
'optimization approaches which aim at selecting those examples that optimize some -LRB- algorithm-dependent -RRB- objective function , such as prediction variance <OTH> , and heuristic methods with uncertainty sampling <OTH> and query-by-committee -LRB- QBC -RRB- <OTH> just to name the most prominent ones '
'Lexical Weighting : -LRB- e -RRB- the lexical weight a27 a14a12a91 a29 a92a93a21 of the block a9 a72 a14a12a91 a19a86a92a93a21 is computed similarly to <CIT> , details are given in Section 34 '
'Predicate argument structures , which consist of complements -LRB- case filler nouns and case markers -RRB- and verbs , have also been used in the task of noun classification <CIT> '
'62 Experimental Settings We utilize a maximum entropy -LRB- ME -RRB- model <CIT> to design the basic classifier for WSD and TC tasks '
'It has been used for a variety of tasks , such as wide-coverage parsing <OTH> , sentence realization <OTH> , learning semantic parsers <CIT> , dialog systems <OTH> , grammar engineering <OTH> , and modeling syntactic priming <OTH> '
'3 Network Evaluation We present an evaluation which has been carried out on an initial set of annotations of English articles from The Wall Street Journal -LRB- covering those annotated at the syntactic level in the Penn Treebank <CIT> -RRB- '
'The labeling agreement was 84 % -LRB- n = 80 ; <CIT> -RRB- '
'These tasks are generally treated as sequential labeling problems incorporating the IOB tagging scheme <CIT> '
'We measure translation performance by the BLEU score <CIT> with one reference for each hypothesis '
'However , the Naive Bayes classifier has been found to perform well for word-sense disambiguation both here and in a variety of other works -LRB- eg , <OTH> , <OTH> , <OTH> , and <OTH> -RRB- '
'<CIT> describe an error-driven transformation-based learning -LRB- TBL -RRB- method for finding NP chunks in texts '
'For practical reasons , the maximum size of a token was set at three for Chinese , andfor forKorean2 Minimum error rate training <CIT> was run on each system afterwardsand BLEU score <OTH> was calculated on the test sets '
'The concept of baseNP has undergone a number of revisions <CIT> but has previously always been tied to extraction from a more completely annotated treebank , whose annotations are subject to other pressures than just initial material up to the head To our knowledge , our gures for inter-annotator agreement on the baseNP task itself 169 -LRB- ie not derived from a larger annotation task -RRB- are the rst to be reported '
'The noun phrase extraction module uses Brill ''s POS tagger -LRB- Brill <OTH> -RRB- and a base NP chunker -LRB- <CIT> -RRB- '
'<CIT> argues that many NLP tasks can be formulated in terms of analogical reasoning , and he applies his PairClass algorithm to a number of problems including SAT verbal analogy tests , synonym\/antonym classification and distinction between semantically similar and semantically associated words '
'Five chunk tag sets , IOB1 , IOB2 , IOE1 , IOE2 <CIT> and SE <OTH> , are commonly used '
'They have made semantic formalisms like those now usually associated with Davison <OTH> attractive in artificial intelligence for many years <CIT> '
'In fact , a limitation of the experiments described in this paper is that the loglinear weights for the glass-box techniques were optimized for BLEU using Ochs algorithm <CIT> , while the linear weights for 55 black-box techniques were set heuristically '
'We computed the LCS and WLCS-based F-measure following <CIT> using both the query pool and the sentence pool as in the previous section '
'<OTH> and <CIT> <OTH> '
'Systems which are able to acquire a small number of verbal subcategorisation classes automatically from corpus text have been described by Brent <OTH> , and Ushioda et al '
'Iterative cost reduction algorithm Input: An SCFG  Output: An equivalent binary SCFG    of  1: Function ITERATIVECOSTREDUCTION( ) 2:         0 3:   for each    0do 4:         ( ) =     ,  0 5:   while  (  ) does not converge do 6:        for each      do 7:             [   ]        (  ) 8:            for each     (  ) do 9:                for each        ,     do 10:                              1 11:           (  )   CKYBINARIZATION(  ,  ) 12:                [   ]    (  ) 13:          for each     (  ) do 14:              for each        ,     do 15:                             + 1 16: return   In the iterative cost reduction algorithm, we first obtain an initial binary SCFG  0 using the synchronous binarization method proposed in (Zhang et al., 2006).'
'The most relevant to our work are <CIT> , Toral and Muoz -LRB- 2006 -RRB- , and Cucerzan -LRB- 2007 -RRB- '
'A different approach in evaluating nonparametric Bayesian models for NLP is statesplitting <CIT> '
'<CIT> Hindi is a verb final , flexible word order language and therefore , has frequent occurrences of non-projectivity in its dependency structures '
'The goal of each selection stage is to select the feature f that maximizes the gain of the log likelihood, where the a and gain of f are derived through following steps: Let the log likelihood of the model be  -=  yx xZysump pL,, )(/|log()( ~ and the empirical expectation of feature f be  E  p (f)= p (x,y)f(x,y) x,y  With the approximation assumption in Berger et al (1996)s paper, the un-normalized component and the normalization factor of the model have the following recursive forms: )|()|( aa exysumxysum SfS =  | Z f + The approximate gain of the log likelihood is computed by  G Sf (a)L(p Sf a )-L(p S ) =- p (x)(logZ Sf,a (x) x  /Z S (x)) +aE  p (f) (1) The maximum approximate gain and its corresponding a are represented as: )(max),(~ a fS GfSL  =D maxarg f 3 A Fast Feature Selection Algorithm The inefficiency of the IFS algorithm is due to the following reasons.'
'<CIT> described the use of a biased PageRank over the WordNet graph to compute word pair semantic relatedness using the divergence of the probability values over the graph created by each word '
'<CIT> used a binary bracketing ITG to segment a sen19 tence while simultaneously word-aligning it to its translation , but the model was trained heuristically with a fixed segmentation '
'Then , it models the correlations between the pivot features and all other features by training linear pivot predictors to predict occurrences of each pivot in the unlabeled data from both domains <CIT> '
'<CIT> proposed a joint model of text and aspect ratings which utilizes a modified LDA topic model to build topics that are representative of ratable aspects , and builds a set of sentiment predictors '
'Furthermore , techniques such as iterative minimum errorrate training <CIT> as well as web-based MT services require the decoder to translate a large number of source-language sentences per unit time '
'Its previous applications <CIT> demonstrated that cooccurrence statistics on a target word is often sufficient for its automatical classification into one of numerous classes such as synsets of WordNet '
'No documentation of tile construction algorithm of the su -LRB- lix lexicon in <CIT> was available '
'It is also related to loglinear models for machine translation <CIT> '
'3 Baseline MT System The phrase-based SMT system used in our experiments is Moses , phrase translation pro ing probabilities , and languag ties are combined in the log-linear model to obtain the best translation best e of the source sentence f : = = M p -RRB- -LRB- maxarg fee ebest -LRB- 2 -RRB- m mm h 1 , -LRB- maxarg f -RRB- e e The weights are set by a discriminative training method using a held-out data set as describ in <CIT> '
'Introduction Verb subcategorizafion probabilities play an important role in both computational linguistic applications <OTH> and psycholinguisfic models of language processing <OTH> '
'For example , in John saw Mary yesterday at the station , only John and Mary are required arguments while the other constituents are optional -LRB- adjuncts -RRB- 3 The problem of SF identification using statistical methods has had a rich discussion in the literature <OTH> -LRB- also see the refences cited in <OTH> -RRB- '
'Given sentence-aligned bi-lingual training data , we first use GIZA + + <CIT> to generate word level alignment '
'This strategy is commonly used in multi-document summarization <CIT> , where the combination step eliminates the redundancy across selected excerpts '
'Previous uses of this model include language modeling <OTH> , machine translation <OTH> , prepositional phrase attachment <OTH> , and word morphology <OTH> '
'In his Xtract system , <CIT> first extracted significant pairs of words that consistently co-occur within a single syntactic structure using statistical scores called distance , strength and spread , and then examined concordances of the bi-grams to find longer frequent multiword units '
'22 Brown clustering algorithm In order to provide word clusters for our experiments , we used the Brown clustering algorithm <CIT> '
'a Hindle and Rooth <OTH> and <CIT> used partial parses generated by Fidditch to study word ~ urrtnc patterns m syntactic contexts '
'<CIT> observed , however , that the piecewiseconstant property could be exploited to characterize the function exhaustively along any line in parameter space , and hence to minimize it globally along that line '
'To overcome the knowledge acquisition bottleneck problem suffered by supervised methods , these methods make use of a small annotated corpus as seed data in a bootstrapping process <OTH> <CIT> '
'The tagging scheme is a variant of the IOB scheme originally put forward by <CIT> '
'The central question in learning is how to set the parameters a , given the training examples b x 1 , y 1 , x 2 , y 2 , : : : , x n , y n Logistic regression and boosting involve different algorithms and criteria for training the parameters a , but recent work <CIT> has shown that the methods have strong similarities '
'<CIT> -RRB- , the tagger for grammatical functions works with lexical and contextual probability measures Pq -LRB- -RRB- '
'4 Semantic Class Induction from Wikipedia Wikipedia has recently been used as a knowledge source for various language processing tasks , including taxonomy construction <OTH> , coreference resolution <OTH> , and English NER -LRB- eg , Bunescu and Pasca <OTH> , Cucerzan <OTH> , Kazama and Torisawa <OTH> , <CIT> et al '
'The translation models and lexical scores were estimated on the training corpus whichwasautomaticallyalignedusingGiza + + <OTH> in both directions between source and target and symmetrised using the growing heuristic <CIT> '
'Inter-annotator agreement is typically measured by the kappa statistic <CIT> , dekappa frequency 00 02 04 06 08 10 0 2 4 6 8 Figure 2 : Distribution of -LRB- inter-annotator agreement -RRB- across the 54 ICSI meetings tagged by two annotators '
'Table 6 shows 3An exception is Golding <OTH> , who uses the entire Brown corpus for training -LRB- 1M words -RRB- and 3\/4 of the Wall Street Journal corpus <CIT> for testing '
'As in <CIT> , confusion networks built around all skeletons are joined into a lattice which is expanded and rescored with language models '
'For example , in machine translation evaluation , approaches such as BLEU <CIT> use n-gram overlap comparisons with a model to judge overall goodness , with higher n-grams meant to capture fluency considerations '
'2 We used the <CIT> to generate the constituency parse and a dependency converter <OTH> to obtain the dependency parse of English sentences '
'Verbs and possible senses in our corpus Both corpora were lemmatized and part-of-speech -LRB- POS -RRB- tagged using Minipar <OTH> and Mxpost <CIT> , respectivelly '
'The method uses a translation model based on IBM Model 1 <CIT> , in which translation candidates of a phrase are generated by combining translations and transliterations of the phrase components , and matching the result against a large corpus '
'Recently used machine learning methods including maximum entropy models <CIT> and support vector machines <OTH> provide grounds for this type of modeling , because it allows various dependent features to be incorporated into the model without the independence assumption '
'Gibbs sampling is not new to the natural language processing community <CIT> '
'In the field of parsing , <CIT> compared parsing errors between graphbased and transition-based parsers '
'CLL has then been applied to a corpus of declarative sentences from the Penn Treebank <CIT> on which it has been shown to perform comparatively well with respect to much less psychologically plausible systems , which are significantly more supervised and are applied to somewhat simpler problems '
'41 Data We used Penn-Treebank <CIT> data , presented in Table 1 '
'1 Introduction Current state-of-the-art statistical parsers <OTH> are trained on large annotated corpora such as the Penn Treebank <CIT> '
'There are two tasks <CIT> for the domain adaptation problem '
'Fox <OTH> , <CIT> and Wellington et al '
'Many of these tasks have been addressed in other fields , for example , hypothesis verification in the field of machine translation <OTH> , sense disambiguation in speech synthesis <CIT> , and relation tagging in information retrieval <OTH> '
'Hierarchical rules were extracted from a subset which has about 35M\/41M words5 , and the rest of the training data were used to extract phrasal rules as in <CIT> '
'In this paper we focus on the second issue , constraining the grammar to the binary-branching Inversion Transduction Grammar of <CIT> '
'The tagger was tested on two corpora-the Brown corpus -LRB- from the Treebank II CDROM <CIT> -RRB- and the Wall Street Journal corpus -LRB- from the same source -RRB- '
'The second alternative used BerkeleyAligner <CIT> , which shares information between the two alignment directions to improve alignment quality '
'Further enhancement of these utilities include compiling collocation statistics <CIT> and semi-automatic gloassary construction <OTH> '
'corpus (Dunning, 1993; Scott, 1997; Rayson et al., 2004).'
'Firstly , <CIT> resorted to heuristics to extract the Stringto-Dependency trees , whereas our approach employs the well formalized CCG grammatical theory '
'Minimum Error Rate training <CIT> over BLEU was used to optimize the weights for each of these models over the development test data '
'Kappa is a better measurement of agreement than raw percentage agreement <CIT> because it factors out the level of agreement which would be reached by random annotators using the same distribution of categories as the real coders '
'We use the log-likelihood X ~ statistic , rather than the Pearson ''s X 2 statistic , as this is thought to be more appropriate when the counts in the contingency table are low <CIT> '
'3 Experimental Results and Discussion We test our parsing models on the CONLL-2007 <CIT> data set on various languages including Arabic , Basque , Catalan , Chinese , English , Italian , Hungarian , and Turkish '
'NP chunks <CIT> and technical terms <OTH> fall into this difficult-toassess category '
'Under the maximum entropy framework <CIT> , evidence from different features can be combined with no assumptions of feature independence '
'31 Background <CIT> introduced the quasisynchronous grammar formalism '
'Our technique is based on a novel Gibbs sampler that draws samples from the posterior distributionofaphrase-basedtranslationmodel <CIT> but operates in linear time with respect to the number of input words -LRB- Section 2 -RRB- '
'We analyze our results using syntactic features extracted from a parse tree generated by Collins parser <CIT> and compare those to models built using features extracted from FrameNets human annotations '
'Weights for the log-linear model are set using the 500-sentence tuning set provided for the shared task with minimum error rate training <CIT> as implemented by Venugopal and Vogel -LRB- 2005 -RRB- '
'both relevant and non-redundant <CIT> , some recent work focuses on improved search <OTH> '
'There are multiple studies <OTH> showing that the agreement between two -LRB- untrained -RRB- native speakers is about upper a15 a12a14a7 to lower a0a4a12a14a7 '
'More specifically , a statistical word alignment model <CIT> is used to acquire a bilingual lexicon consisting of NL substrings coupled with their translations in the target MRL '
'The statistical methods are based on distributional analysis -LRB- we defined a measure called mutual conditioned plausibility , a derivation of the well known mutual information -RRB- , and cluster analysis <OTH> '
'5 Experiments We compare the performance of our forest reranker against n-best reranking on the Penn English Treebank <CIT> '
'Confusion network and re-decoding have been well studied in the combination of different MT systems <CIT> '
'These probabilities are estimated with IBM model 1 <CIT> on parallel corpora '
'However , current sentence alignment models , <CIT> '
'A number of bootstrapping methods have been proposed for NLP tasks -LRB- eg <CIT> , Collins and Singer -LRB- 1999 -RRB- , Riloff and Jones -LRB- 1999 -RRB- -RRB- '
'Dependency relations are produced using a version of the Collins parser <CIT> that has been adapted for building dependencies '
'For each , we give case-insensitive scores on version 06 of METEOR <OTH> with all modules enabled , version 104 of IBMstyle BLEU <CIT> , and version 5 of TER <OTH> '
'For an alignment model , most of these use the Aachen HMM approach <OTH> , the implementation of IBM Model 4 in GIZA + + <OTH> or , more recently , the semi-supervised EMD algorithm <CIT> '
'In the context of part-of-speech tagging , <CIT> argue for the same distinctions made here between discriminative models and discriminative training criteria , and come to the same conclusions '
'One of the applications is in automatic summarization in order to compress sentences extracted for the summary <CIT> '
'Several researchers -LRB- eg , <CIT> -RRB- work on reducing the granularity of sense inventories for WSD '
'Evaluation 61 Evaluation at the Token Level This section compares translation model estimation methods A , B , and C to each other and to <CIT> Model 1 '
'Movie and product reviews have been the main focus of many of the recent studies in this area <CIT> '
'Note that our use of cepts differs slightly from that of <CIT> , inasmuch cepts may not overlap , according to our definition '
'B = <OTH> ; M = <OTH> ; O = our data ; R = <CIT> ; W = <OTH> '
'1 Introduction The last few decades have seen the emergence of multiple treebanks annotated with different grammar formalisms , motivated by the diversity of languages and linguistic theories , which is crucial to the success of statistical parsing <CIT> '
'This normal form allows simpler algorithm descriptions than the normal forms used by <CIT> and Melamed -LRB- 2003 -RRB- '
'Studies on self-training have focused mainly on generative , constituent based parsing <CIT> '
'~ F ~ c ~ R ~ cR -LRB- 2 -RRB- ~ -RRB- ~ -RRB- continue explanations , we begin by mentioning the ` Xtrgct '' tool by Smadja <CIT> '
'The line search is an extension of that described in -LRB- <CIT> 2003 ; Quirk et al 2005 '
'34 Learning algorithm Maximum entropy -LRB- ME -RRB- models <CIT> , also known as log-linear and exponential learning models , has been adopted in the SC classification task '
'Its roots are the same as computational linguistics -LRB- CL -RRB- , but it has been largely ignored in CL until recently <CIT> '
'The recent emphasis on improving these components of a translation system <CIT> is likely due in part to the widespread availability of NLP tools for the language that is most frequently the target : English '
'However more recent results have shown that it can indeed improve parser performance <CIT> '
'4 Experiments We evaluated our classifier-based best-first parser on the Wall Street Journal corpus of the Penn Treebank <CIT> using the standard split : sections 2-21 were used for training , section 22 was used for development and tuning of parameters and features , and section 23 was used for testing '
'For example , the constrained optimization method of <OTH> relies on approximations of sensitivity -LRB- which they call CA -RRB- and specificity2 -LRB- their CR -RRB- ; related techniques <CIT> rely on approximations of true positives , false positives , and false negatives , and , indirectly , recall and precision '
'1 Introduction Aligning parallel texts has recently received considerable attention <CIT> '
'One aspect of VPCs that makes them dicult to extract -LRB- cited in , eg , <CIT> -RRB- is that the verb and particle can be non-contiguous , eg hand the paper in and battle right on '
'the syntax-based system , we ran a reimplementation of the Collins parser <CIT> on the English half of the bitext to produce parse trees , then restructured and relabeled them as described in Section 32 '
'23 Online Learning Again following <OTH> , we have used the single best MIRA <OTH> , which is a margin aware variant of perceptron <CIT> for structured prediction '
'For each span in the chart , we get a weight factor that is multiplied with the parameter-based expectations9 4 Experiments We applied GIZA + + <OTH> to word-align parts of the Europarl corpus <CIT> for English and all other 10 languages '
'2 The WFST Reordering Model The Translation Template Model -LRB- TTM -RRB- is a generative model of phrase-based translation <CIT> '
'al 2006 , <CIT> , et al 2007a -RRB- '
'A tree sequence to string rule 174 A tree-sequence to string translation rule in a forest is a triple <L, R, A> , where L is the tree sequence in source language , R is the string containing words and variables in target language , and A is the alignment between the leaf nodes of L and R This definition is similar to that of <CIT> except our treesequence is defined in forest '
'The weights of these models are determined using the max-BLEU method described in <CIT> '
'We adopted the stop condition suggested in <CIT> the maximization of the likelihood on a cross-validation set of samples which is unseen at the parameter estimation '
'Chinese word segmentation is a well-known problem that has been studied extensively <OTH> and it is known that human agreement is relatively low '
'565 es <CIT> '
'Yarowsky <CIT> proposed an unsupervised method that used heuristics to obtain seed classifications and expanded the results to the other parts of the corpus , thus avoided the need to hand-annotate any examples '
'Work focusses on analyzing subjective features of text or speech , such as sentiment , opinion , emotion or point of view <CIT> '
'There are also attempts at a more fine-grained analysis of accuracy , targeting specific linguistic constructions or grammatical functions <CIT> '
'We train our feature weights using max-BLEU <CIT> and decode with a CKY-based decoder that supports language model scoring directly integrated into the search '
'In previous work <CIT> , we developed an unsupervised learning algorithm that automatically recognizes definite NPs that are existential without syntactic modification because their meaning is universally understood '
'For nonprojective parsing , the analogy to the inside algorithm is the O -LRB- n3 -RRB- matrix-tree algorithm , which is dominated asymptotically by a matrix determinant <CIT> '
'We compare against several competing systems , the first of which is based on the original IBM Model 4 for machine translation <CIT> and the HMM machine translation alignment model <OTH> as implemented in the GIZA + + package <OTH> '
'Probability estimates of the RHS given the LHS are often smoothed by making a Markov assumption regarding the conditional independence of a category on those more than k categories away <CIT> : P -LRB- X Y1Yn -RRB- = P -LRB- Y1 X -RRB- nY i = 2 P -LRB- Yi X , Y1 Yi1 -RRB- P -LRB- Y1 X -RRB- nY i = 2 P -LRB- Yi X , Yik Yi1 -RRB- '
'Because treebank annotation for individual formalisms is prohibitively expensive , there have been a number of efforts to extract TAGs , LFGs , and , more recently , HPSGs , from the Penn Treebank <CIT> '
'For experiment on English , we used the English Penn Treebank -LRB- PTB -RRB- <CIT> and the constituency structures were converted to dependency trees using the same rules as <OTH> '
'In fact , it has already been established that sentence level classification can improve document level analysis <CIT> '
'In this paper , we investigate the effectiveness of structural correspondence learning -LRB- SCL -RRB- <CIT> in the domain adaptation task given by the CoNLL 2007 '
'Starting from a N-Best list generated from a translation decoder , an optimizer , such as Minimum Error Rate -LRB- MER -RRB- <CIT> training , proposes directions to search for a better weight-vector to combine feature functions '
'It compares favorably 505 with conventional phrase-based translation <CIT> on Chinese-English news translation <OTH> '
'The only trainable approaches -LRB- known to the author -RRB- to surface generation are the purely statistical machine translation -LRB- MT -RRB- systems such as <CIT> and the corpus-based generation system described in <OTH> '
'For our experiments we used the following features , analogous to Pharaohs default feature set : P -LRB- -RRB- and P -LRB- -RRB- , the latter of which is not found in the noisy-channel model , but has been previously found to be a helpful feature <CIT> ; the lexical weights Pw -LRB- -RRB- and Pw -LRB- -RRB- <OTH> , which estimate how well the words in translate the words in ; 2 a phrase penalty exp -LRB- 1 -RRB- , which allows the model to learn a preference for longer or shorter derivations , analogous to Koehns phrase penalty <OTH> '
'In the area of statistical machine translation -LRB- SMT -RRB- , recently a combination of the BLEU evaluation metric <OTH> and the bootstrap method for statistical significance testing <OTH> has become popular <CIT> '
'<CIT> , and in some cases , to factor the translation problem so that the baseline MT system can take advantage of the reduction in sparsity by being able to work on word stems '
'We use a tagger based on Adwait Ratnaparkhi ''s method <CIT> '
'(Turney, 2002; Pang et al., 2002; Dave at al., 2003).'
'<CIT> proposes Inversion Transduction Grammars , treating translation as a process of parallel parsing of the source and target language via a synchronized grammar '
'Some previous approaches <CIT> handle unknown words explicitly using ambiguity class components conditioned on various morphological features , and this has shown to produce good tagging results , especially when dealing with incomplete dictionaries '
'Various corpus-based approaches to word sense disambiguation have been proposed <CIT> '
'5http : \/ \/ opennlpsourceforgenet \/ We use the standard four-reference NIST MTEval data sets for the years 2003 , 2004 and 2005 -LRB- henceforth MT03 , MT04 and MT05 , respectively -RRB- for testing and the 2002 data set for tuning6 BLEU4 <CIT> , METEOR <OTH> and multiple-reference Word Error Rate scores are reported '
'<CIT> uses the observed frequencies within a specific syntactic pattern -LRB- subject\/verb , and verb\/object -RRB- to derive a cooccu , -RRB- rence score which is an estimate of mutual information <OTH> '
'1 Introduction Machine translation systems based on probabilistic translation models <CIT> are generally trained using sentence-aligned parallel corpora '
'To summarize , we can describe our system as follows : it is based on <OTH> s implementation of <CIT> , which has been fed at each iteration by a different dataset consisting of the supervised and unsupervised part : precisely , by a concatenation of the manually tagged training data -LRB- WSJ portion of the PTB 3 for English , morphologically disambiguated data from PDT 20 for Czech -RRB- and a chunk of automatically tagged unsupervised data '
'The time complexity of the CKY-based binarization algorithm is -LRB- n3 -RRB- , which is higher than that of the linear binarization such as the synchronous binarization <CIT> '
'presented in <CIT> '
'Until now , translation models have been evaluated either subjectively <OTH> or using relative metrics , such as perplexity with respect to other models <CIT> '
'For Penn Treebank II style annotation <CIT> , in which a nonterminal symbol is a category together with zero or more functional tags , we adopt the following scheme : the atomic pattern a matches any label with category a or functional tag a ; moreover , we define Boolean operators ^ , _ , and : '
'5 The task : Base NP chunking The task is base NP chunking on section 20 of the Wall Street Journal corpus , using sections 15 to 18 of the corpus as training data as in <CIT> '
'4 Experiments and evaluation We carried out an evaluation on the local rephrasing of French sentences , using English as the pivot language2 We extracted phrase alignments of up to 7 word forms using the Giza + + alignment tool <OTH> and the grow-diag-final-and heuristics described in <CIT> on 948,507 sentences of the French-English part of the Europarl corpus <CIT> and obtained some 42 million phrase pairs for which probabilities were estimated using maximum likelihood estimation '
'This paper explores an alternative approach to parsing , based on the perceptron training algorithm introduced in <CIT> '
'A simpler , related idea of penalizing distortion from some ideal matching pattern can be found in the statistical translation <CIT> and word alignment <OTH> models '
'Examples have been class-based D2-gram models <CIT> , smoothing techniques for structural disambiguation <OTH> and word sense disambiguation <OTH> '
'21 Model 2 of <CIT> Both parsing models discussed in this paper inherit a great deal from this model , so we briefly describe its ` progenitive '' features here , describing only how each of the two models of this paper differ in the subsequent two sections '
'Study in collocation extraction using lexical statistics has gained some insights to the issues faced in collocation extraction <CIT> '
'1 Introduction Several recent syntax-based models for machine translation <CIT> can be seen as instances of the general framework of synchronous grammars and tree transducers '
'Solving this first methodological issue , has led to solutions dubbed hereafter as unlexicalized statistical parsing <CIT> '
'1 Introduction Translational equivalence is a mathematical relation that holds between linguistic expressions with the same meaning <CIT> '
'Most existing work to capture labelconsistency , has attempted to create all parenleftbign2parenrightbig pairwise dependencies between the different occurrences of an entity , <CIT> , where n is the number of occurrences of the given entity '
'While the research in statistical machine translation -LRB- SMT -RRB- has made significant progress , most SMT systems <CIT> relyonparallel corpora toextract translation entries '
'In our context , bootstrapping has a similar motivation to the annealing approach of Smith and Eisner <OTH> , which also tries to alter the space of hidden outputs in the E-step over time to facilitate learning in the M-step , though of course the use of bootstrapping in general is quite widespread <CIT> '
'Training Data Our source for syntactically annotated training data was the Penn Treebank <CIT> '
'To deal with the difficulties in parse-to-parse matching , <CIT> utilizes inversion transduction grammar -LRB- ITG -RRB- for bilingual parsing '
'177 Proceedings of EACL ''99 IOB1 IOB2 IOE1 IOE2 [+] [+ IO IO +] (Ramshaw and Marcus, 1995) (Veenstra, 1998) (Argamon et al. , 1998) (Cardie and Pierce, 1998) accuracy 97.58% 96.50% 97.58% 96.77% 97.37% 97.2% precision 92.50% 91.24% 92.41% 91.93% 93.66% 91.47% 91.25% 91.80% 89.0% 91.6 % 90.7% recall F~=I 92.25% 92.37 92.32% 91.78 92.04% 92.23 92.46% 92.20 90.81% 92.22 92.61% 92.04 92.54% 91.89 92.27% 92.03 94.3% 91.6 91.6% 91.6 91.1% 90.9 Table 6: The F~=I scores for the (Ramshaw and Marcus, 1995) test set after training with their training data set.'
'The bracketed portions of Figure 1 , for example , show the base NPs in one sentence from the Penn Treebank Wall Street Journal -LRB- WSJ -RRB- corpus <CIT> '
'Vilain and Day <OTH> identify -LRB- and classify -RRB- name phrases such as company names , locations , etc <CIT> detect noun phrases , by classifying each word as being inside a phrase , outside or on the boundary between phrases '
'(Koehn et al. , 2003) used the following distortion model, which simply penalizes nonmonotonic phrase alignments based on the word distance of successively translated source phrases with an appropriate value for the parameter a71, a36a51a4a39a38 a33 a40a52a42 a33a53a45 a32 a8 a10 a71a26a72a73a25a74 a45a62a75 a74a77a76a24a78 a45 a32 a72 (3) a79a17a80a82a81a84a83a85a15a86a88a87a70a89a91a90 languageis a means communication of MG RA RA b1 b2 b3 b4 Figure 1: Phrase alignment and reordering bi-1 bi fi-1 fi ei-1 ei bi-1 bi fi-1 fi ei-1 ei bi-1 bi fi-1 fi ei-1 ei bi-1 bi fi-1 fi ei-1 ei source target target source target target source source d=MA d=MG d=RA d=RG Figure 2: Four types of reordering patterns 3 The Global Phrase Reordering Model Figure 1 shows an example of Japanese-English phrase alignment that consists of four phrase pairs.'
'We show that link 1For a complete discussion of alignment symmetrization heuristics , including union , intersection , and refined , refer to <CIT> '
'Specifically , stochastic translation lexicons estimated using the IBM method <CIT> from a fairly large sentence-aligned Chinese-English parallel corpus are used in their approach a considerable demand for a resourcedeficient language '
'1 Introduction The field of sentiment classification has received considerable attention from researchers in recent years <CIT> '
'In paraphrase generation , a text unit that matches a pattern P can be rewritten using the paraphrase patterns of P Avarietyofmethodshavebeenproposedonparaphrase patterns extraction <CIT> '
'Chunks as a separate level have also been used in Collins <OTH> and <CIT> '
'We rst recast the problem of estimating the IBM models <CIT> in a discriminative framework , which leads to an initial increase in word-alignment accuracy '
'In practice , when training the parameters of an SMT system , for example using the discriminative methods of <CIT> , the cost for skips of this kind is typically set to a very high value '
'Section 4 compares our results to Itindle ''s ones <CIT> '
'Without removing them , extracted rules can not be triggered until when completely the same strings appear in a text4 6 Performance Evaluation We measured the performance of our robust parsing algorithm by measuring coverage and degree of overgeneration for the Wall Street Journal in the Penn Treebank <CIT> '
'4 Features Features used in our experiments are inspired by previous work on corpus-based approaches for discourse analysis <CIT> '
'Snow etal <CIT> use known hypernym\/hyponym pairs to generate training data for a machine-learning system , which then learns many lexico-syntactic patterns '
'CIT -RRB- '
'We run the decoder with its default settings -LRB- maximum phrase length 7 -RRB- and then use Koehn ''s implementation of minimum error rate training <CIT> to tune the feature weights on the de2 The full name of HTRDP is National High Technology Research and Development Program of China , also named as 863 Program '
'<CIT> represent chunking as tagging problem and the CoNLL2000 shared task <OTH> is now the standard evaluation task for chunking English '
'h1 -LRB- eI1 , fJ1 -RRB- = log Kproductdisplay k = 1 N -LRB- z -RRB- -LRB- T -LRB- z -RRB- , Tk -RRB- N -LRB- T -LRB- z -RRB- -RRB- h2 -LRB- eI1 , fJ1 -RRB- = log Kproductdisplay k = 1 N -LRB- z -RRB- -LRB- T -LRB- z -RRB- , Tk -RRB- N -LRB- S -LRB- z -RRB- -RRB- h3 -LRB- eI1 , fJ1 -RRB- = log Kproductdisplay k = 1 lex -LRB- T -LRB- z -RRB- S -LRB- z -RRB- -RRB- -LRB- T -LRB- z -RRB- , Tk -RRB- h4 -LRB- eI1 , fJ1 -RRB- = log Kproductdisplay k = 1 lex -LRB- S -LRB- z -RRB- T -LRB- z -RRB- -RRB- -LRB- T -LRB- z -RRB- , Tk -RRB- h5 -LRB- eI1 , fJ1 -RRB- = K h6 -LRB- eI1 , fJ1 -RRB- = log Iproductdisplay i = 1 p -LRB- ei ei2 , ei1 -RRB- h7 -LRB- eI1 , fJ1 -RRB- = I 4When computing lexical weighting features <CIT> , we take only terminals into account '
'Our results agree , at least at the level of morphology , with <CIT> '
'Methods that use bigrams <CIT> or trigrams <OTH> cluster words considering as a word ''s context the one or two immediately adjacent words and employ as clustering criteria the minimal loss of average 836 nmtual information and the perplexity improvement respectively '
'We also experimented with a method suggested by Brent <OTH> which applies the binomial test on frame frequency data '
'85 Recently some alignment evaluation metrics have been proposed which are more informative when the alignments are used to extract translation units <CIT> '
'However , the achieved accuracy was not better than that of related work <CIT> based on CRFs '
'Syntax-based MT approaches began with <CIT> , who introduced the Inversion Transduction Grammars '
'For instance , the to-PP frame is poorly '' represented in the syntactically annotated version of the Penn Treebank <CIT> '
'<CIT> , or -LRB- S+T - -RRB- , where no labeled target domain data is available , eg '
'Using a vector-based topic identification process <CIT> , these keywords are used to determine a set of likely values -LRB- including null -RRB- for that attribute '
'For example , <CIT> suggested two different methods : using only the alignment with the maximum probability , the so-called Viterbi alignment , or generating a set of alignments by starting from the Viterbi alignment and making changes , which keep the alignment probability high '
'Success is indicated by the proportion of the original sentence regenerated , as measured by any string comparison method : in our case , using the BLEU metric <CIT> '
'<OTH> 8910 8914 8912 kitchen sink 8926 8955 8940 parser <OTH> 8 , the only one that we were able to train and test under exactly the same experimental conditions -LRB- including the use of POS tags from the tagger of <CIT> -RRB- '
'a larger number of labeled documents , its performance on this corpus is comparable to that of Support Vector Machines and Maximum Entropy models <CIT> '
'In the original work (Brown et al. , 1993) the posterior probability p(eI1|fJ1 ) is decomposed following a noisy-channel approach, but current stateof-the-art systems model the translation probability directly using a log-linear model(Och and Ney, 2002): p(eI1|fJ1 ) = exp parenleftBigsummationtextM m=1 mhm(e I1,fJ1 ) parenrightBig summationdisplay ?eI1 exp parenleftBigsummationtextM m=1 mhm(?eI1,fJ1 ) parenrightBig, (2) with hm different models, m scaling factors and the denominator a normalization factor that can be ignored in the maximization process.'
'They propose a two-level hierarchy , with 5 classes at the first level and 30 classes at the second one ; other researchers <CIT> have used their class scheme and data set '
'1 Introduction The last years have seen a boost of work devoted to the development of machine learning based coreference resolution systems <OTH> '
'In natural language processing , label propagation has been used for document classification <OTH> , word sense disambiguation <CIT> , and sentiment categorization <OTH> '
'This conclusion is supported by the fact that true IMT is not , to our knowledge , used in most modern translator ''s support environments , eg <OTH> '
'We trained the parser on the Penn Treebank <CIT> '
'We obtain aligned parallel sentences and the phrase table after the training of Moses , which includes running GIZA + + <CIT> , grow-diagonal-final symmetrization and phrase extraction <OTH> '
'The PT grammar 2 was extracted from the Penn Treebank <CIT> '
'In order to resolve all Chinese NLDs represented in the CTB, we modify and substantially extend the (Cahill et al. , 2004) (henceforth C04 for short) algorithm as follows: Given the set of subcat frames s for the word w, and a set of paths p for the trace t, the algorithm traverses the f-structure f to: predict a dislocated argument t at a sub-fstructure h by comparing the local PRED:w to ws subcat frames s t can be inserted at h if h together with t is complete and coherent relative to subcat frame s traverse f starting from t along the path p link t to its antecedent a if ps ending GF a exists in a sub-f-structure within f; or leave t without an antecedent if an empty path for t exists In the modified algorithm, we condition the probability of NLD path p (including the empty path without an antecedent) on the GF associated of the trace t rather than the antecedent a as in C04.'
'Finally , following <CIT> and Johnson -LRB- 2007 -RRB- we can instead insist that at most one HMM state can be mapped to any part-of-speech tag '
'We measure semantic similarity using the shortest path length in WordNet <OTH> as implemented in the WordNet Similarity package <CIT> '
'After that , several million instances of people , locations , and other facts were added <CIT> '
'While movie reviews have been the most studied domain , sentiment analysis has extended to a number of new domains , ranging from stock message boards to congressional floor debates <CIT> '
'c2009 Association for Computational Linguistics Structural Correspondence Learning for Parse Disambiguation Barbara Plank Alfa-informatica University of Groningen , The Netherlands bplank @ rugnl Abstract The paper presents an application of Structural Correspondence Learning -LRB- SCL -RRB- <CIT> for domain adaptation of a stochastic attribute-value grammar -LRB- SAVG -RRB- '
'We utilize a maximum entropy -LRB- ME -RRB- model <CIT> to design the basic classifier used in active learning for WSD '
'Some work identifies inflammatory texts -LRB- eg , <OTH> -RRB- or classifies reviews as positive or negative -LRB- <CIT> -RRB- '
'Our statistical tagging model is modified from the standard bigrams <CIT> using Viterbi search plus onthe-fly extra computing of lexical probabilities for unknown morphemes '
'In <CIT> , an undirected graphical model is used for parse reranking '
'4 Experiments Our experiments involve data from two treebanks : the Wall Street Journal Penn treebank <CIT> and the Chinese treebank <OTH> '
'12 Decoding in Statistical Machine Translation <CIT> and <OTH> have discussed the first two of the three problems in statistical machine translation '
'eBonsai first performs syntactic analysis of a sentence using a parser based on GLR algorithm -LRB- MSLR parser -RRB- <OTH> , and provides candidates of its syntactic structure '
'It reconfirms that only allowing sibling nodes reordering as done in SCFG may be inadequate for translational equivalence modeling <CIT> 4 3 -RRB- All the three models on the FBIS corpus show much lower performance than that on the other two corpora '
'have been used in statistical machine translation <OTH> , terminology research and translation aids <OTH> , bilingual lexicography <CIT> , word-sense disambiguation <OTH> and information retrieval in a multilingual environment <OTH> '
'44 Text chunking Next , a rule-based text chunker <CIT> is applied on the tagged sentences to further identify phrasal units , such as base noun phrases NP and verbal units VB '
'The unlabeled data for English we use is the union of the Penn Treebank tagged WSJ data <CIT> and the BLLIP corpus5 For the rest of the languages we use only the text of George Orwells novel 1984 , which is provided in morphologically disambiguated form as part of MultextEast -LRB- but we dont use the annotations -RRB- '
'The results are consistent with the idea in <CIT> '
'There have been many studies of zero-pronoun identification <OTH> <OTH> <CIT> '
'recent advances in parsing technology are due to the explicit stochastic modeling of dependency information <CIT> '
'We use a bootstrap approach in which we first extract 1-to-n word alignments using an existing word aligner , and then estimate the confidence of those alignments to decide whether or not the n words have to be grouped ; if so , this group is conwould thus be completely driven by the bilingual alignment process -LRB- see also <CIT> for related considerations -RRB- '
'For example , non-local features such as same phrases in a document do not have different entity classes were shown to be useful in named entity recognition <CIT> '
'IBM Model 4 parameters are then estimated over this partial search space as an approximation to EM <CIT> '
'Early experiments with syntactically-informed phrases <CIT> , and syntactic reranking of K-best lists <OTH> produced mostly negative results '
'This has been shown both in supervised settings <OTH> and unsupervised settings <CIT> in which constraints are used to bootstrap the model '
'EMD training <CIT> combines generative and discriminative elements '
'We tune all feature weights automatically <OTH> to maximize the BLEU <CIT> score on the dev set '
'Another line of research closely related to our work is the recognition of semantic orientation and sentiment analysis <CIT> '
'<OTH> , <CIT> and others '
'Models of translational equivalence that are ignorant of indirect associations have '' a tendency to be confused by collocates '' <OTH> '
'Networks (Toutanova et al., 2003) 97.24 SVM (Gimenez and M`arquez, 2003) 97.05 ME based a bidirectional inference (Tsuruoka and Tsujii, 2005) 97.15 Guided learning for bidirectional sequence classification (Shen et al., 2007) 97.33 AdaBoost.SDF with candidate features (=2,=1,=100, W-dist) 97.32 AdaBoost.SDF with candidate features (=2,=10,=10, F-dist) 97.32 SVM with candidate features (C=0.1, d=2) 97.32 Text Chunking F=1 Regularized Winnow + full parser output (Zhang et al., 2001) 94.17 SVM-voting (Kudo and Matsumoto, 2001) 93.91 ASO + unlabeled data (Ando and Zhang, 2005) 94.39 CRF+Reranking(Kudo et al., 2005) 94.12 ME based a bidirectional inference (Tsuruoka and Tsujii, 2005) 93.70 LaSo (Approximate Large Margin Update) (Daume III and Marcu, 2005) 94.4 HySOL (Suzuki et al., 2007) 94.36 AdaBoost.SDF with candidate featuers (=2,=1,=, W-dist) 94.32 AdaBoost.SDF with candidate featuers (=2,=10,=10,W-dist) 94.30 SVM with candidate features (C=1, d=2) 94.31 One of the reasons that boosting-based classifiers realize faster classification speed is sparseness of rules.'
'<CIT> and Quirk et al '
'Clusters are created by means of distributional techniques in <OTH> , while in <OTH> low level synonim sets in WordNet are used '
'22 Generalization pseudocode In order to identify the portions in common between the patterns , and to generalize them , we apply the following pseudocode -LRB- Ruiz-Casado et al , in press -RRB- : 1All the PoS examples in this paper are done with Penn Treebank labels <CIT> '
'Kappa coefficient is given in -LRB- 1 -RRB- <CIT> -LRB- 1 -RRB- -RRB- -LRB- 1 -RRB- -LRB- -RRB- -LRB- EP EPAP Kappa = where P -LRB- A -RRB- is the proportion of times the annotators actually agree and P -LRB- E -RRB- is the proportion of times the annotators are expected to agree due to chance 3 '
'On the base of the chunk scheme proposed by Abney <OTH> and the BIO tagging system proposed in <CIT> , many machine learning techniques are used to deal with the problem '
'It has a lower bound of 0 , no upper bound , better scores indicate better translations , and it tends to be highly correlated with the adequacy of outputs ; mWER <CIT> or Multiple Word Error Rate is the edit distance in words between the system output and the closest reference translation in a set '
'A new automatic metric METEOR <CIT> uses stems and synonyms of the words '
'We compare the following model types : conventional -LRB- ie , non-exponential -RRB- word n-gram models ; conventional IBM class n-gram models interpolated with conventional word n-gram models <CIT> ; and model M All conventional n-gram models are smoothed with modified Kneser-Ney smoothing <OTH> , except we also evaluate word n-gram models with Katz smoothing <OTH> '
'6 Comparison With Previous Work The two parsers which have previously reported the best accuracies on the Penn Treebank Wall St Journal are the bigram parser described in <OTH> and the SPATTER parser described in <OTH> '
'P -LRB- d -RRB- P L -LRB- d -RRB- -LRB- 4 -RRB- Statistical approaches to language modeling have been used in much NLP research , such as machine translation <CIT> and speech recognition <OTH> '
'We computed precision , recall and error rate on the entire set for each data set6 For an initial alignment , we used GIZA + + in both directions -LRB- E-to-F and F-to-E , where F is either Chinese -LRB- C -RRB- or Spanish -LRB- S -RRB- -RRB- , and also two different combined alignments : intersection of E-to-F and F-to-E ; and RA using a heuristic combination approach called grow-diag-final <CIT> '
'In the following sections , we will use 2 statistics to measure the the mutual translation likelihood <CIT> '
'Oncetraininghastakenplace , minimumerrorrate training <CIT> is used to tune the parameters i Finally , decoding in Hiero takes place using a CKY synchronous parser with beam search , augmented to permit efficient incorporation of language model scores <OTH> '
'Decomposing the translational equivalence relations in the training data into smaller units of knowledge can improve a models ability to generalize <CIT> '
'Narrative retellings provide a natural , conversational speech sample that can be analyzed for many of the characteristics of speech and language that have been shown to discriminate between healthy and impaired subjects , including syntactic complexity <OTH> and mean pause duration <OTH> '
'291 31 Level of Analysis Research on sentiment annotation is usually conducted at the text <CIT> or at the sentence levels <OTH> '
'For testing purposes , we used the Wall Street Journal part of the Penn Treebank corpus <CIT> '
'2 Task Description 21 Data Representation <CIT> gave mainly two kinds of base NPs representation the open\/close bracketing and IOB tagging '
'26 Tuning procedure The Moses-based systems were tuned using the implementation of minimum error rate training -LRB- MERT -RRB- <CIT> distributed with the Moses decoder , using the development corpus -LRB- dev2009a -RRB- '
'In many cases , improving semi-supervised models was done by seeding these models with domain information taken from dictionaries or ontology <CIT> '
'This is applied to maximize coverage , which is similar as the final in <CIT> '
'(Lin, 2004b).'
'Trained and tested using the same technique as <CIT> '
'By introducing the hidden word alignment variable a, the following approximate optimization criterion can be applied for that purpose: e = argmaxe Pr(e | f) = argmaxe summationdisplay a Pr(e,a | f)  argmaxe,a Pr(e,a | f) Exploiting the maximum entropy (Berger et al. , 1996) framework, the conditional distribution Pr(e,a | f) can be determined through suitable real valued functions (called features) hr(e,f,a),r = 1R, and takes the parametric form: p(e,a | f)  exp Rsummationdisplay r=1 rhr(e,f,a)} The ITC-irst system (Chen et al. , 2005) is based on a log-linear model which extends the original IBM Model 4 (Brown et al. , 1993) to phrases (Koehn et al. , 2003; Federico and Bertoldi, 2005).'
'The phrase bilexicon is derived from the intersection of bidirectional IBM Model 4 alignments , obtained with GIZA + + <CIT> , augmented to improve recall using the grow-diag-final heuristic '
'Work at the University of Dundee <OTH> has shown that the extensive use of fixed text for sequences such as greetings and prestored narratives is beneficial in AAC '
'For many languages , large-scale syntactically annotated corpora have been built -LRB- eg the Penn Treebank <CIT> -RRB- , and many parsing algorithms using CFGs have been proposed '
'More specifically , the work on optimizing preference factors and semantic collocations was done as part of a project on spoken language translation in which the CLE was used for analysis and generation of both English and Swedish <OTH> '
'The data for all our experiments was extracted from the Penn Treebank II Wall Street Journal -LRB- WSJ -RRB- corpus <CIT> '
'The largest corpus that Goldwater and Griffiths <OTH> studied contained 96,000 words , while <CIT> <OTH> used all of the 1,173,766 words in the full Penn WSJ treebank '
'There are several works that try to learn paraphrase pairs from parallel or comparable corpora <CIT> '
'Examples of such knowledge sources include stemming and TF-IDF weighting <CIT> '
'1 Introduction Text chunking has been one of the most interesting problems in natural language learning community since the first work of <CIT> using a machine learning method '
'A few unsupervised metrics have been applied to automatic paraphrase identification and extraction <CIT> '
'<CIT> showed that the learning strategy of bootstrapping from small tagged data led to results rivaling supervised training methods '
'Therefore , whenever we have access to a large amount of labeled data from some source -LRB- out-of-domain -RRB- , but we would like a model that performs well on some new target domain <CIT> , we face the problem of domain adaptation '
'In the LFG-based generation algorithm presented by <CIT> complex named entities -LRB- ie those consisting of more than one word token -RRB- and other multi-word units can be fragmented in the surface realization '
'All submitted runs were evaluated with the automatic metrics : ROUGE <CIT> , which calculates the proportion of n-grams shared between the candidate summary and the reference summaries , and Basic Elements <OTH> , which compares the candidate to the models in terms of head-modifier pairs '
'2 Latent Variable Parsing In latent variable parsing <CIT> , we learn rule probabilities on latent annotations that , when marginalized out , maximize the likelihood of the unannotated training trees '
'Table 3 shows the differences between the treebank ~ utilized in <OTH> on the one hand , and in the work reported here , on the other , is Table 4 shows relevant lSFigures for Average Sentence Length -LRB- ` l ~ raLuing Corpus -RRB- and Training Set Size , for the IBM ManuaLs Corpus , are approximate , and cz ~ e fzom <OTH> '
'We removed all but the first two characters of each POS tag , resulting in a set of 57 tags which more closely resembles that of the Penn TreeBank <CIT> '
'More specialized methods also exist , for example for support vector machines <OTH> and for conditional random fields <CIT> '
'<CIT> used bootstrapping to extend their semantic compatibility model , which they called contextual-role knowledge , by identifying certain cases of easily-resolved anaphors and antecedents '
'For comparison purposes , we consider two different algorithms for our AnswerExtraction module : one that does not bridge the lexical chasm , based on N-gram cooccurrences between the question terms and the answer terms ; and one that attempts to bridge the lexical chasm using Statistical Machine Translation inspired techniques <CIT> in order to find the best answer for a given question '
'a1 Graduated in March 2006 Standard phrase-based translation systems use a word distance-based reordering model in which non-monotonic phrase alignment is penalized based on the word distance between successively translated source phrases without considering the orientation of the phrase alignment or the identities of the source and target phrases <CIT> '
'For the simple bag-of-word bilingual LSA as describedinSection221 , afterSVDonthesparsematrix using the toolkit SVDPACK <OTH> , all source and target words are projected into a lowdimensional -LRB- R = 88 -RRB- LSA-space '
'Sentiment analysis includes a variety of different problems, including: sentiment classification techniques to classify reviews as positive or negative, based on bag of words (Pang et al. , 2002) or positive and negative words (Turney, 2002; Mullen and Collier, 2004); classifying sentences in a document as either subjective or objective (Riloff and Wiebe, 2003; Pang and Lee, 2004); identifying or classifying appraisal targets (Nigam and Hurst, 2004); identifying the source of an opinion in a text (Choi et al. , 2005), whether the author is expressing the opinion, or whether he is attributing the opinion to someone else; and developing interactive and visual opinion mining methods (Gamon et al. , 2005; Popescu and Etzioni, 2005).'
'MT output is evaluated using the standard MT evaluation metric BLEU <CIT> '
'Word association norms based on co-occurrence information have been proposed by <CIT> '
'Some examples of language reuse include collocation analysis <CIT> , the use of entire factual sentences extracted from corpora -LRB- eg , '' ` Toy Story '' is the Academy Award winning animated film developed by Pixar ~ '' -RRB- , and summarization using sentence extraction <OTH> '
'Since a handmade thesaurus is not slfitahle for machine use , and expensive to compile , automatical construction of ~ a thesaurus has been attempted using corpora <CIT> '
'4 Analysis of Experimental Data Most of the existing research in computational linguistics that uses human annotators is within the framework of classification , where an annotator decides , for every test item , on an appropriate tag out of the pre-specified set of tags <CIT> '
'Following Hatzivassiloglou and McKeown <OTH> and <CIT> , we decided to observe how often the words from the headline co-occur with each one of the six emotions '
'<CIT> calls this trade-off specificity ; equivalent observations were made by Church & Hanks -LRB- 1989 -RRB- and Church et al <OTH> , who refer to the tendency for large windows to wash out , smear or defocus those associations exhibited at smaller scales '
'Morphologicaltoolssuch as lemmatizers andPOStaggersarebeingcommonlyusedin extractionsystems ; they areemployedbothfordealingwithtext variationandfor validatingthe candidatepairs : combinationsof functionwordsare typicallyruledout <OTH> , as are the ungrammaticalcombinationsin the systemsthatmake useofparsers -LRB- ChurchandHanks , 1990 ; <CIT> ,1993 ; Basilietal '
'<CIT> report an accuracy of 9733 % on the same data set using a perceptron-based bidirectional tagging model '
'They are generated from the training corpus via the ? diag-and ? ? method <CIT> and smoothed using Kneser-Ney smoothing <OTH> , ? ? one or several n-gram language model -LRB- s -RRB- trained with the SRILM toolkit <OTH> ; in the baseline experiments reported here , we used a trigram model , ? ? a distortion model which assigns a penalty based on the number of source words which are skipped when generating a new target phrase , ? ? a word penalty '
'This is the strategy that is usually adopted in other phrase-based MT approaches <CIT> '
'For example , <CIT> used an English-Chinese bilingual parser based on stochastic transduction grammars to identify terms , including multiword expressions '
'<OTH> and Magerman <OTH> used the clustering algorithm of <CIT> et al '
'According to the Bayes Rule , the problem is transformed into the noisy channel model paradigm , where the translation is the maximum a posteriori solution of a distribution for a channel target text given a channel source text and a prior distribution for the channel source text <CIT> '
'Sometimes , due to data sparseness and\/or limitations in the machine learning paradigm used , we need to extract features from the available representation in a manner that profoundly changes the representation -LRB- as is done in bilexical parsing <CIT> -RRB- '
'Accurate automatic analysis of these aspects of language will augment existing research in the fields of sentiment <CIT> andsubjectivityanalysis -LRB- Wiebeetal '
'In order to extract the linguistic features necessary for the models , all sentences containing the target word were automatically part-of-speech-tagged using a maximum entropy tagger <OTH> and parsed using the Collins parser <CIT> '
'1999 -RRB- , OpenCCG <OTH> and XLE <OTH> , or created semi-automatically <OTH> , or fully automatically extracted from annotated corpora , like the HPSG <OTH> , LFG <OTH> and CCG <OTH> resources derived from the Penn-II Treebank -LRB- PTB -RRB- <CIT> '
'The likelihood ratio is obtained by treating word and Ic as a bigram and computed with the formula in <CIT> '
'The text was split at the sentence level , tokenized and PoS tagged , in the style of the Wall Street Journal Penn TreeBank <CIT> '
'The difference in accuracy between a SVM model applied to RRR dataset (RRR-basic experiment) and the same experiment applied to TB2 dataset (TB2278 Description Accuracy Data Extra Supervision Always noun 55.0 RRR Most likely for each P 72.19 RRR Most likely for each P 72.30 TB2 Most likely for each P 81.73 FN Average human, headwords (Ratnaparkhi et al. , 1994) 88.2 RRR Average human, whole sentence (Ratnaparkhi et al. , 1994) 93.2 RRR Maximum Likelihood-based (Hindle and Rooth, 1993) 79.7 AP Maximum entropy, words (Ratnaparkhi et al. , 1994) 77.7 RRR Maximum entropy, words & classes (Ratnaparkhi et al. , 1994) 81.6 RRR Decision trees (Ratnaparkhi et al. , 1994) 77.7 RRR Transformation-Based Learning (Brill and Resnik, 1994) 81.8 WordNet Maximum-Likelihood based (Collins and Brooks, 1995) 84.5 RRR Maximum-Likelihood based (Collins and Brooks, 1995) 86.1 TB2 Decision trees & WSD (Stetina and Nagao, 1997) 88.1 RRR WordNet Memory-based Learning (Zavrel et al. , 1997) 84.4 RRR LexSpace Maximum entropy, unsupervised (Ratnaparkhi, 1998) 81.9 Maximum entropy, supervised (Ratnaparkhi, 1998) 83.7 RRR Neural Nets (Alegre et al. , 1999) 86.0 RRR WordNet Boosting (Abney et al. , 1999) 84.4 RRR Semi-probabilistic (Pantel and Lin, 2000) 84.31 RRR Maximum entropy, ensemble (McLauchlan, 2001) 85.5 RRR LSA SVM (Vanschoenwinkel and Manderick, 2003) 84.8 RRR Nearest-neighbor (Zhao and Lin, 2004) 86.5 RRR DWS FN dataset, w/o semantic features (FN-best-no-sem) 91.79 FN PR-WWW FN dataset, w/ semantic features (FN-best-sem) 92.85 FN PR-WWW TB2 dataset, best feature set (TB2-best) 93.62 TB2 PR-WWW Table 5: Accuracy of PP-attachment ambiguity resolution (our results in bold) basic experiment) is 2.9%.'
'<CIT> has proposed automatically augmenting a small set of experimenter-supplied seed collocations -LRB- eg , manufacturing plant and plant life for two different senses of the noun plant -RRB- into a much larger set of training materials '
'We evaluate accuracy performance using two automatic metrics : an identity metric , ID , which measures the percent of sentences recreated exactly , and BLEU <CIT> , which gives the geometric average of the number of uni - , bi - , tri - , and four-grams recreated exactly '
'As shown by <CIT> , the Single Malt parser tends to suffer from two problems : error propagation due to the deterministic parsing strategy , typicallyaffectinglongdependenciesmorethan short ones , and low precision on dependencies originating in the artificial root node due to fragmented parses9 The question is which of these problems is alleviatedbythemultipleviewsgivenbythecomponent parsers in the Blended system '
'The definitions of the phrase and lexical translation probabilities are as follows <CIT> '
'The Pearson correlation is calculated over these ten pairs <CIT> '
'They mention that the resulting shallow parse tags are somewhat different than those used by <CIT> , but that they found no significant accuracy differences in training on either set '
'Firstly , rather than induce millions of xRS rules from parallel data , we extract phrase pairs in the standard way <CIT> and associate with each phrase-pair a set of target language syntactic structures based on supertag sequences '
'Previous publications on Meteor <CIT> have described the details underlying the metric and have extensively compared its performance with Bleu and several other MT evaluation metrics '
'The Gaussian prior -LRB- ie , the P k a 2 k = 7 2 k penalty -RRB- has been found in practice to be very effective in combating overfitting of the parameters to the training data <OTH> '
'Finally , the parameters i of the log-linear model -LRB- 18 -RRB- are learned by minimumerror-rate training <OTH> , which tries to set the parameters so as to maximize the BLEU score <CIT> of a development set '
'43 Using Unlabeled Data for Parsing Recent studies on parsing indicate that the use of unlabeled data by self-training can help parsing on the WSJ data , even when labeled data is relatively large <CIT> '
'unlabeled R 100 % 20\/08\/199605 \/ 08\/1997 -LRB- 351 days -RRB- 50 % 20\/08\/199617 \/ 02\/1997 -LRB- 182 days -RRB- 10 % 20\/08\/199624 \/ 09\/1996 -LRB- 36 days -RRB- labeled WSJ 50 % sections 0012 <OTH> 25 % lines 1 292960 <OTH> 5 % lines 1 58284 <OTH> 1 % lines 1 11720 -LRB- 500 sentences -RRB- 005 % lines 1 611 -LRB- 23 sentences -RRB- Table 1 : Corpora used for the experiments : unlabeled Reuters -LRB- R -RRB- corpus for attachment statistics , labeled Penn treebank -LRB- WSJ -RRB- for training the <CIT> parser '
'The future score is based on the source-language words that are still to be translatedthis can be directly inferred from the items bit-stringthis is similar to the use of future scores in Pharoah <CIT> , and in fact we use Pharoahs future scores in our model '
'For comparison purposes , we also computed the value of R 2 for fluency using the BLEU score formula given in <CIT> , for the 7 systems using the same one reference , and we obtained a similar value , 7852 % ; computing the value of R 2 for fluency using the BLEU scores computed with all 4 references available yielded a lower value for R 2 , 6496 % , although BLEU scores obtained with multiple references are usually considered more reliable '
'The current version of the dataset gives semantic tags for the same sentencesas inthe PennTreebank <CIT> , whichareexcerptsfromtheWallStreetJournal '
'This probability is computed using IBMs Model 1 <OTH> : P -LRB- Q A -RRB- = productdisplay qQ P -LRB- q A -RRB- -LRB- 3 -RRB- P -LRB- q A -RRB- = -LRB- 1 -RRB- Pml -LRB- q A -RRB- + Pml -LRB- q C -RRB- -LRB- 4 -RRB- Pml -LRB- q A -RRB- = summationdisplay aA -LRB- T -LRB- q a -RRB- Pml -LRB- a A -RRB- -RRB- -LRB- 5 -RRB- where the probability that the question term q is generated from answer A , P -LRB- q A -RRB- , is smoothed using the prior probability that the term q is generated from the entire collection of answers C , Pml -LRB- q C -RRB- '
'Most SMT models <CIT> try to model word-to-word corresl -RRB- ondences between source and target words using an alignment nmpl -RRB- ing from source l -RRB- osition j to target position i = aj '
'One possible use for this technique is for parser adaptation initially training the parser on one type of data for which hand-labeled trees are available -LRB- eg , Wall Street Journal <CIT> -RRB- and then self-training on a second type of data in order to adapt the parser to the second domain '
'As the most concise definition we take the first sentence of each article , following <CIT> '
'It is true that various term extraction systems have been developed , such as Xtract <CIT> , Termight <OTH> , and TERMS <OTH> among others -LRB- cf '
'<CIT> show that Bloomier filters <OTH> can be used to create perfect hash functions for language models '
'<OTH> and <CIT> -RRB- , we generate training instances as follows : a positive instance is created for each anaphoric NP , NPj , and its closest antecedent , NPi ; and a negative instance is created for NPj paired with each of the intervening NPs , NPi +1 , NPi +2 , , NPj1 '
'<CIT> also work with arguments '
'the Wall Street Journal -LRB- WSJ -RRB- sections of the Penn Treebank <CIT> as training set , tests on BROWN Sections typically result in a 6-8 % drop in labeled attachment scores , although the average sentence length is much shorter in BROWN than that in WSJ '
'On the positive side , recent work exploring the automaticbinarizationofsynchronousgrammars <CIT> has indicated that non-binarizable constructions seem to be relatively rare in practice '
'In contrast , generative models are trained to maximize the joint probability of the training data , which is 1 <CIT> used transformation-based learning <OTH> , which for the present purposes can be tought of as a classi cation-based method '
'We then built separate English-to-Spanish and Spanish-to-English directed word alignments using IBM model 4 <CIT> , combined them using the intersect + grow heuristic <OTH> , and extracted phrase-level translation pairs of maximum length 7 using the alignment template approach <OTH> '
'The second baseline is our implementation of the relevant part of the Wikipedia extraction in <CIT> , taking the first noun after a be verb in the definition sentence , denoted as WikiBL '
'Most previous work on compositionality of MWEs either treat them as collocations <CIT> , or examine the distributional similarity between the expression and its constituents <OTH> '
'In <CIT> , a small set of sample results are presented '
'It extracts all consistent phrase pairs from word-aligned bitext <CIT> '
'Automatic segmentation of spontaneous speech is an open research problem in its own right <OTH> '
'42 Classifier and Features For our AL framework we decided to employ a Maximum Entropy -LRB- ME -RRB- classifier <CIT> '
'The parameters of the models are estimated by iterative maximum-likelihood training on a large parallel corpus of natural language texts using the EM algorithm <CIT> '
'In particular , we adopt the approach of phrase-based statistical machine translation <CIT> '
'For instance , in <CIT> , yd would be the polarity of the document and ysi would indicate whether sentence si is subjective or objective '
'73 EM algorithm The only other application of the EM algorithm to word-sense disambiguation is described in <CIT> '
'We expect that the mean field approximation should demonstrate better results than feed-forward approximation on this task as it is theoretically expected and confirmed on the constituent parsing task <CIT> '
'The results of the comparison with ROUGE-N <CIT> , ROUGE-S -LRB- U -RRB- <CIT> and ROUGE-L <CIT> show that our method correlates more closely with human evaluations and is more robust '
'To obtain these distances , Ratnaparkhis partof-speech -LRB- POS -RRB- tagger <CIT> and Collins parser <OTH> were used to obtain parse trees for the English side of the test corpus '
'Model performance is evaluated using the standard BLEU metric <CIT> which measures average n-gram precision , n 4 , and we use the NIST definition of the brevity penalty for multiple reference test sets '
'Bilexical CFG is at the heart of most modern statistical parsers <CIT> , because the statistics associated with word-specific rules are more informative for disambiguation purposes '
'<CIT> describes a ` semi-unsupervised '' approach to the problem of sense disambiguation of words , also using a set of initial seeds , in this case a few high quality sense annotations '
'CIT -RRB- '
'It is analogous to the step in other translation model induction algorithms that sets all probabilities below a certain threshold to negligible values <CIT> '
'The Brill tagger comes with an English default version also trained on general-purpose language corpora like the PENN TREEBANK <CIT> '
'As to analysis of NPs , there have been a lot of work on statistical techniques for lexical dependency parsing of sentences <CIT> , and these techniques potentially can be used for analysis of NPs if appropriate resources for NPs are available '
'His results may be improved if more sophisticated techniques and larger corpora are used to establish similarity between words -LRB- such as in <CIT> -RRB- '
'RIDF is like MI , but different References <CIT> Word association norms , mutual information , and lexicography Computational Linguistics , 16:1 , pp '
'2We use a POS tagger <CIT> trained on switchboard data with the additional tags of FP -LRB- filled pause -RRB- and FRAG -LRB- word fragment -RRB- '
'Fortunately , there is a straightforward parallel between our object recognition formulation and the statistical machine translation problem of building a lexicon from an aligned bitext <CIT> '
'The kappa <CIT> obtained on this feature was 093 '
'For example , the Penn Treebank <CIT> was annotated with skeletal syntactic structure , and many syntactic parsers were evaluated and compared on the corpus '
'By contrast , <CIT> present three metrics that use syntactic and unlabelled dependency information '
'Some of them are based upon syntactic structure , with PropBank <OTH> being one of the most relevant , building the annotation upon the syntactic representation of the TreeBank corpus <CIT> '
'The results were evaluated using the character\/pinyin-based 4-gram BLEU score <CIT> , word error rate -LRB- WER -RRB- , position independent word error rate -LRB- PER -RRB- , and exact match -LRB- EMatch -RRB- '
'4 are equivalent to a maximum entropy variant of the phrase sense disambiguation approach studied by <CIT> '
'<OTH> propose a tree sequence-based tree to tree translation model and <CIT> et al '
'Class-based n-gram models have also been shown to benefit from their reduced number of parameters when scaling to higher-order n-grams <OTH> , and even despite the increasing size and decreasing sparsity of language model training corpora <CIT> , class-based n-gram models might lead to improvements when increasing the n-gram order '
'Automated evaluation metrics that rate system behavior based on automatically computable properties have been developed in a number of other fields : widely used measures include BLEU <OTH> for machine translation and ROUGE <CIT> for summarisation , for example '
'It can also be considered as an extension from the monolingual to the bilingual case of the well-established methods for semantic or syntactic word clustering as proposed by Schtitze <OTH> , <CIT> , Ruge -LRB- 1995 -RRB- , Rapp -LRB- 1996 -RRB- , Lin -LRB- 1998 -RRB- , and others '
'NeATS computes the likelihood ratio <CIT> to identify key concepts in unigrams , bigrams , and trigrams and clusters these concepts in order to identify major subtopics within the main topic '
'In the last decade or so research on lexical semantics has focused more on sub-problems like word sense disambiguation <CIT> , named entity recognition <OTH> , and vocabulary construction for information extraction <OTH> '
'<OTH> , <CIT> <OTH> , Dave et al '
'We augment each labeled target instance xj with the label assigned by the source domain classifier <CIT> '
'Introduction Translation of two languages with highly different morphological structures as exemplified by Arabic and English poses a challenge to successful implementation of statistical machine translation models <CIT> '
'5We use deterministic sampling , which is useful for reproducibility and for minimum error rate training <CIT> '
'The second model is a maximum entropy model <OTH> , since Klein and Manning <CIT> found that this model yielded higher accuracy than nave Bayes in a subsequent comparison of WSD performance '
'3 Incremental Parsing Method Based on Adjoining Operation In order to avoid the problem of infinite local ambiguity , the previous works have adopted the following approaches : -LRB- 1 -RRB- a beam search strategy <CIT> , -LRB- 2 -RRB- limiting the allowable chains to those actually observed in the treebank <CIT> , and -LRB- 3 -RRB- transforming the parse trees with a selective left-corner transformation <OTH> before inducing the allowable chains and allowable triples <CIT> '
'52 Pseudo-Disambiguation Task Pseudo-disambiguation tasks have become a standard evaluation technique <CIT> and , in the current setting , we may use a nouns neighbors to decide which of two co-occurrences is the most likely '
'Following <CIT> , the prevailing opinion in the research community has been that more complex patterns of word alignment in real bitexts are mostly attributable to alignment errors '
'For this reason , to our knowledge , all discriminative models proposed to date either side-step the problem by choosing simple model and feature structures , such that spurious ambiguity is lessened or removed entirely <OTH> , or else ignore the problem and treat derivations as translations <CIT> '
'Here , we train word alignments in both directions with GIZA + + <CIT> '
'The model employs a stochastic version of an inversion transduction grammar or ITG <CIT> '
'The statistical machine translation approach is based on the noisy channel paradigm and the Maximum-A-Posteriori decoding algorithm <CIT> '
'We evaluate the string chosen by the log-linear model against the original treebank string in terms of exact match and BLEU score (Papineni et al., 821 Syntactic feature Type Definites Definite descriptions SIMPLE DEF simple definite descriptions POSS DEF simple definite descriptions with a possessive determiner (pronoun or possibly genitive name) DEF ATTR ADJ definite descriptions with adjectival modifier DEF GENARG definite descriptions with a genitive argument DEF PPADJ definite descriptions with a PP adjunct DEF RELARG definite descriptions including a relative clause DEF APP definite descriptions including a title or job description as well as a proper name (e.g. an apposition) Names PROPER combinations of position/title and proper name (without article) BARE PROPER bare proper names Demonstrative descriptions SIMPLE DEMON simple demonstrative descriptions MOD DEMON adjectivally modified demonstrative descriptions Pronouns PERS PRON personal pronouns EXPL PRON expletive pronoun REFL PRON reflexive pronoun DEMON PRON demonstrative pronouns (not: determiners) GENERIC PRON generic pronoun (man  one) DA PRON da-pronouns (darauf, daruber, dazu, ) LOC ADV location-referring pronouns TEMP ADV,YEAR Dates and times Indefinites SIMPLE INDEF simple indefinites NEG INDEF negative indefinites INDEF ATTR indefinites with adjectival modifiers INDEF CONTRAST indefinites with contrastive modifiers (einige  some, andere  other, weitere  further, ) INDEF PPADJ indefinites with PP adjuncts INDEF REL indefinites with relative clause adjunct INDEF GEN indefinites with genitive adjuncts INDEF NUM measure/number phrases INDEF QUANT quantified indefinites Table 5: An inventory of interesting syntactic characteristics in IS phrases Label 1 (+ features) Label 2 (+ features) B/A Total D-GIVEN-PRONOUN INDEF-REL 0 19 PERS PRON 39 INDEF ATTR 23 DA PRON 25 SIMPLE INDEF 17 DEMON PRON 19 GENERIC PRON 11 D-GIVEN-PRONOUN D-GIVEN-CATAPHOR 0.1 11 PERS PRON 39 SIMPLE DEF 13 DA PRON 25 DA PRON 10 DEMON PRON 19 GENERIC PRON 11 D-GIVEN-REFLEXIVE NEW 0.11 31 REFL PRON 54 SIMPLE INDEF 113 INDEF ATTR 53 INDEF NUM 32 INDEF PPADJ 26 INDEF GEN 25  Table 6: IS asymmetric pairs augmented with syntactic characteristics 822 2002).'
'The model was trained using minimum error rate training for Arabic <CIT> and MIRA for Chinese <OTH> '
'Given a set of features and a training corpus , the ME estimation process produces a model in which every feature fi has a weight i From <CIT> , we can compute the conditional probability as : p -LRB- o h -RRB- = 1Z -LRB- h -RRB- productdisplay i fi -LRB- h , o -RRB- i -LRB- 2 -RRB- Z -LRB- h -RRB- = summationdisplay o productdisplay i fi -LRB- h , o -RRB- i -LRB- 3 -RRB- The probability is given by multiplying the weights of active features -LRB- ie , those fi -LRB- h , o -RRB- = 1 -RRB- '
'<OTH> and <CIT> with HPSG and LFG grammars '
'Meanwhile , some learning algorithms , like maximum likelihood for conditional log-linear models <OTH> , unsupervised models <OTH> , and models with hidden variables <CIT> , require summing over the scores of many structures to calculate marginals '
'In the past five years , important research on the automatic acquisition of word classes based on lexical distribution has been published <CIT> '
'<CIT> use a pattern-based approach in mining instances of RSRs such as Contrast and Elaboration from large , unannotated corpora '
'These heuristics define a phrase pair to consist of a source and target ngrams of a word-aligned source-target sentence pair such that if one end of an alignment is in the one ngram , the other end is in the other ngram -LRB- and there is at least one such alignment -RRB- <CIT> '
'We employ loglinear models <CIT> for the disambiguation '
'Whereas until recently the focus of research had been on sense disambiguation , papers like Pantel & Lin <OTH> , Neill <OTH> , and <CIT> give evidence that sense induction now also attracts attention '
'(Fleischman et al. , 2003; Jijkoun et al. , 2003).'
'Performance also degrades when the domain of the test set differs from the domain of the training set , in part because the test set includes more OOV words and words that appear only a few times in the training set -LRB- henceforth , rare words -RRB- <CIT> '
'More details about the re-ranking algorithm are presented in <CIT> '
'This is similar to <CIT> s and Charniak97s definition of a separate category for auxiliary verbs '
'The translation models were pharse-based <OTH> created using the GIZA + + toolkit <CIT> '
'12 From Synchronous to Quasi-Synchronous Grammars Because our approach will let anything align to anything , it is reminiscent of IBM Models 15 <CIT> '
'There are cases, though, where the labels consist of several related, but not entirely correlated, properties; examples include mention detectionthe task we are interested in, syntactic parsing with functional tag assignment (besides identifying the syntactic parse, also label the constituent nodes with their functional category, as defined in the Penn Treebank (Marcus et al. , 1993)), and, to a lesser extent, part-of-speech tagging in highly inflected languages.4 The particular type of mention detection that we are examining in this paper follows the ACE general definition: each mention in the text (a reference to a real-world entity) is assigned three types of information:5  An entity type, describing the type of the entity it points to (e.g. person, location, organization, etc)  An entity subtype, further detailing the type (e.g. organizations can be commercial, governmental and non-profit, while locations can be a nation, population center, or an international region)  A mention type, specifying the way the entity is realized  a mention can be named (e.g. John Smith), nominal (e.g. professor), or pronominal (e.g. she).'
'This algorithm can thus be viewed as a large-margin version of the perceptron algorithm for structured outputs <CIT> '
'We -LRB- : an tin -LRB- l 1 ; 11 -LRB- : sam -LRB- ; l ; yl -RRB- olop ; y in other works <OTH> , -LRB- Ca rdi -LRB- : and Pierc <OTH> '
'Self-training is a commonly used technique for semi-supervised learning that has been ap532 plied to several natural language processing tasks <CIT> '
'These instances can be retagged with their countability by using the proposed method and some kind of bootstrapping <CIT> '
'Methods such as <CIT> , <OTH> and <OTH> employ a synchronous parsing procedure to constrain a statistical alignment '
'We compared our system with the concepts in WordNet and Fleischman et als instance\/concept relations <CIT> '
'Relatedness scores are computed for each pair of senses of the grammatically linked pair of words -LRB- w1 ; w2 ; GR -RRB- , using the WordNet-Similarity-103 package and the lesk 759 option <CIT> '
'The release has implementations for BLEU <CIT> , WER and PER error criteria and it has decoding interfaces for Phramer and Pharaoh '
'In particular , we used this method with WordNet <OTH> and using the same training data '
'-LRB- Case-insensitive -RRB- BLEU-4 <CIT> is used as the evaluation metric '
'The corpus is aligned in the word level using IBM Model4 <CIT> '
'Other techniques have tried to quantify the generalizability of certain features across domains <CIT> , or tried to exploit the common structure of related problems <OTH> '
'3 Analysis Results 31 Kappa Statistic Kappa coefficient <CIT> is commonly used as a standard to reflect inter-annotator agreement '
'In this paper , Stanford Named Entity Recognizer <CIT> is used to classify noun phrases into four semantic categories : PERSON , LOCATION , ORGANIZARION and MISC '
'55 Applying F-score Optimization Technique In addition , we can simply apply the F-score optimization technique for the sequence labeling tasks proposed in <CIT> to boost the HySOL performance since the base discriminative models pD -LRB- y x -RRB- and discriminative combination , namely Equation -LRB- 3 -RRB- , in our hybrid model basically uses the same optimization procedure as CRFs '
'32 Classifying speech segments in isolation In our experiments , we employed the well-known classifier SVMlight to obtain individual-document classification scores , treating Y as the positive class and using plain unigrams as features5 Following standard practice in sentiment analysis <CIT> , the input to SVMlight consisted of normalized presence-of-feature -LRB- rather than frequency-of-feature -RRB- vectors '
'2 Word Alignment Framework A statistical translation model <CIT> describes the relationship between a pair of sentences in the source and target languages -LRB- f = fJ1 , e = eI1 -RRB- using a translation probability P -LRB- f e -RRB- '
'Based on these grammars , a great number of SMT models have been recently proposed , including string-to-string model -LRB- Synchronous FSG -RRB- <CIT> , tree-to-string model -LRB- TSG-string -RRB- <OTH> , string-totree model -LRB- string-CFG\/TSG -RRB- <OTH> , tree-to-tree model -LRB- Synchronous CFG\/TSG , Data-Oriented Translation -RRB- <OTH> and so on '
'4 Evaluation As our algorithm works in open domains , we were able to perform a corpus-based evaluation using the Penn WSJ Treebank <CIT> '
'Co-training <CIT> can be informally described in the following manner : # 0F Pick two -LRB- or more -RRB- views of a classification problem '
'Past work has synchronously binarized such rules for efficiency <CIT> '
'The features are similar to the ones used in phrasal systems , and their weights are trained using max-BLEU training <CIT> '
'Thus , <CIT> also proposed an averaged perceptron , where the nal weight vector is 1Collins -LRB- 2002a -RRB- alsoprovidedproofthatguaranteedgood learning for the non-separable case '
'However , as discussed in prior arts <CIT> and this paper , linguistically-informed SCFG is an inadequate model for parallel corpora due to its nature that only allowing child-node reorderings '
'In particular , Hockenmaier and Steedman <OTH> report a generative model for CCG parsing roughly akin to the Collins parser <CIT> specific to CCG '
'This weak supervision has been encoded using priors and initializations <CIT> , specialized models <OTH> , and implicit negative evidence <CIT> '
'The yield of this tree gives the target translation : the gunman was killed by police The Penn English Treebank -LRB- PTB -RRB- <CIT> is our source of syntactic information , largely due to the availability of reliable parsers '
'This is comparable to the accuracy of 9629 % reported by <CIT> on the newswire domain '
'For example , incremental CFG parsing algorithms can be used with the CFGs produced by this transform , as can the Inside-Outside estimation algorithm <OTH> and more exotic methods such as estimating adjoined hidden states <CIT> '
'A few studies <CIT> addressed this defect by selecting the appropriate translation rules for an input span based on its context in the input sentence '
'Tag sets for English are derived from the Penn Treebank <CIT> '
'In <OTH> , target trees were employed to improve the scoring of translation theories '
'First , splitting and merging of sentences <CIT> , which seems related to content planning and aggregation '
'We referred to the studies of <CIT> '
'For this experiment , we used sections 02 21 of the Penn Treebank -LRB- PTB -RRB- <CIT> as the training data and section 23 -LRB- 2416 sentences -RRB- for evaluation , as is now standard '
'The hypothesis scores and tuning are identical to the setup used in <CIT> '
'AL has already been applied to several NLP tasks , such as document classification <OTH> , POS tagging <OTH> , chunking <OTH> , statistical parsing <OTH> , and information extraction <OTH> '
'Only the measures provided by LESK , HSO , VEC , <OTH> , and <CIT> provide a method for predicting adjective similarities ; of these , only LESK and VEC outperform the uninformed baseline on adjectives , while our learned measure achieves a 40 % improvement over the LESK measure on adjectives '
'Phrase tables were learned from the training corpus using the diag-and method <CIT> , and using IBM model 2 to produce initial word alignments -LRB- these authors found this worked as well as IBM4 -RRB- '
'The marginal relevance systems -LRB- MR and MR+IE -RRB- used a simple selection mechanism which does not involve search , inspired by the maximal marginal relevance -LRB- MMR -RRB- approach <CIT> '
'Similar to bidirectional labeling in <CIT> , there are two learning tasking in this model '
'It is possible to use unsupervised learning to train stochastic taggers without the need for a manually annotated corpus by using the Baum-Welch algorithm <CIT> '
'For instance , work has been done in Chinese using the Penn Chinese Treebank <OTH> , in Czech using the Prague Dependency Treebank <CIT> , in French using the French Treebank <OTH> , in German using the Negra Treebank <OTH> , and in Spanish using the UAM Spanish Treebank <OTH> '
'Statistical machine translation is based on the noisy channel model , where the translation hypothesis is searched over the space defined by a translation model and a target language <CIT> '
'The terms graph-based and transition-based were used by <CIT> to describe the difference between MSTParser <CIT> , which is a graph-based parser with an exhaustive search decoder , and MaltParser <OTH> , which is a transition-based parser with a greedy search decoder '
'One of the most relevant work is <CIT> , which proposed to integrate various patterns in order to measure semantic similarity between words '
'Either pruning <OTH> or lossy randomizing approaches <CIT> may result in a compact representation for the application run-time '
'Actually , now that SMT has reached some maturity , we see several attempts to integrate more structure into these systems , ranging from simple hierarchical alignment models <CIT> to syntax-based statistical systems <OTH> '
'This is in sharp contrast to the smoothed fixed-word statistics in most lexicalized parsing models derived from sparse data -LRB- Magerman <OTH> , <CIT> , Charniak -LRB- 1997 -RRB- , etc -RRB- '
'<CIT> has induced clusters by mapping WordNet senses to a more coarse-grained lexical resource '
'Thus , given a hyponym definition -LRB- O -RRB- and a set of candidate hypernym definitions , this method selects the candidate hypernym definition -LRB- E -RRB- which returns the maximum score given by formula -LRB- 1 -RRB- : SC -LRB- O , E -RRB- : E cw -LRB- wi , wj -RRB- -LRB- I -RRB- ` wIEOAwj6E The cooccurrence weight -LRB- cw -RRB- between two words can be given by Cooccurrence Frequency , Mutual Information <CIT> or Association Ratio <OTH> '
'The weights of the models are computed automatically using a variant of the Maximum Bleu training procedure proposed by <CIT> '
'In <CIT> , we proposed a framework that automatically evaluated automatic MT evaluation metrics using only manual translations without further human involvement '
'The training algorithm we used is the improved iterative scaling -LRB- IIS -RRB- described in <CIT> 3 '
'Performance is measured by computing the BLEU scores <CIT> of the systems translations , when compared against a single reference translation per sentence '
'In previous work <OTH> , I described a Maximum Entropy\/Minimum Divergence -LRB- MEMD -RRB- model <CIT> for p -LRB- w -LRB- hi , s -RRB- which incorporates a trigram language model and a translation component which is an analog of the well-known IBM translation model 1 <OTH> '
'Lexical cues of differing complexities have been used , including single words and Ngrams -LRB- eg , <CIT> -RRB- , as well as phrases and lexico-syntactic patterns -LRB- eg , <OTH> -RRB- '
'-LRB- A similar intuition holds for the Machine Translation models generically known as the IBM models <CIT> , which assume that certain words in a source language sentence tend to trigger the usage of certain words in a target language translation of that sentence -RRB- '
'been put forward by <CIT> '
'Substring-based transliteration with a generative hybrid model is very similar to existing solutions for phrasal SMT <CIT> , operating on characters rather than words '
'In the post-editing step , a prediction engine helps to decrease the amount of human interaction <CIT> '
'1 is a set of assumptions sufficient to support the inI , ` rl -RRB- n ` lation given S and R In other words , this is h , ~ crl -RRB- rctal , ion as abduction '' <OTH> , since ~ ! -RRB- -LRB- i , -LRB- ` lion , not deduction , is needed to arrive at the : ~ -RRB- ''d H II I ~ tiOIIS ,4 '
'Algorithms for the computation of first-order associations have been used in lexicography for the extraction of collocations <CIT> and in cognitive psychology for the simulation of associative learning <OTH> '
'While traditional approaches to syntax based MT were dependent on availability of manual grammar , more recent approaches operate within the resources of PB-SMT and induce hierarchical or linguistic grammars from existing phrasal units , to provide better generality and structure for reordering <CIT> '
'The use of such relations -LRB- mainly relations between verbs or nouns and their arguments and modifiers -RRB- for various purposes has received growing attention in recent research <CIT> '
'Nevertheless , as <CIT> and others have argued , semantic representations for natural language need not be higher-order in that ontological promiscuity can solve the problem '
'WordNet sense information has been criticized to be too fine grained <CIT> '
'The intercoder reliability is a constant concern of everyone working with corpora to test linguistic hypotheses <CIT> , and the more so when one is coding for semanto-pragmatic interpretations , as in the case of the analysis of connectives '
'The next section briefly reviews the word alignment based statistical machine translation <CIT> '
'<CIT> describe a method for learning a probabilistic model that maps LFG parse structures in German into LFG parse structures in English '
'While previous researchers have used agglomerative nesting clustering -LRB- eg <CIT> , Futrelle and Gauch -LRB- 1993 -RRB- -RRB- , comparisons with our work are difficult to draw , due to their use of the 1,000 commonest words from their respective corpora '
'The algorithm proposed by <CIT> is labeled as Turney-PairClass '
'The cohesion between words has been evaluated with the mutual information measure , as in <CIT> '
'Using this alignment strategy , we follow <CIT> and compute one alignment for each translation direction -LRB- f e and e f -RRB- , and then combine them '
'2 Combining Classifiers for Chinesewordsegmentation Thetwomachine-learningmodelsweuseinthis work are the maximum entropy model <CIT> and the error-driven transformation-based learning model <OTH> Weusetheformerasthemainworkhorse and the latter to correct some of the errors producedbytheformer '
'<CIT> studied two context delineation methods of English nouns : the window-based and the syntactic , whereby all the different types of syntactic dependencies of the nouns were used in the same feature space '
'Wall-Street Journal -LRB- WSJ -RRB- Sections 15-18 and 20 were used by <CIT> as training and test data respectively for evaluating their base-NP chunker '
'We then used the kappa statistic <CIT> to assess the level of agreement between the three coders with respect to the 2 An agent holds the task initiative during a turn as long as some utterance during the turn directly proposes how the agents should accomplish their goal , as in utterance -LRB- 3c -RRB- '
'We could also use the value of semantic similarity and relatedness measures <CIT> or the existence of hypernym or hyponym relations as features '
'While the tag features , containing WSJ paxt-ofspeech tags <CIT> , have about 45 values , the word features have more than 10,000 values '
'41 Evaluation of Different Features and Models In pilot experiments on a subset of the features , we provide a comparison of HM-SVM with other two learning models , maximum entropy -LRB- MaxEnt -RRB- model <CIT> and SVM model <OTH> , to test the effectiveness of HMSVM on function labeling task , as well as the generality of our hypothesis on different learning 58 Table 3 : Features used in each experiment round '
'Their weights are optimized wrt BLEU score using the algorithm described in <CIT> '
'GIZA + + <OTH> , an implementation of the IBM <CIT> and HMM -LRB- ? -RRB- '
'As for parser , we train three off-shelf maximum-entropy parsers <OTH> using the Arabic , Chinese and English Penn treebank <CIT> '
'Our decoder is a phrase-based multi-stack implementation of the log-linear model similar to Pharaoh <CIT> '
'Words in test data that have not been seen in training are deterministically assigned the POS tag that is assigned by the tagger described in <CIT> '
'<CIT> describes various strategies for the decomposition of the decoding into multiple translation models using the Moses decoder '
'We created a dependency training corpus based on the Penn Treebank <CIT> , or more specifically on the HPSG Treebank generated from the Penn Treebank -LRB- see section 22 -RRB- '
'34 Feature Representation Ranking Models Following previous work on sentiment classi cation <CIT> , we represent each review as a vector of lexical features '
'Perceptron Learning a discriminative structure prediction model with a perceptron update was first proposed by <CIT> '
'English POS tags were assigned by MXPOST <CIT> , which was trained on the training data described in Section 41 '
'ACM Transactions on Computer-Human Interaction (TOCHI), 11(3).</rawString> </citation> <citation valid=''true''> <authors> <author>M E Pollack</author> </authors> <title>Intelligent technology for an aging population: The use of AI to assist elders with cognitive impairment</title> <date>2005</date> <journal>AI Magazine</journal> <pages>26--2</pages> <contexts> <context>rch on developing SDS for home-care and tele-care applications, Examples include scheduling appointments over the phone (Zajicek et al. 2004, Wolters et al., submitted), interactive reminder systems (Pollack, 2005), symptom management systems (Black et al. 2005) or environmental control systems (Clarke et al. 2005).'
'For example , extractive text summarization generates a summary by selecting a few good sentences from one or more articles on the same topic <CIT> '
'<CIT> showed that the MSTParser and MaltParser produce different errors '
'<OTH> proposed sentence alignment techniques based on dynamic programming , using sentence length and lexical mapping information '
'For instance , BLEU and ROUGE <OTH> are based on n-gram precisions , METEOR <CIT> and STM <OTH> use word-class or structural information , Kauchak <OTH> leverages on paraphrases , and TER <OTH> uses edit-distances '
'One is distortion model <CIT> which penalizes translations according to their jump distance instead of their content '
'CIT -RRB- '
'Results for chunking Penn Treebank data were previously presented by several authors <CIT> '
'1 Introduction The recent advances in statistical machine translation have been achieved by discriminatively training a small number of real-valued features based either on -LRB- hierarchical -RRB- phrase-based translation <CIT> or syntax-based translation <OTH> '
'Manual processes , such as lexicon development could be automated in the future using standard contextbased , word distribution methods <CIT> , or other corpus-based techniques '
'Separating the scoring from the source language reordering also has the advantage that the approach in essence is compatible with other approaches such as a traditional PSMT system <CIT> or a hierarchical phrase system <OTH> '
'(Koehn et al., 2003).'
'In order to create the necessary SMT language and translation models, they used:  Giza++ (Och & Ney, 2003);2  the CMU-Cambridge statistical toolkit;3  the ISI ReWrite Decoder.4 Translation was performed from EnglishFrench and FrenchEnglish, and the resulting translations were evaluated using a range of automatic metrics: BLEU (Papineni et al. , 2002), Precision and Recall 2http://www.isi.edu/och/Giza++.html 3http://mi.eng.cam.ac.uk/prc14/toolkit.html 4http://www.isi.edu/licensed-sw/rewrite-decoder/ 185 (Turian et al. , 2003), and Wordand Sentence Error Rates.'
'<OTH> , sometimes augmented by an HMM-based model or Och and Neys Model 6 <OTH> '
'<OTH> , is not very useful for applications like statistical machine translation , <CIT> , for which an accurate word-to-word alignment between the source and the target languages is critical for high quality translations '
'Candidate term Segment result of GPWS for one sentence , in which term appears \/ \/ \/ \/ \/ \/ \/ \/ \/ \/ \/ \/ \/ \/ \/ \/ \/ \/ \/ \/ \/ \/ \/ \/ \/ \/ \/ \/ \/ \/ \/ \/ \/ Table 2 : Examples of candidates eliminated by GPWS 5 Relative frequency ratio against background corpus Relative frequency ratio -LRB- RFR -RRB- is a useful method to be used to discover characteristic linguistic phenomena of a corpus when compared with another <OTH> '
'Many methods have been proposed to compute distributional similarity between words , eg , <CIT> , <OTH> , <OTH> and <OTH> '
'1 Introduction Decoding is one of the three fundamental problems in classical SMT -LRB- translation model and language model being the other two -RRB- as proposed by IBM in the early 1990s <CIT> '
'For example , minimum entropy regularization <CIT> , aims to maximize the conditional likelihood of labeled data while minimizing the conditional entropy of unlabeled data : summationdisplay i logp -LRB- y -LRB- i -RRB- x -LRB- i -RRB- -RRB- 122bardblbardbl2H -LRB- y x -RRB- -LRB- 3 -RRB- This approach generally would result in sharper models which can be data-sensitive in practice '
'Introduction Many applications that process natural language can be enhanced by incorporating information about the probabilities of word strings ; that is , by using statistical language model information <OTH> '
'While in this paper we evaluated our framework on the discovery of concepts , we have recently proposed fully unsupervised frameworks for the discovery of different relationship types <CIT> '
'Such a technique has been used with TER to combine the output of multiple translation systems <CIT> '
'<OTH> of running GIZA + + <CIT> in both directions and then merging the alignments using the grow-diag-final heuristic '
'Recent lexicalized stochastic parsers such as <CIT> , Charniak -LRB- 1997 -RRB- , and others add additional features to each constituent , the most important being the head word of the parse constituent '
'So far , most previous work on domain adaptation for parsing has focused on data-driven systems <CIT> , ie systems employing -LRB- constituent or dependency based -RRB- treebank grammars <OTH> '
'34 Perspectives for automatic paraphrase extraction There is a growing amount of work on automatic extraction of paraphrases from text corpora <CIT> '
'Others have introduced alternative discriminative training methods <CIT> , in which a recurring challenge is scalability : to train many features , we need many train218 ing examples , and to train discriminatively , we need to search through all possible translations of each training example '
'Our system is a re-implementation of the phrase-based system described in Koehn <OTH> , and uses publicly available components for word alignment <CIT> 1 , decoding <OTH> 2 , language modeling <OTH> 3 and finite-state processing <OTH> 4 '
'<OTH> and the HMM alignment model of <OTH> '
'31 Word Sequence Classification Similar to English text chunking <CIT> , the word sequence classification model aims to classify each word via encoding its context features '
'The table also shows Cohen ''s to , an agreement measure that corrects for chance agreement <CIT> ; the most important t value in the table is the value of 07 for the two human judges , which can be interpreted as sufficiently high to indicate that the task is reasonably well defined '
'Note that the need to consider segmentation and alignment at the same time is also mentioned in <OTH> , and related issues are reported in <CIT> '
'Unconstrained CL corresponds exactly to a conditional maximum entropy model <CIT> '
'<CIT> 1993 -RRB- , make use of both positive and negative instances of performing a task '
'204 422 Correlation between TREC nuggets and non-text features Analyzing the features used could let us understand summarization better <CIT> '
'For this work , an off-the-shelf maximum entropy tagger 10 <CIT> was used '
'Our evaluation metrics is casesensitive BLEU-4 <CIT> '
'For example , the distancebased reordering model <CIT> allows a decoder to translate in non-monotonous order , under the constraint that the distance between two phrases translated consecutively does not exceed a limit known as distortion limit '
'These three parsers have given the best reported parsing results on the Penn Treebank Wall Street Journal corpus <CIT> '
'Recently there have been some improvements to the Charniak parser , use n-best re-ranking as reported in <OTH> and selftraining and re-ranking using data from the North American News corpus -LRB- NANC -RRB- and adapts much better to the Brown corpus <CIT> '
'1 Introduction <CIT> introduced minimum error rate training -LRB- MERT -RRB- as an alternative training regime to the conditional likelihood objective previously used with log-linear translation models <CIT> '
'We evaluate our results with case-sensitive BLEU-4 metric <CIT> '
'(2002) do not use a feature selection technique, employing instead an objective function which includes a Table 4 Values of Savings (a, b) for various values of a, b. ab Savings (a, b) 1100,000 2,692.7 110 48.6 11100 83.5 1011,000 280.0 1,00110,000 1,263.9 10,00150,000 2,920.2 50,001100,000 4,229.8 Collins and Koo Discriminative Reranking for NLP Gaussian prior on the parameter values, thereby penalizing parameter values which become too large: a C3  arg min a  LogLossa X k0:::m a 2 k 7 2 k  28 Closed-form updates under iterative scaling are not possible with this objective function; instead, optimization algorithms such as gradient descent or conjugate gradient methods are used to estimate parameter values.'
'4 Experiments We evaluated the ISBN parser on all the languages considered in the shared task <CIT> '
'For each pivot feature k , we use a loss function L k , -LRB- -RRB- 2 1 -RRB- -LRB- wxwxpL i i T ikk + = -LRB- 1 -RRB- where the function p k -LRB- x i -RRB- indicates whether the pivot feature k occurs in the instance x i , otherwise xif xp ik ik 0 1 1 -RRB- -LRB- -RRB- = , where the weight vector w encodes the correspondence of the non-pivot features with the pivot feature k <CIT> '
'Bilingual configurations that condition on tprime , wprime -LRB- 2 -RRB- are incorporated into the generative process as in <CIT> '
'Their weights are calculated by deleted interpolation <CIT> '
'To generate word alignments we use GIZA + + <CIT> , which implements both the IBM Models of Brown et al '
'During training , the early update strategy of <CIT> is used : when the correct state item falls out of the beam at any stage , parsing is stopped immediately , and the model is updated using the current best partial item '
'The phoneme prediction and sequence modeling are considered as tagging problems and a Perceptron HMM <CIT> is used to model it '
'The system used for baseline experiments is two runs of IBM Model 4 <OTH> in the GIZA + + <CIT> implementation , which includes smoothing extensions to Model 4 '
'In particular , mutual information <CIT> and other statistical methods such as <OTH> and frequency-based methods such as <OTH> exclude infrequent phrases because they tend to introduce too much noise '
'Inspired by the idea of graph based algorithms to collectively rank and select the best candidate , research efforts in the natural language community have applied graph-based approaches on keyword selection <OTH> , text summarization <OTH> , word sense disambiguation <OTH> , sentiment analysis <CIT> , and sentence retrieval for question answering <OTH> '
'By comparing derivation trees for parallel sentences in two languages , instances of structural divergences <OTH> can be automatically detected '
'Language models , such as N-gram class models <OTH> and Ergodic Hidden Markov Models <OTH> were proposed and used in applications such as syntactic class -LRB- POS -RRB- tagging for English <CIT> , clustering and scoring of recognizer sentence hypotheses '
'5 Discussion and Future Work The work in this paper substantially differs from previous work in SMT based on the noisy channel approach presented in <CIT> '
'22 Co-occurrence-based approaches The second class of algorithms uses cooccurrence statistics <CIT> '
'An early exception to this was <CIT> itself , where Model 2 used function tags during the training process for heuristics to identify arguments -LRB- eg , the TMP tag on the NP in Figure 1 disqualifies the NP-TMP from being treated as an argument -RRB- '
'Other metrics assess the impact of alignments externally , eg , different alignments are tested by comparing the corresponding MT outputs using automated evaluation metrics -LRB- eg , BLEU <CIT> or METEOR <OTH> -RRB- '
'This approach is also used in base-NP chunking <CIT> and named entity recognition <OTH> as well as word segmentation '
'Then , the method of <CIT> can be used to compute the probability of every possible edge conditioned on the presence of ki , p -LRB- yiprime = kprime yi = k , x -RRB- , using K1ki Multiplying this probability by p -LRB- yi = k x -RRB- yields the desired two edge marginal '
'Standard MET <CIT> iterative parameter estimation under IBM BLEU <OTH> is performed on the corresponding development set '
'Item Form: a32 a2 a49a51 a15 a52 a49 a51a16a33 Goal: a32a35a34 a49 a51 a15 a23a4a3 a12 a0a36a5 a24 a49 a51a37a33 Inference Rules Scan component d, a10a38a8 a7 a8 a0 : a39a41a40a43a42a44 a44a45 a23a25a24 a49 a5a47a46 a49 a2 a23a25a24 a5a49a48 a49 a51 a50 a23a25a24 a49 a5a47a46 a49 a20a43a5 a3a22 a23a25a24 a5a49a48 a49 a51 a51a14a52 a52 a53 a54a55 a55 a56 a23a25a24 a49 a5a47a46 a49 a2 a23a25a24 a5a49a48 a49 a51 a50 a23a25a24 a49 a5a47a46 a49a23 a19a57a24 a10a13a12 a19 a24 a23a25a24 a5a49a48 a49 a51 a58a59 a59 a60 Compose: a61a63a62a65a64 a66a68a67a69 a64 a66a71a70 a61a35a72a37a64 a66a68a67a73 a64 a66a71a70a36a74a76a75 a32a78a77 a64 a66a76a67a69 a64 a66a80a79a81a73 a64 a66 a14 a62a82a64 a66 a14 a72a37a64 a66 a33 a10 a77 a64 a66 a67a69 a64 a66a37a83 a73 a64 a66 a18 Figure 3: Logic C (C for CKY) These constraints are enforced by the d-span operators a84 and a85 . Parser C is conceptually simpler than the synchronous parsers of Wu (1997), Alshawi et al.'
'Several authors <OTH> have attempted to improve results by using supplementary fields of information in the electronic version of the Longman Dictionary of Contemporary English -LRB- LDOCE -RRB- , in particular , the box codes and subject codes provided for each sense '
'Daume allows an extra degree of freedom among the features of his domains , implicitly creating a two-level feature hierarchy with one branch for general features , and another for domain specific ones , but does not extend his hierarchy further <CIT> -RRB- '
'Then , we apply a grow-diag-final algorithm which is widely used in bilingual phrase extraction <CIT> to monolingual alignments '
'The translation quality is evaluated by BLEU metric <CIT> , as calculated by mteval-v11bpl with case-insensitive matching of n-grams , where n = 4 '
'The second voting model is a maximum entropy model <OTH> , since <CIT> found that this model yielded higher accuracy than naive Bayes in a subsequent comparison of WSD performance '
'While transfer learning was proposed more than a decade ago <OTH> , its application in natural language processing is still a relatively new territory <CIT> , and its application in relation extraction is still unexplored '
'Other classes , such as the ones below can be extracted using lexico-statistical tools , such as in <CIT> , and then checked by a human '
'These include the perceptron <CIT> and its large-margin variants <OTH> '
'The other intriguing issue is how our anchor-based method for shared argument identification can benefit from recent advances in coreference and zero-anaphora resolution <CIT> '
'Another important direction is classifying sentences as subjective or objective , and classifying subjective sentences or clauses as positive or negative <CIT> '
'<CIT> proposed a method to retrieve collocations by combining bigrams whose cooccurrences are greater than a given threshold 3 '
'Discriminative models do not only have theoretical advantages over generative models , as we discuss in Section 2 , but they are also shown to be empirically favorable over generative models when features and objective functions are fixed <CIT> '
'3 Feature selection <CIT> proposed an iterative procedure of adding news features to feature set driven by data '
'One of our goals was to use for this study only information that could be annotated reliably <CIT> , as we believe this will make our results easier to replicate '
'task , originally introduced in <CIT> and also described in <OTH> , brackets just base NP constituents5 '
'Following <OTH> , we used the version 11a NIST BLEU script with its default settings to calculate the BLEU scores <CIT> based on case-insensitive ngram matching , where n is up to 4 '
'<CIT> predicates the sentiment orientation of a review by the average semantic orientation of the phrases in the review that contain adjectives or adverbs , which is denoted as the semantic oriented method '
'The Logllkelihood Ratio , G 2 , is a mathematically well-grounded and accurate method for calculating how ` surprising '' an event is <CIT> '
'417 structure of semantic networks was proposed in <CIT> , with a disambiguation accuracy of 509 % measured on all the words in the SENSEVAL-2 data set '
'We describe the experiment in greater detail 2The particular verbs selected were looked up in <OTH> and the class for each verb in the classification system defined in <OTH> was selected with some discussion with linguists '
'So fitr , we have implemented the following , : sentence ~ dignment btLsed-on word correspondence information , word correspondence estimation by cooccnl ` rence-ffequency-based methods in GMe mid Church -LRB- 19 ~ H -RRB- and Kay and R6scheisen <OTH> , structured Imttehlng of parallel sentences <OTH> , and case Dame acquisition of Japanese verbs <OTH> '
'of ACL 1990 <CIT> , F Smadja , Retrieving collocations fi ` cma text : XTRACT , -LRB- 1993 -RRB- '
'322 Features We used eight features <CIT> and their weights for the translations '
'-LRB- <CIT> discusses the recovery of one kind of empty node , viz '
'This idea is the same as <CIT> '
'<CIT> present a lexical similarity model based on random walks on graphs derived from WordNet ; Rao et al '
'For example , syntactic features <CIT> can be computed this way and are used in our system '
'Due to the parameter interdependencies introduced by the one-to-one assumption , we are unlikely to find a method for decomposing the assignments into parameters that can be estimated independently of each other as in Brown et al <OTH> -RRB- '
'2 Disperp and Distortion Corpora 21 Defining Disperp The ultimate reason for choosing one SCM over another will be the performance of an MT system containing it , as measured by a metric like BLEU <CIT> '
'22 Statistical Parsers Pioneered by the IBM natural language group <OTH> and later pursued by , for example , Schabes , Roth , and Osborne <OTH> , Jelinek et al '
'3ThePOS taggers The two POS taggers used in the experiments are TNT , a publicly available Markov model tagger <OTH> , and a reimplementation of the maximum entropy -LRB- ME -RRB- tagger MXPOST <CIT> '
'In earlier IBM translation systems <CIT> each English word would be generated by , or ` aligned to '' , exactly one formal language word '
'52 Experimental Results Following <OTH> and other work on general-purpose generators , BLEU score <CIT> , average NIST simple string accuracy -LRB- SSA -RRB- and percentage of exactly matched sentences are adopted as evaluation metrics '
'Then the two models and a search module are used to decode the best translation <CIT> '
'For process -LRB- 3 -RRB- , machine-learning methods are usually used to classify subjective descriptions into bipolar categories <CIT> or multipoint scale categories <CIT> '
'Then , those structurally matched parallel sentences are used as a source for acquiring lexical knowledge snch as verbal case frames <OTH> '
'Above the phrase level , some models perform no reordering <OTH> , some have a simple distortion model that reorders phrases independently of their content <CIT> , and some , for example , the Alignment Template System <OTH> , hereafter ATS , and the IBM phrase-based system <OTH> , have phrase-reordering models that add some lexical sensitivity '
'In fact, when the perceptron update rule of (Dekel et al. , 2004)  which modifies the weights of every divergent node along the predicted and true paths  is used in the ranking framework, it becomes virtually identical with the standard, flat, ranking perceptron of Collins (2002).5 In contrast, our approach shares the idea of (Cesa-Bianchi et al. , 2006a) that if a parent class has been predicted wrongly, then errors in the children should not be taken into account. We also view this as one of the key ideas of the incremental perceptron algorithm of (Collins and Roark, 2004), which searches through a complex decision space step-by-step and is immediately updated at the first wrong move.'
'Recently , severalmethods <CIT> have been proposed with similar motivation to ours '
'It has been used for diverse problems such as machine translation and sense disambiguation <OTH> '
'In this study we have concentrated on the NPs ? ? term extraction , which comprises the focus of interest in several studies <OTH> '
'An alternative training criterion therefore directly optimizes translation quality as measured by an automatic evaluation criterion <CIT> '
'<CIT> proposed a simple feature augmentation method to achieve domain adaptation '
'In open-domain opinion extraction , some approaches use syntactic features obtained from parsed input sentences <CIT> , as is commonly done in semantic role labeling '
'We utilize a maximum entropy -LRB- ME -RRB- model <CIT> to design the basic classifier used in active learning for WSD '
'They can be used for discriminative training of reordering models <CIT> '
'The metric we used is the kappa statistic <CIT> , which factors out the agreement that is expected by chance : -RRB- -LRB- 1 -RRB- -LRB- -RRB- -LRB- EP EPAP = where P -LRB- A -RRB- is the observed agreement among the raters , and P -LRB- E -RRB- is the expected agreement , ie , the probability that the raters agree by chance '
'We trained a Chinese Treebank-style tokenizer and partof-speech tagger , both using a tagging model based on a perceptron learning algorithm <CIT> '
'We tuned our system on the development set devtest2006 for the EuroParl tasks and on nc-test2007 for CzechEnglish , using minimum error-rate training <CIT> to optimise BLEU score '
'5 Related Work Evidence from the surrounding context has been used previously to determine if the current sentence should be subjective\/objective <CIT> -RRB- and adjacency pair information has been used to predict congressional votes <OTH> '
'41 Applications to phrase-based SMT Aphrase-basedtranslationmodelcanbeestimated in two stages : first a parallel corpus is aligned at the word-level and then phrase pairs are extracted <CIT> '
'These blocks are used to compute the results in the fourth column : the BLEU score <CIT> with a153 reference translation using a153 - grams along with 95 % confidence interval is reported 4 '
'22 Implementation of GIZA + + GIZA + + is an implementation of ML estimators for several statistical alignment models , including IBM Model 1 through 5 <CIT> , HMM <OTH> and Model 6 <OTH> '
'An exception is the use of similarity for alleviating the sparse data problem in language modeling <CIT> '
'The majority of these systems used models belonging to one of the twodominantapproachesindata-drivendependency parsinginrecentyears <CIT> : In graph-based models , every possible dependency graph for a given input sentence is given a score that decomposes into scores for the arcs of the graph '
'579 The MaxEnt algorithm associates a set of weights -LRB- ij -RRB- i = 1nj = 1m with the features , which are estimated during the training phase to maximize the likelihood of the data <CIT> '
'We have also applied our more general unification grammar acquisition methodology to the TIGER Treebank <OTH> and Penn Chinese Treebank <OTH> , extracting wide-coverage , probabilistic LFG grammar 361 Computational Linguistics Volume 31 , Number 3 approximations and lexical resources for German <CIT> and Chinese <OTH> '
'The last issue is how our binarization performs on a lexicalized parser , like <CIT> '
'33 Grid Line Search Our implementation of a grid search is a modified version of that proposed in <CIT> '
'Toremedythis situation , we can borrow the probabilistic model of PHARAOH , and define the parsing model as : Pr -LRB- d e -LRB- d -RRB- -RRB- = productdisplay dd w -LRB- r -LRB- d -RRB- -RRB- -LRB- 4 -RRB- which is the product of the weights of the rules used in a derivation d The rule weight , w -LRB- X , -RRB- , is in turn defined as : P -LRB- -RRB- 1P -LRB- -RRB- 2Pw -LRB- -RRB- 3Pw -LRB- -RRB- 4 exp -LRB- -RRB- 5 where P -LRB- -RRB- and P -LRB- -RRB- are the relative frequencies of and , and Pw -LRB- -RRB- and Pw -LRB- -RRB- are 176 the lexical weights <CIT> '
'However , their decoder is outperformed by phrase-based decoders such as <CIT> , <OTH> , and <OTH> '
'Many machine learning techniques have been developed to tackle such random process tasks , which include Hidden Markov Models -LRB- HMMs -RRB- <OTH> , Maximum Entropy Models -LRB- MEs -RRB- <CIT> , Support Vector Machines -LRB- SVMs -RRB- <OTH> , etc Among them , SVMs have high memory capacity and show high performance , especially when the target classification requires the consideration of various features '
'Strube and <CIT> , and for Coreference Resolution <OTH> '
'Finally , we would like to investigate the incorporation of unsupervised methods for WSD , such as the heuristically-based methods of <OTH> and <OTH> , and the theoretically purer bootstrapping method of <CIT> '
'We use a program to label syntactic arguments with the roles they are playing <OTH> , and the rules for complement\/adjunct distinction given by <CIT> to never allow deletion of the complement '
'To quickly -LRB- and approximately -RRB- evaluate this phenomenon , we trained the statistical IBM wordalignment model 4 <OTH> ,1 using the GIZA + + software <OTH> for the following language pairs : ChineseEnglish , Italian English , and DutchEnglish , using the IWSLT-2006 corpus <OTH> for the first two language pairs , and the Europarl corpus <CIT> for the last one '
'However , in yet unpublished work we found that at least for the computation of synonyms and related words neither syntactical analysis nor singular value decomposition lead to significantly better results than the approach described here when applied to the monolingual case <OTH> , so we did not try to include these methods in our system '
'Similar adaptations of the Matrix-Tree Theorem have been developed independently and simultaneouslyby <CIT> andMcDonaldand Satta -LRB- 2007 -RRB- ; see Section 5 for more discussion '
'<CIT> reports 88 % labeled precision and recall on individual parse constituents on data from the Penn Treebank , roughly consistent with our finding of at least 13 % error '
'The agreement was statistically significant -LRB- Kappa = 0650 -RRB- 001 for Japanese and Kappa = 0748,0 -RRB- 001 for English <CIT> -RRB- '
'In the following experiments , the NIST BLEU score is used as the evaluation metric <CIT> , which is reported as a percentage in the following sections '
'The majority of this research was done on extending the tree structure -LRB- finding new synsets <CIT> or enriching WN with new relationships <OTH> -RRB- rather than improving the quality of existing concept\/synset nodes '
'<OTH> 9417 Li and Roth <OTH> 9302 9464 Table 2 : Baseline results on three shallow parsing tasks : the NP-Chunking task <CIT> ; the CoNLL-2000 Chunking task <OTH> ; and the Li & Roth task <OTH> , which is the same as CoNLL-2000 but with more training data and a different test section '
'In computational linguistics , our pattern discovery procedure extends over previous approaches that use surface patterns as indicators of semantic relations between nouns or verbs -LRB- <CIT> inter alia -RRB- '
'Our intuition comes from an observation by <CIT> regarding multiple tokens of words in documents '
'In such cases , additional information may be coded into the HMM model to achieve higher accuracy <CIT> '
'22 Table 5 : Comparison with previous best results : -LRB- Top : POS tagging , Bottom : Text Chunking -RRB- POS tagging F = 1 Perceptron <CIT> 9711 Dep '
'A totally different approach to improving the accuracy of our parser is to use the idea of selftraining described in <CIT> '
'For instance , the most relaxed IBM Model-1 , which assumes that any source word can be generated by any target word equally regardless of distance , can be improved by demanding a Markov process of alignments as in HMM-based models <OTH> , or implementing a distribution of number of target words linked to a source word as in IBM fertility-based models <CIT> '
'Metrics in the Rouge family allow for skip n-grams <OTH> ; Kauchak and Barzilay <OTH> take paraphrasing into account ; metrics such as METEOR <CIT> and GTM <OTH> calculate both recall and precision ; METEOR is also similar to SIA <OTH> in that word class information is used '
'Lexical cues of differing complexities have been used , including single words and Ngrams -LRB- eg , <CIT> -RRB- , as well as phrases and lexico-syntactic patterns -LRB- eg , <OTH> -RRB- '
'The simple idea that words in a source chunk are typically aligned to words in a single possible target chunk is used to discard alignments which link words from 2We use IBM-1 to IBM-5 models <CIT> implemented with GIZA + + <OTH> '
'There is also work on grouping senses of other inventories using information in the inventory <CIT> along with information retrieval techniques <OTH> '
'In our experiments , we used a dependency parser only in English -LRB- a version of the Collins parser <CIT> that has been adapted for building dependencies -RRB- but not in the other language '
'Automatic Evaluation Measures A variety of automatic evaluation methods have been recently proposed in the machine translation community <CIT> '
'According to current tagger comparisons <OTH> , and according to a comparsion of the results presented here with those in <CIT> , the Maximum Entropy framework seems to be the only other approach yielding comparable results to the one presented here '
'One is to find unknown words from corpora and put them into a dictionary -LRB- eg , <OTH> -RRB- , and the other is to estimate a model that can identify unknown words correctly -LRB- eg , <OTH> -RRB- '
'Early work employed a diverse range of features in a linear classifier -LRB- commonly referred to as feature-based approaches -RRB- , including lexical features , syntactic parse features , dependency features and semantic features <CIT> '
'Since an existing study incorporates these relations ad hoc <CIT> , they are apparently crucial in accurate disambiguation '
'Collocation : Collocations were extracted from a seven million word sample of the Longman English Language Corpus using the association ratio <CIT> and outputted to a lexicon '
'The underlying translation model is Model 2 from <CIT> '
'Agglomerative clustering -LRB- eg , <CIT> -RRB- can produce hierarchical word categories from an unannotated corpus '
'Much research has been carried out recently in this area <OTH> '
'A superset of the parallel data was word aligned by GIZA union <OTH> and EMD <CIT> '
'1 Empty categories however seem different , in that , for the most part , their location and existence is determined , not by observable data , but by explicitly constructed linguistic principles , which 1 Both <CIT> and Higgins -LRB- 2003 : 100 -RRB- are explicit about this predisposition '
'David <CIT> showed it was accurate in the word sense disambiguation '
'Wu <OTH> adopted chammls that eliminate syntactically unlikely alignments and Wang et al '
'Generative and discriminative models have been comparedanddiscussedagreatdeal <OTH> , including for NLP models <CIT> '
'As comparison , <CIT> used seed sets consisting of 7 words in their word valence annotation experiments , while Turney <OTH> used minimal seed sets consisting of only one positive and one negative word -LRB- excellent and poor -RRB- in his experiments on review classification '
'An acceptable agreement for most NLP classification tasks lies between 07 and 08 <CIT> '
'We performed experiments with two statistical classifiers : the decision tree induction system C45 <OTH> and the Tilburg Memory-Based Learner -LRB- TiMBL -RRB- <OTH> '
'We also trained a baseline model with GIZA + + <CIT> following a regimen of 5 iterations of Model 1 , 5 iterations of HMM , and 5 iterations of Model 4 '
'Both <CIT> and Wagner et al '
'The four models we compare are a maximum a posteriori -LRB- MAP -RRB- method and three discriminative training methods , namely the boosting algorithm <CIT> , the average perceptron <CIT> and the minimum sample risk method <OTH> '
'Secondly , while all taggers use lexical information , and , indeed , it is well-known that lexical probabilities are much more revealing than tag sequence probabilities <OTH> , most taggers make quite limited use of lexical probabilities -LRB- compared with , for example , the bilexical probabilities commonly used in current statistical parsers -RRB- '
'METRIC FORMULA Frequency (Guiliano, 1964) x yf Pointwise Mutual Information [PMI] (Church & Hanks, 1990) ( )xy x y2log /P P P True Mutual Information [TMI] (Manning, 1999) ( )xy 2 xy x ylog /P P P P Chi-Squared ( 2 ) (Church and Gale, 1991) { }{ },, 2( ) i X X Y Y i j i j i j j f     T-Score (Church & Hanks, 1990) 1 2 2 2 1 2 1 2 x x s s n n  + C-Values4 (Frantzi, Anadiou & Mima 2000) 2 is not nested 2 log ( ) log ( ) 1 ( ) ( ) a a b T a f f f b P T         where is the candidate string f( ) is its frequency in the corpus T is the set of candidate terms that contain P(T ) is the number of these candidate terms 609 1,700 of the three-word phrases are attested in the Lexile corpus.'
'<CIT> avoided the problem by precomputing the oracle translations in advance '
'Probabilistic generative models like IBM 1-5 (Brown et al., 1993), HMM (Vogel et al., 1996), ITG (Wu, 1997), and LEAF (Fraser and Marcu, 2007) define formulas for P(f | e) or P(e, f), with ok-voon ororok sprok at-voon bichat dat erok sprok izok hihok ghirok totat dat arrat vat hilat ok-drubel ok-voon anok plok sprok at-drubel at-voon pippat rrat dat ok-voon anok drok brok jok at-voon krat pippat sat lat wiwok farok izok stok totat jjat quat cat lalok sprok izok jok stok wat dat krat quat cat lalok farok ororok lalok sprok izok enemok wat jjat bichat wat dat vat eneat lalok brok anok plok nok iat lat pippat rrat nnat wiwok nok izok kantok ok-yurp totat nnat quat oloat at-yurp lalok mok nok yorok ghirok clok wat nnat gat mat bat hilat lalok nok crrrok hihok yorok zanzanok wat nnat arrat mat zanzanat lalok rarok nok izok hihok mok wat nnat forat arrat vat gat Figure 1: Word alignment exercise (Knight, 1997).'
'Much of this work has utilized the fundamental concept of semantic orientation , <CIT> ; however , sentiment analysis still lacks a unified field theory '
'Official DUC scoring utilizes the jackknife procedure and assesses significance using bootstrapping resampling <CIT> '
'Many approaches for POS tagging have been developed in the past , including rule-based tagging <OTH> , HMM taggers <OTH> , maximum-entropy models <OTH> , cyclic dependency networks <OTH> , memory-based learning <OTH> , etc All of these approaches require either a large amount of annotated training data -LRB- for supervised tagging -RRB- or a lexicon listing all possible tags for each word -LRB- for unsupervised tagging -RRB- '
'The quality of the translation output is mainly evaluated using BLEU , with NIST <OTH> and METEOR <CIT> as complementary metrics '
'Note that although the source of the data is the same as in Section 5 , as <CIT> did '
'The statistical significance often evaluate whether two words are independant using hypothesis tests such as t-score <OTH> , the X2 , the log-likelihood <CIT> and Fishers exact test <OTH> '
'C , A , and B are computed for training dataset D as C = summationtext M m = 1 y -LRB- m -RRB- y -LRB- m -RRB- , A = summationtext M m = 1 y -LRB- m -RRB- , and B = summationtext M m = 1 y -LRB- m -RRB- In <CIT> , y -LRB- m -RRB- was approximated by using the discriminative and logistic functions shown in Eqs '
'Various methods <CIT> of automatically acquiring synonyms have been proposed '
'Then the same system weights are applied to both IncHMM and Joint Decoding - based approaches , and the feature weights of them are trained using the max-BLEU training method proposed by <CIT> and refined by Moore and Quirk -LRB- 2008 -RRB- '
'Various machine learning strategies have been proposed to address this problem , including semi-supervised learning <OTH> , domain adaptation <CIT> , multi-task learning <OTH> , self-taught learning <OTH> , etc A commonality among these methods is that they all require the training data and test data to be in the same feature space '
'One possible conclusion from the POS tagging literature is that accuracy is approaching the limit , and any remaining improvement is within the noise of the Penn Treebank training data <CIT> '
'Stress is an attribute of syllables , but syllabification is a non-trivial task in itself <CIT> '
'One promising approach extends standard Statistical Machine Translation -LRB- SMT -RRB- techniques <CIT> to the problems of monolingual paraphrase identification and generation '
'Other commonly used measures include kappa <CIT> and relative utility <OTH> , both of which take into account the performance of a summarizer that randomly picks passages from the original document to produce an extract '
'45 <CIT> proposed an MI-based measure , which he used to show that nouns could be reliably clustered based on their verb co-occurrences '
'So far , this approach has been taken by a lot of researchers <CIT> '
'Three recent papers in this area are <CIT> , Hindle -LRB- 1990 -RRB- , and Smadja and McKeown -LRB- 1990 -RRB- '
'We utilize the OpenNLP MaxEnt implementation2 of the maximum entropy classification algorithm <CIT> to train classification models for each lemma and part-of-speech combination in the training corpus '
'This generates tens of millions features , so we prune those features that occur fewer than 10 total times , as in <CIT> '
'Recently some researchers have pointed out the importance of the lexicon and proposed lexicalized models <CIT> '
'A reranking parser -LRB- see also <CIT> -RRB- is a layered model : the base layer is a generative statistical PCFG parser that creates a ranked list of k parses -LRB- say , 50 -RRB- , and the second layer is a reranker that reorders these parses using more detailed features '
'To achieve step -LRB- 1 -RRB- , we first apply a set of headfinding rules which are similar to those described in <CIT> '
'Obviously , these productions are not in the normal form of an ITG , but with the method described in <CIT> , they can be normalized '
'We set all weights by optimizing Bleu <OTH> using minimum error rate training -LRB- MERT -RRB- <CIT> on a separate development set of 2,000 sentences -LRB- Indonesian or Spanish -RRB- , and we used them in a beam search decoder <OTH> to translate 2,000 test sentences -LRB- Indonesian or Spanish -RRB- into English '
'31 Context Extraction We adopted dependency structure as the context of words since it is the most widely used and wellperforming contextual information in the past studies <CIT> '
'<CIT> considered the same problem and presented a set of supervised machine learning approaches to it '
'The first one is a hypotheses testing approach <OTH> while the second one is closer to a model estimating approach <CIT> '
'Since this trade-off is also affected by the settings of various pruning parameters , we compared decoding time and translation quality , as measured by BLEU score <CIT> , for the two models on our first test set over a broad range of settings for the decoder pruning parameters '
'<CIT> showed that it is possible to use only a few of those semantically oriented words -LRB- namely , excellent and poor -RRB- to label other phrases co-occuring with them as positive or negative '
'However , much recent work in machine learning and statistics has turned away from maximum-likelihood in favor of Bayesian methods , and there is increasing interest in Bayesian methods in computational linguistics as well <CIT> '
'After the parser produces a semantic feature structure representation of the sentence , predicate mapping rules then match against that representation in order to produce a predicate language representation in the style of Davidsonian event based semantics <CIT> , as mentioned above '
'2 The alignment Algorithm 21 Estimation of translation probabilities The translation probabilities are estimated using a method based on <CIT> , which is summarized in the following subsection , 211 '
'In the Link Grammar framework <OTH> , strictly local contexts are naturally combined with long-distance information coming from long-range trigrams '
'Minimum-error-rate training <CIT> are conducted on dev-set to optimize feature weights maximizing the BLEU score up to 4grams , and the obtained feature weights are blindly applied on the test-set '
'Given the training pairs , any sequence predictor can be used , for example a Conditional Random Field -LRB- CRF -RRB- <OTH> or a structured perceptron <CIT> '
'For example , in the IBM Models <CIT> , each word ti independently generates 0 , 1 , or more 2Note that we refer to t as the target sentence , even though in the source-channel model , t is the source sentence which goes through the channel model P -LRB- s t -RRB- to produce the observed sentence s words in the source language '
'We do not use particular lexicosyntactic patterns , as previous attempts have <CIT> '
'<OTH> , a trigram target language model , an order model , word count , phrase count , average phrase size functions , and whole-sentence IBM Model 1 logprobabilities in both directions <CIT> '
'Our approach thus provides an even more extreme version of automatic con rmation generation than that used by <CIT> where only a small eort is required by the developer '
'For example , in machine translation , BLEU score <CIT> is developed to assess the quality of machine translated sentences '
'The first one , GIZA-Lex , is obtained by running the GIZA + +2 implementation of the IBM word alignment models <CIT> on the initial parallel corpus '
'But such general word lists were shown to perform worse than statistical models built on sufficiently large in-domain training sets of movie reviews <CIT> '
'It assumes that the distance of the positions relative to the diagonal of the -LRB- j , i -RRB- plane is the dominating factor : r -LRB- i _ j I -RRB- p -LRB- ilj , J , I -RRB- = -LRB- 7 -RRB- , Ei , = l r -LRB- i '' j -RRB- As described in <CIT> , the EM algorithm can be used to estimate the parameters of the model '
'1 Introduction Corpus-derived distributional semantic spaces have proved valuable in tackling a variety of tasks , ranging from concept categorization to relation extraction to many others <CIT> '
'One of the advantages of these methods is that a wide variety of features such as dependency trees and sequences of words can easily be incorporated <CIT> '
'We then built separate directed word alignments for EnglishX andXEnglish -LRB- X -LCB- Indonesian , Spanish -RCB- -RRB- using IBM model 4 <CIT> , combined them using the intersect + grow heuristic <OTH> , and extracted phrase-level translation pairs of maximum length seven using the alignment template approach <OTH> '
'Intuitively speaking , the gaps on the target-side will lead to exponential complexity in decoding with integrated language models -LRB- see Section 3 -RRB- , as well as synchronous parsing <CIT> '
'Now with the availability of large-scale corpus , automatic acquisition of word compositions , especially word collocations from them have been extensively studie <CIT> '
'Figure 1 gives an example dependency graph for the sentence Mr Tomash will remain as a director emeritus , whichhasbeenextractedfromthe Penn Treebank <CIT> '
'<CIT> use pointwise KLdivergence between multiple language models for scoring both phraseness and informativeness of phrases '
'We parsed a 125-million word newspaper corpus with Minipar , 1 a descendent of Principar <OTH> , and extracted dependency relationships from the parsed corpus '
'Discriminative training has been used mainly for translation model combination <OTH> and with the exception of <CIT> , has not been used to directly train parameters of a translation model '
'In WASP , GIZA + + <CIT> is used to obtain the best alignments from the training examples '
'We ran the baseline semisupervised system for two iterations -LRB- line 2 -RRB- , and in contrast with <CIT> we found that the best symmetrization heuristic for this system was union , which is most likely due to our use of fully linked alignments which was discussed at the end of Section 3 '
'Recent work on the automatic acquisition of multilingual LFG resources from treebanks for Chinese , German and Spanish <CIT> has shown that given a suitable treebank , it is possible to automatically acquire high quality LFG resources in a very short space of time '
'In Machine Translation , for example , sentences are produced using application-specific decoders , inspired by work on speech recognition <CIT> , whereas in Summarization , summaries are produced as either extracts or using task-specific strategies <OTH> '
'Maximum Entropy models implement the intuition that the best model is the one that is consistent with the set of constraints imposed by the evidence but otherwise is as uniform as possible <CIT> '
'Endemic structural ambiguity , which can lead to such difficulties as trying to cope with the many thousands of possible parses that a grammar can assign to a sentence , can be greatly reduced by adding empirically derived probabilities to grammar rules <OTH> and by computing statistical measures of lexical association <OTH> '
', 1989 -RRB- , eg , lexicography <CIT> , information retrieval <OTH> , text input <OTH> , etc This paper will touch on its feasibility in topic identification '
'However , it can not handle long-distance reorderings properly and does not exploit discontinuous phrases and linguistically syntactic structure features <CIT> '
'Salience Feature Pronoun Name Nominal TOP 075 017 008 HIGH 055 028 017 MID 039 040 021 LOW 020 045 035 NONE 000 088 012 Table 2 : Posterior distribution of mention type given salience -LRB- taken from <CIT> -RRB- 33 Modifications to the H&K Model Next , we discuss the potential weaknesses of H&K s model and propose three modifications to it '
'Our human word alignments do not distinguish between Sure and Probable links <CIT> '
'521 Generate English Annotated Corpus from Wikipedia Wikipedia provides a variety of data resources for NER and other NLP research <CIT> '
'Results are reported using lowercase BLEU <CIT> '
'We have processed the Susanne corpus <OTH> and Penn treebank <CIT> to provide tables of word and subtree alignments '
'Parameters were tuned with MERT algorithm <CIT> on the NIST evaluation set of 2003 -LRB- MT03 -RRB- for both the baseline systems and the system combination model '
'Any way to enforce linguistic constraints will result in a reduced need for data , and ultimately in more complete models , given the same amount of data <CIT> '
'1 Introduction Phrase-based modeling method <CIT> is a simple , but powerful mechanism to machine translation since it can model local reorderings and translations of multiword expressions well '
'In the area of statistical machine translation -LRB- SMT -RRB- , recently a combination of the BLEU evaluation metric <OTH> and the bootstrap method for statistical significance testing <OTH> has become popular <CIT> '
'It can be proven that the probability distribution p satisfying the above assumption is the one with the highest entropy, is unique and has the following expone ntial form (Berger et al. 1996): (1)  = = k j cajf jcZcap 1 ),( )( 1)|( a where Z(c) is a normalization factor, fj(a,c) are the values of k features of the pair (a,c) and correspond to the linguistic cues of c that are relevant to predict the outcome a. Features are extracted from the training data and define the constraints that the probabilistic model p must satisfy.'
'It generates a vector of 5 numeric values for each phrase pair:  phrase translation probability: ( f|e) = count( f, e) count(e),(e| f) = count( f, e) count( f) 2http://www.phramer.org/  Java-based open-source phrase based SMT system 3http://www.isi.edu/licensed-sw/carmel/ 4http://www.speech.sri.com/projects/srilm/ 5http://www.iccs.inf.ed.ac.uk/pkoehn/training.tgz 150  lexical weighting (Koehn et al. , 2003): lex( f|e,a) = nproductdisplay i=1 1 |{j|(i, j)  a}| summationdisplay (i,j)a w(fi|ej) lex(e|f,a) = mproductdisplay j=1 1 |{i|(i, j)  a}| summationdisplay (i,j)a w(ej|fi)  phrase penalty: ( f|e) = e; log(( f|e)) = 1 2.2 Decoding We used the Pharaoh decoder for both the Minimum Error Rate Training (Och, 2003) and test dataset decoding.'
'This algorithm and its many variants are widely used in the computational linguistics community <CIT> '
'32 73 Unknown Words and Parts of Speech When the parser encounters an unknown word , the first-best tag delivered by <CIT> tagger is used '
'Recently , sentiment classification has become popular because of its wide applications <CIT> '
'This information can be annotated reliably -LRB- a1a3a2a5a4a7a6a9a8 a10a12a11a14a13a16a15 and a1a17a2a5a4a19a18a20a8 a10a12a11a14a13a16a21 -RRB- 4 4Following <CIT> , we use the a22 statistic to estimate reliability of annotation '
'a0 subsequence S1 S2 a0 subsequence S1 S2 a0 subsequence S1 S2 Becoming 1 1 Becoming-is a1 a2 a1 a2 astronaut-DREAM 0 a1 a2 DREAM 1 1 Becoming-my a1a4a3a5a1a4a3 astronaut-ambition 0 a1 a2 SPACEMAN 1 1 SPACEMAN-DREAM a1a4a3a5a1 a2 astronaut-is 0 1 a 1 0 SPACEMAN-ambition 0 a1 a2 astronaut-my 0 a1 ambition 0 1 SPACEMAN-dream a1 a3 0 cosmonaut-DREAM a1 a3 0 1 an 0 1 SPACEMAN-great a1 a2 0 cosmonaut-dream a1 a3 0 astronaut 0 1 SPACEMAN-is 1 1 cosmonaut-great a1 a2 0 cosmonaut 1 0 SPACEMAN-my a1a6a1 cosmonaut-is 1 0 dream 1 0 a-DREAM a1 a7 0 cosmonaut-my a1 0great 1 0 a-SPACEMAN 1 0 great-DREAM 1 0 is 1 1 2 a-cosmonaut 1 0 2 great-dream 1 0 my 1 1 a-dream a1 a7 0 is-DREAM a1 a2 a1 Becoming-DREAM a1a4a8a5a1 a7 a-great a1 a3 0 is-ambition 0 a1 Becoming-SPACEMAN a1a6a1 a-is a1 0 is-dream a1 a2 0 Becoming-a 1 0 a-my a1 a2 0 is-great a1 0 Becoming-ambition 0 a1 a7 an-DREAM 0 a1 a3 is-my 1 1 2 Becoming-an 0 1 an-SPACEMAN 0 1 my-DREAM a1 1 Becoming-astronaut 0 a1 an-ambition 0 a1 a3 my-ambition 0 1 Becoming-cosmonaut a1 0 an-astronaut 0 1 my-dream a1 0 Becoming-dream a1a4a8 0 an-is 0 a1 my-great 1 0 Becoming-great a1 a7 0 an-my 0 a1 a2 2002; Lin and Hovy, 2003; Lin, 2004a; Lin, 2004b; Soricut and Brill, 2004).'
'As expected , as we double the size of the data , the BLEU score <CIT> increases '
'In Section 3 we review <CIT> s method for recovering English NLDs in treebank-based LFG approximations '
'The use of structured prediction to SMT is also investigated by <CIT> '
'There has also been previous work on determining whether a given text is factual or expresses opinion <CIT> ; again this work uses a binary distinction , and supervised rather than unsupervised approaches '
'Finally, methods in the literature more focused on a specific disambiguation task include statistical methods for the attachment of hyponyms under the most likely hypernym in the WordNet taxonomy (Snow et al., 2006), structural approaches based on semantic clusters and distance metrics (Pennacchiotti and Pantel, 2006), supervised machine learning methods for the disambiguation of meronymy relations (Girju et al., 2003), etc. 6 Conclusions In this paper we presented a novel approach to disambiguate the glosses of computational lexicons and machine-readable dictionaries, with the aim of alleviating the knowledge acquisition bottleneck.'
'Then we compute the same ratio of machine translation sentence to source sentence , and take the output of p-norm function as a feature : -RRB- __ \/ __ -LRB- -RRB- -LRB- s csrcoflengthtoflenght Ptf norm = -LRB- 7 -RRB- Features based on parse score The usual practice to model the wellformedness of a sentence is to employ the n-gram language model or compute the syntactic structure similarity <CIT> '
'<CIT> and Jing -LRB- 2000 -RRB- propose a cut-and-paste strategy as a computational process of automatic abstracting and a sentence reduction strategy to produce concise sentences '
'In this paper , we employed the Chinese word segmentation tool <OTH> that achieved about 093-096 recall\/precision rates in the SIGHAN-3 word segmentation task <CIT> '
'52 Bleu : Automatic Evaluation BLEU <CIT> is a system for automatic evaluation of machine translation '
'The part of the 1Release 2 of this data set can be obtained t ` rmn the Linguistic Data Consortium with Catalogue number LDC94T4B -LRB- http://wwwldcupennedu/ldc/nofranmhtml -RRB- 2There are 48 labels defined in <CIT> , however , three of ttmm do not appear in the corpus '
'<OTH> , we used the MXPOST <CIT> tagger trained on training data to provide part-of-speech tags for the development and the test set , and we used 10way jackknifing to generate tags for the training set '
'4.1 Baseline Our baseline system is a fairly typical phrasebased machine translation system (Finch and Sumita, 2008a) built within the framework of a feature-based exponential model containing the following features: Table 1: Language Resources Corpus Train Dev Eval NC Spanish sentences 74K 2,001 2,007 words 2,048K 49,116 56,081 vocab 61K 9,047 8,638 length 27.6 24.5 27.9 OOV (%)  5.2 / 2.9 1.4 / 0.9 English sentences 74K 2,001 2,007 words 1,795K 46,524 49,693 vocab 47K 8,110 7,541 length 24.2 23.2 24.8 OOV (%)  5.2 / 2.9 1.2 / 0.9 perplexity  349 / 381 348 / 458 EP Spanish sentences 1,404K 1,861 2,000 words 41,003K 50,216 61,293 vocab 170K 7,422 8,251 length 29.2 27.0 30.6 OOV (%)  2.4 / 0.1 2.4 / 0.2 English sentences 1,404K 1,861 2,000 words 39,354K 48,663 59,145 vocab 121K 5,869 6,428 length 28.0 26.1 29.6 OOV (%)  1.8 / 0.1 1.9 / 0.1 perplexity  210 / 72 305 / 125 Table 2: Testset 2009 Corpus Test NC Spanish sentences 3,027 words 80,591 vocab 12,616 length 26.6  Source-target phrase translation probability  Inverse phrase translation probability  Source-target lexical weighting probability  Inverse lexical weighting probability  Phrase penalty  Language model probability  Lexical reordering probability  Simple distance-based distortion model  Word penalty For the training of the statistical models, standard word alignment (GIZA++ (Och and Ney, 2003)) and language modeling (SRILM (Stolcke, 2002)) tools were used.'
'The classifier uses mutual information -LRB- MI -RRB- scores rather than the raw frequences of the occurring patterns <CIT> '
'31 Golden-standard-based criteria In the domain of machine translation systems , an increasingly accepted way to measure the quality of a system is to compare the outputs it produces with a set of reference translations , considered as an approximation of a golden standard <CIT> '
'Also , PMI-IR is useful for calculating semantic orientation and rating reviews <CIT> '
'Using this heuristic , BABAR identifies existential definite NPs in the training corpus using our previous learning algorithm <CIT> and resolves all occurrences of the same existential NP with each another '
'6 Related Work A description of the IBM models for statistical machine translation can be found in <CIT> '
'This is similar to work by several other groups which aims to induce semantic classes through syntactic co-occurrence analysis <CIT> , although in our case the contexts are limited to selected patterns , relevant to the scenario '
'<CIT> proposed a structured model based on CRFs for jointly classifying the sentiment of text at varying levels of granularity '
'164 and Itai , 1990 ; Dagan et al , 1995 ; Kennedy and Boguraev , 1996a ; Kennedy and Boguraev , 1996b -RRB- '
'We then scored each query pair -LRB- q1 , q2 -RRB- in this subset using the log-likelihood ratio <CIT> between q1 and q2 , which measures the mutual dependence within the context of web search queries <OTH> '
'Using the log-linear form to model p -LRB- e f -RRB- gives us the flexibility to introduce overlapping features that can represent global context while decoding -LRB- searching the space of candidate translations -RRB- and rescoring -LRB- ranking a set of candidate translations before performing the argmax operation -RRB- , albeit at the cost of the traditional source-channel generative model of translation proposed in <CIT> '
'Research in the first category aims to identify specific types of nonanaphoric phrases , with some identifying pleonastic it -LRB- using heuristics -LRB- eg , Paice and Husk <OTH> , Lappin and Leass <OTH> , Kennedy and Boguraev <OTH> -RRB- , supervised approaches -LRB- eg , Evans <OTH> , Muller <OTH> , Versley et al '
'Relative frequency ratio -LRB- RFR -RRB- of terms between two different corpora can also be used to discover domain-oriented multi-word terms that are characteristic of a corpus when compared with another <OTH> '
'Semantic features are used for classifying entities into semantic types such as name of person , organization , or place , while syntactic features characterize the kinds of dependency 5It is worth noting that the present approach can be recast into one based on constraint relaxation <CIT> '
'Other factors that distinguish us from previous work are the use of all phrases proposed by a phrase-based system , and the use of a dependency language model that also incorporates constituent information -LRB- although see <CIT> for related approaches -RRB- '
'Discriminative training with hidden variables has been handled in this probabilistic framework <CIT> , but we choose Equation 3 for efficiency '
'Dependency representation has been used for language modeling , textual entailment and machine translation <CIT> , to name a few tasks '
'Many adaptation methods operate by simple augmentations of the target feature space , as we have donehere <OTH> '
'17 The justification for this is that there is an estimated 3 % error rate in the hand-assigned POS tags in the treebank <CIT> , and we didnt want this noise to contribute to dependency errors '
'We develop this intuition into a technique called synchronous binarization <CIT> which binarizes a synchronous production or treetranduction rule on both source and target sides simultaneously '
'To deal with this question , we use ATIS p-o-s trees as found in the Penn Treebank <CIT> '
'In this paper we present MapReduce implementations of training algorithms for two kinds of models commonly used in statistical MT today : a phrasebased translation model <CIT> and word alignment models based on pairwise lexical translation trained using expectation maximization <OTH> '
'Then the two models and a search module are used to decode the best translation <CIT> '
'Graph-based algorithms for classification into subjective\/objective or positive\/negative language units have been mostly used at the sentence and document level <CIT> , instead of aiming at dictionary annotation as we do '
'We used the heuristic combination described in <CIT> and extracted phrasal translation pairs from this combined alignment as described in <OTH> '
'Most work on corpora of naturally occurring language 244 Michael R Brent From Grammar to Lexicon either uses no a priori grammatical knowledge <OTH> , or else it relies on a large and complex grammar <CIT> '
'<OTH> -RRB- -RRB- , and others identifying non-anaphoric definite descriptions -LRB- using rule-based techniques -LRB- eg , Vieira and Poesio <OTH> -RRB- and unsupervised techniques -LRB- eg , <CIT> -RRB- -RRB- '
'In the following section , we follow the notation in <CIT> '
'Several approaches have been proposed in the context of word sense disambiguation <CIT> , named entity -LRB- NE -RRB- classification <OTH> , patternacquisitionforIE <OTH> , or dimensionality reduction for text categorization -LRB- TC -RRB- <OTH> '
'3 Margin Perceptron Algorithm for Sequence Labeling Weextendedaperceptronwithamargin <OTH> to sequence labeling in this study , as <CIT> extended the perceptron algorithm to sequence labeling '
'Our work in sentence reformulation is different from cut-and-paste summarization <CIT> in many ways '
'Statistical approaches , which depend on a set of unknown parameters that are learned from training data , try to describe the relationship between a bilingual sentence pair <OTH> '
'To compute the degree of interaction between two proteins D4 BD and D4 BE , we use the information-theoretic measure of pointwise mutual information <CIT> , which is computed based on the following quantities : 1 '
'Most clustering schemes <OTH> use the average entropy reduction to decide when two words fall into the same cluster '
'If we assign a probability a13a15a14a17a16 a10a12a11a5a19a18a2 a3a5a21a20 to each pair of strings a16 a10 a11a5a12a22 a2a4a3a5 a20, then according to Bayes decision rule, we have to choose the English string that maximizes the product of the English language model a13a23a14a24a16 a10 a11a5 a20 and the string translation model a13a15a14a17a16a25a2 a3a5a26a18a10a27a11a5a28a20 . Many existing systems for statistical machine translation (Wang and Waibel, 1997; Nieen et al. , 1998; Och and Weber, 1998) make use of a special way of structuring the string translation model like proposed by (Brown et al. , 1993): The correspondence between the words in the source and the target string is described by alignments which assign one target word position to each source word position.'
'The Spanish corpus was parsed using the MST dependency parser <OTH> trained using dependency trees generated from the the English Penn Treebank <CIT> and Spanish CoNLL-X data <OTH> '
'It is today common practice to use phrases as translation units <CIT> instead of the original word-based approach '
'This setting is reminiscent of the problem of optimizing feature weights for reranking of candidate machine translation outputs , and we employ an optimization technique similar to that used by <CIT> for machine translation '
'For more detail , explanations and experiments see <CIT> '
'51 Agreement between translators In an attempt to quantify the agreement between the two groups of translators , we computed the Kappa coefficient for annotation tasks , as defined by <CIT> '
'Using the ME principle , we can combine information from a variety of sources into the same language model <CIT> '
'CRF (baseline)] 97.18 97.21  Table 7: POS tagging results of the previous top systems for PTB III data evaluated by label accuracy system test additional resources JESS-CM (CRF/HMM) 95.15 1G-word unlabeled data 94.67 15M-word unlabeled data (Ando and Zhang, 2005) 94.39 15M-word unlabeled data (Suzuki et al., 2007) 94.36 17M-word unlabeled data (Zhang et al., 2002) 94.17 full parser output (Kudo and Matsumoto, 2001) 93.91  [supervised CRF (baseline)] 93.88  Table 8: Syntactic chunking results of the previous top systems for CoNLL00 shared task data (F=1 score) 30-31 Aug. 1996 and 6-7 Dec. 1996 Reuters news articles, respectively.'
'Note that generative hybrids are the norm in SMT , where translation scores are provided by a discriminative combination of generative models <CIT> '
'For instance , on unsupervised part-ofspeech tagging , EM requires over 100 iterations to reach its peak performance on the Wall-Street Journal <CIT> '
'When evaluated against the state-of-the-art, phrase-based decoder Pharaoh (Koehn, 2004), using the same experimental conditions  translation table trained on the FBIS corpus (7.2M Chinese words and 9.2M English words of parallel text), trigram language model trained on 155M words of English newswire, interpolation weights a65 (Equation 2) trained using discriminative training (Och, 2003) (on the 2002 NIST MT evaluation set), probabilistic beam a90 set to 0.01, histogram beam a58 set to 10  and BLEU (Papineni et al. , 2002) as our metric, the WIDL-NGLM-Aa86 a129 algorithm produces translations that have a BLEU score of 0.2570, while Pharaoh translations have a BLEU score of 0.2635.'
'These transtbr rules are pairs of corresponding rooted substructures , where a substructure <OTH> is a connected set of arcs and nodes '
'Therefore , <CIT> introduced skip-bigram statistics for the evaluation of machine translation '
'<CIT> and manual methods '
'And indeed , the agreement figures went up from K = 063 to K = 068 -LRB- ignoring doubts -RRB- when we did so , ie , within the ` tentative '' margins of agreement according to <CIT> -LRB- 068 -LRB- _ x -LRB- 08 -RRB- '
'The reason may be that shorter dependencies are often modifier of nouns such as determiners or adjectives or pronouns modifying their direct neighbors , while longer dependencies typically represent modifiers of the root or the main verb in a sentenc <CIT> '
'For the MER training <CIT> , we modify Koehns MER trainer <OTH> to train our system '
'The parameters of the refined productions Ax By Cz , where Ax is a subcategory of A , By of B , and Cz of C , can then be estimated in various ways ; past work has included both generative <CIT> and discriminative approaches <OTH> '
'In the work of <CIT> on extracting collocations , preference was given to constructions whose constituents appear in a fixed order , a similar -LRB- and more generally implemented -RRB- version of our assumption here that asymmetric constructions are more idiomatic than symmetric ones '
'4 Structural Correspondence Learning SCL -LRB- Structural Correspondence Learning -RRB- <CIT> is a recently proposed domain adaptation technique which uses unlabeled data from both source and target domain to learn correspondences between features from different domains '
'al 2006 -RRB- , we are interested in applying alternative metrics such a Meteor <CIT> '
'52 Assigning complex ambiguity tags In the tagging literature -LRB- eg , <CIT> -RRB- an ambiguity class is often composed of the set of every possible tag for a word '
'Finally , we plan to apply the model to other paraphrasing tasks including fully abstractive document summarisation <CIT> '
'The other 5 have been suggested for Dutch by <OTH> '
'We performed a comparison between the existing CFG filtering techniques for LTAG <OTH> and HPSG <OTH> , using strongly equivalent grammars obtained by converting LTAGs extracted from the Penn Treebank <CIT> into HPSG-style '
'For the MER training <CIT> , Koehns MER trainer <OTH> is modified for our system '
'Therefore , estimating a natural language model based on the maximum entropy -LRB- ME -RRB- method <CIT> has been highlighted recently '
'953 2 Bilexicalization of Inversion Transduction Grammar The Inversion Transduction Grammar of <CIT> models word alignment between a translation pair of sentences by assuming a binary synchronous tree on top of both sides '
'Even before the 2006 shared task , the parsers of <CIT> and Charniak -LRB- 2000 -RRB- , originally developed for English , had been adapted for dependency parsing of Czech , and the parsing methodology proposed by Kudo and Matsumoto <OTH> and Yamada and Matsumoto <OTH> had been evaluated on both Japanese and English '
'A few studies <CIT> addressed this defect by selecting the appropriate translation rules for an input span based on its context in the input sentence '
'We first determine lexical heads of nonterminal nodes by using Bikels implementation of Collins head detection algorithm9 <CIT> '
'For instance , word alignment models are often trained using the GIZA + + toolkit <OTH> ; error minimizing training criteria such as the Minimum Error Rate Training <OTH> are employed in order to learn feature function weights for log-linear models ; and translation candidates are produced using phrase-based decoders <CIT> in combination with n-gram language models <OTH> '
'But the lack of corpora has led to a situation where much of the current work on parsing is performed on a single domain using training data from that domain the Wall Street Journal -LRB- WSJ -RRB- section of the Penn Treebank <CIT> '
'In <OTH> , as well as other similar works <CIT> , only left-toright search was employed '
'This operation does not change the collection of phrases or rules extracted from a hypothesized alignment , see , for instance , <CIT> '
'We tested the techniques described above with the previous Bakeoffs data5 <CIT> '
'The modify features involve the dependency parse tree for the sentence , obtained by first parsing the sentence <CIT> and then converting the tree into its dependency representation <OTH> '
'In a factored translation model other factors than surface form can be used , such as lemma or part-of-speech <CIT> '
'Therefore , P -LRB- g l e -RRB- is the sum of the probabilities of generating g from e over all possible alignments A , in which the position i in the target sentence g is aligned to the position ai in the source sentence e : P -LRB- gle -RRB- = I l m e ~ , ~ '' IT t -LRB- g # le = jla -LRB- a ~ Ij , l , m -RRB- = al = 0 amm0j = l m ! e 1 '' I ~ t -LRB- g # l e , -RRB- a -LRB- ilj , t , m -RRB- -LRB- 3 -RRB- j = l i = 0 <CIT> also described how to use the EM algorithm to estimate the parameters a -LRB- i I j , l , m -RRB- and $ -LRB- g I e -RRB- in the aforementioned model '
'12 As such , we resort to an approximation : Voted Perceptron training <CIT> '
'(see Brown et al. , 1993 for a detailed mathematical description of the model and the formula for computing the probability of an alignment and target string given a source string).'
'Our evaluation metric is case-insensitive BLEU-4 <CIT> , as defined by NIST , that is , using the shortest -LRB- as opposed to closest -RRB- reference sentence length for the brevity penalty '
'211 Pointwise Mutual Information This measure for word similarity was first used in this context by <CIT> '
'2 Related Work The issue of MWE processing has attracted much attention from the Natural Language Processing -LRB- NLP -RRB- community , including Smadja , 1993 ; Dagan and Church , 1994 ; Daille , 1995 ; 1995 ; McEnery et al , 1997 ; Wu , 1997 ; Michiels and Dufour , 1998 ; Maynard and Ananiadou , 2000 ; Merkel and Andersson , 2000 ; Piao and McEnery , 2001 ; Sag et al , 2001 ; Tanaka and Baldwin , 2003 ; Dias , 2003 ; Baldwin et al , 2003 ; Nivre and Nilsson , 2004 Pereira et al , '
'405 PRF 1 proposed 383 437 408 multinomial mixture 360 374 367 Newman <OTH> 318 353 334 cosine 603 114 192 - skew divergence <OTH> 730 155 255 Lins similarity <CIT> 691 096 169 CBC <CIT> 981 060 114 Table 3 : Precision , recall , and F-measure '
'The model parameters are trained using minimum error-rate training <CIT> '
'Forced decoding arises in online discriminative training , where model updates are made toward the most likely derivation of a gold translation <CIT> '
'We used treebank grammars induced directly from the local trees of the entire WSJ section of the Penn Treebank <CIT> -LRB- release 3 -RRB- '
'<CIT> finds significant bigrams using an estimate of z-score -LRB- deviation from an expected mean -RRB- '
'The results so far mainly come from studies where a parser originally developed for English , such as the Collins parser <CIT> , is applied to a new language , which often leads to a signicant decrease in the measured accuracy <CIT> '
'Tag test data using the POS-tagger described in <CIT> '
'The third estimates the equivalence based on word alignment composed using templates or translation probabilities derived from a set of parallel text <CIT> '
'Classes can be induced directly from the corpus using distributional clustering <CIT> or taken from a manually crafted taxonomy <OTH> '
'Table 2 shows the dependency accuracy , root accuracy and complete match scores for our best parser -LRB- Model 2 with label set B -RRB- in comparison with <CIT> -LRB- Model 3 -RRB- , Charniak -LRB- 2000 -RRB- , and Yamada and Matsumoto <OTH> 5 It is clear that , with respect to unlabeled accuracy , our parser does not quite reach state-of-the-art performance , even if we limit the competition to deterministic methods such as that of Yamada and Matsumoto <OTH> '
'Within NLP , applications include sentiment-analysis problems <CIT> and content selection for text generation <OTH> '
'Some researchers apply shallow or partial parsers <CIT> to acquiring specific patterns from texts '
'The task of classifying several different uses of definite descriptions <CIT> is somewhat analogous to that for bare nouns '
'This results in two forbidden alignment structures , shown in Figure 1 , called inside-out transpositions in <CIT> '
'In <CIT> the model is optimized to produce a block orientation and the target sentence is used only for computing a sentence level BLEU '
'For extracting simple noun phrases we first used Ramshaw and Marcuss base NP chunker <CIT> '
're-ranking 1 uses the score of the rst model as a feature in addition to the non-local features as in <CIT> '
'The optimal bilingual parsing tree for a given sentence-pair can be computed using dynamic programming -LRB- DP -RRB- algorith <CIT> '
'The hierarchical phrase translation pairs are extracted in a standard way <OTH> : First , the bilingual data are word alignment annotated by running GIZA + + <CIT> in two directions '
'We were already using a generative statistical model for part-of-speech tagging <OTH> , and more recently , had begun using a generative statistical model for name finding <OTH> '
'Motivated by our goal of representing syntax , we used part-of-speech -LRB- POS -RRB- tags as labeled by a maximum entropy tagger <CIT> '
'<OTH> , but its performance was worse than our centroid baseline '
'12 Evaluation In this paper we report results using the BLEU metric <CIT> , however as the evaluation criterion in GALE is HTER <OTH> , we also report in TER <OTH> '
'24 METEOR Given a pair of strings to compare -LRB- a system translation and a reference translation -RRB- , METEOR <CIT> first creates a word alignment between the two strings '
'For this present work , we use Dunnings log-likelihood ratio statistics <CIT> defined as follows : sim = aloga + blogb + clogc + dlogd -LRB- a + b -RRB- log -LRB- a + b -RRB- -LRB- a + c -RRB- log -LRB- a + c -RRB- -LRB- b + d -RRB- log -LRB- b + d -RRB- -LRB- c + d -RRB- log -LRB- c + d -RRB- + -LRB- a + b + c + d -RRB- log -LRB- a + b + c + d -RRB- For each bilingual pattern EiJj , we compute its similarity score and qualify it as a bilingual sequence-to-sequence correspondence if no equally strong or stronger association for monolingual constituent is found '
'Mathematical details are fully described in <CIT> '
'Following the broad shift in the field from finite state transducers to grammar transducers <OTH> , recent approaches to phrase-based alignment have used synchronous grammar formalisms permitting polynomial time inference <CIT> '
'In the domain adaptation track , participants were provided with English training data from the Wall Street Journal portion of the Penn Treebank <CIT> converted to dependencies <OTH> to train parsers to be evaluated on material in the biological -LRB- development set -RRB- and chemical -LRB- test set -RRB- domains <OTH> , and optionally on text from the CHILDES database <OTH> '
'31 Definition The following set-up , adapted from <CIT> , was used for all three discriminative training methods : 266 Training data is a set of input-output pairs '
'The parameters , j , were trained using minimum error rate training <OTH> to maximize the BLEU score <CIT> on a 150 sentence development set '
'Typically , a phrase-based SMT system includes a feature that scores phrase pairs using lexical weights <CIT> which are computed for two directions : source to target and target to source '
'As was demonstrated in <CIT> , even a minimal set of local explicit features achieves results which are non-significantly different from a carefully chosen set of explicit features , given the language independent definition of locality described in section 2 '
'The Penn Treebank results reported here for the Markov model approach are at least equivalent to those reported for the Maximum Entropy approach in <CIT> '
'Although a large number of studies have been made on learning paraphrases , for example <CIT> , there are only a few studies which address the connotational difference of paraphrases '
'3 Bi-Stream HMMs for Transliteration Standard IBM translation models <CIT> can be used to obtain letter-to-letter translations '
'It turns out that while problems of coverage and ambiguity prevent straightforward lookup , injection of gazetteer matches as features in machine-learning based approaches is critical for good performance <CIT> '
'3 The Framework 31 The Algorithm Our transductive learning algorithm , Algorithm 1 , is inspired by the Yarowsky algorithm <CIT> '
'42 Binarization Schemes Besides the baseline <CIT> and iterative cost reduction binarization methods , we also perform right-heavy and random synchronous binarizations for comparison '
'Independently , in AI an effort arose to encode large amounts of commonsense knowledge <CIT> '
'1 The Baseline Maximum Entropy Model We started with a maximum entropy based tagger that uses features very similar to the ones proposed in <CIT> '
'We collect substring rationales for a sentiment classification task <CIT> and use them to obtain significant accuracy improvements for each annotator '
'Chunking For NP chunking , <OTH> used data extracted from section 15-18 of the WSJ as a fixed train set and section 20 as a fixed test set , the same data as <CIT> '
'Firstly , we run GIZA + + <CIT> on the training corpus in both directions and then apply the ogrow-diag-finalprefinement rule <OTH> to obtain many-to-many word alignments '
'In the multilingual parsing track , participants train dependency parsers using treebanks provided for ten languages : Arabic <OTH> , Basque <OTH> , Catalan <OTH> , Chinese <OTH> , Czech <OTH> , English <CIT> , Greek <OTH> , Hungarian <OTH> , Italian <OTH> , and Turkish <OTH> '
'<CIT> , 1996 -RRB- , a single inconsistency in a test set tree will very likely yield a zero percent parse accuracy for the particular test set sentence '
'32 -LRB- m , n -RRB- - cousin Classification The classifier for learning coordinate terms relies on the notion of distributional similarity , ie , the idea that two words with similar meanings will be used in similar contexts <CIT> '
'5-gram word language models in English are trained on a variety of monolingual corpora <CIT> '
'2 Related Work There has been a large and diverse body of research in opinion mining , with most research at the text <CIT> , sentence <OTH> or word <OTH> level '
'This was overcome by a probabilistic model which provides probabilities of discriminating a correct parse tree among candidates of parse trees in a log-linear model or maximum entropy model <CIT> with many features for parse trees <OTH> '
'2.2 Automatic evaluation metric Since the official evaluation criterion for WMT09 is human sentence ranking, we chose to minimize a linear combination of two common evaluation metrics, BLEU and TER (Papineni et al., 2002; Snover et al., 2006), during system development and tuning: TERBLEU 2 Although we are not aware of any work demonstrating that this combination of metrics correlates better than either individually in sentence ranking, Yaser Al-Onaizan (personal communication) reports that it correlates well with the human evaluation metric HTER.'
'Examples of such contexts are verb-object relations and noun-modifier relations , which were traditionally used in word similarity tasks from non-parallel corpora <OTH> '
'The refined grammar is estimated using a variant of the forward-backward algorithm <CIT> '
'(McDonald and Satta, 2007; Smith and Smith, 2007).'
'<OTH> and <CIT> discuss different ways of generalizing the tree-level crosslinguistic correspondence relation , so it is not confined to single tree nodes , thereby avoiding a continuity assumption '
'1 Introduction : Defining SCMs The work presented here was done in the context of phrase-based MT <CIT> '
'This model is very similar to <CIT> '
'We also report the result of our translation quality in terms of both BLEU <CIT> and TER <OTH> against four human reference translations '
'One could use the estimated co-occurrences from a small sample to compute the test statistics , most commonly Pearsons chi-squared test , the likelihood ratio test , Fishers exact test , cosine similarity , or resemblance -LRB- Jaccard coefficient -RRB- <CIT> '
'Similar observations have been made in the context of tagging problems using maximum-entropy models <CIT> '
'Since most phrases appear only a few times in training data , a phrase pair translation is also evaluated by lexical weights <CIT> or term weighting <OTH> as additional features to avoid overestimation '
'2 Our statistical engine 21 The statistical models In this study , we built an SMT engine designed to translate from French to English , following the noisy-channel paradigm flrst described by <CIT> '
'A la <CIT> , and Kudo and Matsumato -LRB- 2000 -RRB- , we use the IOB tagging style for modeling and classification '
'CIT -RRB- '
'5 External Knowledge Sources 51 Lexical Dependencies Features derived from n-grams of words and tags in the immediate vicinity of the word being tagged have underpinned the world of POS tagging for many years <CIT> , and have proven to be useful features in WSD <OTH> '
'NULL -RRB- Compared with the B-Chunk and I-Chunk used in <CIT> , structural relations 99 and 90 correspond to B-Chunk which represents the first word of the chunk , and structural relations 00 and 09 correspond to I-Chunk which represnts each other in the chunk while 90 also means the beginning of the sentence and 09 means the end of the sentence '
'We trained three Arabic-English syntax-based statistical MT systems <CIT> using max-B training <OTH> : one on a newswire development set , one on a weblog development set , and one on a combined development set containing documents from both genres '
'The reliability for the two annotation tasks -LRB- - statistics <CIT> -RRB- was of 094 and 090 respectively '
'Different models have been presented in the literature , see for instance <CIT> '
'Thus , our generative model is a quasi-synchronous grammar , exactly as in <CIT> 3 When training on target sentences w , therefore , we tune the model parameters to maximize notsummationtextt p -LRB- t , w -RRB- as in ordinary EM , but rather 3Our task here is new ; they used it for alignment '
'We then train IBM models <CIT> using the GIZA + + package <OTH> '
'<CIT> shows that parsing a binary SCFG is in O -LRB- w 6 -RRB- while parsing SCFG is NP-hard in general <OTH> '
'Beyond WordNet <OTH> , a wide range of resources has been developed and utilized , including extensions to WordNet <CIT> and resources based on automatic distributional similarity methods <OTH> '
'The model was trained on sections 221 from the English Penn Treebank <CIT> '
'The modifications are made to deal with the efficiency issue due to the fact that there is a very large number of features and training samples in our task , compared to only 8 features used in <CIT> '
'Let W1 , W2 be the vocabulary sizes of the two languages , and N = -LCB- A1 , , AN -RCB- be the set of nonterminals with indices 1 , , N <CIT> also showed that ITGs can be equivalently be defined in two other ways '
'It is shown that -LRB- 2,2 -RRB- - BRCGs induce inside-out alignments <CIT> and cross-serial discontinuous translation units -LRB- CDTUs -RRB- ; both phenomena can be shown to occur frequently in many hand-aligned parallel corpora '
'ps -LRB- arc -RRB- is increased by 1110 1 \/ -LRB- k +1 -RRB- if the hypothesis ranking k in the system s contains the arc <CIT> '
'1 Introduction Word alignment is a critical component in training statistical machine translation systems and has received a significant amount of research , for example , <CIT> , including work leveraging syntactic parse trees , eg , <OTH> '
'21 Word Sequence Classification Similar to English text chunking <CIT> , the word sequence classification model aims to classify each word via encoding its context features '
'Finally , following Haghighi and Klein <OTH> and <CIT> we can instead insist that at most one HMM state can be mapped to any part-of-speech tag '
'WLCS -LRB- w , d -RRB- = summationtextmi = 0 f -LRB- ki -RRB- We then compute the following quantities , where is word length , and f1 is the inverse of f P -LRB- w , d -RRB- = f1 -LRB- WLCS -LRB- w , d -RRB- f -LRB- w -RRB- -RRB- R -LRB- w , d -RRB- = f1 -LRB- WLCS -LRB- w , d -RRB- f -LRB- d -RRB- -RRB- F -LRB- w , d -RRB- = -LRB- 1 +2 -RRB- R -LRB- w , d -RRB- P -LRB- w , d -RRB- R -LRB- w , d -RRB- +2 P -LRB- w , d -RRB- In effect , P -LRB- w , d -RRB- examines how close the longest common substring is to w and R -LRB- w , d -RRB- how close it is to d Following <CIT> , we use = 8 , assigninggreaterimportancetoR -LRB- w , d -RRB- '
'Our corpora were automatically aligned with Giza + + <OTH> in both directions between source and target and symmetrised using the intersection heuristic <CIT> '
'The initial state contains terminal items , whose labels are the POS tags given by the tagger of <CIT> '
'Previous approaches include supervised learning <OTH> , <OTH> , vectorial similarity computed between an initial abstract and sentences in the given document , intradocument similarities <OTH> , or graph algorithms <CIT> , <OTH> , <OTH> '
'We then describe the two main paradigms for learning and inference , in this years shared task as well as in last years , which we call transition-based parsers -LRB- section 52 -RRB- and graph-based parsers -LRB- section 53 -RRB- , adopting the terminology of <CIT> 5 Finally , we give an overview of the domain adaptation methods that were used -LRB- section 54 -RRB- '
'This is similar to Model 3 of <CIT> , but without null-generated elements or re-ordering '
'<OTH> , <OTH> , <OTH> , <OTH> , <OTH> , <OTH> -RRB- , and to pick those ingredients which are known to be con ~ i -RRB- utationally ` tractable '' in some sense '
'From this data , we use the the GHKM minimal-rule extraction algorithm of <CIT> to yield rules like : NP-C -LRB- x0 : NPB PP -LRB- IN -LRB- of x1 : NPB -RRB- -RRB- $ x1 de x0 Though this rule can be used in either direction , here we use it right-to-left -LRB- Chinese to English -RRB- '
'In the years since the appearance of the first papers on using statistical models for bilingual lexicon compilation and machine translation <CIT> , large amount of human effort and time has been invested in collecting parallel corpora of translated texts '
'Much previous work on unsupervised grammar induction has used gold-standard partof-speech tags <CIT> '
'The translation model used in (Koehn et al. , 2003) is the product of translation probability a34a35a4 a29 a0 a33 a6 a29 a2 a33 a8 and distortion probability a36a37a4a39a38 a33a41a40a43a42a44a33a46a45 a32 a8, a3a5a4a35a29 a0 a30 a32 a6 a29 a2 a30 a32 a8 a10 a30 a47 a33a49a48 a32 a34a35a4 a29 a0a22a33 a6 a29 a2 a33a50a8 a36a51a4a39a38 a33 a40a52a42 a33a53a45 a32 a8 (1) where a38 a33 denotes the start position of the source phrase translated into the a54 -th target phrase, and a42 a33a53a45 a32 denotes the end position of the source phrase translated into the a4a53a54 a40a56a55 a8 -th target phrase.'
'All corpora are formatted in the IOB sequence representation <CIT> '
'For a full discussion of previous work , please see <OTH> , or see <CIT> for work relating to synonym resolution '
'37 3 Semi-supervised Domain Adaptation 31 Structural Correspondence Learning Structural Correspondence Learning <CIT> exploits unlabeled data from both source and target domain to find correspondences among features from different domains '
'The syntactic and part-of-speech informations were obtained from the part of the corpus processed in the Penn Treebank project <CIT> '
'The recall problem is usually addressed by increasing the amount of text data for extraction -LRB- taking larger collections <CIT> -RRB- or by developing more surface patterns <OTH> '
'The sentences were processed using Collins parser <CIT> to generate parse-trees automatically '
'<OTH> on CTB 50 and <CIT> on CTB 40 since they reported the best performances on joint word segmentation and POS tagging using the training materials only derived from the corpora '
'Our method is based on the Extended String Subsequence Kernel -LRB- ESK -RRB- <OTH> which is a kind of convolution kernel <OTH> '
'<CIT> -RRB- '
'Related work includes <CIT> , Zens and Ney -LRB- 2003 -RRB- and Wellington et al '
'Lastly , collocations are domain-dependent <CIT> and language-dependent '
'To further emphasize the importance of morphology in MT to Czech , we compare the standard BLEU <CIT> of a baseline phrasebased translation with BLEU which disregards word forms -LRB- lemmatized MT output is compared to lemmatized reference translation -RRB- '
'Word features are introduced primarily to help with unknown words , as in <OTH> '
'As a basis mapping function we used a generalisation of the one used by Grefenstette <OTH> and <CIT> '
'Given a set of features and a training corpus , the MaxEnt estimation process produces a model in which every feature fi has a weight i We can compute the conditional probability as <CIT> : p -LRB- o h -RRB- = 1Z -LRB- h -RRB- productdisplay i ifi -LRB- h , o -RRB- -LRB- 1 -RRB- Z -LRB- h -RRB- = summationdisplay o productdisplay i ifi -LRB- h , o -RRB- -LRB- 2 -RRB- The conditional probability of the outcome is the product of the weights of all active features , normalized over the products of all the features '
'As discussed in <CIT> , undirected graphical models do not seem to be suitable for history-based parsing models '
'One is to use a stochastic gradient descent -LRB- SGD -RRB- or Perceptron like online learning algorithm to optimize the weights of these features directly for MT <CIT> '
'For instance , one might be interested in frequencies of co-occurences of a word with other words and phrases -LRB- collocations -RRB- <CIT> , or one might be interested in inducing wordclasses from the text by collecting frequencies of the left and right context words for a word in focus <OTH> '
'Similar to Goldwater and Griffiths <OTH> and <CIT> <OTH> , Toutanova and <CIT> <OTH> also use Bayesian inference for POS tagging '
'Starting from the parallel training corpus , provided with direct and inverted alignments , the socalled union alignment <CIT> is computed '
'While theoretically sound , this approach is computationally challenging both in practice <OTH> and in theory <OTH> , may suffer from reference reachability problems <OTH> , and in the end may lead to inferior translation quality <CIT> '
'Proceedings of the 40th Annual Meeting of the Association for In a key step for locating important sentences , NeATS computes the likelihood ratio <CIT> to identify key concepts in unigrams , bigrams , and trigrams1 , using the ontopic document collection as the relevant set and the off-topic document collection as the irrelevant set '
'5 Results We present results that compare our system against the baseline Pharaoh implementation <CIT> and MER training scripts provided for this workshop '
'Statistical approaches , which depend on a set of unknown parameters that are learned from training data , try to describe the relationship between a bilingual sentence pair <CIT> '
'42 Impact of Paraphrases on Machine Translation Evaluation The standard way to analyze the performance of an evaluation metric in machine translation is to compute the Pearson correlation between the automatic metric and human scores <CIT> '
'In fact , many studies that try to exploit Wikipedia as a knowledge source have recently emerged <CIT> '
'These texts were not seen at the training phase which means that neither the 6Since Brill ''s tagger was trained on the Penn tag-set <CIT> we provided an additional mapping '
'also <CIT> and Nivre , 2007 -RRB- '
'4 Corpus Annotation For our corpus , we selected 1,000 sentences containing at least one comma from the Penn Treebank <CIT> WSJ section 00 , and manually annotated them with comma information3 '
'But we did not use any LM estimate to achieve early stopping as suggested by <CIT> '
'More details on these standard criteria can be found for instance in <CIT> '
'3However , the binary-branching SCFGs used by <CIT> and Alshawi et al '
'Recently there have been some studies addressing domain adaptation from different perspectives <CIT> '
'Works on word similarity and word sense disambiguation are generally based on statistical methods designed for large or even very large corpora <CIT> '
'In this paper we use the phrase-based system of <CIT> as our underlying model '
'This sequential property is well suited to HMMs <OTH> , in which the jumps from the current aligned position can only be forward '
'Thus the alignment set is denoted as -RCB- & -RRB- ,1 -LRB- -RRB- , -LCB- -LRB- ialiaiA ii = We adapt the bilingual word alignment model , IBM Model 3 <CIT> , to monolingual word alignment '
'In particular , we use a feature augmentation technique recently introduced by <CIT> , and active learning <OTH> to perform domain adaptation of WSD systems '
'In this paper , we give an overview of NLPWin , a multi-application natural language analysis and generation system under development at Microsoft Research <OTH> , incorporating analysis systems for 7 languages -LRB- Chinese , English , French , German , Japanese , Korean and Spanish -RRB- '
'2 Background 21 Previous Work 211 Research on Phrase-Based SMT The original work on statistical machine translation was carried out by researchers at IBM <CIT> '
'However , while discriminative models promise much , they have not been shown to deliver significant gains 1We class approaches using minimum error rate training <CIT> frequency count based as these systems re-scale a handful of generative features estimated from frequency counts and do not support large sets of non-independent features '
'Recently , many phrase reordering methods have been proposed , ranging from simple distancebased distortion model <OTH> , flat reordering model <CIT> , lexicalized reordering model <OTH> , to hierarchical phrase-based model <OTH> and classifier-based reordering model with linear features <OTH> '
'Examples include Wus <CIT> ITG and Chiangs hierarchical models <OTH> '
'This is possible because of the availability of statistical parsers , which can be trained on human-annotated treebanks <CIT> for multiple languages ; -LRB- 2 -RRB- The binding theory is used as a guideline and syntactic structures are encoded as features in a maximum entropy coreference system ; -LRB- 3 -RRB- The syntactic features are evaluated on three languages : Arabic , Chinese and English -LRB- one goal is to see if features motivated by the English language can help coreference resolution in other languages -RRB- '
'Much like kappa statistics proposed by <CIT> , existing employments of majority class baselines assume an equal set of identical potential mark-ups , ie attributes and their values , for all markables '
'A very common case of this in the CoNLL dataset is that of documents containing references to both The China Daily , a newspaper , and China , the country <CIT> '
'Many of the previous studies of Bio-NER tasks have been based on machine learning techniques including Hidden Markov Models -LRB- HMMs -RRB- <OTH> , the dictionary HMM model <OTH> and Maximum Entropy Markov Models -LRB- MEMMs -RRB- <CIT> '
'Finally , in section 4 we add additional features to the maxent model , and chain these models into a conditional markov model -LRB- CMM -RRB- , as used for tagging <CIT> or earlier NER work <OTH> '
'Previous approaches , eg , <OTH> and <OTH> , have all used the Brown algorithm for clustering <CIT> '
'We used the Berkeley Parser4 to learn such grammars from Sections 2-21 of the Penn Treebank <CIT> '
'Along similar lines , <CIT> combine a generative model of word alignment with a log-linear discriminative model trained on a small set of hand aligned sentences '
'2 Related Work Previous work on polarity disambiguation has used contextual clues and reversal words <CIT> '
'In the context of headline generation , simple statistical models are used for aligning documents and headlines <OTH> , based on IBM Model 1 <CIT> '
'<CIT> defined two local search operations for their 1-to-N alignment models 3 , 4 and 5 '
'In order to create the necessary SMT language and translation models, they used:  Giza++ (Och & Ney, 2003);2  the CMU-Cambridge statistical toolkit;3  the ISI ReWrite Decoder.4 Translation was performed from EnglishFrench and FrenchEnglish, and the resulting translations were evaluated using a range of automatic metrics: BLEU (Papineni et al. , 2002), Precision and Recall 2http://www.isi.edu/och/Giza++.html 3http://mi.eng.cam.ac.uk/prc14/toolkit.html 4http://www.isi.edu/licensed-sw/rewrite-decoder/ 185 (Turian et al. , 2003), and Wordand Sentence Error Rates.'
'The sentences included in the gold standard were chosen at random from the BNC , subject to the condition that they contain a verb which does not occur in the training sections of the WSJ section of the PTB <CIT> '
'Only recently have robust knowledge-based methods for some of these tasks begun to appear , and their performance is still not very good , as seen above in our discussion of using WordNet as a semantic network ; 33 as for checking the plausibility of a hypothesis on the basis of causal knowledge about the world , we now have a much better theoretical grasp of how such inferences could be made <OTH> , but we are still quite a long way from a general inference engine '
'The principle of maximum entropy states that when one searches among probability distributions that model the observed data -LRB- evidence -RRB- , the preferred one is the one that maximizes the entropy -LRB- a measure of the uncertainty of the model -RRB- <CIT> '
'Usually the IBM Model 1 , developed in the statistical machine translation field <CIT> , is used to construct translation models for retrieval purposes in practice '
'et al , 1994 ; Brill and Resnik , 1994 ; <CIT> and Brooks , 1995 ; Merlo et al , 1997 -RRB- '
'52 Adding lexical information Gildea <OTH> shows that removing the lexical dependencies in Model 1 of <CIT> -LRB- that is , not conditioning on w h when generating w s -RRB- decreases labeled precision and recall by only 05 % '
'The translation quality on the TransType2 task in terms of WER , PER , BLEU score <CIT> , and NIST score <OTH> is given in Table 4 '
'81 The Averaged Perceptron Algorithm with Separating Plane The averaged perceptron algorithm <CIT> has previously been applied to various NLP tasks <CIT> for discriminative reranking '
'Other researchers have also reported similar problems of excessive resource demands with the ` collect all neighbors '' model <OTH> '
'Many corpus based statistical methods have been proposed to solve this problem , including supervised learning algorithms <OTH> , weakly supervised learning algorithms <CIT> , unsupervised learning algorithms -LRB- or word sense discrimination -RRB- <OTH> , and knowledge based algorithms <OTH> '
'Two disjoint corpora are used in steps 2 and 5 , both consisting of complete articles taken from the Wall Street Journal Treebank Corpus <CIT> '
'But there is also extensive research focused on including linguistic knowledge in metrics <CIT> among others '
'<CIT> tested the claim on about 37,000 examples and found that when a polysemous word appeared more than once in a discourse , they took on the majority sense for the discourse 998 % of the time on average '
'One popular approach is to use a log-linear parsing model and maximize the conditional likelihood function <OTH> '
'This is based on the idea from <CIT> that rare words in the training set are similar to unknown words in the test set , and can be used to learn how to tag the unknown words that will be encountered during testing '
'Then , to solve p E C in equation -LRB- 8 -RRB- is equivalent to solve h that maximize the loglikelihood : = -LRB- x -RRB- log zj , -LRB- z -RRB- + x i -LRB- 10 -RRB- h = argmax kV -LRB- h -RRB- Such h can be solved by one of the numerical algorithm called the Improved Iteratire Scaling Algorithm <CIT> '
'There are many techniques for transliteration and back-transliteration , and they vary along a number of dimensions : phoneme substitution vs character substitution heuristic vs generative vs discriminative models manual vs automatic knowledge acquisition We explore the third dimension , where we see several techniques in use : Manually-constructed transliteration models , eg , <CIT> '
'The annotation scheme <OTH> is modeled to a certain extent on that of the Penn Treebank <CIT> , with crucial differences '
'The mutual information of a cooccurrence pair , which measures the degree of association between the two words <CIT> , is defined as <OTH> : P -LRB- xly -RRB- I -LRB- x , y -RRB- - log 2 P -LRB- x , y -RRB- _ log 2 -LRB- 1 -RRB- P -LRB- x -RRB- P -LRB- y -RRB- P -LRB- x -RRB- = log 2 P -LRB- y -LRB- x -RRB- P -LRB- Y -RRB- where P -LRB- x -RRB- and P -LRB- y -RRB- are the probabilities of the events x and y -LRB- occurrences of words , in our case -RRB- and P -LRB- x , y -RRB- is the probability of the joint event -LRB- a cooccurrence pair -RRB- '
'Following <CIT> , we use syntactic dependencies between words to model their semantic properties '
'32 ITG Constraints In this section , we describe the ITG constraints <CIT> '
'We therefore ran the dependency model on a test corpus tagged with the POS-tagger of <CIT> , which is trained on the original Penn Treebank -LRB- see HWDep -LRB- + tagger -RRB- in Table 3 -RRB- '
'Several authors have used mutual information and similar statistics as an objective function for word clustering <CIT> , for automatic determination of phonemic baseforms <OTH> , and for language modeling for speech recognition <OTH> '
'We use a standard data set <CIT> consisting of sections 15-19 of the WSJ corpus as training and section 20 as testing '
'In one set of experiments , we generated lexicons for PEOPLE and ORGANIZATIONS using 2500 Wall Street Journal articles from the Penn Treebank <CIT> '
'A comparison of the two approaches can be found in Koehn , <CIT> '
'a65 The rest of the factors denote distorsion probabilities -LRB- d -RRB- , which capture the probability that words change their position when translated from one language into another ; the probability of some French words being generated from an invisible English NULL element -LRB- pa6 -RRB- , etc See <CIT> or <OTH> for a detailed discussion of this translation model and a description of its parameters '
'We use ROUGE <CIT> to assess summary quality using common n-gram counts and longest common subsequence -LRB- LCS -RRB- measures '
'Dynamic programming is applied to bilingual sentence alignment in most of previous works <CIT> '
'To evaluate the performance of a parser , NP chunks can usefully be evaluated by a gold standard ; many systems <CIT> use the Penn Treebank for this type of evaluation '
'In particular , it shows systematically better F-Measure and Accuracy measures over all other metrics showing an improvement of -LRB- 1 -RRB- at least 286 % in terms of F-Measure and 396 % in terms of Accuracy and -LRB- 2 -RRB- at most 661 % in terms of FMeasure and 674 % in terms of Accuracy compared to the second best metric which is also systematically the word N-gram overlap similarity measure used by <CIT> '
'Their experiments were performed using a decoder based on IBM Model 4 using the translation techniques developed at IBM <CIT> '
'A superset of the parallel data was word aligned by GIZA union <CIT> and EMD <OTH> '
'To group the letters into classes , we employ a hierarchical clustering algorithm <CIT> '
'Previous research in automatic acquisition focuses primarily on the use of statistical techniques , such as bilingual alignment <CIT> or extraction of syntactic constructions from online dictionaries and corpora <OTH> '
'These sentences were parsed with the Collins parser <CIT> '
'The precision of the extracted information can be improved significantly by using machine learning methods to filter out noise <CIT> '
'9 <CIT> report that , for translation reranking , such local updates -LRB- towards the oracle -RRB- outperform bold updates -LRB- towards the gold standard -RRB- '
'To achieve robust training , Daume III and Marcu <OTH> employed the averaged perceptron <CIT> and ALMA <OTH> '
'However , our representation of the model conceptually separates some of the hyperparameters which are not separated in <CIT> , and we found that setting these hyperparameters with different values from one another was critical for improving performance '
'6 Related works After the work of <CIT> , many machine learning techniques have been applied to the basic chunking task , such as Support Vector Machines <OTH> , Hidden Markov Model <OTH> , Memory Based Learning <OTH> , Conditional Random Fields <OTH> , and so on '
'Different news articles reporting on the same event are commonly used as monolingual comparable corpora , from which both paraphrase patterns and phrasal paraphrases can be derived <CIT> '
'We ran GIZA + + <OTH> on the training corpus in both directions with IBM model 4 , and then applied the refinement rule described in <CIT> to obtain a many-to-many word alignment for each sentence pair '
'<OTH> , <CIT> -RRB- , source extraction -LRB- eg Bethard et al '
'7For details about the Bleu evaluation metric , see <CIT> '
'<CIT> 8802 -LRB- +082 -RRB- + unlabeled data -LRB- 17M 27M words -RRB- 8841 -LRB- +039 -RRB- + supplied gazetters 8890 -LRB- +049 -RRB- + add dev '
'Interestingly , the interannotator agreement on SWITCHBOARD -LRB- a0a2a1 a3a5a4a7a6a9a8a9a6 -RRB- is higher than on the lecture corpus <OTH> and higher than the a0 - score reported by <CIT> for the ICSI meeting data used by Murray et al '
'In the results we describe here , we use mutual information <CIT> as the metric for neighborhood pruning , pruning which occurs as the network is being generated '
'Our work builds upon Turneys work on semantic orientation <CIT> and synonym learning <CIT> , in which he used a PMI-IR algorithm to measure the similarity of words and phrases based on Web queries '
'2 We illustrate the rule extraction with an example from the tree-to-tree translation model based on tree sequence alignment <CIT> without losing of generality to most syntactic tree based models '
'MET <CIT> was carried out using a development set , and the BLEU score evaluated on two test sets '
'However morphosyntactic features alone can not verify the terminological status of the units extracted since they can also select non terms <CIT> '
'There has thus been a trend recently towards robust wide-coverage semantic construction -LRB- eg , <CIT> -RRB- '
'This model is trained on approximately 5 million sentence pairs of Hansard -LRB- Canadian parliamentary -RRB- and UN proceedings which have been aligned on a sentence-by-sentence basis by the methods of <CIT> , and then further aligned on a word-by-word basis by methods similar to <CIT> '
'In contrast , globally optimized clustering decisions were reported in <OTH> and <OTH> , where all clustering possibilities are considered by searching on a Bell tree representation or by using the Learning as Search Optimization -LRB- LaSO -RRB- framework <OTH> respectively , but the first search is partial and driven by heuristics and the second one only looks back in text '
'<OTH> compare taggers trained and tested on the Wall Street Journal <CIT> and the Lancaster-Oslo-Bergen <OTH> corpora and find that the results for the WSJ perform significantly worse '
'The training samples are respectively used to create the models PT ^ G , PCHUNK , PBUILD , and PCMECK , all of which have the form : k p -LRB- a , b -RRB- = II _ ij -LRB- o , b ~ j -LRB- 1 -RRB- j -- 1 where a is some action , b is some context , ~ '' is a nor4 Model Categories Description Templates Used TAG See <CIT> CHUNK chunkandpostag -LRB- n -RRB- \* BUILD CHECK chunkandpostag -LRB- m , n -RRB- \* cons -LRB- n -RRB- cons -LRB- re , n -RRB- \* cons -LRB- m , n , p -RRB- T punctuation checkcons -LRB- n -RRB- \* checkcons -LRB- m , n -RRB- \* production surround -LRB- n -RRB- \* The word , POS tag , and chunk tag of nth leaf '
'We also record for each token its derivational root , using the CELEX <OTH> database '
'1 Church and Hanks <CIT> thus emphasize the importance of human judgment used in conjunction with these tools '
'<OTH> , in which we translate a source-language sentence f into the target-language sentence e that maximizes a linear combination of features and weights :1 e , a = argmax e , a score -LRB- e , a , f -RRB- -LRB- 1 -RRB- = argmax e , a Msummationdisplay m = 1 mhm -LRB- e , a , f -RRB- -LRB- 2 -RRB- where a represents the segmentation of e and f into phrases and a correspondence between phrases , and each hm is a R-valued feature with learned weight m The translation is typically found using beam search <CIT> '
'Drawing on <CIT> analysis of the Yarowsky algorithm , we perform bootstrapping by entropy regularization : we maximize a linear combination of conditional likelihood on labeled data and confidence -LRB- negative Renyi entropy -RRB- on unlabeled data '
'3 GM Representation of IBM MT Models In this section we present a GM representation for IBM model 3 <CIT> in fig '
'Previous work has demonstrated that this scoring function is able to provide high discrimination power for a variety of applications <OTH> '
'22 Learning Algorithm For learning coreference decisions , we used a Maximum Entropy <CIT> model '
'Since we need knowledge-poor Daille , 1996 -RRB- induction , we can not use human-suggested filtering Chi-squared -LRB- G24 -RRB- 2 <OTH> Z-Score <OTH> Students t-Score <OTH> n-gram list in accordance to each probabilistic algorithm '
'In the first , a separate language model is trained on each column of the database and these models are then used to segment and label a given text sequence <CIT> '
'The toolkit also implements suffix-array grammar extraction <OTH> and minimum error rate training <CIT> '
'We use the neural network approximation <CIT> to perform inference in our model '
'We compared our system Lynx against a freely available phrase-based decoder Pharaoh <CIT> '
'Recently several latent variable models for constituent parsing have been proposed <CIT> '
'<CIT> presented randomized language model based on perfect hashing combined with entropy pruning to achieve further memory reductions '
'In particular , since we treat each individual speech within a debate as a single document , we are considering a version of document-level sentiment-polarity classification , namely , automatically distinguishing between positive and negative documents <CIT> '
'Most current statistical models <CIT> treat the aligned sentences in the corpus as sequences of tokens that are meant to be words ; the goal of the alignment process is to find links between source and target words '
'In addition , IC is stable even for relatively low frequency words , which can be contrasted with Fano ''s mutual information formula recently used by <CIT> to compute word cooccurrence patterns in a 44 million word corpus of Associated Press news stories '
'A similar view underlies the class-based methods cited in Section 243 <CIT> '
'We have used a state-of-the-art Chinese handwriting recognizer <OTH> developed by ATC , CCL , ITRI , Taiwan as the basis of our experiments '
'We also use minimum error-rate training <CIT> to tune our feature weights '
'Thenthewordalignment is refined by performing grow-diag-final method <CIT> '
'<CIT> Hindi is a verb final , flexible word order language and therefore , has frequent occurrences of non-projectivity in its dependency structures '
'173 The standard features for genre classification models include words , part-of-speech -LRB- POS -RRB- tags , and punctuation <OTH> , but constituent-based syntactic categories have also been explored <OTH> '
'In the following sections , we present the best performing set of feature templates as determined on the development data set using only the supervised training setting ; our feature templates have thus not been influenced nor extended by the unsupervised data13 11The full list of tags , as used by <CIT> , also makes the underlying Viterbi algorithm unbearably slow '
'In this sense , instead of measuring only the categorial agreement between annotators with the kappa statistic <CIT> or the performance of a system in terms of precision\/recall , we could take into account the hierarchical organization of the categories or concepts by making use of measures considering the hierarchical distance between two concepts such as proposed by <OTH> or <OTH> '
'Such a similarity is calculated by using the WordNet : : Similarity tool <CIT> , and , concretely , the Wu-Palmer measure , as defined in Equation1 <OTH> '
'Furthermore , I plan to apply my parsers in other domains -LRB- eg , biomedical data -RRB- <CIT> besides treebank data , to investigate the effectiveness and generality of my approaches '
'<CIT> studied open domain relation extraction , for which they manually identified several common relation patterns '
'Once we obtain the augmented phrase table , we should run the minimum-error-rate training <CIT> with the augmented phrase table such that the model parameters are properly adjusted '
'As such , we quantify success based on ROUGE <CIT> scores '
'(Ng and Low 2004, Toutanova et al, 2003, Brants 2000, Ratnaparkhi 1996, Samuelsson 1993).'
'In each iteration of local search , we look in the neighborhood of the current best alignment for a better alignment <CIT> '
'To address this drawback , we proposed a new method3 to compute a more reliable and smoothed score in the undefined case , based on the IBM model 1 <CIT> '
'One is the longest common subsequence -LRB- LCS -RRB- based approach <CIT> '
'5 Experimental Evaluation To perform empirical evaluations of the proposed methods , we considered the task of parsing the Penn Treebank Wall Street Journal corpus <CIT> '
'In Section 3 , we will present a Perceptron like algorithm <CIT> to obtain the parameters '
'F-Struct Feats Grammar Rules -LCB- PRED = PRO , NUM = SG PER = 3 , GEN = FEM -RCB- PRP-nom -LRB- = -RRB- she -LCB- PRED = PRO , NUM = SG PER = 3 , GEN = FEM -RCB- PRP-acc -LRB- = -RRB- her Table 5 : Lexical item rules with case markings 4 A History-Based Generation Model The automatic generation grammar transform presented in <CIT> provides a solution to coarse-grained and -LRB- in fact -RRB- inappropriate independence assumptions in the basic generation model '
'Moreover , it was -LRB- without imposing determinism -RRB- the inference technique employed in <OTH> '
'Oxford, UK: Oxford University Press.</rawString> </citation> <citation valid=''true''> <authors> <author>L A Black</author> <author>C McMeel</author> <author>M McTear</author> <author>N Black</author> <author>R Harper</author> <author>M Lemon</author> </authors> <title>Implementing autonomy in a diabetes management system</title> <date>2005</date> <journal>J Telemed Telecare</journal> <volume>11</volume> <contexts> <context>-care applications, Examples include scheduling appointments over the phone (Zajicek et al. 2004, Wolters et al., submitted), interactive reminder systems (Pollack, 2005), symptom management systems (Black et al. 2005) or environmental control systems (Clarke et al. 2005).'
'We dealt with this by either limiting the translation probability from the null word <CIT> at the hypothetical 0-position <CIT> over a threshold during the EM training , or setting SHo -LRB- j -RRB- to a small probability 7r instead of 0 for the initial null hypothesis H0 '
'Such metrics have been introduced in other fields , including PARADISE <OTH> for spoken dialogue systems , BLEU <OTH> for machine translation ,1 and ROUGE <CIT> for summarisation '
'Our features were based on those in <CIT> '
'In prior research , ILP was used as a postprocessing step to remove redundancy and make other global decisions about parameters <CIT> '
'Previous work used all possible pre xes and suf xes ranging in length from 1 to k characters , with k = 4 <CIT> , and k = 10 <OTH> '
'Texts are represented by dependency parse trees -LRB- using the Minipar parser <CIT> -RRB- and templates by parse sub-trees '
'<CIT> employsthez-scoreinconjunction with several heuristics -LRB- eg , the systematic occurrenceof two lexical items at the same distanceintext -RRB- andextractspredicativecollocations , 1Eg , -LRB- Frantziet al ,2000 ; Pearce ,2001 ; Goldmanet al , 2001 ; ZaiuInkpenandHirst ,2002 ; Dias ,2003 ; Seretanetal '
'We evaluated performance by measuring WER -LRB- word error rate -RRB- , PER -LRB- position-independent word error rate -RRB- , BLEU <CIT> and TER -LRB- translation error rate -RRB- <OTH> using multiple references '
'(Zollmann et al., 2008).'
'Whereas most of the work on English has been based on constituency-based representations , partly inuenced by the availability of data resources such as the Penn Treebank <CIT> , it has been argued that free constituent order languages can be analyzed more adequately using dependency-based representations , which is also the kind of annotation found , for example , in the Prague Dependency Treebank of Czech <OTH> '
'In our VB experiments we set i = j = 01 , i -LCB- 1 , , T -RCB- , j -LCB- 1 , , V -RCB- , which yielded the best performance on most reported metrics in <CIT> '
'Note that , since the FrameNet data does not include deep syntactic tree annotation , we processed the FrameNet data with Collins parser <CIT> , consequently , the experiments on FrameNet relate to automatic syntactic parse trees '
'While transfer learning was proposed more than a decade ago <OTH> , its application in natural language processing is still a relatively new territory <CIT> , and its application in relation extraction is still unexplored '
'NER proves to be a knowledgeintensive task , and it was reassuring to observe that System Resources Used F1 + LBJ-NER Wikipedia , Nonlocal Features , Word-class Model 9080 <OTH> Semi-supervised on 1Gword unlabeled data 8992 <OTH> Semi-supervised on 27Mword unlabeled data 8931 <OTH> Wikipedia 8802 <OTH> Non-local Features 8724 <OTH> Non-local Features 8717 + <CIT> Non-local Features 8686 Table 7 : Results for CoNLL03 data reported in the literature '
'Second , instead of disambiguating phrase senses as in <CIT> , we model word selection independently of the phrases used in the MT models '
'<OTH> , and <CIT> et al '
'Early examples of this work include <CIT> ; more recent models include <OTH> '
'Instead of computing all intersections , <CIT> only computes critical intersections where highest-score translations will change '
'<CIT> ; <OTH> -RRB- '
'<CIT> Model 3 integrates the detection and resolution of WH-traces in relative clauses into a lexicalized PCFG '
'Given training data consisting of parallel sentences: }1),,{( )()( Sief ii =, our Model-1 training for t(f|e) is as follows:  =  = S s ss e efefceft 1 )()(1 ),;|()|(  Where 1 e  is a normalization factor such that 0.1)|( =  j j eft ),;|( )()( ss efefc denotes the expected number of times that word e connects to word f.   == = = l i i m j jl k k ss eeff eft eft efefc 11 1 )()( ),(),( )|( )|( ),;|(  With the conditional probability t(f|e), the probability for an alignment of foreign string F given English string E is in (1):  = = + = m j n i ijm eft l EFP 1 0 )|( )1( 1 )|( (1) The probability of alignment F given E: )|( EFP is shown to achieve the global maximum under this EM framework as stated in (Brown et al. ,1993).'
'As such , discourse markers play an important role in the parsing of natural language discourse <CIT> , and their correspondence with discourse relations can be exploited for the unsupervised learning of discourse relations <CIT> '
'Table 2 shows the results for English projective dependency trees extracted from the Penn Treebank <CIT> using the rules of Yamada and Matsumoto -LRB- 2003 -RRB- '
'Mean number of instances of paraphrase phenomena per sentence <CIT> '
'1 Introduction Bilingual word alignment is first introduced as an intermediate result in statistical machine translation -LRB- SMT -RRB- <CIT> '
'We then replaced fi with its associated z-score k $ , e k $ , e is the strength of code frequency f at Lt , and represents the standard deviation above the average of frequency fave , t Referring to Smadja ''s definition <CIT> , the standard deviation at at Lt and strength kf , t of the code frequencies are defined as shown in formulas 1 and 2 '
'It has been shown that the methods can be ported to other languages and treebanks <CIT> , including Cast3LB <OTH> '
'Others try to accommodate both syntactic and lexical differences between the candidate translation and the reference , like CDER <OTH> , which employs a version of edit distance for word substitution and reordering ; METEOR <CIT> , which uses stemming and WordNet synonymy ; and a linear regression model developed by <OTH> , which makes use of stemming , WordNet synonymy , verb class synonymy , matching noun phrase heads , and proper name matching '
'Most recently , Yarowsky used an unsupervised learning procedure to perform WSD <CIT> , although this is only tested on disambiguating words into binary , coarse sense distinction '
'For each word in LDV , three existing thesauri are consulted : Rogets Thesaurus <OTH> , Collins COBUILD Thesaurus <OTH> , and WordNet <OTH> '
'((:I (:Q DET NAMED-ENTITY) ENTER[V] (:Q THE ROOM[N])) (:I (:Q DET FEMALE-INDIVIDUAL) HAVE[V] (:Q DET ROOM[N])) (:I (:Q DET FEMALE-INDIVIDUAL) SLEEP[V]) (:I (:Q DET FEMALE-INDIVIDUAL) HAVE[V] (:Q DET (:F PLUR CLOTHE[N]))) (:I (:Q DET (:F PLUR CLOTHE[N])) WASHED[A])) Here the upper-case sentences are automatically generated verbalizations of the abstracted LFs shown beneath them.1 The initial development of KNEXT was based on the hand-constructed parse trees in the Penn Treebank version of the Brown corpus, but subsequently Schubert and collaborators refined and extended the system to work with parse trees obtained with statistical parsers (e.g., that of Collins (1997) or Charniak (2000)) applied to larger corpora, such as the British National Corpus (BNC), a 100 million-word, mixed genre collection, along with Web corpora of comparable size (see work of Van Durme et al.'
'This merging of contexts is different than clustering words <CIT> , but is applicable , as word clustering relies on knowing which contexts identify the same category '
'<OTH> , various classification models and linguistic features have been proposed to improve the classification performance <CIT> '
'Like baseNP chunking <CIT> , content chunk parsing is also a kind of shallow parsing '
'The weights 1 , , M are typically learned to directly minimize a standard evaluation criterion on development data -LRB- eg , the BLEU score ; Papineni et al , <OTH> -RRB- using numerical search <CIT> '
'Adwait <CIT> estimates a probability distribution for tagging using a maximum entropy approach '
'We obtained word alignments of training data by first running GIZA + + <CIT> and then applying the refinement rule grow-diag-final-and <OTH> '
'Charniak <OTH> gives a thorough explanation of the equations for an HMM model , and Kupiec <OTH> describes an HMM tagging system in detail '
'1 Introduction Various papers use phrase-based translation systems <OTH> that have shown to improve translation quality over single-word based translation systems introduced in <CIT> '
'The various extraction measures have been discussed in great detail in the literature <OTH> , their performance has been compared <CIT> , and the methods have been combined to improve overall performance <OTH> '
'42 Approximated BLEU We used the BLEU score <CIT> as the loss function computed by : BLEU -LRB- E ; E -RRB- = exp 1N Nsummationdisplay n = 1 log pn -LRB- E , E -RRB- BP -LRB- E , E -RRB- -LRB- 7 -RRB- where pn -LRB- -RRB- is the n-gram precision of hypothesized translations E = -LCB- et -RCB- Tt = 1 given reference translations E = -LCB- et -RCB- Tt = 1 and BP -LRB- -RRB- 1 is a brevity penalty '
'Table 2 summarizes the characteristics of the training corpus used for training the parameters of Model 4 proposed in <CIT> '
'Most of researchers focus on how to extract useful textual features -LRB- lexical , syntactic , punctuation , etc -RRB- for determining the semantic orientation of the sentences using machine learning algorithm <OTH> '
'<CIT> searches with the full model , but makes assumptions about the the amount of reordering the language model can trigger in order to limit exploration '
'Methods for doing so , for stochastic parser output , are described by Johnson <OTH> and <CIT> '
'1 Introduction Summarizing spoken documents has been extensively studied over the past several years <CIT> '
'1 Introduction Previous corpus-based sense disambiguation methods require substantial amounts of sense-tagged training data <OTH> or aligned bilingual corpora <CIT> '
'Table 8 compares the F1 results of our baseline model with Nakagawa and Uchimoto <OTH> and <CIT> on CTB 30 '
'Section 4 concludes the paper with a critical assessment of the proposed approach and a discussion of the prospects for application in the construction of corpora comparable in size and quality to existing treebanks -LRB- such as , for example , the Penn Treebank for English <CIT> or the TIGER Treebank for German <OTH> -RRB- '
'For a more detailed introduction to maximum entropy estimation see <CIT> '
'34 -RRB- 31 Probabilistic model In the probabilistic formulation <CIT> , the task of learning taxonomies from a corpus is seen as a probability maximization problem '
'We augment Collins head-driven model 2 <CIT> to incorporate a semantic label on each internal node '
'Such metrics have been introduced in other fields , including PARADISE <OTH> for spoken dialogue systems , BLEU <CIT> for machine translation ,1 and ROUGE <OTH> for summarisation '
'It was later applied by <CIT> as a way to determine if a sequence of N words -LRB- Ngram -RRB- came from an independently distributed sample '
'Co-Training has been used before in applications like word-sense disambiguation <CIT> , web-page classification <OTH> and namedentity identification <OTH> '
'The IBM models , together with a Hidden Markov Model -LRB- HMM -RRB- , form a class of generative models that are based on a lexical translation model P -LRB- fj ei -RRB- where each word fj in the foreign sentence fm1 is generated by precisely one word ei in the sentence el1 , independently of the other translation decisions <CIT> '
'Experiment Implementation : We apply SVM algorithm to construct our classifiers which has been shown to perform better than many other classification algorithms <CIT> '
'2 WordNet-based semantic relatedness measures 21 Basic measures Two similarity\/distance measures from the Perl package WordNet-Similarity written by <CIT> are used '
'The third exploits automatic subjectivity analysis in applications such as review classification -LRB- eg , <CIT> -RRB- , mining texts for product reviews -LRB- eg , <OTH> -RRB- , summarization -LRB- eg , <OTH> -RRB- , information extraction -LRB- eg , <OTH> -RRB- , 1Note that sentiment , the focus of much recent work in the area , is a type of subjectivity , specifically involving positive or negative opinion , emotion , or evaluation '
'<CIT> and Hogan et al '
'For the correct identification of phrases in a Korean query , it would help to identify the lexical relations and produce statistical information on pairs of words in a text corpus as in <CIT> '
'For example , the sentence I went to California last May would be marked for base NPs as : I went to California last May I 0 0 I B I indicating that the NPs are I , California and last May This approach has been studied in <CIT> '
'In the context of statistical machine translation <CIT> , we may interpretE as an English sentence , F its translation in French , and A a representation of how the words correspond to each other in the two sentences '
'Among them , <CIT> have proposed a way to exploit bilingual dictionnaries at training time '
'While recent proposals for evaluation of MT systems have involved multi-parallel corpora <CIT> , statistical MT algorithms typically only use one-parallel data '
'Of course , many applications require smoothing of the estimated distributionsthis problem also has known solutions in MapReduce <CIT> '
'203 Estimating the parameters for these models is more difficult (and more computationally expensive) than with the models considered in the previous section: rather than simply being able to count the word pairs and alignment relationships and estimate the models directly, we must use an existing model to compute the expected counts for all possible alignments, and then use these counts to update the new model.7 This training strategy is referred to as expectationmaximization (EM) and is guaranteed to always improve the quality of the prior model at each iteration (Brown et al., 1993; Dempster et al., 1977).'
'The best prosodic label sequence is then , L = argmax L nproductdisplay i P -LRB- li -RRB- -LRB- 6 -RRB- To estimate the conditional distribution P -LRB- li -RRB- we use the general technique of choosing the maximum entropy -LRB- maxent -RRB- distribution that estimates the average of each feature over the training data <CIT> '
'Other scores for the word arc are set as in <CIT> '
'Model interpolation in this case perSystem Training Heldout LR LP MAP Brown ; T Brown ; H 760 754 MAP Brown ; T WSJ ; 24 769 771 Gildea WSJ ; 2-21 861 866 MAP WSJ ; 2-21 WSJ ; 24 869 871 Charniak <OTH> WSJ ; 2-21 WSJ ; 24 867 866 Ratnaparkhi <OTH> WSJ ; 2-21 863 875 <CIT> <OTH> WSJ ; 2-21 881 883 Charniak -LRB- 2000 -RRB- WSJ ; 2-21 WSJ ; 24 896 895 <CIT> -LRB- 2000 -RRB- WSJ ; 2-21 896 899 Table 4 : Parser performance on WSJ ; 23 , baselines '
'The learning algorithm , which is illustrated in <CIT> , proceeds as follows '
'POS tag the text using <CIT> '
'One is to use a stochastic gradient descent -LRB- SGD -RRB- or Perceptron like online learning algorithm to optimize the weights of these features directly for MT <CIT> '
'Second , phrase translation pairs are extracted from the word aligned corpus <CIT> '
'62 Translation Results For the translation experiments , we report the two accuracy measures BLEU <CIT> and NIST <OTH> as well as the two error rates word error rate -LRB- WER -RRB- and positionindependent word error rate -LRB- PER -RRB- '
'1 Introduction Statistical language modeling has been widely used in natural language processing applications such as Automatic Speech Recognition -LRB- ASR -RRB- , Statistical Machine Translation -LRB- SMT -RRB- <CIT> and Information Retrieval -LRB- IR -RRB- <OTH> '
'22 The Translation Model We adapted Model 1 <CIT> to our purposes '
'These lists are rescored with the following models : -LRB- a -RRB- the different models used in the decoder which are described above , -LRB- b -RRB- two different features based on IBM Model 1 <CIT> , -LRB- c -RRB- posterior probabilities for words , phrases , n-grams , and sentence length <OTH> , all calculated over the Nbest list and using the sentence probabilities which the baseline system assigns to the translation hypotheses '
'The tensor has been adapted with a straightforward extension of pointwise mutual information <CIT> for three-way cooccurrences , following equation 4 '
'1 <CIT> proposes a method for word sense -LRB- translation -RRB- disambiguation that is based on a bootstrapping technique , which we refer to here as Monolingual Bootstrapping -LRB- MB -RRB- '
'Starting from a word-based alignment for each pair of sentences , the training for the algorithm accepts all contiguous bilingual phrase pairs -LRB- up to a predetermined maximum length -RRB- whose words are only aligned with each other <CIT> '
'HMMs have been used many times for POS tagging and chunking , in supervised , semisupervised , and in unsupervised settings <CIT> '
'Collocations were extracted according to the method described in <CIT> by moving a window on texts '
'Such approaches have shown promise in applications such as web page classification <OTH> , named entity classification <OTH> , parsing <CIT> , and machine translation <OTH> '
'The techniques examined are Structural Correspondence Learning -LRB- SCL -RRB- <OTH> and Self-training <CIT> '
'Also , adding a constituent size\/distance effect , as described by Schubert <OTH> and as used by some researchers in parsing -LRB- eg Lesmo and Torasso <OTH> and <CIT> -RRB- would almost certainly improve parsing '
'One such model is the IBM Model 1 <CIT> '
'Forest reranking with a language model can be performed over this n-ary forest using the cube growing algorithm of <CIT> '
'First , we show how one can use an existing statistical translation model <CIT> in order to automatically derive a statistical TMEM '
'We set our space usage to match the 308 bytes per n-gram reported in <CIT> and held out just over 1M unseen n-grams to test the error rates of our models '
'These results were achieved using the statistical alignments provided by model 5 <CIT> and smoothed 11-grams and 6-grams , respectively '
'157 ena or the linguist ''s abstraction capabilities -LRB- eg knowledge about what is relevant in the context -RRB- , they tend to reach a 95-97 % accuracy in the analysis of several languages , in particular English <CIT> '
'This score measures the precision of unigrams , bigrams , trigrams and fourgrams with respect to a reference translation with a penalty for too short sentences <CIT> '
'It acquires a set of synchronous lexical entries by running the IBM alignment model <CIT> and learns a log-linear model to weight parses '
'Most related to our approach , <CIT> used inversion transduction grammarsa synchronous context-free formalism <CIT> for this task '
'In this paper we extend this work to represent sets of situation-specific events not unlike scripts , caseframes <CIT> , and FrameNet frames <OTH> '
'Previous research in this area includes several models which incorporate hidden variables <CIT> '
'In order to build models that perform well in new -LRB- target -RRB- domains we usually find two settings <CIT> : In the semi-supervised setting the goal is to improve the system trained on the source domain using unlabeled data from the target domain , and the baseline is that of the system c2008 '
'Based on this assumption , <CIT> stored all bigrams of words along with their relative position , p -LRB- -5 -LRB- p _ ~ 5 -RRB- '
'<CIT> used this method for word sense disambiguation '
'Generation of paraphrase examples was also investigated <CIT> '
'<OTH> -RRB- and on speech repair detection and correction -LRB- eg '
'As in other work , we collapsed AI -RRB- VP and Pl ? Jl '' to the same label when calculating these scores <CIT> '
'<CIT> and <OTH> claim that fine-grained semantic distinctions are unlikely to be of practical value for many applications '
'ther background on this method of hypothesis testing the reader is referred to <CIT> '
'Our method is based on a decision list proposed by Yarowsky <CIT> '
'As in much recent empirical work in discourse processing <CIT> , we performed an intercoder reliability study investigating agreement in annotating the times '
'Different optimization techniques are available , like the Simplex algorithm or the special Minimum Error Training as described in <CIT> '
'41 Experimental Setup We use the whole Penn Treebank corpus <CIT> as our data set '
'<OTH> , -LRB- 3 -RRB- thesaurus categories <CIT> , -LRB- 4 -RRB- translation in another language <CIT> , -LRB- 5 -RRB- automatically induced clusters with sublexical representation <OTH> , and -LRB- 6 -RRB- hand-crafted lexicons <OTH> '
'Following the phrase extraction phase in PHARAOH , we eliminate word gaps by incorporating unaligned words as part of the extracted NL phrases <CIT> '
'Many probabilistic evaluation models have been published inspired by one or more of these feature types <OTH> <OTH> <OTH> <CIT> <CIT> <OTH> <OTH> , but discrepancies between training sets , algorithms , and hardware environments make it difficult , if not impossible , to compare the models objectively '
'<CIT> proposed a Perceptron like learning algorithm to solve sequence classification in the traditional left-to-right order '
'Recently , many works combined a MRD and a corpus for word sense disambiguatio <CIT> '
'5 Augmenting the corpus with an extracted dictionary Previous research <CIT> has shown that including word aligned data during training can improve translation results '
'2 Statistical Word Alignment Model According to the IBM models <CIT> , the statistical word alignment model can be generally represented as in equation -LRB- 1 -RRB- '
'Our experimental results display that our SDB model achieves a substantial improvement over the baseline and significantly outperforms XP + according to the BLEU metric <CIT> '
'We evaluate this method over the part of speech tagged portion of the Penn Treebank corpus <CIT> '
'1 Introduction Bilingual word alignment is first introduced as an intermediate result in statistical machine translation -LRB- SMT -RRB- <CIT> '
'OHara and Wiebe <OTH> also make use of high level features , in their case the Penn Treebank <CIT> and FrameNet <OTH> to classify prepositions '
'Our approach to STC uses a thesaurus based on corpus statistics <CIT> for real-valued similarity calculation '
'We use a bidirectional search strategy <OTH> , and our algorithm is based on Perceptron learning <CIT> '
'In recent years , many researchers build alignment links with bilingual corpora <CIT> '
'Related Work The first application of log-linear models to parsing is the work of Ratnaparkhi and colleagues <CIT> '
'Beam-search parsing using an unnormalized discriminative model , as in <CIT> , requires a slightly different search strategy than the original generative model described in Roark <OTH> '
'Following the evaluation methodology of Wong and Mooney <OTH> , we performed 4 runs of the standard 10-fold cross validation and report the averaged performance in this section using the standard automatic evaluation metric BLEU <CIT> and NIST <OTH> 2 '
'The resulting memory limitations alone can prevent the practical learning of highly split grammars <CIT> '
'The algorithm is essentially the same as the one introduced in <CIT> '
'Models of this type include : <CIT> , which use semantic word clustering , and <OTH> , which uses variablelength context '
'In another generation approach , <CIT> look for pairs of slotted word lattices that share many common slot fillers ; the lattices are generated by applying a multiplesequence alignment algorithm to a corpus of multiple news articles about the same events '
'43 Baselines 431 Word Alignment We used the GIZA + + implementation of IBM word alignment model 4 <CIT> for word alignment , and the heuristics described in <OTH> to derive the intersection and refined alignment '
'Tbest = argmax T P -LRB- T F -RRB- -LRB- 1 -RRB- P -LRB- T F -RRB- = productdisplay X Y in T Feats = -LCB- ai ai -LRB- X -RRB- -RCB- P -LRB- X Y X , Feats -RRB- -LRB- 2 -RRB- 3 Disambiguation Models The basic generation model presented in <CIT> used simple probabilistic context-free grammars '
'glish -LRB- previously used for self-training of parsers <CIT> -RRB- '
'A monotonic segmentation copes with monotonic alignments , that is , j -LRB- k ? ? aj -LRB- ak following the notation of <CIT> '
'Numerous experiments have shown parallel bilingual corpora to provide a rich source of constraints for statistical analysis <CIT> '
'In particular , this method has been used for word sense disambiguation <CIT> and thesaurus construction <CIT> '
'Most previous work exploiting unsupervised training data for inferring POS tagging models has focused on semi-supervised methods in the in which the learner is provided with a lexicon specifying the possible tags for each word <OTH> or a small number of prototypes for each POS <CIT> '
'Since then this idea has been applied to several tasks , including word sense disambiguation <CIT> and named-entity recognition <CIT> '
'The theory has been applied in probabilistic language modeling <OTH> , natural language processing <CIT> , as well as computational vision <OTH> '
'Finally , since non-projective constructions often involve long-distance dependencies , the problem is closely related to the recovery of empty categories and non-local dependencies in constituency-based parsing <CIT> '
'However , in the experiments described here , we focus on alignment at the level of sentences , this for a number of reasons : First , sentence alignments have so far proven their usefulness in a number of applications , eg bilingual lexicography <OTH> , automatic translation verification <OTH> and the automatic acquisition of knowledge about translation <CIT> '
'<CIT> demonstrates the case of binary SCFG parsing , where six string boundary variables , three for each language as in monolingual CFG parsing , interact with each other , yielding an O -LRB- N6 -RRB- dynamic programming algorithm , where N is the string length , assuming the two paired strings are comparable in length '
'Similar to <CIT> , each word in the confusion network is associated with a word posterior probability '
'accuracy Training data Turney <OTH> 66 % unsupervised <CIT> 8715 % supervised Aue & Gamon -LRB- 2005 -RRB- 914 % supervised SO 7395 % unsupervised SM+SO to increase seed words , then SO 7485 % weakly supervised Table 7 : Classification accuracy on the movie review domain Turney <OTH> achieves 66 % accuracy on the movie review domain using the PMI-IR algorithm to gather association scores from the web '
'We ran GIZA + + <CIT> on the training corpus in both directions using its default setting , and then applied the refinement rule diagand described in <OTH> to obtain a single many-to-many word alignment for each sentence pair '
'2 Related Work ThisworkbuildsuponthatofMcCarthyetal <OTH> which acquires predominant senses for target words from a large sample of text using distributional similarity <CIT> to provide evidence for predominance '
'The measures2 Mutual Information -LRB- a0a2a1 -RRB- <OTH> , the log-likelihood ratio test <CIT> , two statistical tests : t-test and a3a5a4 - test , and co-occurrence frequency are applied to two sets of data : adjective-noun -LRB- AdjN -RRB- pairs and preposition-noun-verb -LRB- PNV -RRB- triples , where the AMs are applied to -LRB- PN , V -RRB- pairs '
'234 ADV Non-specific adverbial BNF Benefemtive CLF It-cleft CLR ''Closely related'' DIR Direction DTV Dative EXT Extent HLN Headline LGS Logical subject L0C Location MNI~ Manner N0M Nominal PRD Predicate PRP Purpose PUT Locative complement of ''put'' SBJ Subject TMP Temporal TPC Topic TTL Title V0C Vocative Grammatical DTV 0.48% LGS 3.0% PRD 18.% PUT 0.26% SBJ 78.% v0c 0.025% Figure 1: Penn treebank function tags 53.% Form/Function 37.% Topicalisation 2.2% 0.25% NOM 6.8% 2.5% TPC 100% 2.2% 1.5% ADV 11.% 4.2% 9.3% BN''F 0.072% 0.026% 0.13% DIR 8.3% 3.0% 41.% EXT 3.2% 1.2% 0.013% LOC 25.% 9.2% MNR 6.2% 2.3% PI~ 5.2% 1.9% 33.% 12.% Miscellaneous 9.5% CLR 94.% 8.8% CLF 0.34% 0.03% HLN 2.6% 0.25% TTL 3.1% 0.29% Figure 2: Categories of function tags and their relative frequencies one project that used them at all: (Collins, 1997) defines certain constituents as complements based on a combination of label and function tag information.'
'More recently , <CIT> use the distinction between pronouns , nominals and proper nouns 660 in their unsupervised , generative model for coreference resolution ; for their model , this is absolutely critical for achieving better accuracy '
'The most frequently used resource for synonym extraction is large monolingual corpora <CIT> '
'32 Translation quality Table 2 presents the impact of parse quality on a treelet translation system , measured using BLEU <CIT> '
'The feasibility of such post-parse deepening -LRB- for a statistical parser -RRB- is demonstrated by <CIT> '
'While transfer learning was proposed more than a decade ago <OTH> , its application in natural language processing is still a relatively new territory <CIT> , and its application in relation extraction is still unexplored '
'Tillmann and Zhang <OTH> and <CIT> et al '
'Such text-oriented ranking methods can be applied to tasks ranging from automated extraction of keyphrases , to extractive summarization and word sense disambiguation <CIT> '
'We run the decoder with its default settings and then use Koehn ''s implementation of minimum error rate training <CIT> to tune the feature weights on the development set '
'Next we use the conclusions from two psycholinguistic experiments on ranking the Cf-list , the salience of discourse entities in prepended phrases <OTH> and the ordering of possessor and possessed in complex NPs <OTH> , to try to improve the performance of LRC '
'No pretagged text is necessary for Hidden Markov Models <CIT> '
'<CIT> used the averaged perceptron <CIT> '
'For handling word identities , one could follow the approach used for handling the POS tags <OTH> and view the POS tags and word identities as two separate sources of information '
'22 Using Log-Likelihood-Ratios to Estimate Word Translation Probabilities Our method for computing the probabilistic translation lexicon LLR-Lex is based on the the Log2http : \/ \/ wwwfjochcom\/GIZA + + html Likelihood-Ratio -LRB- LLR -RRB- statistic <CIT> , which has also been used by Moore -LRB- 2004a ; 2004b -RRB- and Melamed -LRB- 2000 -RRB- as a measure of word association '
': ~ The difl ` erent kinds of noun chunks covered by our grmnmar are listed below and illustrated with exmnples : a combination of a non-obligatory deternfiner , optional adjectives or cardinals and the noun 1Other types of lexicalised PCFGs have been -LRB- h ! scrib -LRB- ''d in <OTH> , <CIT> , <OTH> , -LRB- Chcll -RRB- a and lelinek , 1998 -RRB- mid <OTH> '
'Much research has been done to improve tagging accuracy using several different models and methods , including : hidden Markov models -LRB- HMMs -RRB- <OTH> , <OTH> ; rule-based systems <OTH> , <OTH> ; memory-based systems <OTH> ; maximum-entropy systems <CIT> ; path voting constraint systems <OTH> ; linear separator systems <OTH> ; and majority voting systems <OTH> '
'3 Model 1 and Model 2 l ~ cl -RRB- lacing the -LRB- l -LRB- ~ , t -RRB- endence on aj-l in the HMM alignment mo -LRB- M I -RRB- y a del -RRB- endence on j , we olltain a model wlfich -LRB- : an lie seen as a zero-order Hid -LRB- l -LRB- mMarkov Model which is similar to Model 2 1 -RRB- rot -RRB- ose -LRB- t t\/y <CIT> '
'In particular , we use a randomly-selected corpus the first five columns as information-like consisting of a 67 million word subset of the TREC Similarly , since the last four columns share databases <OTH> '
'The model scaling factors M1 are trained with respect to the final translation quality measured by an error criterion <CIT> '
'Some research into factored machine translation has been published by <CIT> '
'The feature weights for the overall translation models were trained using <CIT> minimum-error-rate training procedure '
'The standard method to overcome this problem to use the model in both directions -LRB- interchanging the source and target languages -RRB- and applying heuristic-based combination techniques to produce a refined alignment <CIT> henceforth referred to as RA Several researchers have proposed algorithms for improving word alignment systems by injecting additional knowledge or combining different alignment models '
'This tagging scheme is the IOB scheme originally put forward by <CIT> '
'212 Research on Syntax-Based SMT A number of researchers <OTH> have proposed models where the translation process involves syntactic representations of the source and\/or target languages '
'<CIT> , 2004 -RRB- and lscript22 regularization <OTH> '
'Lexical collocation functions , especially those determined statistically , have recently attracted considerable attention in computational linguistics <CIT> mainly , though not exclusively , for use in disambiguation '
'Previous research in automatic acquisition focuses primarily on the use of statistical techniques , such as bilingual alignment <CIT> , or extraction of syntactic constructions from online dictionaries and corpora <OTH> '
'However , <CIT> found that it is actually harmful to restrict phrases to constituents in parse trees , because the restriction would cause the system to miss many reliable translations , such as the correspondence between there is in English and es gibt -LRB- it gives -RRB- in German '
'Pivots are features occurring frequently and behaving similarly in both domains <CIT> '
'A very impor232 Author Best Hindle and Rooth <OTH> 800 % Resnik and Hearst <OTH> 839 % WN Resnik and Hearst <OTH> 750 % Ratnaparkhi et al '
'Most systems extract co-occurrence and syntactic information from the words surrounding the target term , which is then converted into a vector-space representation of the contexts that each target term appears in <CIT> '
'Table 1 shows the impact of increasing reordering window length <CIT> on translation quality for the ? dev06 ? ? data2 Increasing the reordering window past 2 has minimal impact on translation quality , implying that most of the reordering effects across Spanish and English are well modeled at the local or phrase level '
'thresholding (DeNero and Klein, 2007).'
'Parse selection constitutes an important part of many parsing systems <OTH> '
'<CIT> used skip-chain Conditional Random Fields to model pragmatic dependencies between paired meeting utterances -LRB- eg QUESTION-ANSWER relations -RRB- , and used a combination of lexical , prosodic , structural and discourse features to rank utterances by importance '
'Since it is not feasible to maximize the likelihood of the observations directly , we maximize the expected log likelihood by considering the EM auxiliary function , in a similar manner to that used for modelling contextual variations of phones for ASR <OTH> '
'Recall that the log likelihood of our model is : d parenleftBigg Lorig -LRB- Dd ; d -RRB- i -LRB- d , i , i -RRB- 2 2 2d parenrightBigg i -LRB- , i -RRB- 2 2 2 We now introduce a new variable d = d , and plug it into the equation for log likelihood : d parenleftBigg Lorig -LRB- Dd ; d + -RRB- i -LRB- d , i -RRB- 2 2 2d parenrightBigg i -LRB- , i -RRB- 2 2 2 The result is the model of <CIT> , where the d are the domain-specific feature weights , and d are the domain-independent feature weights '
'Agreement among annotators was measured using the K statistic <CIT> '
'We divided these case roles into four types by location in the article as in (Iida et al., 2006), i) the case role depends on the predicate or the predicate depends on the case role in the intra-sentence (dependency relations), ii) the case role does not depend on the predicate and the predicate does not depend on the case role in the intra-sentence (zeroanaphoric (intra-sentential)), iii) the case role is not in the sentence containing the predicate (zeroanaphoric (inter-sentential)), and iv) the case role and the predicate are in the same phrase (in same phrase).'
'21 Heuristic Grammar Induction Grammar based SMT models almost exclusively follow the same two-stage approach to grammar induction developed for phrase-based methods <CIT> '
'Line 4 and 5 are similar to the phrase extraction algorithm by <CIT> '
'External information such as the discourse or domain dependency of each word sense <CIT> is expected to lead to system improvement '
'With IOB2 representation <CIT> , the problem of Chinese chunking can be regarded as a sequence labeling task '
'This contrasts with alternative alignment models such as those of Melamed <OTH> and <CIT> , which impose a one-to-one constraint on alignments '
'For each feature function , there is a model parameter i The best word segmentation W \* is determined by the decision rule as = = = M i ii W M W WSfWSScoreW 0 0 \* -RRB- , -LRB- maxarg -RRB- , , -LRB- maxarg -LRB- 2 -RRB- Below we describe how to optimize s Our method is a discriminative approach inspired by the Minimum Error Rate Training method proposed in <CIT> '
'2 Word-to-Word Bitext Alignment We will study the problem of aligning an English sentence to a French sentence and we will use the word alignment of the IBM statistical translation models <CIT> '
'This incremental process can be iterated to the point that the system 1 It is not just a matter of time , but also of required linguistic skills -LRB- see for example <CIT> -RRB- '
'<CIT> use cascaded processing for full parsing with good results '
'predict correctly the label of a test instance xN +1 is bounded by 2N +1 EN +1 bracketleftbigd + D bracketrightbig2 where D = D -LRB- w , , -RRB- = radicalBigsummationtext N i = 12i This result is used to explain the convergence of weighted or voted perceptron algorithms <CIT> '
'10Both Pharoah and our system have weights trained using MERT <CIT> on sentences of length 30 words or less , to ensure that training and test conditions are matched '
'These features are calculated by mining the parse trees , and then could be used for resolution by using manually designed rules <OTH> , or using machine-learning methods <CIT> '
'5 Effectiveness Comparison 51 English-Chinese ATIS Models Both the transfer and transducer systems were trained and evaluated on English-to-Mandarin Chinese translation of transcribed utterances from the ATIS corpus <OTH> '
'It is possible to recognize a common structure of these works , based on a typical bootstrap schema <CIT> : Step 1 : Initial unsupervised categorization '
'It has been shown that both Nave Bayes and SVMs perform with similar accuracy on different sentiment tagging tasks <CIT> '
'Note that the translation direction is inverted from what would be normally expected ; correspondingly the models built around this equation are often called invertedtranslationmodels <CIT> '
'1 Introduction The research presented in this paper forms part of an ongoing effort to develop methods to induce wide-coverage multilingual LexicalFunctional Grammar -LRB- LFG -RRB- <OTH> resources from treebanks by means of automatically associating LFG f-structure information with constituency trees produced by probabilistic parsers <CIT> '
'(DeRose 1988; Cutting et al 1992; Merialdo 1994).'
'We trained log linear models with theperceptronalgorithm <CIT> usingfea746 Markov order Classification Task 0 1 2 S1 -LRB- no multi-word constituent start -RRB- 967 969 969 E1 -LRB- no multi-word constituent end -RRB- 973 973 973 Table 2 : Classification accuracy on development set for binary classes S1 and E1 , for various Markov orders '
'Other models -LRB- <CIT> , Xiong et al '
'1 Introduction Shallow parsing has received a reasonable amount of attention in the last few years -LRB- for example <CIT> -RRB- '
'The best accuracies are observed when the labelsarecreatedfromdistributionallysimilarwords using <CIT> dependency-based similarity measure -LRB- Depend -RRB- '
'While they train the parameters using a maximum a posteriori estimator , we extend the MERT algorithm <CIT> to take the evaluation metric into account '
'For classi cation , we use a maximum entropy model <CIT> , from the logistic regression package in Weka <OTH> , with all default parameter settings '
'Parametertuningwasdonewithminimum error rate training <CIT> , which was used to maximize BLEU <OTH> '
'Alignment models to structure the translation model are introduced in <CIT> '
'As the training data from DVDs is much more similar to books than that from kitchen <CIT> , we should give the data from DVDs a higher weight '
'Since Odds = P/(1  P), we multiply both sides of Definition 3 by (1P(U|E))1 to obtain, P(U|E) 1P(U|E) = P(E|U)P(U) P(E)(1P(U|E)) (7) By substituting Equation 6 in Equation 7 and later, applying the multiplication rule P(U|E)P(E) = P(E|U)P(U) to it, we will obtain: P(U|E) P(U|E) = P(E|U)P(U) P(E|U)P(U) (8) We proceed to take the log of the odds in Equation 8 (i.e. logit) to get: log P(E|U)P(E|U) = log P(U|E)P(U|E) log P(U)P(U) (9) While it is obvious that certain words tend to cooccur more frequently than others (i.e. idioms and collocations), such phenomena are largely arbitrary (Smadja, 1993).'
'Indeed , the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules <CIT> '
'We present two approaches to SMT-based query expansion , both of which are implemented in the framework of phrase-based SMT <CIT> '
'In particular , most of the work on parsing with kernel methods has focussed on kernels over parse trees <CIT> '
'Most current approaches emphasize within-sentence dependencies such as the distortion in <CIT> , the dependency of alignment in HMM <OTH> , and syntax mappings in <OTH> '
'At the same time , grammar theoreticians have proposed various generative synchronous grammar formalisms for MT , such as Synchronous Context Free Grammars -LRB- S-CFG -RRB- <CIT> or Synchronous Tree Adjoining Grammars -LRB- S-TAG -RRB- <OTH> '
'However , Moores Law , the driving force of change in computing since then , has opened the way for recent progress in the field , such as Statistical Machine Translation -LRB- SMT -RRB- <CIT> '
'Its still possible to use MSA if , for example , the input is pre-clustered to have the same constituent ordering -LRB- <CIT> -RRB- '
'ROUGE <CIT> is an evaluation metric designed to evaluate automatically generated summaries '
'In TAC 2008 Summarization track , all submitted runs were scored with the ROUGE <CIT> and Basic Elements -LRB- BE -RRB- metrics <OTH> '
'5 The Experimental Results We used the Penn Treebank WSJ corpus <CIT> to perform empirical experiments on the proposed parsing models '
'Since word senses are often associated with domains <CIT> , word senses can be consequently distinguished by way of determining the domain of each description '
'First , we can construct an infinite number of more specialized PCFGs by splitting or refining the PCFGs nonterminals into increasingly finer states ; this leads to the iPCFG or infinite PCFG <CIT> '
'Machine translation based on a deeper analysis of the syntactic structure of a sentence has long been identified as a desirable objective in principle -LRB- consider <CIT> -RRB- '
'W -LRB- S , T -RRB- = summationdisplay uS , vT w -LRB- u , v -RRB- Globally optimal minimum cuts can be found in polynomial time and near-linear running time in practice , using the maximum flow algorithm <CIT> '
'The words with the highest association probabilities are chosen as acquired words for entity e 41 Base Model I Using the translation model I <CIT> , where each word is equally likely to be aligned with each entity , we have p -LRB- w e -RRB- = 1 -LRB- l + 1 -RRB- m mproductdisplay j = 1 lsummationdisplay i = 0 p -LRB- wj ei -RRB- -LRB- 1 -RRB- where l and m are the lengths of entity and word sequences respectively '
'The alignment a J 1 that has the highest probability -LRB- under a certain model -RRB- is also called the Viterbi alignment -LRB- of that model -RRB- : a J 1 = argmax a J 1 p -LRB- f J 1 , a J 1 e I 1 -RRB- -LRB- 8 -RRB- A detailed comparison of the quality of these Viterbi alignments for various statistical alignment models compared to human-made word alignments can be found in <CIT> '
'This approach has also been used by -LRB- Dagan and Itai , 1994 ; Gale et al , 1992 ; Shiitze , 1992 ; Gale et al , 1993 ; Yarowsky , 1995 ; Gale and Church , 1Lunar is not an unknown word in English , Yeltsin finds its translation in the 4-th candidate '
'Based on the proofs in <CIT> and Li et al '
'corpora and corpus query tools has been particularly significant in the area of compiling and developing lexicographic materials <OTH> and in the area of creating various kinds of lexical resources , such as WordNet <OTH> and FrameNet <OTH> '
'Our experience suggests that disjunctive LFs are an important capability , especially as one seeks to make grammars reusable across applications , and to employ domain-specific , sentence-level paraphrases <CIT> '
'This also makes our grammar weakly equivalent to an inversion transduction grammar <CIT> , although the conversion would create a very large number of new nonterminal symbols '
'The statistic <CIT> is recast as : -LRB- fs , w -RRB- -LRB- sys , sys -RRB- = agr -LRB- fs , w -RRB- -LRB- sys , sys -RRB- P agr -LRB- fs , -RRB- -LRB- sys , sys -RRB- N P agr -LRB- fs , -RRB- -LRB- sys , sys -RRB- N In this modified form , -LRB- fs , w -RRB- represents the divergence in relative agreement wrt f s for target noun w , relative to the mean relative agreement wrt f s over all words '
'Clark <OTH> reports results on a corpus containing 12 million terms , Schcurrency1utze <OTH> on one containing 25 million terms , and <CIT> on one containing 365 million terms '
'They give a probabilistic formation of paraphrasing which naturally falls out of the fact that they use techniques from phrase-based statistical machine translation: e2 = argmax e2:e2negationslash=e1 p(e2|e1) (1) where p(e2|e1) = summationdisplay f p(f|e1)p(e2|f,e1) (2)  summationdisplay f p(f|e1)p(e2|f) (3) Phrase translation probabilities p(f|e1) and p(e2|f) are commonly calculated using maximum likelihood estimation (Koehn et al., 2003): p(f|e) = count(e,f)summationtext f count(e,f) (4) where the counts are collected by enumerating all bilingual phrase pairs that are consistent with the 197 conseguido .opportunitiesequalcreatetofailedhasprojecteuropeanthe oportunidadesdeigualdadlahanoeuropeoproyectoel Figure 1: The interaction of the phrase extraction heuristic with unaligned English words means that the Spanish phrase la igualdad aligns with equal, create equal, and to create equal.'
'However , in yet unpublished work we found that at least for the computation of synonyms and related words neither syntactical analysis nor singular value decomposition lead to significantly better results than the approach described here when applied to the monolingual case <CIT> , so we did not try to include these methods in our system '
'These range from twoword to multi-word , with or without syntactic structure <CIT> '
'HockenmaierandSteedman <OTH> showedthat a CCG corpus could be created by adapting the Penn Treebank <CIT> '
'2 Prior Work Statistical machine translation , as pioneered by IBM <OTH> , is grounded in the noisy channel model '
'The notation will assume ChineseEnglish word alignment and ChineseEnglish MT Here we adopt a notation similar to <CIT> '
'The complexities of 15 restricted alignment problems in two very different synchronous grammar formalisms of syntax-based machine translation , inversion transduction grammars -LRB- ITGs -RRB- <CIT> and a restricted form of range concatenation grammars -LRB- -LRB- 2,2 -RRB- - BRCGs -RRB- <OTH> , are investigated '
'In all other respects , our work departs from previous research on broad -- coverage 16 I t I I I I I i ! I i I I I I I I I I I I I i I 1 , I I I I I i I 1 I I I I probabilistic parsing , which either attempts to learn to predict gr ~ rarn ~ tical structure of test data directly from a training treebank <OTH> , or employs a grammar and sometimes a dictionary to capture linguistic expertise directly <OTH> , but arguably at a less detailed and informative level than in the research reported here '
'The algorithm is slightly different from other online training algorithms <CIT> in that we keep and update oracle translations , which is a set of good translations reachable by a decoder according to a metric , ie BLEU <OTH> '
'For instance , several studies have shown that BLEU correlates with human ratings on machine translation quality <CIT> '
'The SemEval-2010 task we present here builds on thework ofNakov <CIT> , where NCs are paraphrased by combinations of verbs and prepositions '
'In order to build models that perform well in new -LRB- target -RRB- domains we usually find two settings <CIT> '
'<CIT> , Tillmann -LRB- 2003 -RRB- , and Vogel et al '
'(Lin, 2004; Lin and Och, 2004).'
'The two annotators agreed on the annotations of 385\/453 turns , achieving 8499 % agreement -LRB- Kappa = 068 <CIT> -RRB- '
'So , we pre-tagged the input to the Bikel parser using the MXPOST tagger <CIT> '
'For instance , some approaches coarsely discriminate between biographical and non-biographical information <CIT> , whileothersgobeyondbinary distinction by identifying atomic events eg , occupation and marital status that are typically included in a biography <OTH> '
'One of the popular statistical machine translation paradigms is the phrase-based model -LRB- PBSMT -RRB- <CIT> '
'The assumptions we made were the following:  a lexical token in one half of the translation unit (TU) corresponds to at most one non-empty lexical unit in the other half of the TU; this is the 1:1 mapping assumption which underlines the work of many other researchers (Ahrenberg et al (2000), Brew and McKelvie (1996), Hiemstra (1996), Kay and Rscheisen (1993), Tiedmann (1998), Melamed (2001) etc);  a polysemous lexical token, if used several times in the same TU, is used with the same meaning; this assumption is explicitly used by Gale and Church (1991), Melamed (2001) and implicitly by all the previously mentioned authors;  a lexical token in one part of a TU can be aligned to a lexical token in the other part of the TU only if the two tokens have compatible types (part-of-speech); in most cases, compatibility reduces to the same POS, but it is also possible to define other compatibility mappings (e.g. participles or gerunds in English are quite often translated as adjectives or nouns in Romanian and vice-versa);  although the word order is not an invariant of translation, it is not random either (Ahrenberg et al (2000)); when two or more candidate translation pairs are equally scored, the one containing tokens which are closer in relative position are preferred.'
'As with conventional smoothing methods <CIT> , triangulation increases the robustness of phrase translation estimates '
'More specifically , the latter system uses the IBM-1 lexical parameters <CIT> for computing the translation probabilities of two possible new tuples : the one resulting when the null-aligned-word is attached to Table 6 Evaluation results for experiments on n-gram size incidence '
'Step 3 -RRB- Answer Extraction : We select the top 5 ranked sentences and return them as <CIT> , 1997 -RRB- , can be used to capture the binary dependencies between the head of each phrase '
'2 Summary of approaches Given a source language sentence f, statistical machine translation defines the translation task as selecting the most likely target translation e under a model P(e|f), i.e.: e(f) = argmax e P(e|f) = argmax e msummationdisplay i=1 hi(e,f)i where the argmax operation denotes a search through a structured space of translation ouputs in the target language, hi(e,f) are bilingual features of e and f and monolingual features of e, and weights i are trained discriminitively to maximize translation quality (based on automatic metrics) on held out data (Och, 2003).'
'On the other end of the spectrum , character-based bitext mapping algorithms <OTH> are limited to language pairs where cognates are common ; in addition , they may easily be misled by superficial differences in formatting and page layout and must sacrifice precision to be computationally tractable '
'Freund and Schapire <OTH> originally proposed the averaged parameter method ; it was shown to give substantial improvements in accuracy for tagging tasks in <CIT> '
'Hw6 : Implement beam search and reduplicate the POS tagger described in <CIT> '
'1 Introduction Supervised statistical parsers attempt to capture patterns of syntactic structure from a labeled set of examples for the purpose of annotating new sentences with their structure <CIT> '
'Thus , we can compute the source dependency LM score in the same way we compute the target side score , using a procedure described in <OTH> '
'A number of other re532 searchers <CIT> have described previous work on preprocessing methods '
'For our studies here , the parser employed was that of <CIT> applied to the sentences of the British National Corpus <OTH> '
'The most commonly used automatic evaluation metrics , BLEU <CIT> and NIST <OTH> , are based on the assumption that The closer a machine translation is to a professional human translation , the better it is <CIT> '
'As with many dependency parsers <CIT> , we handle non-projective -LRB- ie crossing -RRB- arcs by transforming them into noncrossing arcs with augmented labels1 Because our syntactic derivations are equivalent to those of <OTH> , we use their HEAD methods to projectivise the syntactic dependencies '
'4 Semi-Supervised Training for Word Alignments Intuitively , in approximate EM training for Model 4 <CIT> , the E-step corresponds to calculating the probability of all alignments according to the current model estimate , while the M-step is the creation of a new model estimate given a probability distribution over alignments -LRB- calculated in the E-step -RRB- '
'The kappa statistic <CIT> has become the de facto standard to assess inter-annotator agreement '
'However , they make different types of errors , which can be seen as a reflection of their theoretical differences <CIT> '
'We just assign these rules a constant score trained using our implementation of Minimum Error Rate Training <CIT> , which is 07 in our system '
'3 Monolingual comparable corpus : Similar to the methods in <CIT> , we construct a corpus of comparable documents from a large corpus D of news articles '
'We can stipulate the time line to be linearly ordered -LRB- although it is not in approaches that build ignorance of relative times into the representation of time <CIT> nor in approaches using branching futures <OTH> -RRB- , and we can stipulate it to be dense -LRB- although it is not in the situation calculus -RRB- '
'In cases where the number of gold tags is different than the number of induced tags , some must necessarily remain unassigned <CIT> '
'Modulo more minor differences , these notions are close to the ideas of interpretation as abduction <CIT> and generation as abduction -LRB- ltobbs et al -LRB- 1990:26 -28 -RRB- -RRB- , where we take abduction , in the former case for instance , to be a process returning a temporal-causal structure which can explain the utterance in context '
'Then P -LRB- eI1jfj1 -RRB- = summationtextaI 1 P -LRB- eI1 , aI1jfj1 -RRB- <CIT> '
'In the general language UPenn annotation efforts for the WSJ sections of the Penn Treebank <CIT> , sentences are annotated with POS tags , parse trees , as well as discourse annotation from the Penn Discourse Treebank <OTH> , while verbs and verb arguments are annotated with Propbank rolesets <OTH> '
'These methods often involve using a statistic such as 2 <OTH> or the log likelihood ratio <CIT> to create a score to measure the strength of correlation between source and target words '
'Some methods only extract paraphrase patternsusingnewsarticlesoncertaintopics <CIT> , while some others need seeds as initial input <OTH> '
'Again we used Mohammad and Hirsts <OTH> method along with <CIT> distributional measure to determine the distributional closeness of two thesaurus concepts '
'-LRB- Collins parser <CIT> always predicts a flat NP for such configurations -RRB- '
'Stage 2 processing is then free to assign to the compound any bracketing for which it 3The design of this level of Lucy is influenced by <CIT> , which advocates a level of ` surfaey '' logical form with predicates close to actual English words and a structure similar to the syntactic structure of the sentence '
'The feature weights are tuned by the modified Koehns MER <CIT> trainer '
'SCL for Discriminative Parse Selection So far , pivot features on the word level were used <CIT> '
'In particular , we adopt the approach of phrase-based statistical machine translation <CIT> '
'This makes it suitable for discriminative SMT training , which is still a challenge for large parameter sets <CIT> '
'1409 cally , and experimentally <CIT> '
'Unfortunately , as was shown by Fraser and Marcu <OTH> AER can have weak correlation with translation performance as measured by BLEU score <CIT> , when the alignments are used to train a phrase-based translation system '
'Decoding Conditions For tuning of the decoder ''s parameters , minimum error training <CIT> with respect to the BLEU score using was conducted using the respective development corpus '
'We perform a statistical analysis that provides information that complements the information provided by Cohen ''s Kappa <CIT> '
'We would expect better performance with the more accurate approximation based on variational inference proposed and evaluated in <CIT> '
'The fertility for the null word is treated specially <CIT> '
'(Berger et al. , 1996).'
'The rules extracted from the training bitext have the following features: a114 P( | )andP( | ), the latter of which is not found in the noisy-channel model, but has been previously found to be a helpful feature (Och and Ney 2002); 210 Chiang Hierarchical Phrase-Based Translation a114 the lexical weights P w ( | )andP w ( | ), which estimate how well the words in  translate the words in  (Koehn, Och, and Marcu 2003); 4 a114 a penalty exp(1) for extracted rules, analogous to Koehns phrase penalty (Koehn 2003), which allows the model to learn a preference for longer or shorter derivations.'
'1 Introduction Recent work in machine translation has evolved from the traditional word <CIT> and phrase based <OTH> models to include hierarchical phrase models <OTH> and bilingual synchronous grammars <OTH> '
'The traditional estimation method for word 98 alignment models is the EM algorithm <CIT> which iteratively updates parameters to maximize the likelihood of the data '
'Such transformations are typically denoted as paraphrases in the literature , where a wealth of methods for their automatic acquisition were proposed <CIT> '
'Most of the early work in this area was based on postulating generative probability models of language that included parse structures <CIT> '
'Pure statistical machine translation <CIT> mltst in principle recover the most probable alignment out of all possible alignments between the input and a translation '
'Method dev test Finkel et al , 2005 <CIT> baseline CRF 8551 + non-local features 8686 Krishnan and Manning , 2006 <OTH> baseline CRF 8529 + non-local features 8724 Table 5 : Summary of performance with POS\/chunk tags by TagChunk '
'The subsequent construction of translation table was done in exactly the same way as explained 4 in <CIT> '
'16In fact , we have experimented with other tagger combinations and configurations as wellwith the TnT <OTH> , MaxEnt <CIT> and TreeTagger <OTH> , with or without the Morce tagger in the pack ; see below for the winning combination '
'The Attr cells summarize the performance of the 6 models on the wiki table that are based on attributional similarity only <CIT> '
'Starting out with a chunking pipeline , which uses a classical combination of tagger and chunker , with the Stanford POS tagger <OTH> , the YamCha chunker <OTH> and the Stanford Named Entity Recognizer <CIT> , the desire to use richer syntactic representations led to the development of a parsing pipeline , which uses Charniak and Johnsons reranking parser <OTH> to assign POS tags and uses base NPs as chunk equivalents , while also providing syntactic trees that can be used by feature extractors '
'2 Related Work The issue of MWE processing has attracted much attention from the Natural Language Processing -LRB- NLP -RRB- community , including Smadja , 1993 ; Dagan and Church , 1994 ; Daille , 1995 ; 1995 ; McEnery et al , 1997 ; <CIT> , 1997 ; Michiels and Dufour , 1998 ; Maynard and Ananiadou , 2000 ; Merkel and Andersson , 2000 ; Piao and McEnery , 2001 ; Sag et al , 2001 ; Tanaka and Baldwin , 2003 ; Dias , 2003 ; Baldwin et al , 2003 ; Nivre and Nilsson , 2004 Pereira et al , '
'In the February 2004 version of the PropBank corpus , annotations are done on top of the Penn TreeBank II parse trees <CIT> '
'We present two approaches to SMT-based query expansion , both of which are implemented in the framework of phrase-based SMT <CIT> '
'Our work expands on the general approach taken by <OTH> but arrives at insights similar to those of the most recent work <CIT> , albeit in a completely different manner '
'For efficiency reasons we report results on sentences of length 30 words or less10 The syntax-based method gives a BLEU <CIT> score of 2504 , a 046 BLEU point gain over Pharoah '
'Others use sentence cohesion <OTH> , agreement\/disagreement between speakers <CIT> , or structural adjacency '
'Grammar rules were induced with the syntaxbased SMT system SAMT described in <OTH> , which requires initial phrase alignments that we generated with GIZA + + <CIT> , and syntactic parse trees of the target training sentences , generated by the Stanford Parser <OTH> pre-trained on the Penn Treebank '
'The latter group did an experiment early on in which they found that manual tagging took about twice as long as correcting -LRB- automated tagging -RRB- , with about twice the interannotator disagreement rate and an error rate that was about 50 % higher <CIT> '
'Stochastic taggers use both contextual and morphological information , and the model parameters are usually defined or updated automatically from tagged texts -LRB- Cerf-Danon and E1-Beze 1991 ; Church 1988 ; <CIT> et al 1992 ; Dermatas and Kokkinakis 1988 , 1990 , 1993 , 1994 ; Garside , Leech , and Sampson 1987 ; Kupiec 1992 ; Maltese \* Department of Electrical Engineering , Wire Communications Laboratory -LRB- WCL -RRB- , University of Patras , 265 00 Patras , Greece '
'SRILM <OTH> can produce classes to maximize the mutual information between the classes I -LRB- C -LRB- wt -RRB- ; C -LRB- wt 1 -RRB- -RRB- , as described in <CIT> '
'<CIT> found that human summarization can be traced back to six cut-andpaste operations of a text and proposed a revision method consisting of sentence reduction and combination modules with a sentence extraction part '
'However , the fact that the DGSSN uses a large-vocabulary tagger <CIT> as a preprocessing stage may compensate for its smaller vocabulary '
'In our framework , we employ a simple HMM-based tagger , where the most probable tag sequence , a29a30 , given the words , a31 , is output <OTH> : a29 a30 a20a22a32a34a33a36a35a38a37a39a32a41a40 a42 a43a45a44 a30a47a46 a31a49a48a17a20a22a32a34a33a50a35a38a37a39a32a41a40 a42 a43a45a44 a31 a46a30 a48 a43a51a44 a30 a48 Since we do not have enough data which is manually tagged with part-of-speech tags for our applications , we used Penn Treebank <CIT> as our training set '
'3 Candidates extraction on Suffix array Suffix array -LRB- also known as String PATarray -RRB- <OTH> is a compact data structure to handle arbitrary-length strings and performs much powerful on-line string search operations such as the ones supported by PAT-tree , but has less space overhead '
'A CHECK move requests the partner to confirm information that the speaker has some reason to believe , but is not entirely sure about <CIT> '
'The reader is referred to Schmid <OTH> and <CIT> for details '
'We propose using distributional similarity -LRB- using <CIT> -RRB- as an approximation of semantic distancebetweenthewordsinthetwoglosses , rather than requiring an exact match '
'For our experiments we used the following features , analogous to Pharaohs default feature set : P -LRB- -RRB- and P -LRB- -RRB- , the latter of which is not found in the noisy-channel model , but has been previously found to be a helpful feature <OTH> ; the lexical weights Pw -LRB- -RRB- and Pw -LRB- -RRB- <CIT> , which estimate how well the words in translate the words in ; 2 a phrase penalty exp -LRB- 1 -RRB- , which allows the model to learn a preference for longer or shorter derivations , analogous to Koehns phrase penalty <CIT> '
'<CIT> presented an approach that significantly reduces the amount of labeled data needed for word sense disambiguation '
'Pointwise mutual information <OTH> was used to measure strength of selection restrictions for instance by <CIT> '
'There has been a sizable amount of research on structure induction ranging fromlinearsegmentation <OTH> tocontent modeling <OTH> '
'In another line of research , <CIT> and <OTH> have shown that it is possible to reduce the need for supervision with the help of large amounts of unannotated data '
'In modern lexicalized parsers , POS tagging is often interleaved with parsing proper instead of being a separate preprocessing module <CIT> '
'3 Related Work Many methods have been developed for automatically identifying subjective -LRB- opinion , sentiment , attitude , affect-bearing , etc -RRB- words , eg , <CIT> '
'However , <CIT> showed that for natural language and text processing tasks , conditional models are usually better than joint likelihood models '
'al 2003b -RRB- 147 is -LRB- B -RRB- eginning , -LRB- I -RRB- nside or -LRB- O -RRB- utside of a chunk <CIT> '
'However , with their system trained on the medical corpus and then tested on the Wall Street Journal corpus <CIT> , they achieve an overall prediction accuracy of only 54 % '
'We solve this using the local search defined in <CIT> '
'The extraction procedure utilizes a head percolation table as introduced by Magerman <OTH> in combination with a variation of <CIT> approach to the differentiation between complement and adjunct '
'We can incorporate each model into the system in turn , and rank the results on a test corpus using BLEU <CIT> '
'Most systems for automatic role-semantic analysis have used constituent syntax as in the Penn Treebank <CIT> , although there has also been much research on the use of shallow syntax <OTH> in SRL '
'Pereira <OTH> , Curran <OTH> and <CIT> use syntactic features in the vector definition '
'3 Hebrew Simple NP Chunks The standard definition of English base-NPs is any noun phrase that does not contain another noun phrase , with possessives treated as a special case , viewing the possessive marker as the first word of a new base-NP <CIT> '
'(1) Here has(h,x) is a binary function that returns true if the history h has feature x.Inour experiments, we focused on such information as whether or not a string is found in a dictionary, the length of the string, what types of characters are used in the string, and what part-of-speech the adjacent morpheme is. Given a set of features and some training data, the M.E. estimation process produces a model, which is represented as follows (Berger et al. , 1996; Ristad, 1997; Ristad, 1998): P(f|h)= producttext i  g i (h,f) i Z  (h) (2) Z  (h)= summationdisplay f productdisplay i  g i (h,f) i.'
'Artificial ungrammaticalities have been used in various NLP tasks <CIT> The idea of an automatically generated ungrammatical treebank was proposed by Foster <OTH> '
'Other authors have applied this approach to language modeling <OTH> '
'A quick search in the Penn Treebank <CIT> shows that about 17 % of all sentences contain parentheticals or other sentence fragments , interjections , or unbracketable constituents '
'2 F 1 - score Maximization Training of LRM We first review the F 1 - score maximization training method for linear models using a logistic function described in <CIT> '
'In this respect , it resembles bilingual bracketing <CIT> , but our model has more lexical items in the blocks with many-to-many word alignment freedom in both inner and outer parts '
'41 Experimental Setup Like several previous work -LRB- eg , Mullen and Collier <OTH> , <CIT> and Lee <OTH> , Whitelaw et al '
'Much research has been done to improve tagging accuracy using several different models and methods , including : hidden Markov models -LRB- HMMs -RRB- <OTH> , <OTH> ; rule-based systems <OTH> , <OTH> ; memory-based systems <OTH> ; maximum-entropy systems <OTH> ; path voting constraint systems <OTH> ; linear separator systems <OTH> ; and majority voting systems <OTH> '
'In recent years , many researchers have tried to automatically evaluate the quality of MT and improve the performance of automatic MT evaluations <CIT> because improving the performance of automatic MT evaluation is expected to enable us to use and improve MT systems efficiently '
'While we have observed reasonable results with both G 2 and Fisher ''s exact test , we have not yet discussed how these results compare to the results that can be obtained with a technique commonly used in corpus linguistics based on the mutual information -LRB- MI -RRB- measure <CIT> : I -LRB- x , y -RRB- -- log 2 P -LRB- x , y -RRB- -LRB- 4 -RRB- P -LRB- x -RRB- P -LRB- y -RRB- In -LRB- 4 -RRB- , y is the seed term and x a potential target word '
'The weighting parameters of these features were optimized in terms of BLEU by the approach of minimum error rate training <CIT> '
'Concluding Remarks Formalisms for finite-state and context-free transduction have a long history <OTH> , and such formalisms have been applied to the machine translation problem , both in the finite-state case <OTH> and the context-free case <CIT> '
'We accordingly introduce approaches which attempt to include semantic information into the coreference models from a variety of knowledge sources , eg WordNet <OTH> , Wikipedia <CIT> and automatically harvested patterns <OTH> '
'For evaluation , we use IBMs BLEU score <CIT> to measure the performance of the SMS normalization '
'They compare two data representations and report that a representation with bracket structures outperforms the IOB tagging representation introduced by <CIT> '
'Learning in this context consisted of estimating the parameters of the model with simple likelihood based techniques , but incorporating various smoothing and back-off estimation tricks to cope with the sparse data problems <CIT> '
'Doing inference with representations close to natural language has also been advocated by Jerry Hobbs , as in <CIT> '
'In Hirschberg and Nakatani <OTH> , average reliability <CIT> of segmentinitial labels among 3 coders on 9 monologues produced by the same speaker , labeled using text and speech , is8 or above for both read and spontaneous speech ; values of at least 8 are typically viewed as representing high reliability -LRB- see Section 32 -RRB- '
'In all other respects , our work departs from previous research on broad -- coverage 16 I t I I I I I i ! I i I I I I I I I I I I I i I 1 , I I I I I i I 1 I I I I probabilistic parsing , which either attempts to learn to predict gr ~ rarn ~ tical structure of test data directly from a training treebank <OTH> , or employs a grammar and sometimes a dictionary to capture linguistic expertise directly <OTH> , but arguably at a less detailed and informative level than in the research reported here '
'We want to avoid training a metric that as5Or , in a less adversarial setting , a system may be performing minimum error-rate training <CIT> signs a higher than deserving score to a sentence that just happens to have many n-gram matches against the target-language reference corpus '
'Similarly , Murdock and Croft <OTH> adopted a simple translation model from IBM model 1 <CIT> and applied it to QA '
'These relations are then used for various tasks , ranging from the interpretation of a noun sequence <OTH> or a prepositional phrase <OTH> , to resolving structural ambiguity <OTH> , to merging dictionary senses for WSD <CIT> '
'2 Extracting paraphrases Much previous work on extracting paraphrases <CIT> has focused on finding identifying contexts within aligned monolingual sentences from which divergent text can be extracted , and treated as paraphrases '
'The training of IBM model 4 was implemented by the GIZA + + package <CIT> '
'In the geometric interpolation above , the weight n controls the relative veto power of the n-gram approximation and can be tuned using MERT <CIT> or a minimum risk procedure <OTH> '
'In this work , we study a method for obtaining word phrases that is based on Stochastic Inversion Transduction Grammars that was proposed in <CIT> '
'Pattern-based IE approaches employ seed data to learn useful patterns to pinpoint required fields values <OTH> '
'Of particular relevance are class-based language models -LRB- eg , <CIT> -RRB- '
'The first constraints are based on inversion transduction grammars -LRB- ITG -RRB- <CIT> '
'When the value of Ilw, r,w''ll is unknown, we assume that A and C are conditionally independent given B. The probability of A, B and C cooccurring is estimated by PMLE( B ) PMLE( A[B ) PMLE( C[B ), where PMLE is the maximum likelihood estimation of a probability distribution and P.LE(B) = II*,*,*ll'' P. ,~E(AIB ) = II*,~,*ll '' P, LE(CIB) = When the value of Hw, r, w~H is known, we can obtain PMLE(A, B, C) directly: PMLE(A, B, C) = [[w, r, wll/[[*, *, *H Let I(w,r,w ~) denote the amount information contained in Hw, r,w~]]=c. Its value can be corn769 simgindZe(Wl, W2) = ~''~(r,w)eTCwl)NTCw2)Aresubj.of.obj-of} min(I(Wl, r, w), I(w2, r, w) ) simHindte, (Wl, W2) = ~,(r,w)eT(w,)nT(w2) min(I(wl, r, w), I(w2, r, w)) ]T(Wl)NT(w2)I simcosine(Wl,W2) = x/IZ(w~)llZ(w2)l 2x IT(wl)nZ(w2)l simDice(Wl, W2) = iT(wl)l+lT(w2) I simJacard (Wl, W2) = T(wl )OT(w2)l T(wl) + T(w2)l-IT(Wl)rlT(w2)l Figure 1: Other Similarity Measures puted as follows: I(w,r,w'') = _ Iog(PMLE(B)PMLE(A]B)PMLE(CIB)) --(-log PMLE(A, B, C)) log IIw,r,wflll*,r,*ll -IIw,r,*ll xll*,r,w''ll It is worth noting that I(w,r,w'') is equal to the mutual information between w and w'' (Hindle, 1990).'
'We use the same set of binary features as in previous work on this dataset <CIT> '
'At each training-set size , a new copy of the network is trained under each of the following conditions : -LRB- 1 -RRB- using SULU , -LRB- 2 -RRB- using SULU but supplying only the labeled training examples to synthesize , -LRB- 3 -RRB- standard network training , -LRB- 4 -RRB- using a re-implementation of an algorithm proposed by <CIT> , and -LRB- 5 -RRB- using standard network training but with all training examples labeled to establish an upper bound '
'<CIT> -RRB- , less prior work exists for bilingual acquisition of domain-specific translations '
'1 Introduction Word alignmentdetection of corresponding words between two sentences that are translations of each otheris usually an intermediate step of statistical machine translation -LRB- MT -RRB- <CIT> , but also has been shown useful for other applications such as construction of bilingual lexicons , word-sense disambiguation , projection of resources , and crosslanguage information retrieval '
'1 Introduction Empty categories -LRB- also called null elements -RRB- are used in the annotation of the PENN treebank <CIT> in order to represent syntactic phenomena like constituent movement -LRB- eg whextraction -RRB- , discontinuous constituents , and missing elements -LRB- PRO elements , empty complementizers and relative pronouns -RRB- '
'Collocation Dictionary of Modern Chinese Lexical Words , Business Publisher , China Yuan Liu , et al 1993 '
'The probability distributions of these binary classifiers are learned using maximum entropy model <CIT> '
'Weischedel ''s group <OTH> examines unknown words in the context of part-of-speech tagging '
'We use the publicly available ROUGE toolkit <CIT> tocomputerecall , precision , andF-scorefor ROUGE-1 '
'Recently , Wikipedia is emerging as a source for extracting semantic relationships <CIT> '
'The observation that shallow syntactic information can be extracted using local information by examining the pattern itself , its nearby context and the local part-of-speech information has motivated the use of learning methods to recognize these patterns <CIT> '
'We report case-insensitive scores for version 06 of METEOR <OTH> with all modules enabled , version 104 of IBM-style BLEU <CIT> , and version 5 of TER <OTH> '
'It is dubious whether SWD is useful regarding recall-oriented metrics like METEOR <CIT> , since SWD removes information in source sentences '
'All the feature weights -LRB- s -RRB- were trained using our implementation of Minimum Error Rate Training <CIT> '
'In the first approach , heuristic rules are used to find the dependencies <OTH> or penalties for label inconsistency are required to handset ad-hoc <CIT> '
'2 Statistical Translation Engine A word-based translation engine is used based on the so-called IBM-4 model <CIT> '
'<CIT> uses a mutual-information based metric derived from the distribution of subject , verb and object in a large corpus to classify nouns '
'In analyzing opinions <OTH> , judging document-level subjectivity <CIT> , and answering opinion questions <OTH> , the output of a sentence-level subjectivity classification can be used without modification '
'Theyalsoappliedself-training to domain adaptation of a constituency parser <CIT> '
'In addition to collocation translation , there is also some related work in acquiring phrase or term translations from parallel corpus <OTH> '
'<CIT> applied an internet-based technique to the semantic orientation classification of phrases , which had originally been developed for word sentiment classification '
'Hyponymy relations were extracted from definition sentences <CIT> '
'The cube-pruning by Chiang <OTH> and the lazy cube-pruning of <CIT> and Chiang <OTH> turn the computation of beam pruning of CYK decoders into a top-k selection problem given two columns of translation hypotheses that need to be combined '
'In designing LEAF , we were also inspired by dependency-based alignment models <CIT> '
'1 Introduction Since their appearance , string-based evaluation metrics such as BLEU <CIT> and NIST <OTH> have been the standard tools used for evaluating MT quality '
'622 We also identified a length effect similar to that studied by <CIT> for self-training -LRB- using a reranker and large seed , as detailed in Section 2 -RRB- '
'51 Pharaoh The baseline system we used for comparison was Pharaoh <CIT> , a freely available decoder for phrase-based translation models : p -LRB- e f -RRB- = p -LRB- f e -RRB- pLM -LRB- e -RRB- LM pD -LRB- e , f -RRB- D length -LRB- e -RRB- W -LRB- e -RRB- -LRB- 10 -RRB- We ran GIZA + + <OTH> on the training corpus in both directions using its default setting , and then applied the refinement rule diagand described in <CIT> to obtain a single many-to-many word alignment for each sentence pair '
'42 Data The data comes from the CoNLL 2000 shared task <OTH> , which consists of sentences from the Penn Treebank Wall Street Journal corpus <CIT> '
'Dredze et al also indicated that unlabeled dependency parsing is not robust to domain adaptation <CIT> '
'In addition , we developed a word clustering procedure -LRB- based on a standard approach <CIT> -RRB- that optimizes conditional word clusters '
'Specifically , three features are used to instantiate the templates : POS tags on both sides : We assign POS tags using the MXPOST tagger <CIT> for English and Chinese , and Connexor for Spanish '
'Many 649 similarity measures and weighting functions have been proposed for distributional vectors ; comparative studies include Lee <OTH> , Curran <OTH> and <CIT> '
'Also , we chose to average each individual perceptron <CIT> prior to Bayesian averaging '
'Wiebe <OTH> uses <CIT> style distributionally similar adjectives in a cluster-and-label process to generate sentiment lexicon of adjectives '
'Gildea and Jurafsky <OTH> used a supervised learning method to learn both the identifier of the semantic roles defined in FrameNet such as theme , target , goal , and the boundaries of the roles <OTH> '
'To simplify , the plausibility of a detected esl is roughly inversely proportional to the number of mutually excluding syntactic structures in the text segment that generated the esl -LRB- see <OTH> for details -RRB- '
'Alignment is often used in training both generative and discriminative models <CIT> '
'In the past two or three years , this kind of verification has been attempted for other aspects of semantic interpretation : by Passonneau and Litman <OTH> for segmentation and by Kowtko , Isard , and Doherty <OTH> and Carletta et al '
'We adopt the approach of <CIT> , using a small set of patterns to build relation models , and extend their work by re ning the training and classi cation process using parameter optimization , topic segmentation and syntactic parsing '
'960 12 Alignment with Mixture Distribution Several papers have discussed the first issue , especially the problem of word alignments for bilingual corpora <CIT> , <OTH> , <OTH> , <OTH> , <OTH> '
'We measure this association using pointwise Mutual Information -LRB- MI -RRB- <CIT> '
'<OTH> and <CIT> <OTH> classified sentiment polarity of reviews at the document level '
'There are many method proposed to extract rigid expressions from corpora such as a method of focusing on the binding strength of two words <OTH> ; the distance between words <CIT> ; and the number of combined words and frequency of appearance <OTH> '
'Estimation of the parameters has been described elsewhere <CIT> '
'It differs from the many approaches where -LRB- 1 -RRB- is defined by a stochastic synchronous grammar <CIT> and from transfer-based systems defined by context-free grammars <OTH> '
'This approach to minimally supervised classifier construction has been widely studied <CIT> , especially in cases in which the features of interest are orthogonal in some sense <OTH> '
'Due to the lack of a good Arabic parser compatible with the Sakhr tokenization that we used on the source side , we did not test the source dependency LM for Arabic-to-English MT When extracting rules with source dependency structures , we applied the same well-formedness constraint on the source side as we did on the target side , using a procedure described by <OTH> '
'Its roots are the same as computational linguistics -LRB- CL -RRB- , but it has been largely ignored in CL until recently <CIT> '
'Unlabeled dependencies can be readily obtained by processing constituent trees , such as those in the Penn Treebank <CIT> , with a set of rules to determine the lexical heads of constituents '
'These models include a standard unlexicalized PCFG parser , a head-lexicalized parser <CIT> , and a maximum-entropy inspired parser <OTH> '
'For example , in the WSJ corpus , part of the Penn Treebank 3 release <CIT> , the string in -LRB- 1 -RRB- is a variation 12-gram since off is a variation nucleus that in one corpus occurrence is tagged as a preposition -LRB- IN -RRB- , while in another it is tagged as a particle -LRB- RP -RRB- '
'42 Features For our experiments , we use a feature set analogous to the default feature set of Pharaoh <CIT> '
'For a comparison , we also include the ROUGE-1 Fscores <CIT> of each system output against the human compressed sentences '
'<OTH> , Warnke et al '
'We measure translation performance by the BLEU <OTH> and METEOR <CIT> scores with multiple translation references '
'F-Measure with an appropriate setting of will be useful during the development process of new alignment models , or as a maximization criterion for discriminative training of alignment models <CIT> '
'The <CIT> model does not use context-free rules , but generates the next category using zeroth order Markov chains -LRB- see Section 33 -RRB- , hence no information about the previous sisters is included '
'7 This discussion could also be cast in an information theoretic framework using the notion of ` mutual information '' <OTH> , estimating the variance of the degree of match in order to find a frequency-threshold <CIT> '
'In several papers <OTH> , selection criteria for single word trigger pairs were studied '
'See <CIT> for more details on this approach '
'Word alignment was carried out by running Giza + + implementation of IBM Model 4 initialized with 5 iterations of Model 1 , 5 of the HMM aligner , and 3 iterations of Model 4 <OTH> in both directions and then symmetrizing using the grow-diag-final-and heuristic <CIT> '
'A common choice for the local probabilistic classifier is maximum entropy classifiers <CIT> '
'CIT -RRB- '
'Illustrative clusterings of this type can also be found in Pereira , Tishby , and Lee <OTH> , <CIT> , Kneser and Ney <OTH> , and Brill et al '
'The decision rule was based on the standard loglinear interpolation of several models , with weights tunedbyMERTonthedevelopmentse <CIT> '
'Since manual word alignment is an ambiguous task , we also explicitly allow for ambiguous alignments , ie the links are marked as sure -LRB- S -RRB- or possible -LRB- P -RRB- <CIT> '
'Finally , we compare against the mapping from WordNet to the Oxford English Dictionary constructed in <CIT> , equivalent to clustering based solely on the OED feature '
'We have -LRB- 11 -RRB- Hypernym Patterns based on patterns proposed by <OTH> and <OTH> , -LRB- 12 -RRB- Sibling Patterns which are basically conjunctions , and -LRB- 13 -RRB- Part-of Patterns based on patterns proposed by <OTH> and <OTH> '
'6 Related Work and Discussion There are several studies that used automatically extracted gazetteers for NER <CIT> '
'In our decoder , we incorporate two pruning techniques described by <CIT> '
'2 Phrase-based statistical machine translation Phrase-based SMT uses a framework of log-linear models <CIT> to integrate multiple features '
'And third , 1This <CIT> baseNP data set is available via ftp : \/ \/ ftpcisupennedu\/pub\/chunker \/ 2Software for generating the data is available from http://lcg-wwwuiaacbe/conl199/npb/ 50 with the FZ = I rate which is equal to -LRB- 2 \* precision \* recall -RRB- \/ -LRB- precision + recall -RRB- '
'In <OTH> , finite-state machine translation is based on <CIT> and is used for decoding the target language string '
'2 Related Research Several researchers <CIT> have already proposed methods for binarizing synchronous grammars in the context of machine translation '
'An automatic metric which uses base forms and synonyms of the words in order to correlate better to human judgements has been 1 proposed in <CIT> '
'4 Experiments We evaluate the accuracy of HPSG parsing with dependencyconstraintsontheHPSGTreebank <OTH> , which is extracted from the Wall Street Journal portion of the Penn Treebank <CIT> 1 '
'This method is employed in <CIT> '
'This resembles the re-ranking approach <CIT> '
'However , recent progress in machine translation and the continuous improvement on evaluation metrics such as BLEU <CIT> suggest that SMT systems are already very good at choosing correct word translations '
'Practically , the grammar relaxation is done via the introduction of non-standard CCG rules <CIT> '
'For determining whether an opinion sentence is positive or negative , we have used seed words similar to those produced by <OTH> and extended them to construct a much larger set of semantically oriented words with a method similar to that proposed by <CIT> '
'Others proposed distributional similarity measures between words <CIT> '
'2 Motivation Automatic subjectivity analysis methods have been used in a wide variety of text processing applications , such as tracking sentiment timelines in online forums and news <OTH> , review classification <CIT> , mining opinions from product reviews <OTH> , automatic expressive text-to-speech synthesis <OTH> , text semantic analysis <OTH> , and question answering <OTH> '
'If the input consists of sevWe also adopt the approximation that treats every sentence with its reference as a separate corpus <CIT> so that ngram counts are not accumulated , and parallel processing of sentences becomes possible '
'Feature weight tuning was carried out using minimum error rate training , maximizing BLEU scores on a held-out development set <CIT> '
'Since we approach decoding as xR transduction , the process is identical to that of constituencybased algorithms <CIT> '
'Others try to accommodate both syntactic and lexical differences between the candidate translation and the reference , like CDER <OTH> , which employs a version of edit distance for word substitution and reordering ; or METEOR <CIT> , which uses stemming and WordNet synonymy '
'Many techniques which have been studied for the purpose of machine translation , such as word sense disambiguation <CIT> , anaphora resolution <OTH> , and automatic pattern extraction from corpora <OTH> , can accelerate the further enhancement of sentiment analysis , or other NLP tasks '
'For phrase-based translation model training , we used the GIZA + + toolkit <CIT> '
'andw2 iscomputedusinganassociationscorebased on pointwise mutual information , asdefinedbyFano <OTH> and used for a similar purpose in <CIT> , as well as in many other studies in corpus linguistics '
'The traditional method of evaluating similarity in a semantic network by measuring the path length between two nodes <OTH> also captures this , albeit indirectly , when the semantic network is just an IS-A hierarchy : if the minimal path of IS-A links between two nodes is long , that means it is necessary to go high in the taxonomy , to more abstract concepts , in order to find their least upper bound '
'In this spirit , we introduce a generalization of the classic k-gram models , widely used for string processing <CIT> , to the case of trees '
'See Table 4 in <CIT> for the detail '
'Detail of the Bakeoff data sets is in <CIT> '
'This kind of smoothing has also been used in the generative parser of <CIT> and has been shown to have a relatively good performance for language modeling <OTH> '
'In <CIT> , the authors use the transcripts of debates from the US Congress to automatically classify speeches as supporting or opposing a given topic by taking advantage of the voting records of the speakers '
'Training on about 40,000 sentences <CIT> achieves a crossing brackets rate of 107 , a better value than our 163 value for regular parsing or the 113 value assuming perfect segmentation\/tagging , but even for similar text types , comparisons across languages are of course problematic '
'<OTH> proposed using GIZA + + <CIT> to align words between the backbone and hypothesis '
'Clustering-based approaches usually represent word contexts as vectors and cluster words based on similarities of the vectors <CIT> '
'In particular , this holds for the SCFG implementing Inversion 3For two sequences of numbers , the notation y -LRB- z stands for y y , z z : y -LRB- z Transduction Grammar <CIT> '
'Labeled data for one domain might be used to train a initial classifier for another -LRB- possibly related -RRB- domain , and then bootstrapping can be employed to learn new knowledge from the new domain <CIT> '
'Proposals have recently been made for protocols for the collection of human discourse segmentation data <OTH> and for how to evaluate the validity of judgments so obtained <CIT> '
'12 Statistical modeling for translation Earlier work in statistical machine translation <CIT> is based on the noisy-channel formulation where T = arg max T p -LRB- TjS -RRB- = argmax T p -LRB- T -RRB- p -LRB- SjT -RRB- -LRB- 1 -RRB- where the target language model p -LRB- T -RRB- is further decomposed as p -LRB- T -RRB- \/ productdisplay i p -LRB- tijti1 , , tik +1 -RRB- where k is the order of the language model and the translation model p -LRB- SjT -RRB- has been modeled by a sequence of five models with increasing complexity <CIT> '
'The SENSEVAL '' ~ tan -LRB- lard is clearly beaten by the earlier results of <CIT> -LRB- 965 % precision -RRB- and Schiitze -LRB- 1992 -RRB- -LRB- 92 % precision -RRB- '
'3The usefulness of position varies significantly in different genres <CIT> '
'Then the words are tagged as inside a phrase -LRB- I -RRB- , outside a phrase -LRB- O -RRB- or beginning of a phrase -LRB- B -RRB- <OTH> '
'In supervised domain adaptation <CIT> , besides the labeled source data , we have access to a comparably small , but labeled amount of target data '
'This was a difcult challenge as many participants in the task failed to obtain any meaningful gains from unlabeled data <OTH> '
'? ? Initial phrase pairs are identified following the procedure typically employed in phrase based systems <CIT> '
'For this purpose , we adopt the view of the ITG constraints as a bilingual grammar as , eg , in <CIT> '
'This formulation is similar to the energy minimization framework , which is commonly used in image analysis <OTH> and has been recently applied in natural language processing <CIT> '
'c2005 Association for Computational Linguistics Automatic identification of sentiment vocabulary : exploiting low association with known sentiment terms Michael Gamon Anthony Aue Natural Language Processing Group Natural Language Processing Group Microsoft Research Microsoft Research mgamon @ microsoftcom anthaue @ microsoftcom Abstract We describe an extension to the technique for the automatic identification and labeling of sentiment terms described in <CIT> and Turney and Littman -LRB- 2002 -RRB- '
'While this technique has been successfully applied to parsing the ATIS portion in the Penn Treebank <CIT> , it is extremely time consuming '
'As argued in <CIT> , Kappa values of 08 or higher are desirable for detecting associations between several coded variables ; we were thus satisfied with the level of agreement achieved '
'<CIT> described the use of minimum error training directly optimizing the error rate on automatic MT evaluation metrics such as BLEU '
'The most relevant to our work are Kazama and Torisawa <OTH> , Toral and Muoz <OTH> , and <CIT> <OTH> '
'Due to its popularity for unsupervised POS induction research <CIT> and its often-used tagset , for our initial research , we use the Wall Street Journal -LRB- WSJ -RRB- portion of the Penn Treebank <OTH> , with 36 tags -LRB- plus 9 punctuation tags -RRB- , and we use sections 00-18 , leaving held-out data for future experiments4 Defining frequent frames as those occurring at 4Even if we wanted child-directed speech , the CHILDES database <OTH> uses coarse POS tags '
'2 Related Work The issue of MWE processing has attracted much attention from the Natural Language Processing -LRB- NLP -RRB- community , including <CIT> , 1993 ; Dagan and Church , 1994 ; Daille , 1995 ; 1995 ; McEnery et al , 1997 ; Wu , 1997 ; Michiels and Dufour , 1998 ; Maynard and Ananiadou , 2000 ; Merkel and Andersson , 2000 ; Piao and McEnery , 2001 ; Sag et al , 2001 ; Tanaka and Baldwin , 2003 ; Dias , 2003 ; Baldwin et al , 2003 ; Nivre and Nilsson , 2004 Pereira et al , '
'Estimated clues are derived from the parallel data using , for example , measures of co-occurrence -LRB- eg the Dice coefficient <OTH> -RRB- , statistical alignment models -LRB- eg IBM models from statistical machine translation <CIT> -RRB- , or string similarity measures -LRB- eg the longest common sub-sequence ratio <OTH> -RRB- '
'The tag propagation\/elimination scheme is adopted from <CIT> '
'The data consists of sections of the Wall Street Journal part of the Penn TreeBank <CIT> , with information on predicate-argument structures extracted from the PropBank corpus <OTH> '
'Previous authors have used numerous HMM-based models <CIT> and other types of networks including maximum entropy models <OTH> , conditional Markov models <OTH> , conditional random elds -LRB- CRF -RRB- <OTH> , and cyclic dependency networks <OTH> '
'As an example , consider the fiat NP structures that are in the Penn Treebank <CIT> '
'On the other hand , statistical MT employing IBM models <CIT> translates an input sentence by the combination of word transfer and word re-ordering '
'Note that apart from previous work <CIT> we use complete skip-chain -LRB- contextanswer -RRB- edges in hc -LRB- x , y -RRB- '
'Six features from <CIT> were used as baseline features '
'NER proves to be a knowledgeintensive task , and it was reassuring to observe that System Resources Used F1 + LBJ-NER Wikipedia , Nonlocal Features , Word-class Model 9080 <OTH> Semi-supervised on 1Gword unlabeled data 8992 <OTH> Semi-supervised on 27Mword unlabeled data 8931 <CIT> Wikipedia 8802 <OTH> Non-local Features 8724 <CIT> Non-local Features 8717 + <OTH> Non-local Features 8686 Table 7 : Results for CoNLL03 data reported in the literature '
'5 Related Work There has not been much previous work on graphical models for full parsing , although recently several latent variable models for parsing have been proposed <CIT> '
'Generalized Forward Backward Reestimation Generalization of the Forward and Viterbi Algorithm In English part of speech taggers , the maximization of Equation -LRB- 1 -RRB- to get the most likely tag sequence , is accomplished by the Viterbi algorithm <OTH> , and the maximum likelihood estimates of the parameters of Equation -LRB- 2 -RRB- are obtained from untagged corpus by the ForwardBackward algorithm <CIT> '
'Of the several slightly different definitions of a base NP in the literature we use for the purposes of this work the definition presented in <CIT> and used also by <OTH> and others '
'Furthermore , the BLEU score performance suggests that our model is not very powerful , but some interesting hints can be found in Table 3 when we compare our method with a 5-gram language model to a state-of-the-art system Moses <OTH> based on various evaluation metrics , including BLEU score , NIST score <OTH> , METEOR <CIT> , TER <OTH> , WER and PER '
'Finally , the loglikelihood ratios test -LRB- henceforth LLR -RRB- <CIT> is applied on each set of pairs '
'<CIT> make a similar assumption '
'<CIT> proposes a method for word sense disambiguation , which is based on Monolingual Bootstrapping '
'This set of context vectors is then clustered into a predetermined number of coherent clusters or context groups using Buckshot <OTH> , a combination of the EM algorithm and agglomerative clustering '
'A subst(req, cons(c, argo)) st ^ rel(c, z) s2 ~(i,k,=,;~z[p~(:) ^ ~(~)]) (Vi,j,w)n(i,j,w) D (3z)cn(i,j,z,w) (Vi,j, k, w, z, c, rel)prep(i, j, w) ^ np(j, k, x) A rel(c, z) In 3 ptXi, k,,~z[w(c, z)], <c>, Req(w)) For example, the first axiom says that there is a sentence from point i to point k asserting eventuality e if there is a noun phrase from i to j referring to z and a verb phrase from j to k denoting predicate p with arguments arg8 and having an associated requirement req, and there is (or, for $3, can be assumed to be) an eventuality e of p''s being true of , where c is related to or coercible from x (with an assumability cost of $20), and the requirement req associated with p can be proved or, for $10, assumed to hold of the arguments of p. The symbol c&el denotes the conjunction of eventualities e and el (See Hobbs (1985b), p. 35).'
'31 The Corpus The systems are applied to examples from the Penn Treebank <CIT> a corpus of over 45 million words of American English annotated with both part-of-speech and syntactic tree information '
'The alignment of sentences can be done sufficiently well using cues such as sentence length <OTH> or cognates <OTH> '
'Some of this work focuses on classifying the semantic orientation of individual words or phrases , using linguistic heuristics or a pre-selected set of seed words <CIT> '
'As in phrasebased translation model estimation , ? also contains two lexical weights <CIT> , counters for number of target terminals generated '
'Abduction has been applied to the solution of local pragmatics problems <CIT> and to story understanding <OTH> '
'<CIT> argue that these restrictions reduce our ability to model translation equivalence effectively '
'Word alignment is also a required first step in other algorithms such as for learning sub-sentential phrase pairs <OTH> or the generation of parallel treebanks <CIT> '
'Intuitively , if we are able to find good correspondences through linking pivots , then the augmented source data should transfer better to a target domain <CIT> '
'Then we use both Moses decoder and its suppo We run the decoder with its d then use Moses '' implementation of minimum error rate training <CIT> to tune the feature weights on the development set '
'2 Inside-out alignments <CIT> identified so-called inside-out alignments , two alignment configurations that can not be induced by binary synchronous context-free grammars ; these alignment configurations , while infrequent in language pairs such as EnglishFrench <OTH> , have been argued to be frequent in other language pairs , incl '
'1 Data Data for 64 verbs -LRB- shown in Table 1 -RRB- was collected from three corpora ; The British National Corpus -LRB- BNC -RRB- -LRB- http ` J\/infooxacuk\/bnc \/ indexhtml -RRB- , the Penn Treehank parsed version of the Brown Corpus -LRB- Brown -RRB- , and the Penn Treebank Wall Street Journal corpas -LRB- WSJ -RRB- <CIT> '
'<CIT> -LRB- Z&C 08 -RRB- generated CTB 30 from CTB 40 '
'Analyze resulting findings to determine a progression of competence In <OTH> we discuss the initial steps we took in this process , including the development of a list of error codes documented by a coding manual , the verification of our manual and coding scheme by testing inter-coder reliability in a subset of the corpus -LRB- where we achieved a Kappa agreement score <CIT> of a0 a1a3a2a5a4a7a6 -RRB- 2 , and the subsequent tagging of the entire corpus '
'2 Related Work Given its potential usefulness in coreference resolution , anaphoricity determination has been studied fairly extensively in the literature and can be classified into three categories : heuristic rule-based <OTH> , statistics-based <CIT> and learning-based <OTH> '
'This evaluation shows that our WIDL-based approach to generation is capable of obtaining headlines that compare favorably , in both content and fluency , with extractive , state-of-the-art results <OTH> , while it outperforms a previously-proposed abstractive system by a wide margin <OTH> '
'6 Concluding remarks Our work presents a set of improvements on previous state of the art of Grammar Association: first, by providing better language models to the original system described in (Vidal et al. , 1993); second, by setting the technique into a rigorous statistical framework, clarifying which kind of probabilities have to be estimated by association models; third, by developing a novel and especially adequate association model: Loco C. On the other hand, though experimental results are quite good, we find them particularly relevant for pointing out directions to follow for further improvement of the Grammar Association technique.'
'We set all feature weights by optimizing Bleu <OTH> directly using minimum error rate training -LRB- MERT -RRB- <CIT> on the tuning part of the development set -LRB- dev-test2009a -RRB- '
'We obtained word alignments of training data by first running GIZA + + <OTH> and then applying the refinement rule grow-diag-final-and <CIT> '
'When we trained external Chinese models , we used the same unlabeled data set as <CIT> , including the bilingual dictionary '
'The training data is aligned using the LEAF technique <CIT> '
'<CIT> et al 2006 -RRB- '
'This is con rmed by the translation experiments in which the evaluation data sets were translated using the servers translation engines and the translation quality was evaluated using the standard automatic evaluation metrics BLEU <OTH> and METEOR <CIT> where scores range between 0 -LRB- worst -RRB- and 1 -LRB- best -RRB- '
'Given an input training corpus of such derivations D = d1 dn, a vector feature function on derivations vectorF(d), and an initial weight vector vectorw, the perceptron performs two steps for each training example di  D:  Decode: d = argmaxdD(src(di)) parenleftBig vectorw vectorF(d) parenrightBig  Update: vectorw = vectorw + vectorF(di) vectorF(d) where D(src(d)) enumerates all possible derivations with the same source side as d. To improve generalization, the final feature vector is the average of all vectors found during learning (Collins, 2002).'
'1 Introduction Given a source-language -LRB- eg , French -RRB- sentence f , the problem of machine translation is to automatically produce a target-language -LRB- eg , English -RRB- translation e The mathematics of the problem were formalized by <CIT> , and re-formulated by <OTH> in terms of the optimization e = arg maxe Msummationdisplay m = 1 mhm -LRB- e , f -RRB- -LRB- 1 -RRB- where fhm -LRB- e , f -RRB- g is a set of M feature functions and fmg a set of weights '
'We also trained an HMM aligner as described in <CIT> and used the posteriors of this model as features '
'Rule-based taggers <OTH> use POS-dependent constraints defined by experienced linguists '
'We use the similarity proposed by <CIT> '
'Note that all systems were optimized using a non-deterministic implementation of the Minimum Error Rate Training described in <CIT> '
'2 Recap of BLEU , ROUGE-W and METEOR The most commonly used automatic evaluation metrics , BLEU <CIT> and NIST <OTH> , are based on the assumption that The closer a machine translation is to a promt1 : Life is like one nice chocolate in box ref : Life is just like a box of tasty chocolate ref : Life is just like a box of tasty chocolate mt2 : Life is of one nice chocolate in box Figure 1 : Alignment Example for ROUGE-W fessional human translation , the better it is <CIT> '
'21 Relationship Types There is a large body of related work that deals with discovery of basic relationship types represented in useful resources such as WordNet , including hypernymy <CIT> , synonymy <OTH> and meronymy <OTH> '
'<CIT> proposes two approximate models based on the variational approach '
'1087 Model 3 of <CIT> is a zero-order alignment model like Model 2 including in addition fertility paranmters '
'Among these methods , SVM is shown to perform better than other methods <CIT> '
'3 Maximum Entropy ME models implement the intuition that the best model is the one that is consistent with the set of constraints imposed by the evidence , but otherwise is as uniform as possible <CIT> '
'31 Candidate NPs Noun phrases were extracted using <CIT> -RRB- '
'Many statistical metrics have been proposed , including pointwise mutual information -LRB- MI -RRB- <OTH> , mean and variance , hypothesis testing -LRB- t-test , chisquare test , etc -RRB- , log-likelihood ratio -LRB- LR -RRB- <CIT> , statistic language model <OTH> , and so on '
'766 System Beam Error % <OTH> 5 337 <OTH> 1 290 <CIT> 289 Guided Learning , feature B 3 285 <OTH> all 285 <OTH> 284 <OTH> 276 Guided Learning , feature E 1 273 Guided Learning , feature E 3 267 Table 4 : Comparison with the previous works According to the experiments shown above , we build our best system by using feature set E with beam width B = 3 '
'We also have an additional held-out translation set , the development set , which is employed by the MT system to train the weights of its log-linear model to maximize BLEU <CIT> '
'The translation component is an analog of the IBM model 2 <CIT> , with parameters that are optimized for use with the trigram '
'The toolkit also implements suffixarray grammar extraction <OTH> and minimum error rate training <CIT> '
'Pattern-based approaches are known for their high accuracy in recognizing instances of relations if the patterns are carefully chosen , either manually <OTH> or via automatic bootstrapping <OTH> '
'Our model uses an exemplar memory that consists of 133566 verb-role-noun triples extracted from the Wall Street Journal and Brown parts of the Penn Treebank <CIT> '
'Hypotheses for unknown words , both stochastic <OTH> , and connectionist <OTH> have been applied to unlimited vocabulary taggers '
'For example , if we make a mean-field assumption , with respect to hidden structure and weights , the variationalalgorithmforapproximatelyinferringthe distribution over and trees y resembles the traditional EM algorithm very closely <CIT> '
'2 Maximum Entropy In this bakeoff , our basic model is based on the framework described in the work of <CIT> which was applied for English POS tagging '
'1 Introduction Previous work on sentiment categorization makes an implicit assumption that a single score can express the polarity of an opinion text <CIT> '
'For examples , see <CIT> '
'Since the introduction of BLEU <CIT> the basic n-gram precision idea has been augmented in a number of ways '
'1 Introduction The best performing systems for many tasks in natural language processing are based on supervised training on annotated corpora such as the Penn Treebank <CIT> and the prepositional phrase data set first described in <OTH> '
'However , as pointed out in <CIT> , there is no reason to believe that the resulting parameters are optimal with respect to translation quality measured with the Bleu score '
'The initial state contains terminal items , whose labels are the POS tags given by <CIT> '
'imum error rate training -LRB- MERT -RRB- <CIT> to maximize BLEU score <OTH> '
'As an example of it s application , N-gram co-occurrence is used for evaluating machine translations <CIT> '
'3 The Log-Likelihood-Ratio Association Measure We base all our association-based word-alignment methods on the log-likelihood-ratio -LRB- LLR -RRB- statistic introduced to the NLP community by <CIT> '
'Recently , some generic methods were proposed to handle context-sensitive inference <CIT> , but these usually treat only a single aspect of context matching -LRB- see Section 6 -RRB- '
'The discrepancy between DEV performance and TEST performance is due to temporal distance from TRAIN and high variance in BLEU score11 We also compared our model with Pharaoh <CIT> '
'51 Experimental setup The baseline model was Hiero with the following baseline features <OTH> : two language models phrase translation probabilities p -LRB- f e -RRB- and p -LRB- e f -RRB- lexical weighting in both directions <CIT> word penalty penalties for : automatically extracted rules identity rules -LRB- translating a word into itself -RRB- two classes of number\/name translation rules glue rules The probability features are base-100 logprobabilities '
'Finally we use Minimum Error Training -LRB- MET -RRB- <CIT> to train log-linear scaling factors that are applied to the WFSTs in Equation 1 '
'5 Related Work Automatically finding sentences with the same meaning has been extensively studied in the field of automatic paraphrasing using parallel corpora and corporawith multiple descriptionsof the same events <CIT> '
'As mentioned in Section 22 , there are words which have two or more candidate POS tags in the PTB corpus <CIT> '
'Finally , the fourth and fifth feature functions corresponded to two lexicon models based on IBM Model 1 lexical parameters p -LRB- t s -RRB- <CIT> '
'Using an Maximum Entropy approach to POS tagging , <CIT> reports a tagging accuracy of 966 % on the Wall Street Journal '
'2 Motivation Automatic subjectivity analysis methods have been used in a wide variety of text processing applications , such as tracking sentiment timelines in online forums and news <OTH> , review classification <CIT> , mining opinions from product reviews <OTH> , automatic expressive text-to-speech synthesis <OTH> , text semantic analysis <OTH> , and question answering <OTH> '
'Some studies exploit topically related articles derived from multiple news sources <CIT> '
'We also note that <CIT> found movie reviews to be the most 2Indeed , although our choice of title was completely independent of his , our selections were eerily similar '
'c2006 Association for Computational Linguistics Robust PCFG-Based Generation using Automatically Acquired LFG Approximations Aoife Cahill1 and Josef van Genabith1,2 1 National Centre for Language Technology (NCLT) School of Computing, Dublin City University, Dublin 9, Ireland 2 Center for Advanced Studies, IBM Dublin, Ireland {acahill,josef}@computing.dcu.ie Abstract We present a novel PCFG-based architecture for robust probabilistic generation based on wide-coverage LFG approximations (Cahill et al. , 2004) automatically extracted from treebanks, maximising the probability of a tree given an f-structure.'
'The model cleanly incorporates both syntax and lexical semantics using quasi-synchronous dependency grammars <CIT> '
'Previous workthe generative models described in <CIT> and the earlier version of these models described in Collins -LRB- 1997 -RRB- conditioned on punctuation as surface features of the string , treating it quite differently from lexical items '
'Dunning (1993) argues for the use of G 2 rather than X 2, based on the claim that the sampling distribution of G 2 approaches the true chi-square distribution quicker than the sampling distribution of X 2 . However, Agresti (1996, page 34) makes the opposite claim: The sampling distributions of X 2 and G 2 get closer to chi-squared as the sample size n increasesThe convergence is quicker for X 2 than G 2 . In addition, Pedersen (2001) questions whether one statistic should be preferred over the other for the bigram acquisition task and cites Cressie and Read (1984), who argue that there are some cases where the Pearson statistic is more reliable than the log-likelihood statistic.'
'A variety of synset similarity measures based on properties of WordNet itself have been proposed ; nine such measures are discussed in <CIT> , including gloss-based heuristics <CIT> , information-content based measures <OTH> , and others '
'This con rms <CIT> s nding that in sentence level evaluation , long n-grams in BLEU are not bene cial '
'3 <CIT> describes the cube growing algorithm in further detail , including the precise form of the successor function for derivations '
'CIT -RRB- '
'Below is an example of the initial-state tagging of a sentence from the Penn Treebank <CIT> , where an underscore is to be read as or '
'The performance of cross-language information retrieval with a uniform T is likely to be limited in the same way as the performance of conventional information retrieval without term-frequency information , ie , where the system knows which terms occur in which documents , but not how often <OTH> '
'Although LDD annotation is actually provided in Treebanks such as the Penn Treebank <CIT> over which they are typically trained , most probabilistic parsers largely or fully ignore this information '
'Log-likelihood ratio -LRB- G2 -RRB- <CIT> with respect to a large reference corpus , Web 1T 5-gram Corpus <OTH> , is used to capture the contextually relevant nouns '
'In <CIT> , target trees were employed to improve the scoring of translation theories '
'As with other randomised models we construct queries with the appropriate sanity checks to lower the error rate efficiently <CIT> '
'3.1 Part-of-Speech (POS) of Neighboring Words We use 7 features to encode this knowledge source: a0a2a1a4a3a6a5a7a0a8a1a10a9a11a5a7a0a8a1a13a12a14a5a15a0a17a16a6a5a15a0a2a12a18a5a7a0a19a9a20a5a15a0a17a3, where a0a8a1 a21 (a0 a21 ) is the POS of thea6 th token to the left (right) ofa0, and a0a17a16 is the POS of a0 . A token can be a word or a punctuation symbol, and each of these neighboring tokens must be in the same sentence asa0 . We use a sentence segmentation program (Reynar and Ratnaparkhi, 1997) and a POS tagger (Ratnaparkhi, 1996) to segment the tokens surroundinga0 into sentences and assign POS tags to these tokens.'
'35 Adding Context to the Model Next , we added of a stochastic POS tagger <OTH> to provide a model of context '
'In examining the combination of the two types of parsing , <CIT> utilized similar approaches to our empirical analysis '
'We have achieved average results in the CoNLL domain adaptation track open submission <CIT> '
'<CIT> also worked on one of our data sets '
'To make the model more practical in parameter estimation , we assume the features in feature set FS are independent from each other , thus : = FSFi AFiPAFSP -RRB- , -LRB- -RRB- , -LRB- -LRB- 5 -RRB- Under this PCFG+PF model , the goal of a parser is to choose a parse that maximizes the following score : -RRB- , -LRB- maxarg -RRB- -LRB- 1 AFS i i i n i T PSTScore = = -LRB- 6 -RRB- Our model is thus a simplification of more sophisticated models which integrate PCFGs with features , such as those in Magerman <OTH> , <CIT> and Goodman -LRB- 1997 -RRB- '
'The relatedness between two word senses is computed using a measure of semantic relatedness defined in the WordNet : : Similarity software package <CIT> , which is a suite of Perl modules implementing a number WordNet-based measures of semantic relatedness '
'24 Comparison with Hybrid Model SSL based on a hybrid generative\/discriminative approach proposed in <CIT> has been defined as a log-linear model that discriminatively combines several discriminative models , pDi , and generative models , pGj , such that : R -LRB- y x ; , , -RRB- = producttext i p Di -LRB- y x ; i -RRB- i producttext j p Gj -LRB- xj , y ; j -RRB- j summationtext y producttext i p Di -LRB- y x ; i -RRB- i producttext j p Gj -LRB- xj , y ; j -RRB- j , where = -LCB- i -RCB- Ii = 1 , and = -LCB- -LCB- i -RCB- Ii = 1 , -LCB- j -RCB- I+J j = I +1 -RCB- '
'We present results in the form of search error analysis and translation quality as measured by the BLEU score <CIT> on the IWSLT 06 text translation task <OTH> 1 , comparing Cube Pruning with our two-pass approach '
'The features are the same as those in <CIT> '
'In the usual case considered by <CIT> and discussed by Manning and Sch utze -LRB- 1999 -RRB- , the right-hand side of the equation is larger than the left-hand side '
'We examine Structural Correspondence Learning -LRB- SCL -RRB- <CIT> for this task , and compare it to several variants of Self-training <OTH> '
'<CIT> and <OTH> use syntactic markers to increase the significance of the data '
'Such studies follow the empiricist approach to word meaning summarized best in the famous dictum of the British 3 linguist J.R. Firth: You shall know a word by the company it keeps. (Firth, 1957, p. 11) Context similarity has been used as a means of extracting collocations from corpora, e.g. by Church & Hanks (1990) and by Dunning (1993), of identifying word senses, e.g. by Yarowski (1995) and by Schutze (1998), of clustering verb classes, e.g. by Schulte im Walde (2003), and of inducing selectional restrictions of verbs, e.g. by Resnik (1993), by Abe & Li (1996), by Rooth et al.'
'2 Related Work Question Answering has attracted much attention from the areas of Natural Language Processing , Information Retrieval and Data Mining <CIT> '
'3 Algorithm As in previous work <CIT> , our computations are based on a partially lemmatized version of the British National Corpus -LRB- BNC -RRB- which has the function words removed '
'toilet/bathroom Since the word ''facility'' is the subject of ''employ'' and is modified by ''new'' in (3), we retrieve other words that appeared in the same contexts and obtain the following two groups of selectors (the log A column shows the likelihood ratios (Dunning, 1993) of these words in the local contexts):  Subjects of ''employ'' with top-20 highest likelihood ratios: word freq, Iog,k word freq ORG'' 64 50.4 plant 14 31.0 company 27 28.6 operation 8 23.0 industry 9 14.6 firm 8 13.5 pirate 2 12.1 unit 9 9.32 shift 3 8.48 postal service 2 7.73 machine 3 6.56 corporation 3 6.47 manufacturer 3 6.21 insurance company 2 6.06 aerospace 2 5.81 memory device 1 5.79 department 3 5.55 foreign office 1 5.41 enterprise 2 5.39 pilot 2 537 *ORG includes all proper names recognized as organizations 18  Modifiees of ''new'' with top-20 highest likelihood ratios: word freq log,k post 432 952.9 issue 805 902.8 product 675 888.6 rule 459 875.8 law 356 541.5 technology 237 382.7 generation 150 323.2 model 207 319.3 job 260 269.2 system 318 251.8 word freq log )~ bonds 223 245.4 capital 178 241.8 order 228 236.5 version 158 223.7 position 236 207.3 high 152 201.2 contract 279 198.1 bill 208 194.9 venture 123 193.7 program 283 183.8 Since the similarity between Sense 1 of ''facility'' and the selectors is greater than that of other senses, the word ''facility'' in (3) is tagged ''Sense The key innovation of our algorithm is that a polysemous word is disambiguated with past usages of other words.'
'In recent years , many researchers have tried to automatically evaluate the quality of MT and improve the performance of automatic MT evaluations <CIT> because improving the performance of automatic MT evaluation is expected to enable us to use and improve MT systems efficiently '
'The model we use is similar to that of <CIT> '
'We evaluate the system generated summaries using the automatic evaluation toolkit ROUGE <CIT> '
'<CIT> has presented an unsupervised opinion classification algorithm called SO-PMI -LRB- Semantic Orientation Using Pointwise Mutual Information -RRB- '
'Standard sequence prediction models are highly effective for supertagging , including Hidden Markov Models <OTH> , Maximum Entropy Markov Models <OTH> , and Conditional Random Fields <OTH> '
'11 This low agreement ratio is also re ected in a measure called the statistic <CIT> '
'Early work by <CIT> falls within this framework '
'In tabh ; 2 , the accuracy rate of the Net-Tagger is cOrolLated to that of a trigram l -RRB- msed tagger <OTH> and a lIidden Markov Model tagger <CIT> which were '
'In the sequel , we use Collinss statistical parser <CIT> as our canonical automated approximation of the Treebank '
'For details on these feature functions , please refer to <CIT> '
'In <CIT> , shallow syntactic analysis such as POS tagging and morphological analysis were incorporated in a phrasal decoder '
'<OTH> , a robust risk minimization classifier , based on a regularized winnow method <OTH> -LRB- henceforth RRM -RRB- and a maximum entropy classifier <CIT> -LRB- henceforth MaxEnt -RRB- '
'Various machine learning strategies have been proposed to address this problem , including semi-supervised learning <OTH> , domain adaptation <CIT> , multi-task learning <OTH> , self-taught learning <OTH> , etc A commonality among these methods is that they all require the training data and test data to be in the same feature space '
'Since we need knowledge-poor Daille , 1996 -RRB- induction , we can not use human-suggested filtering Chi-squared -LRB- G24 -RRB- 2 <CIT> Z-Score <OTH> Students t-Score <CIT> n-gram list in accordance to each probabilistic algorithm '
'That is , phrases are heuristically extracted from word-level alignments produced by doing GIZA + + training on the corresponding parallel corpora <CIT> '
'The system used for baseline experiments is two runs of IBM Model 4 <CIT> in the GIZA + + <OTH> implementation , which includes smoothing extensions to Model 4 '
'The problem itself has started to get attention only recently <CIT> '
'They are based on the sourcechannel approach to statistical machine translation <CIT> '
'1 Introduction Sentiment classification is a special task of text categorization that aims to classify documents according to their opinion of , or sentiment toward a given subject -LRB- eg , if an opinion is supported or not -RRB- <CIT> '
'1 Introduction Deep and accurate text analysis based on discriminative models is not yet efficient enough as a component of real-time applications , and it is inadequate to process Web-scale corpora for knowledge acquisition <OTH> or semi-supervised learning <CIT> '
'3 Length Model : Dynamic Programming Given the word fertility de nitions in IBM Models <CIT> , we can compute a probability to predict phrase length : given the candidate target phrase -LRB- English -RRB- eI1 , and a source phrase -LRB- French -RRB- of length J , the model gives the estimation of P -LRB- J eI1 -RRB- via a dynamic programming algorithm using the source word fertilities '
'More recently , the problem has been tackled using statistics-based <CIT> and learning-based <OTH> methods '
'These were : BLEU <OTH> , NIST <OTH> , WER -LRB- Word Error Rate -RRB- , PER -LRB- Position-independent WER -RRB- , GTM -LRB- General Text Matcher -RRB- , and METEOR <CIT> '
'Accordingly , in <CIT> we used a machine learning based coreference resolution system to provide an extrinsic evaluation of the utility of WordNet and Wikipedia relatedness measures for NLP applications '
'The maximum entropy classier <CIT> used is Le Zhang ''s Maximum Entropy Modeling Toolkit and the L-BFGS parameter estimation algorithm with gaussian prior smoothing <OTH> '
'One option would be to leverage unannotated text <CIT> '
'Model Bits \/ Character ASCII Huffman code each char Lempel-Ziv -LRB- Unix TM compress -RRB- Unigram -LRB- Huffman code each word -RRB- Trigram Human Performance 8 5 443 21 -LRB- Brown , personal communication -RRB- 176 <CIT> 125 <OTH> The cross entropy , H , of a code and a source is given by : H -LRB- source , code -RRB- = ~ ~ Pr -LRB- s , h I source -RRB- log 2 Pr -LRB- s I h , code -RRB- s h where Pr -LRB- s , h I source -RRB- is the joint probability of a symbol s following a history h given the source '
'These later inductive phases may rely on some level of a priori knowledge , like for example the naive case relations used in the ARIOSTO_LEX system <OTH> '
'<CIT> says that 067 a10a14a11a15a10 08 allows just tentative conclusions to be drawn '
'Rmnshaw and Marcus <OTH> introdu -LRB- : e -LRB- l a 1 -RRB- aseNl '' whi -LRB- : h is a non-re -LRB- : ursive NIL They used trmlsfornmtion-1 -RRB- ase -LRB- l learning to i -LRB- lentif ~ y n -LRB- \/ nrecto ` sire l -RRB- aseNPs in a s -LRB- mtence '
'We are encoding the knowledge as axioms in what is for the most part first-order logic , described in <CIT> , although quantification over predicates is sometimes convenient '
'3 <CIT> and Kleins Coreference Model To gauge the performance of our model , we compare it with a Bayesian model for unsupervised coreference resolution that was recently proposed by <CIT> and Klein <OTH> '
'The association relationship between two words can be indicated by their mutual information , which can be further used to discover phrases -LRB- <CIT> -RRB- '
'<CIT> used a Bloomier filter to encode a LM '
'<OTH> from the Penn Treebank <CIT> WSJ corpus '
'Several non-linear objective functions , such as F-score for text classification <OTH> , and BLEU-score and some other evaluation measures for statistical machine translation <CIT> , have been introduced with reference to the framework of MCE criterion training '
'Firstly , there is also H -LRB- RB -RRB- A -LRB- ADVP -RRB- declined H -LRB- VBD -RRB- H -LRB- VP -RRB- the dollar A -LRB- DT -RRB- H -LRB- NN -RRB- C -LRB- NP-SBJ -RRB- H -LRB- VP -RRB- H -LRB- S -RRB- Figure 2 : A tree with constituents marked the top-down method , which is a version of the algorithm described by Hockenmaier et al <OTH> , but used for translating into simple -LRB- AB -RRB- CG rather than the Steedmans Combinatory Categorial Grammar -LRB- CCG -RRB- <OTH> '
'(Turney, 2002; Pang et al., 2002; Dave at al., 2003).'
'Ramshaw and Marcus <CIT> first represented base noun phrase recognition as a machine learning problem '
'Approaches include word substitution systems <CIT> , phrase substitution systems <OTH> , and synchronous context-free grammar systems <OTH> , all of which train on string pairs and seek to establish connections between source and target strings '
'A variety of unsupervised WSD methods , which use a machinereadable dictionary or thesaurus in addition to a corpus , have also been proposed <CIT> '
'In his analysis of Yarowsky <OTH> , <CIT> formulates several variants of bootstrapping '
'To make this paper comparable to <CIT> , we use English-French notation in this section '
'<CIT> reports that we should not rely on the assumption of a normal distribution when performing statistical text analysis and suggests that parametric analysis based on the binomial or multinomial distributions is a better alternative for smaller texts '
'<CIT> used transformation-based learning , an error-driven learning technique introduced by Eric Bn11 <OTH> , to locate chunks in the tagged corpus '
'7 For a more detailed discussion , see Berger , Della Pietra , and Della Pietra <OTH> and <CIT> <OTH> '
'146 23 Approximating ISBNs <CIT> proposes two approximations for inference in ISBNs , both based on variational methods '
'Because of its central role in building machine translation systems and because of the complexity of the task , sub-sentential alignment of parallel corpora continues to be an active area of research <CIT> , and this implies a continuing demand for manually created or human-verified gold standard alignments for development and evaluation purposes '
'In future work we plan to experiment with richer representations , eg including long-range n-grams <OTH> , class n-grams <CIT> , grammatical features <OTH> , etc '' '
'The approach is in the spirit of <CIT> on retrieving collocations from text corpora , but is more integrated with parsing '
'Experimentation The corpus used in shallow parsing is extracted from the PENN TreeBank <CIT> of 1 million words -LRB- 25 sections -RRB- by a program provided by Sabine Buchholz from Tilburg University '
'We collected training samples from the Brown Corpus distributed with the Penn Treebank <CIT> '
'Recent research in open information extraction <CIT> has shown that we can extract large amounts of relational data from open-domain text with high accuracy '
'Rather than learning how strings in one language map to strings in another, however, translation now involves learning how systematic patterns of errors in ESL learners English map to corresponding patterns in native English 2.2 A Noisy Channel Model of ESL Errors If ESL error correction is seen as a translation task, the task can be treated as an SMT problem using the noisy channel model of (Brown et al. , 1993): here the L2 sentence produced by the learner can be regarded as having been corrupted by noise in the form of interference from his or her L1 model and incomplete language models internalized during language learning.'
'Results on the provided 2000sentence development set are reported using the BLEU metric <CIT> '
'3 Evaluation of Algorithms All four algorithms were run on a 3900 utterance subset of the Penn Treebank annotated corpus <CIT> provided by Charniak and Ge -LRB- 1998 -RRB- '
'Like <CIT> , we give our model information about the basic types of pronouns in English '
'Averaging has been shown to help reduce overfitting <CIT> '
'Though several algorithms <CIT> have been proposed 100 -LRB- 9o -LRB- 80 -LRB- 4O -LRB- 20 -LRB- 1000 goo 80 ~ 41111 2 @ 5 10 15 20 25 30 5 10 15 20 25 30 iteration of EM iteration of EM -LRB- a -RRB- -LRB- b -RRB- Figure 1 : Plots of -LRB- a -RRB- training and -LRB- b -RRB- test perplexity versus number of iterations of the EM algorithm , for the aggregate Markov model with C = 32 classes '
'These weights or scaling factors can be optimized with respect to some evaluation criterion <CIT> '
'<OTH> , various classification models and linguistic features have been proposed to improve the classification performance <CIT> '
'Several models were introduced for these problems , for example , the Hidden Markov Model -LRB- HMM -RRB- <OTH> , Maximum Entropy Model -LRB- ME -RRB- <CIT> , and Conditional Random Fields -LRB- CRFs -RRB- <OTH> '
'Our framework makes use of the log-frequency Bloom filter presented in <CIT> , and described briefly below , to compute smoothed conditional n-gram probabilities on the fly '
'5 Related Work As discussed in footnote 3 , <CIT> and McDonald et al '
'214 Model Features Our MST models are based on the features described in <CIT> ; specifically , we use features based on a dependency nodes form , lemma , coarse and fine part-of-speech tag , and morphologicalstring attributes '
'We compare an ordinary PCFG estimated with maximum likelihood <CIT> and the HDP-PCFG estimated using the variational inference algorithm described in Section 26 '
'See <CIT> for additional work using perceptron algorithms to train tagging models , and a more thorough description of the theory underlying the perceptron algorithm applied to ranking problems '
'2 The ME Tagger The ME tagger is based on <CIT> s POS tagger and is described in Curran and Clark -LRB- 2003 -RRB- '
'This could , for example , aid machine-translation evaluation , where it has become common to evaluate systems by comparing their output against a bank of several reference translations for the same sentences <CIT> '
'methods for syntactic SMT held to this assumption in its entirety <CIT> '
'This corpus contains annotations of semantic PASs superimposed on the Penn Treebank -LRB- PTB -RRB- <CIT> '
'<OTH> , <CIT> <OTH> , Dave et al '
'Significant neighbor-based co-occurrence : As discussed in <CIT> , it is possible to measure the amount of surprise to see two neighboring words in a corpus at a certain frequency under the assumption of independence '
'2 Block Orientation Bigrams This section describes a phrase-based model for SMT similar to the models presented in <CIT> '
'For the evaluation of translation quality , we used the BLEU metric <CIT> , which measures the n-gram overlap between the translated output and one or more reference translations '
'The decoding process is very similar to those described in <CIT> : It starts from an initial empty hypothesis '
'Moreover , the inference procedure for each sentence pair is non-trivial , proving NP-complete for learning phrase based models <OTH> or a high order polynomial -LRB- O -LRB- f 3 e 3 -RRB- -RRB- 1 for a sub-class of weighted synchronous context free grammars <CIT> '
'<OTH> 866 867 119 611 Collins <OTH> 887 885 092 667 Charniak and Johnson <OTH> 901 901 074 701 This Paper 903 900 078 685 all sentences LP LR CB 0CB Klein and Manning <OTH> 863 851 131 572 <CIT> et al '
'With this constraint , each of these binary trees is unique and equivalent to a parse tree of the canonical-form grammar in <CIT> '
'73 122 Baseline System and Experimental Setup We take BBNs HierDec , a string-to-dependency decoder as described in <OTH> , as our baseline for the following two reasons : It provides a strong baseline , which ensures the validity of the improvement we would obtain '
'This tagging scheme is the IOB scheme originally put forward by Ramshaw and Marcus <CIT> '
'<OTH> and <CIT> et al '
'The implementation of the algorithm is one that has a core of code that can run on either the Penn Treebank <CIT> or on the Chinese Treebank '
'One approach to translate terms consists in using a domain-specific parallel corpus with standard alignment techniques <CIT> to mine new translations '
'We use SUMMA <OTH> to generate generic and query-based multi-document summaries and evaluate them using ROUGE evaluation metrics <CIT> relative to human generated summaries '
'The initial phase relies on a parser that draws on the SPECIALIST Lexicon <OTH> and the Xerox Part-of-Speech Tagger <CIT> to produce an underspecified categorial analysis '
'In marked contrast to annotated training material for partof-speech tagging, (a) there is no coarse-level set of sense distinctions widely agreed upon (whereas part-of-speech tag sets tend to differ in the details); (b) sense annotation has a comparatively high error rate (Miller, personal communication, reports an upper bound for human annotators of around 90% for ambiguous cases, using a non-blind evaluation method that may make even this estimate overly optimistic); and (c) no fully automatic method provides high enough quality output to support the ''annotate automatically, correct manually'' methodology used to provide high volume annotation by data providers like the Penn Treebank project (Marcus et al. , 1993).'
'This negation handling is similar to that used in <CIT> '
'To estimate combination weights , we extend the F 1 - score maximization training algorithm for LRM described in <CIT> '
'<CIT> grouped nouns into thesaurus-like lists based on the similarity of their syntactic contexts '
'Our method was applied to 23 million words of the WSJ that were automatically tagged with Ratnaparkhi ''s maximum entropy tagger <CIT> and chunked with the partial parser CASS <OTH> '
'task <OTH> , and reported errors in the range of 26 % are common '
'A pipage approach <OTH> has been proposed for MCKP , but we do not use this algorithm , since it requires costly partial enumeration and solutions to many linear relaxation problems '
'The acquisition of clues is a key technology in these research efforts , as seen in learning methods for document-level SA <CIT> and for phraselevel SA <OTH> '
'The feature weights i are trained in concert with the LM weight via minimum error rate -LRB- MER -RRB- training <CIT> '
'Our next steps will be to take a closer look at the following work : clustering of similar words <CIT> , topic signatures <CIT> and Kilgariffs sketch engine <OTH> '
'31 NP Our NP chunks are very similar to the ones of <CIT> '
'Most of the early work in this area was based on postulating generative probability models of language that included parse structure <CIT> '
'The model weights are trained using the improved iterative scaling algorithm <CIT> '
'We experimented with two independent , arguably complementary techniques for clustering and aligning a predicate argument based approach that extracts more general templates containing one predicate and a ROUGE <CIT> based 265 approach that can extract templates containing multiple verbs '
'We show that the method of <CIT> , which was presented as a simple preprocessing step , is actually equivalent , except our representation explicitly separates hyperparameters which were tied in his work '
'The loglinear model feature weights were learned using minimum error rate training -LRB- MERT -RRB- <CIT> with BLEU score <OTH> as the objective function '
'To reduce it we exploit the one sense per collocation property <CIT> '
'WordNet has been criticized for being overly finegrained <CIT> , we are using it here because it is the sense inventory used by Erk et al '
'2 Related Work Two different approaches have been proposed for Sentence Compression : purely statistical methodologies <CIT> and hybrid linguistic\/statistic methodologies <OTH> '
'This is concordant with the usage in the maximum entropy literature <CIT> '
'43 Baselines 431 Word Alignment We used the GIZA + + implementation of IBM word alignment model 4 <CIT> for word alignment , and the heuristics described in <CIT> to derive the intersection and refined alignment '
'Much previous work has been done on this problem and many different methods have been used: Church''s PARTS (1988) program uses a Markov model; Bourigault (1992) uses heuristics along with a grammar; Voutilainen''s NPTool (1993) uses a lexicon combined with a constraint grammar; Juteson and Katz (1995) use repeated phrases; Veenstra (1998), Argamon, Dagan & Krymolowski(1998) and Daelemaus, van den Bosch & Zavrel (1999) use memory-based systems; Ramshaw & Marcus (In Press) and Cardie & Pierce (1998) use rule-based systems.'
'There are similarities with dependency grammars here because such constraint graphs are also produced by dependency grammars <OTH> <CIT> '
'Learned vowels include -LRB- in order of generation probability -RRB- : e , a , o , u , i , y Learned sonorous consonants include : n , s , r , l , m Learned non-sonorous consonants include : d , c , t , l , b , m , p , q The model bootstrapping is good for dealing with too many parameters ; we see a similar approach in <CIT> march from Model 1 to Model 5 '
'<CIT> managed to extract LFG subcategorisation frames and paths linking long distance dependencies reentrancies from f-structures generated automatically for the PennII treebank trees and used them in an long distance dependency resolution algorithm to parse new text '
'Second , phrase translation pairs are extracted from the word alignment corpus <CIT> '
'The scores were then weighted by the inverse of their height in the tree and then summed together , similarly to the procedure in <OTH> '
'Then , we run GIZA + + <CIT> on the corpus to obtain word alignments in both directions '
'Berry et al (1993)) to yield W  W = U  S  V T as Figure 3 shows, where, for some order R lessmuch min(M,N) of the decomposition, U is a MR left singular matrix with rows ui, i = 1,,M, S is a RR diagonal matrix of singular values s1  s2    sR greatermuch 0, and V is NR a right singular matrix with rows vj, j = 1,,N. For each i, the scaled R-vector uiS may be viewed as representing wi, thei-th word in the vocabulary, and similarly the scaled R-vector vjS as representing dj, j-th document in the corpus.'
'This cost can often be substantial , as with the Penn Treebank <OTH> '
'This characteristic of our corpus is similar to problems with noisy and comparable corpora <OTH> , and it prevents us from using methods developed in the MT community based on clean parallel corpora , such as <CIT> '
'It forms a baseline for performance evaluations , but is prone to sparse data problems <CIT> '
'1 Introduction Word alignmentdetection of corresponding words between two sentences that are translations of each otheris usually an intermediate step of statistical machine translation -LRB- MT -RRB- <CIT> , but also has been shown useful for other applications such as construction of bilingual lexicons , word-sense disambiguation , projection of resources , and crosslanguage information retrieval '
'The basic phrase reordering model is a simple unlexicalized , context-insensitive distortion penalty model <CIT> '
'Corpus Time Period Size Articles Words New Indian Express -LRB- English -RRB- 20070101 to 20070831 2,359 347,050 Dinamani -LRB- Tamil -RRB- 20070101 to 20070831 2,359 256,456 Table 1 : Statistics on Comparable Corpora From the above corpora , we first extracted all the NEs from the English side , using the Stanford NER tool <CIT> '
'In general , they can be divided into two major categories , namely lexicalized models <CIT> and un-lexicalized models <OTH> '
'The candidates were then ranked according to the scores assigned by four association measures : the log-likelihood ratio G2 <CIT> , Pearsons chi-squared statistic X2 <OTH> , the t-score statistic t <OTH> , and mere cooccurrence frequency f4 TPs were identified according to the definition of Krenn <OTH> '
'5 Conclusions and Future Work The paper compares Structural Correspondence Learning <CIT> with -LRB- various instances of -RRB- self-training <OTH> for the adaptation of a parse selection model to Wikipedia domains '
'Second , we discuss the work done by <CIT> who use clustering of paraphrases to induce rewriting rules '
'Word alignment is newer , found only in a few places <CIT> '
'Uses for k-best lists include minimum Bayes risk decoding <OTH> , discriminative reranking <OTH> , and discriminative training <CIT> '
'<OTH> , <CIT> -RRB- '
'The disambiguation model of this parser is based on a maximum entropy model <CIT> '
'The reliability for the two annotation tasks -LRB- - statistics <CIT> -RRB- was of 094 and 090 respectively '
'2 Previous Work We briefly outline the most important existing methods and cite error rates on a standard English data set , sections 03-06 of the Wall Street Journal -LRB- WSJ -RRB- corpus <CIT> , containing nearly 27,000 examples '
'<CIT> -RRB- , and emotion studies -LRB- eg '
'<OTH> , bilingual sentences are trained by GIZA + + <CIT> in two directions -LRB- from source to target and target to source -RRB- '
'We have begun experimenting with log likelihood ratio <CIT> as a thresholding technique '
'33 Language Model -LRB- LM -RRB- As a second baseline we use the classification based on the language model using overlapping ngram sequences -LRB- n was set to 8 -RRB- as suggested by <CIT> for the English language '
'Method Source Spearman <OTH> Wikipedia 019048 <OTH> WordNet 033035 <OTH> Rogets 055 <CIT> WordNet 055 <OTH> Web corpus , WN 056 <OTH> ODP 065 <OTH> Wikipedia 075 SVM Web corpus , WN 078 Table 9 : Comparison with previous work for WordSim353 '
'Following the suggestions in <CIT> , Core et al consider kappa scores above 067 to indicate significant agreement and scores above 08 reliable agreement '
'These rules are learned using a word alignment model , which finds an optimal mapping from words to MR predicates given a set of training sentences and their correct MRs Word alignment models have been widely used for lexical acquisition in SMT <CIT> '
'While work on subjectivity analysis in other languages is growing -LRB- eg , Japanese data are used in <CIT> , Chinese data are used in <OTH> , and German data are used in <OTH> -RRB- , much of the work in subjectivity analysis has been applied to English data '
'While this technique has been sttccessfully applied to parsing lhe ATIS portion in the Penn Treebank <CIT> , it is extremely time consuming '
'<CIT> and Lin and Och -LRB- 2004 -RRB- proposed an LCS-based automatic evaluation measure called ROUGE-L '
'In <CIT> , the definition words were used as initial sense indicators , automatically tagging the target word examples containing them '
'We follow IBM Model 1 <CIT> and assume that each word in an utterance is generated by exactly one role in the parallel frame Using standard EM to learn the role to word mapping is only sufficient if one knows to which level in the tree the utterance should be mapped '
'We compared this nonprobabilistic DOP model against tile probabilistic DOP model -LRB- which estimales the most probable parse for each sentence -RRB- on three different domains : tbe Penn ATIS treebank <CIT> , the Dutch OVIS treebank <OTH> and tile Penn Wall Street Journal -LRB- WSJ -RRB- treebank <CIT> '
'<CIT> used the averaged perceptron <CIT> '
'However , with the algorithms proposed in <CIT> , it is possible to develop a general-purpose decoder that can be used by all the parsing-based systems '
'We also show that integrating our case prediction model improves the quality of translation according to BLEU <CIT> g2 and human evaluation '
'A CYK-style decoder has to rely on binarization to preprocess the grammar as did in <CIT> to handle multi-nonterminal rules '
'The results of these studies have important applications in lexicography , to detect lexicosyntactic regularities -LRB- <CIT> and Hanks , 19901 <OTH> , such as , for example ~ support verbs -LRB- eg ` make-decision '' -RRB- prepositional verbs -LRB- eg ` rely-upon '' -RRB- idioms , semantic relations -LRB- eg ` part_of '' -RRB- and fixed expressions -LRB- eg ` kick the bucket '' -RRB- '
'Following the perspective of <CIT> , a minimal set of phrase blocks with lengths -LRB- m , n -RRB- where either m or n must be greater than zero results in the following types of blocks : 1 '
'For a class bigram model , find : V -- + C to maximize ~ -LRB- T -RRB- = ~ I\/L = I p -LRB- wi I -LRB- wl -RRB- -RRB- p -LRB- -LRB- wi -RRB- l -LRB- wi-1 -RRB- -RRB- -RRB- -RRB- Alternatively , perplexity <OTH> or average mutual information <CIT> can be used as the characteristic value for optimization '
'O ` Hara and Wiebe <OTH> make use of Penn Treebank <CIT> and FrameNet <OTH> to classify prepositions '
'6 The Experiments We used the Penn Treebank <CIT> to perform empirical experiments on the proposed parsing models '
'For example , <OTH> developed a system to identify inflammatory texts and <CIT> developed methods for classifying reviews as positive or negative '
'Parameters were tuned with minimum error-rate training <CIT> on the NIST evaluation set of 2006 -LRB- MT06 -RRB- for both C-E and A-E '
'We used a maximummatching algorithm and a dictionary compiled from the CTB <OTH> to do segmentation , and trained a maximum entropy part-ofspeech tagger <OTH> and TAG-based parser <OTH> on the CTB to do tagging and parsing4 Then the same feature extraction and model-training was done for the PDN corpus as for the CTB '
'<CIT> proposed a coreference resolution approach which also explores the information from the syntactic parse trees '
'Therefore , we determine the maximal translation probability of the target word e over the source sentence words : p ibm1 -LRB- e f J 1 -RRB- = max j = 0 , , J p -LRB- e f j -RRB- -LRB- 18 -RRB- where f 0 is the empty source word <CIT> '
'-LRB- KD1 , 2371 -RRB- 23 Reliability To evaluate the reliability of the annotation , we use the kappa coe cient -LRB- K -RRB- <CIT> , which measures pairwise agreement between a set of coders making category judgements , correcting for expected chance agreement '
'Occasionally , in 59 sentences out of 2416 on section 23 of the Wall Street Journal Penn Treebank <CIT> , the shift-reduce parser fails to attach a node to a head , producing a disconnected graph '
'We build sentencespecific zero-cutoff stupid-backoff <CIT> 5-gram language models , estimated using 47B words of English newswire text , and apply them to rescore each 10000-best list '
'The techniques examined are Structural Correspondence Learning -LRB- SCL -RRB- <CIT> and Self-training <OTH> '
'52 Evaluation Criteria For the automatic evaluation , we used the criteria from the IWSLT evaluation campaign <OTH> , namely word error rate -LRB- WER -RRB- , positionindependent word error rate -LRB- PER -RRB- , and the BLEU and NIST scores <CIT> '
'Our process of extraction of rules as synchronous trees and then converting them to synchronous CFG rules is most similar to that of <CIT> '
'SGD was recently used for NLP tasks including machine translation <CIT> and syntactic parsing <OTH> '
'22 STT : A Statistical Tree-based Tagger The aim of statistical or probabilistic tagging <CIT> is to assign the most likely sequence of tags given the observed sequence of words '
'The NIST MT03 test set is used for development , particularly for optimizing the interpolation weights using Minimum Error Rate training <CIT> '
'For instance <OTH> , <OTH> <OTH> all automatically acquire large TAGs for English from the Penn Treebank <CIT> '
'Toward a Task-based Gold Standard for Evaluation of NP Chunks and Technical Terms Nina Wacholder Rutgers University nina @ scilsrutgersedu Peng Song Rutgers University psong @ paulrutgersedu Abstract We propose a gold standard for evaluating two types of information extraction output - noun phrase -LRB- NP -RRB- chunks <CIT> and technical terms <OTH> '
'The relationship between the translation model and the alignment model is given by : Pr -LRB- fJ1 jeI1 -RRB- = X aJ1 Pr -LRB- fJ1 ; aJ1jeI1 -RRB- -LRB- 3 -RRB- In this paper , we use the models IBM-1 , IBM4 from <CIT> and the HiddenMarkovalignmentmodel -LRB- HMM -RRB- from <OTH> '
'Based on this theoretical cornerstone , <CIT> presented a PCFG-based chart generator using wide-coverage LFG approximations automatically extracted from the Penn-II treebank '
'Many grammars , such as finite-state grammars -LRB- FSG -RRB- , bracket\/inversion transduction grammars -LRB- BTG\/ITG -RRB- <CIT> , context-free grammar -LRB- CFG -RRB- , tree substitution grammar -LRB- TSG -RRB- <OTH> and their synchronous versions , have been explored in SMT '
'The problem is that with such a definition of collocations , even when improved , one identifies not only collocations but freecombining pairs frequently appearing together such as lawyer-client ; doctor-hospital , as pointed out by <CIT> '
'The parsing algorithm was CKY-style parsing with beam thresholding , which was similar to ones used in <OTH> '
'The experiments were performed using the Wall Street Journal -LRB- WSJ -RRB- corpus of the University of Pennsylvania <CIT> modified as described in <OTH> and <OTH> '
'The approach is related , but not identical , to distributional similarity -LRB- for details , see <CIT> and <OTH> -RRB- '
'While the BBN model does not perform at the level of Model 2 of <CIT> on Wall Street Journal text , it is also less language-dependent , eschewing the distance metric -LRB- which relied on specific features of the English Treebank -RRB- in favor of the ` bigrams on nonterminals '' model '
'Maximum Entropy Modeling -LRB- MaxEnt -RRB- <CIT> and Support Vector Machine -LRB- SVM -RRB- <OTH> were used to build the classifiers in our solution '
'They can be seen as extensions of the simpler IBM models 1 and 2 <CIT> '
'Previous authors have used numerous HMM-based models <OTH> and other types of networks including maximum entropy models <CIT> , conditional Markov models <OTH> , conditional random elds -LRB- CRF -RRB- <OTH> , and cyclic dependency networks <OTH> '
'Unlike <CIT> , in the shared task we used only the simplest feed-forward approximation , which replicates the computation of a neural network of the type proposed in <OTH> '
'<CIT> used a corpus-based algorithm '
'More recently , EM has been used to learn hidden variables in parse trees ; these can be head-childannotations <OTH> , latent head features <CIT> , or hierarchicallysplit nonterminal states <OTH> '
'33 CRFs and Perceptron Learning Perceptron training for conditional models <CIT> is an approximation to the SGD algorithm , using feature counts from the Viterbi label sequence in lieu of expected feature counts '
'In none of these cases did we repeat minimum-error-rate training ; all these systems were trained using max-B The metrics we tested were : METEOR <CIT> , version 06,usingtheexact,Porter-stemmer , andWordNet synonmy stages , and the optimized parameters = 081 , = 083 , = 028 as reported in <OTH> '
'Each item is associated with a stack whose signa12Specifically a B-hypergraph , equivalent to an and-or graph <OTH> or context-free grammar <OTH> '
'Under the maximum entropy framework <CIT> , evidence from different features can be combined with no assumptions of feature independence '
'Paraphrases can also be automatically acquired using statistical methods as shown by <CIT> '
'The formally syntax-based model for SMT was first advocated by <CIT> '
'Therefore , the results are more informative than a simple agreement average <CIT> '
'in that order <CIT> '
'Aggregate models based on higher-order n-grams <CIT> might be able to capture multi-word structures such as noun phrases '
'Feature function scaling factors m are optimized based on a maximum likelihood approach <CIT> or on a direct error minimization approach <CIT> '
'(Brown et al. , 1993; Vogel et al. , 1996; Garca-Varea et al. , 2002; Ahrenberg et al. , 1998; Tiedemann, 1999; Tufis and Barbu, 2002; Melamed, 2000).'
'Instead of using a single system output as the skeleton , we employ a minimum Bayes-risk decoder to select the best single system output from the merged N-best list by minimizing the BLEU <CIT> loss '
'(He et al., 2008).'
'Dirichlet priors can be used to bias HMMs toward more skewed distributions <CIT> , which is especially useful in the weakly supervised setting consideredhere '
'Here we used the averaged perceptron <CIT> , where the weight matrix used to classify the test data is the average of all of the matrices posited during training , ie , a1 a62 a52 a49 a62 a49 a42a51a50a53a52 a1 a42 42 Multicomponent architecture Task specific and external training data are integrated with a two-component perceptron '
'The polarity value proposed by <CIT> is as follows '
'The percentage agreement for each of the features is shown in the following table : feature percent agreement form 100 % intentionality 749 % awareness 935 % safety 907 % As advocated by <CIT> , we have used the Kappa coefficient <OTH> as a measure of coder agreement '
'To regularize the model we take as the final model the average of all weight vectors posited during training <CIT> '
'3 Bilingual Task : An Application for Word Alignment 31 Sentence and word alignment Bilingual alignment methods <CIT> '
'The first is identifying words and phrases that are associated with subjectivity , for example , that think is associated with private states and that beautiful is associated with positive sentiments -LRB- eg , <CIT> -RRB- '
'The same probabilities are also included using 50 hard word classes derived from the parallel corpus using the GIZA + + mkcls utility <CIT> '
'Many methods for calculating the similarity have been proposed <CIT> '
'Approaches include word substitution systems <OTH> , phrase substitution systems <CIT> , and synchronous context-free grammar systems <OTH> , all of which train on string pairs and seek to establish connections between source and target strings '
'Next , we learn our polarity classifier using positive and negative reviews taken from two movie 611 review datasets , one assembled by <CIT> and the other by ourselves '
'Sentiment classification is a well studied problem <CIT> and in many domains users explicitly 1We use the term aspect to denote properties of an object that can be rated by a user as in Snyder and Barzilay <OTH> '
'In contrast , semi-supervised domain adaptation <CIT> is the scenario in which , in addition to the labeled source data , we only have unlabeled and no labeled target domain data '
'After that , we used three types of methods for performing a symmetrization of IBM models : intersection , union , and refined methods <CIT> '
'Several automatic sentence alignment approaches have been proposed based on sentence length <CIT> and lexical information <OTH> '
'These parameters 1 8 are tuned by minimum error rate training <CIT> on the dev sets '
'However , another approach is to train a separate out-of-domain parser , and use this to generate additional features on the supervised and unsupervised in-domain data <CIT> '
'To extract such word clusters we used suffix arrays proposed in Yamamoto and <CIT> and the pointwise mutual information measure , see Church and Hanks -LRB- 1990 -RRB- '
'A third of this is syntactically parsed as part of the Penn Treebank <CIT> and has dialog act annotation <OTH> '
'To overcome these limitations , many syntaxbased SMT models have been proposed <CIT> '
'Our test set is 3718 sentences from the English Penn treebank <CIT> which were translated into German '
'The third exploits automatic subjectivity analysis in applications such as review classification -LRB- eg , <CIT> -RRB- , mining texts for product reviews -LRB- eg , <OTH> -RRB- , summarization -LRB- eg , <OTH> -RRB- , information extraction -LRB- eg , <OTH> -RRB- , 1Note that sentiment , the focus of much recent work in the area , is a type of subjectivity , specifically involving positive or negative opinion , emotion , or evaluation '
'Only recently the issue has drawn attention : <CIT> present an initial analysis of the factors that influence system performance in content selection '
'-LRB- subjective -RRB- So far , none of the studies in sentiment detection <CIT> or opinion extraction <OTH> have specifically looked at the role of superlatives in these areas '
'It is difficult to compare these with previous work , but <CIT> report that in a completely unsupervised setting , their MRF model , which uses a large set of additional features and a more complex estimation procedure , achieves an average 1-to-1 accuracy of 413 % '
'In this paper , a new part-of-speech tagging method hased on neural networks -LRB- Net-Tagger -RRB- is presented and its performance is compared to that of a llMM-tagger <CIT> and a trigrambased tagger <OTH> '
'We determined appropriate training parameters and network size based on intermediate validation 1We used a publicly available tagger <CIT> to provide the tags '
'<CIT> s parser and its reimplementation and extension by Bikel -LRB- 2002 -RRB- have by now been applied to a variety of languages : English <CIT> , Czech <CIT> , German <OTH> , Spanish <CIT> , French <OTH> , Chinese <OTH> and , according to Dan Bikels web page , Arabic '
'1 Introduction Parsing technology has come a long way since Charniak <OTH> demonstrated that a simple treebank PCFG performs better than any other parser -LRB- with F175 accuracy -RRB- on parsing the WSJ Penn treebank <CIT> '
'<CIT> approached chunking by using a machine learning method '
'<OTH> , <CIT> <OTH> -RRB- , a sentence -LRB- eg , Liu et al '
'The recent approaches used pair-wise alignment algorithms based on symmetric alignments from a HMM alignment model <OTH> or edit distance alignments allowing shifts <CIT> '
'Antonyms often indicate the discourse relation of contrast <CIT> '
'For example , a statistical machine translation system such as ISIs AlTemp SMT system <CIT> can generate a list of n-best alternative translations given a source sentence '
'The per-state models in this paper are log-linear models , building upon the models in <CIT> and <OTH> , though some models are in fact strictly simpler '
'This is the traditional approach for glass-box smoothing <CIT> '
'The former extracts collocations within a fixed window <CIT> '
'Similarly , <CIT> uses a six content word window to extract significant collocations '
'The text was split at the sentence level , tokenized and PoS tagged , in the style of the Wall Street Journal Penn TreeBank <OTH> '
'Data and Parameters To facilitate comparison with previous work , we trained our models on sections 2-21 of the WSJ section of the Penn tree-bank <CIT> '
'Each linked fragment pair consists of a source-language side and a target-language side , similar to <CIT> '
'For example , the lexicalized grammars of <CIT> and Charniak -LRB- 1997 -RRB- and the statesplit grammars of Petrov et al '
'4 Related work Algorithms for retrieving collocations has been described <CIT> <OTH> '
'Part-ofspeech taggers are used in a few applications , such as speech synthesis <OTH> and question answering <OTH> '
'Hence our classifier evaluation omits those two word positions , leading to n2 classifications for a string of length n Table 1 shows statistics from sections 2-21 of the Penn WSJ Treebank <CIT> '
'Minimum-error-rate training was done using Koehns implementation of <CIT> minimum-error-rate model '
'The work reported in this paper is most closely related to work on statistical machine translation , particularly the IBM-style work on CANDIDE <CIT> '
'CIT -RRB- '
'These domains have been commonly used in prior work on summarization <CIT> '
'The analyser -- and therefore the generator-includes exception lists derived from WordNet <OTH> '
'In practice , 7 - \/ is very large and the model ''s expectation Efj can not be computed directly , so the following approximation <OTH> is used : n E fj , ~ E15 -LRB- hi -RRB- p -LRB- tilhi -RRB- fj -LRB- hi , ti -RRB- i = 1 where fi -LRB- hi -RRB- is the observed probability of the history hi in the training set '
'4 Experiment 41 Evaluation Method We evaluated each sentence compression method using word F-measures , bigram F-measures , and BLEU scores <CIT> '
'<CIT> show that exploiting all contiguous word blocks in phrase-based alignment is better than focusing on syntactic constituents only '
'It is an implementation of Models 1-4 of <CIT> , where each of these models produces a Viterbi alignment '
'For regularization purposes we adopt an average perceptron <CIT> which returns for each y , y = 1T summationtextTt = 1 ty , the average of all weight vectors ty posited during training '
'Its applications range from sentence boundary disambiguation <CIT> to part-of-speech tagging <CIT> , parsing <CIT> and machine translation <OTH> '
'For the named entity features , we used a fairly standard feature set , similar to those described in <CIT> '
'In contrast , semi-supervised domain adaptation <CIT> is the scenario in which , in addition to the labeled source data , we only have unlabeled and no labeled target domain data '
'For instance , for Maximum Entropy , I picked <OTH> for the basic theory , <OTH> for an application -LRB- POS tagging in this case -RRB- , and <CIT> for more advanced topics such as optimization and smoothing '
'On the other hand , the thesaurus-based method of Yarowsky <OTH> may suffer from loss of information -LRB- since it is semi-class-based -RRB- as well as data sparseness -LRB- since H Classes used in Resnik <OTH> are based on the WordNet taxonomy while classes of <CIT> et al '
'Identification of Terms To-be Transliterated -LRB- TTT -RRB- must not be confused with recognition of Named Entities -LRB- NE -RRB- <CIT> '
'<OTH> who employ clusters of related words constructed by the Brown clustering algorithm <CIT> for syntactic processing of texts '
'To this extent , we cast the supersense tagging problem as a sequence labeling task and train a discriminative Hidden Markov Model -LRB- HMM -RRB- , based on that of <CIT> , on the manually annotated Semcor corpus <OTH> '
'We then propose a relatively simple yet effective method for resolving translation disambiguation using mutual information -LRB- MI -RRB- <CIT> statistics obtained only from the target document collection '
'Notice that most in-context and dictionary translations of source words are bounded within the same category in a typical thesaurus such as the LLOCE <OTH> and CILIN <OTH> '
'-LRB- HICSS-35 </booktitle> <contexts> <context> documents , genres also work on an intra-document , or page segment level because a single document can contain instances of multiple genres , eg , contact information , list of publications , CV , see <OTH> '
'The Xerox tagger <CIT> comes with a set of rules that assign an unknown word a set of possible pos-tags -LRB- ie , POS-class -RRB- on the basis of its ending segment '
'1 Introduction Mainstream approaches in statistical parsing are based on nondeterministic parsing techniques , usually employing some kind of dynamic programming , in combination with generative probabilistic models that provide an n-best ranking of the set of candidate analyses derived by the parser <CIT> '
'This step can be seen as a multi-label , multi-class call classi cation problem for customer care applications <CIT> '
'tile data put tbrward by ll , amshaw and Marcus <OTH> '
'In this year , CoNLL-2007 shared task <OTH> focuses on multilingual dependency parsing based on ten different languages <CIT> and domain adaptation for English <CIT> without taking the languagespecific knowledge into consideration '
'For example , bilingual lexicographers can use bitexts to discover new cross-language lexicalization patterns <OTH> ; students of foreign languages can use one half of a bitext to practice their reading skills , referring to the other half for translation when they get stuck <OTH> '
'41 Translation Modeling We can test our models utility for translation by transforming its parameters into a phrase table for the phrasal decoder Pharaoh <CIT> '
'1 Introduction Word alignment is an important component of a complete statistical machine translation pipeline <CIT> '
'Other similar work includes the mention detection -LRB- MD -RRB- task <OTH> and joint probabilistic model of coreference <CIT> '
'have been proposed <OTH> '
'5 Datasets For evaluation we selected two domain adaptation datasets : spam <OTH> and sentiment <CIT> '
'One option would be to leverage unannotated text <CIT> '
'The problem itself has started to get attention only recently <CIT> '
'Not only is this beneficial in terms of parsing complexity , but smaller rules can also improve a translation models ability to generalize to new data <CIT> '
'task <CIT> , and reported errors in the range of 26 % are common '
'The natural next step in sentence alignment is to account for word ordering in the translation model , eg , the models described in <CIT> could be used '
'It is interesting to constrast this method with the ` parse-parse-match '' approaches that have been reported recently for producing parallel bracketed corpora <OTH> '
'Suhm and Waibel <OTH> and Eckert , Gallwitz , and Niemann <OTH> each condition a recognizer LM on left-to-right DA predictions and are able to 366 Stolcke et al Dialogue Act Modeling show reductions in word error rate of 1 % on task-oriented corpora '
'41 Corpora Sentence compression systems have been tested on product review data from the Ziff-Davis -LRB- ZD , henceforth -RRB- Corpus by Knight and Marcu <OTH> , general news articles by <CIT> and biomedical articles <OTH> '
'The methods for calculating relative frequencies <OTH> and lexical weights <CIT> are also adapted for the weighted matrix case '
'For instance , word alignment models are often trained using the GIZA + + toolkit <CIT> ; error minimizing training criteria such as the Minimum Error Rate Training <CIT> are employed in order to learn feature function weights for log-linear models ; and translation candidates are produced using phrase-based decoders <OTH> in combination with n-gram language models <OTH> '
'Both training and testing sentences were processed using Collins parser <CIT> to generate parse-tree automatically '
'This linear model is learned using a variant of the incremental perceptron algorithm <CIT> '
'32 Translation performance For the experiments reported in this section , we used feature weights trained with minimum error rate training <CIT> Because MERT ignores the denominator in Equation 1 , it is invariant with respect to the scale of the weight vector the Moses implementation simply normalises the weight vector it finds by its lscript1-norm '
'<OTH> , Pedersen <OTH> , Yarowsky and Florian <OTH> -RRB- as well as maximum entropy models -LRB- eg , Dang and Palmer <OTH> , <CIT> and Manning <OTH> -RRB- '
'Starting with bilingualphrasepairsextractedfromautomatically aligned parallel text <CIT> , these PSCFG approaches augment each contiguous -LRB- in source and target words -RRB- phrase pair with a left-hand-side symbol -LRB- like the VP in the example above -RRB- , and perform a generalization procedure to form rules that include nonterminal symbols '
'The IBM model 1 <CIT> is used to find an initial estimate of the translation probabilities '
'The accuracy of the generator outputs was evaluated by the BLEU score <CIT> , which is commonly used for the evaluation of machine translation and recently used for the evaluation of generation <OTH> '
'It us widely acknowledged that word sense d~samblguatmn (WSD) us a central problem m natural language processing In order for computers to be able to understand and process natural language beyond simple keyword matching, the problem of d~samblguatmg word sense, or dlscermng the meamng of a word m context, must be effectively dealt with Advances in WSD v, ill have slgmficant Impact on apphcatlons hke information retrieval and machine translation For natural language subtasks hke part-of-speech tagging or s)ntactm parsing, there are relatlvely well defined and agreed-upon cnterm of what it means to have the ''correct'' part of speech or syntactic structure assigned to a word or sentence For instance, the Penn Treebank corpus (Marcus et al, 1993) pro~ide~,t large repo.~tory of texts annotated w~th partof-speech and s}ntactm structure mformatlon Tv.o independent human annotators can achieve a high rate of agreement on assigning part-of-speech tags to words m a g~ven sentence Unfortunately, th~s us not the case for word sense assignment F~rstly, it is rarely the case that any two dictionaries will have the same set of sense defimtmns for a g~ven word Different d~ctlonanes tend to carve up the ''semantic space'' m a different way, so to speak Secondly, the hst of senses for a word m a typical dmtmnar~ tend to be rather refined and comprehensive This is especmlly so for the commonly used words which have a large number of senses The sense dustmctmn between the different senses for a commonly used word m a d~ctmnary hke WoRDNET (Miller, 1990) tend to be rather fine Hence, two human annotators may genuinely dusagree m their sense assignment to a word m context The agreement rate between human annotators on word sense assignment us an Important concern for the evaluatmn of WSD algorithms One would prefer to define a dusamblguatlon task for which there us reasonably hlgh agreement between human annotators The agreement rate between human annotators will then form the upper ceiling against whmh to compare the performance of WSD algorithms For instance, the SENSEVAL exerclse has performed a detaded study to find out the raterannotator agreement among ~ts lexicographers taggrog the word senses (Kllgamff, 1998c, Kllgarnff, 1998a, Kflgarrlff, 1998b) 2 A Case Study In this-paper, we examine the ~ssue of raterannotator agreement by comparing the agreement rate of human annotators on a large sense-tagged corpus of more than 30,000 instances of the most frequently occurring nouns and verbs of Enghsh This corpus is the intersection of the WORDNET Semcor corpus (Miller et al, 1993) and the DSO corpus (Ng and Lee, 1996, Ng, 1997), which has been independently tagged wlth the refined senses of WORDNET by two separate groups of human annotators The Semcor corpus us a subset of the Brown corpus tagged with ~VoRDNET senses, and consists of more than 670,000 words from 352 text files Sense taggmg was done on the content words (nouns, ~erbs, adjectives and adverbs) m this subset The DSO corpus consists of sentences drawn from the Brown corpus and the Wall Street Journal For each word w from a hst of 191 frequently occurring words of Enghsh (121 nouns and 70 verbs), sentences containing w (m singular or plural form, and m its various reflectional verb form) are selected and each word occurrence w ~s tagged w~th a sense from WoRDNET There ~s a total of about 192,800 sentences in the DSO corpus m which one word occurrence has been sense-tagged m each sentence The intersection of the Semcor corpus and the DSO corpus thus consists of Brown corpus sentences m which a word occurrence w is sense-tagged m each sentence, where w Is one of.the 191 frequently oc-,currmg English nouns or verbs Since this common pomon has been sense-tagged by two independent groups of human annotators, ~t serves as our data set for investigating inter-annotator agreement in this paper 3 Sentence Matching To determine the extent of inter-annotator agreement, the first step ~s to match each sentence m Semcor to its corresponding counterpart In the DSO corpus This step ~s comphcated by the following factors 1 Although the intersected portion of both corpora came from Brown corpus, they adopted different tokemzatmn convention, and segmentartan into sentences differed sometimes 2 The latest versmn of Semcor makes use of the senses from WORDNET 1 6, whereas the senses used m the DSO corpus were from WoRDNET 15 1 To match the sentences, we first converted the senses m the DSO corpus to those of WORDNET 1 6 We ignored all sentences m the DSO corpus m which a word is tagged with sense 0 or -1 (A word is tagged with sense 0 or -1 ff none of the given senses m WoRDNFT applies ) 4, sentence from Semcor is considered to match one from the DSO corpus ff both sentences are exactl) ldent~cal or ff the~ differ only m the pre~ence or absence of the characters '' (permd) or -'' (hyphen) For each remaining Semcor sentence, taking into account word ordering, ff 75% or more of the words m the sentence match those in a DSO corpus sentence, then a potential match ~s recorded These i -kctua[ly, the WORD~q''ET senses used m the DSO corpus were from a shght variant of the official WORDNE''I 1 5 release Th~s ssas brought to our attention after the pubhc release of the DSO corpus potential matches are then manually verffied to ensure that they are true matches and to ~eed out any false matches Using this method of matching, a total of 13,188 sentence-palrs contasnmg nouns and 17,127 sentence-pa~rs containing verbs are found to match from both corpora, ymldmg 30,315 sentences which form the intersected corpus used m our present study 4 The Kappa Statistic Suppose there are N sentences m our corpus where each sentence contains the word w Assume that w has M senses Let 4 be the number of sentences which are assigned identical sense b~ two human annotators Then a simple measure to quantify the agreement rate between two human annotators Is Pc, where Pc, = A/N The drawback of this simple measure is that it does not take into account chance agreement between two annotators The Kappa statistic a (Cohen, 1960) is a better measure of rater-annotator agreement which takes into account the effect of chance agreement It has been used recently w~thm computatmnal hngu~stlcs to measure raterannotator agreement (Bruce and Wmbe, 1998, Carletta, 1996, Veroms, 1998) Let Cj be the sum of the number of sentences which have been assigned sense 3 by annotator 1 and the number of sentences whmh have been assigned sense 3 by annotator 2 Then P~-P~ 1-P~ where M j=l and Pe measures the chance agreement between two annotators A Kappa ~alue of 0 indicates that the agreement is purely due to chance agreement, whereas a Kappa ~alue of 1 indicates perfect agreement A Kappa ~alue of 0 8 and above is considered as mdmatmg good agreement (Carletta, 1996) Table 1 summarizes the inter-annotator agreement on the mtersected corpus The first (becond) row denotes agreement on the nouns (xerbs), wh~le the lass row denotes agreement on all words combined The a~erage ~ reported m the table is a s~mpie average of the individual ~ value of each word The agreement rate on the 30,315 sentences as measured by P= is 57% This tallies with the figure reported ~n our earlier paper (Ng and Lee, 1996) where we performed a quick test on a subset of 5,317 sentences,n the intersection of both the Semcor corpus and the DSO corpus 10 [] mm m m m m m mm m m m m mm m m m Type Num of v, ords A N [ P~ Avg Nouns 121 7,676 13,188 I 0 582 0 300 Verbs 70 9,520 17,127 I 0 555 0 347 All I 191 I 17,196 30,315 I 056T 0317 Table 1 Raw inter-annotator agreement 5 Algorithm Since the rater-annotator agreement on the intersected corpus is not high, we would like to find out how the agreement rate would be affected if different sense classes were in use In this section, we present a greedy search algorithm that can automatmalb derive coarser sense classes based on the sense tags assigned by two human annotators The resulting derived coarse sense classes achmve a higher agreement rate but we still maintain as many of the original sense classes as possible The algorithm is given m Figure 1 The algorithm operates on a set of sentences where each sentence contains an occurrence of the word w whmh has been sense-tagged by two human annotators At each Iteration of the algorithm, tt finds the pair of sense classes Ct and Cj such that merging these two sense classes results in the highest t~ value for the resulting merged group of sense classes It then proceeds to merge Cz and C~ Thin process Is repeated until the ~ value reaches a satisfactory value ~,~t,~, which we set as 0 8 Note that this algorithm is also applicable to deriving any coarser set of classes from a refined set for any NLP tasks in which prior human agreement rate may not be high enough Such NLP tasks could be discourse tagging, speech-act categorization, etc 6 Results For each word w from the list of 121 nouns and 70 verbs, ~e applied the greedy search algorithm to each set of sentences in the intersected corpus contaming w For a subset of 95 words (53 nouns and 42 verbs), the algorithm was able to derive a coarser set of 2 or more senses for each of these 95 words such that the resulting Kappa ~alue reaches 0 8 or higher For the other 96 words, m order for the Kappa value to reach 0 8 or higher, the algorithm collapses all senses of the ~ord to a single (trivial) class Table 2 and 3 summarizes the results for the set of 53 nouns and 42 ~erbs, respectively Table 2 md~cates that before the collapse of sense classes, these 53 nouns have an average of 7 6 senses per noun There is a total of 5,339 sentences in the intersected corpus containing these nouns, of which 3,387 sentences were assigned the same sense by the two groups of human annotators The average Kappa statistic (computed as a simple average of the Kappa statistic of ~he mdlwdual nouns) is 0 463 After the collapse of sense classes by the greedy search algorithm, the average number of senses per noun for these 53 nouns drops to 40 Howe~er, the number of sentences which have been asmgned the same coarse sense by the annotators increases to 5,033 That is, about 94 3% of the sentences have been assigned the same coarse sense, and that the average Kappa statistic has improved to 0 862, mgmfymg high rater-annotator agreement on the derived coarse senses Table3 gl~es the analogous figures for the 42 verbs, agmn mdmatmg that high agreement is achieved on the coarse sense classes den~ed for verbs 7 Discussion Our findings on rater-annotator agreement for word sense tagging indicate that for average language users, it is quite dl~cult to achieve high agreement when they are asked to assign refned sense tags (such as those found in WORDNET) given only the scanty definition entries m the WORDNET dlctionary and a few or no example sentences for the usage of each word sense Thin observation agrees wlth that obtmned m a recent study done by (Veroms, 1998), where the agreement on sense-tagging by naive users was also not hlgh Thus It appears that an average language user is able to process language wlthout needing to perform the task of dlsamblguatmg word sense to a very fine-grained resolutmn as formulated m a tradltlonal dmtlonary In contrast, expert lexicographers tagged the ~ ord sense in the sentences used m the SENSEVAL exerclse, where high rater-annotator agreement was reported There are also fuller dlctlonary entries m the HECTOR dlctlonary used and more e<amples showing the usage of each word sense m HECTOR These factors are likely to have contributed to the difference in rater-annotator agreement observed m the three studies conducted We also examined the coarse sense classes derived by the greedy search algorithm Vv''e found some interesting groupings of coarse senses for nouns which ~e hst in Table 4 From Table 4, it is apparent that the greedy search algorithm can derive interesting groupings of word senses that correspond to human mtmtwe judgment of sense graz}.ulanty It Is clear that some of the disagreement between the two groups of human annotators can be attributed solely to the overly refined senses of WoRDNET As an example, there is a total Ii loop: let Ct,, C M denote the current M sense classes ~* +--oo for all z,3 such that 1 <, < 3 < M let C[,,C~w_ 1 denote the resulting M 1 sense classes by mergmg C, and C 3 compute ~(C[,, C~/_t) ff ~(C,, C~4_x) > ~* then ~'' +~(C~,,C~_t), z* +~, ~* +end for merge the sense class C,.'
'We use the IBM Model 1 <CIT> and the Hidden Markov Model -LRB- HMM , <OTH> -RRB- to estimate the alignment model '
'We provide results using a range of automatic evaluation metrics : BLEU <CIT> , Precision and Recall <OTH> , and Wordand Sentence Error Rates '
'ROUGE-L and ROUGE-1 are supposed to be appropriate for the headline gener853 ation task <CIT> '
'The common types of features include contextual <CIT> , co-occurrence <OTH> , and syntactic dependency <CIT> '
'32 Results and Discussion The BLEU scores <CIT> for 10 direct translations and 4 sets of heuristic selections 4Admittedly , in typical instances of such chains , English would appear earlier '
'Furthermore , WASP1 + + employs minimum error rate training <CIT> to directly optimize the evaluation metrics '
'6 The Experiments To investigate the e ects of lookahead on our family of deterministic parsers , we ran empirical experiments on the standard the Penn Treebank <CIT> datasets '
'We have already shown in Section 3 how to solve -LRB- a -RRB- ; here we avoid -LRB- b -RRB- by maximizing conditional likelihood , marginalizing out the hidden variable , denotedz : max vector summationdisplay x , y p -LRB- x , y -RRB- log summationdisplay z pvector -LRB- y , z x -RRB- -LRB- 17 -RRB- This sort of conditional training with hidden variables was carried out by <CIT> , for example , in reranking ; it is related to the information bottleneck method <OTH> and contrastive estimation <OTH> '
'This paper continues a line of research on online discriminative training <CIT> , extending that of Watanabe et al '
'For comparison , we use the MT training program , GIZA + + <OTH> , the phrase-base decoder , Pharaoh <CIT> , and the wordbased decoder , Rewrite <OTH> '
'5 Comparison with related work Preliminary work on SF extraction from coq ~ ora was done by <OTH> and <OTH> '
'Recentworkconsidersadamagedtagdictionary by assuming that tags are known only for words that occur more than once or twice <CIT> '
'An existing method to combine multiple parsing algorithms is the ensemble approach <OTH> , which was reported to be useful in improving dependency parsing <CIT> '
'33 BLEU Score The BLEU score <CIT> measures the agreement between a hypothesiseI1 generated by the MT system and a reference translation eI1 '
'As far as the log-linear combination of float features is concerned , similar training procedures have been proposed in <CIT> '
'Proceedings of the 40th Annual Meeting of the Association for <CIT> , a number of other algorithms have been developed '
'However , most of them do not build a NEs resource but exploit external gazetteers <OTH> , <CIT> '
'Then P -LRB- eI1jfj1 -RRB- = summationtextaI 1 P -LRB- eI1 , aI1jfj1 -RRB- <OTH> '
'to estimale a model -LRB- clustering words -RRB- , and measured the I -LRB- L distancd ~ between ` l ` he K -RRB- , distance -LRB- relative Clt , l : Opy -RRB- , which is widely used in information theory and sta , tist , ics , is a , nleasur ,2 of ` dista , n -LRB- : c '' l -RRB- ~ -LRB- , wcen two distributions 52 Experiment 2 : Qualitative Evaluation We extracted roughly 180,000 case fl : anles from the bracketed WSJ -LRB- Wall Street Journal -RRB- corpus of the Penn Tree Bank <CIT> as co-occurrence data '
'http://ducnistgov </title> <date> 2004 </date> <journal> Journal of the Association for Computing Machinery </journal> <volume> 16 </volume> <pages> 264 -- 285 </pages> <contexts> <context> <OTH> , Message Understanding Conferences -LRB- MUC -RRB- <OTH> , TIPSTER SUMMAC Text Summarization Evaluation <OTH> , Document Understanding Conference -LRB- DUC -RRB- <OTH> , and Text Summarization Challenge -LRB- TSC -RRB- <OTH> , have attested the importance of this topic '
'The row labeled Precision shows the precision of the extracted information -LRB- ie , how many entries are correct , according to a human annotator -RRB- estimated by random sampling and manual evaluation of 1 % of the data for each table , similar to <CIT> '
'<OTH> , <CIT> -RRB- '
'We use the union , re ned and intersection heuristics de ned in <CIT> which are used in conjunction with IBM Model 4 as the baseline in virtually all recent work on word alignment '
'This algorithm is proved to converge -LRB- ie , there are no more updates -RRB- in the separable case <CIT> 1 Thatis , ifthereexistweightvectorU -LRB- with U = 1 -RRB- , -LRB- -RRB- 0 -RRB- , and R -LRB- -RRB- 0 -RRB- that satisfy : i , y Y xi -LRB- xi , yi -RRB- U -LRB- xi , y -RRB- U , i , y Y xi -LRB- xi , yi -RRB- -LRB- xi , y -RRB- R , the number of updates is at most R2\/2 '
'<CIT> , Mihalcea and Moldovan -LRB- 2000 -RRB- , and Mihalcea -LRB- 2002 -RRB- have made further research to obtain large corpus of higher quality from an initial seed corpus '
'There are many POS taggers developed using different techniques for many major languages such as transformation-based error-driven learning <OTH> , decision trees <OTH> , Markov model <CIT> , maximum entropy methods <OTH> etc for English '
'The list is obtained by first extracting the phrases with - TMP function tags from the PennTree bank , and taking the words in these phrases <CIT> '
'These include scripts for creating alignments from a parallel corpus , creating phrase tables and language models , binarizing phrase tables , scripts for weight optimization using MERT <CIT> , and testing scripts '
'The standard solution is to approximate the maximum probability translation using a single derivation <CIT> '
'It is known that PMI gives undue importance to low frequency events <CIT> , therefore the evaluation considers only pairs of genes that occur at least 5 times in the whole corpus '
'To model p -LRB- t , a s -RRB- , we use a standard loglinear approach : p -LRB- t , a s -RRB- exp bracketleftBiggsummationdisplay i ifi -LRB- s , t , a -RRB- bracketrightBigg where each fi -LRB- s , t , a -RRB- is a feature function , and weights i are set using Ochs algorithm <CIT> to maximize the systems BLEU score <OTH> on a development corpus '
'Context extraction begins with a Maximum Entropy POS tagger and chunker <CIT> '
'For each cell in the contingency table , the expected counts are : mi j = ni + n + jn + + The measures are calculated as <OTH> : 2 = i ; j -LRB- ni j mi j -RRB- 2 mi j LL = 2 i ; j log2 n 2i j mi j Log-likelihood ratios <CIT> are more appropriate for sparse data than chi-square '
'1 Introduction During the last few years , SMT systems have evolved from the original word-based approach <CIT> to phrase-based translation systems <OTH> '
'Minimum error rate training -LRB- MERT -RRB- with respect to BLEU score was used to tune the decoders parameters , and performed using the technique proposed in <CIT> '
'Head word -LRB- and its part-of-speech tag -RRB- of the constituent After POS tagging , a syntactic parser <CIT> was then used to obtain the parse tree for the sentence '
'Otherwise they are generated along with the words using the same approach as in <CIT> '
'We use the maximum entropy tagging method described in <OTH> for the experiments , which is a variant of <CIT> modified to use HMM state features '
'The PropBank superimposes an annotation of semantic predicate-argument structures on top of the Penn Treebank -LRB- PTB -RRB- <CIT> '
'3 Variational Bayes for ITG Goldwater and Griffiths <OTH> and <CIT> <OTH> show that modifying an HMM to include a sparse prior over its parameters and using Bayesian estimation leads to improved accuracy for unsupervised part-of-speech tagging '
'As a measure of association , we use the loglikelihood-ratio statistic recommended by <CIT> , which is the same statistic used by Melamed to initialize his models '
'Since that time , however , increasingly large amounts of language model training data have become available ranging from approximately one billion words -LRB- the Gigaword corpora from the Linguistic Data Consortium -RRB- to trillions of words <CIT> '
'This approach , however , does not have a theoretical guarantee on optimality unless certain nontrivial conditions are satisfied <CIT> '
'Most prior work on the speci c problem of categorizing expressly opinionated text has focused on the binary distinction of positive vs negative <CIT> '
'This is the scenario considered by <CIT> for POS tagging : how to construct an accurate tagger given a set of tags and a few example words for each of those tags '
'The pchemtb-closed shared task <CIT> is used to illustrate our models '
'1 Introduction The field of sentiment classification has received considerable attention from researchers in recent years <CIT> '
'This paper presents an empirical study measuring the effectiveness of our evaluation functions at selecting training sentences from the Wall Street Journal -LRB- WSJ -RRB- corpus <CIT> for inducing grammars '
'We then rank-order the P X|Y MI XY M Z Pr Z|Y MI ZY G092log [P X P Y P X P Y ] f Y [P XY P XY ] f XY [P XY P XY ] f XY M iG13X,X} jG13Y,Y} (f ij G09 ij ) 2 ij f XY G09 XY XY (1G09( XY /N)) f XY G09 XY f XY (1G09(f XY /N)) Table 1: Probabilistic Approaches METHOD FORMULA Frequency (Guiliano, 1964) f XY Pointwise Mutual Information (MI) (Fano, 1961; Church and Hanks, 1990) log (P / PP) 2XY XY Selectional Association (Resnik, 1996) Symmetric Conditional Probability (Ferreira and Pereira, 1999) P / PP XY X Y 2 Dice Formula (Dice, 1945) 2 f / (f +f ) XY X Y Log-likelihood (Dunning, 1993; (Daille, 1996).'
'6 Related work Evidence from the surrounding context has been used previously to determine if the current sentence should be subjective\/objective <CIT> and adjacency pair information has been used to predict congressional votes <OTH> '
'The topic signatures are automatically generated for each specific term by computing the likelihood ratio -LRB- - score -RRB- between two hypotheses <CIT> '
'Based on the word alignment results , if the aligned target words of any two adjacent foreign linguistic phrases can also be formed into two valid adjacent phrase according to constraints proposed in the phrase extraction algorithm by <CIT> , they will be extracted as a reordering training sample '
'These wordbased models are used to find the latent wordalignments between bilingual sentence pairs , from which a weighted string transducer can be induced -LRB- either finite state <CIT> or synchronous context free grammar <OTH> -RRB- '
'These heuristics are extensions of those developed for phrase-based models <CIT> , and involve symmetrising two directional word alignments followed by a projection step which uses the alignments to find a mapping between source words and nodes in the target parse trees <OTH> '
'Iterating between these two 1 Note that these problems are associated with corpus-based approaches in general , and have been identified by a number of researchers <CIT> '
'The parameters of the MT system were optimized on MTEval02 data using minimum error rate training <CIT> '
'Our starting point is the work done by Zettlemoyer and Collins on parsing using relaxed CCG grammars <CIT> -LRB- ZC07 -RRB- '
'Note that unlike the constructions in <CIT> and <OTH> no errors are possible for ngrams stored in the model '
'That is a significant shortcoming , because in many domains , hard or soft global constraints on the label sequence are motivated by common sense : For named entity recognition , a phrase that appears multiple times should tend to get the same label each time <CIT> '
'<CIT> proposed to eliminate objective sentences before the sentiment classification of documents '
'Model weights were also trained following <CIT> '
'By contrast , in the training method proposed by <CIT> , the discriminative function f -LRB- x ; w -RRB- is estimated to maximize the F 1 - score of training dataset D This training method employs an approximate form of the F 1 - score obtained by using a logistic function '
'Most of the reported work on paraphrase generation from arbitrary input sentences uses machine learning techniques trained on sentences that are known or can be inferred to be paraphrases of each other <CIT> '
'The resulting training procedure is analogous to the one presented in <CIT> and <OTH> '
'He uses a specic reliability statistic , , for his measurements , but <CIT> implicitly assumes kappa-like metrics are similar enough in practice for the rule of thumb to apply to them as wellA detailed discussion on the differences and similarities of these , and other , measures is provided by Krippendorff <OTH> ; in this article we will use Cohens <OTH> to investigate the value of the 08 reliability cut-off for computational linguistics '
'We used \* TH \* = 3 following '' a very rough rule of thumb '' used for word-based mutual information in <CIT> '
'The query tions , the syntax , semantics , and abstract knowledge representation have type declarations <CIT> which help to detect malformed representations '
'Also relevant is previous work that applied machine learning approaches to MT evaluation , both with human references <CIT> and without <OTH> '
'Finally , recent efforts have also looked at transfer learning mechanisms for sentiment analysis , eg , see <CIT> '
'31 Data The English data set consists of the Wall Street Journal sections 2-24 of the Penn treebank <CIT> , converted to dependency format '
'<CIT> lter training instances based on Part-of-Speech -LRB- POS -RRB- tags , and Soricut and Marcu <OTH> use syntactic features to identify sentence-internal RST structure '
'Finally , knowledge of polarity can be combined with corpus-based collocation extraction methods <CIT> to automatically produce entries for the lexical functions used in MeaningText Theory <OTH> for text generation '
'Our evaluation metrics are BLEU <CIT> and NIST , which are to perform caseinsensitive matching of n-grams up to n = 4 '
'Part-of-speech features Based on the lexical categories produced by GATE <OTH> , each token xi is classified into one of a set of coarse part-of-speech tags : noun , verb , adverb , wh-word , determiner , punctuation , etc We do the same for neighboring words in a -LRB- 2 , +2 -RRB- window in order to assist noun phrase segmentation '
'Unsupervised Learning : Results To test the effectiveness of the above unsupervised learning algorithm , we ran a number of experiments using two different corpora and part of speech tag sets : the Penn Treebank Wall Street Journal Corpus <CIT> and the original Brown Corpus <OTH> '
'33 Language Model We estimate P -LRB- s -RRB- using n-gram LMs trained on data from the Web , using Stupid Backoff <CIT> '
'We use these tuples to calculate a balanced f-score against the gold alignment tuples4 Method Dict size f-score Gold 28 1000 Monotone 39 689 IBM-1 <CIT> 30 803 IBM-4 <CIT> 29 869 IP 28 959 The last line shows an average f-score over the 8 tied IP solutions '
'We split the treebank into training -LRB- sections 0-18 -RRB- , development <OTH> and test -LRB- sections 22-24 -RRB- as in <CIT> '
'The a0 coefficient is computed as follows: a0 a47 a1a32a2 a9 a1 a30 a68 a9 a1a32a30 Carletta (1996) reports that content analysis researchers generally think of a0a34a33 a49a36a35a37 as good reliability, with a49a36a35a38a40a39a37a41 a0 a41a25a49a36a35a37 allowing tentative conclusions to be drawn. All that remains is to define the chance agreement probability a1 a30 . Let a1a32a41 a1 a30 a7 and a1a32a42 a1 a30 a7 be the fraction of utterances that begin or end one or more segments in segmentation a30 respectively.'
'<CIT> proposed such a method for word sense disambiguation , which we refer to as monolingual bootstrapping '
'The COlllillOil poini ; s regarding collocations appear to be , as <CIT> suggestsl : they are m ` bil ; rary -LRB- it is nol ; clear why to ` Bill through '' means to ` fail '' -RRB- , th -LRB- '' y are domain-dependent -LRB- ` interest rate '' , ` stock market '' -RRB- , t ; hey are recurrenl ; and cohesive lo ~ xical clusters : the presence of one of the '
'<OTH> and <CIT> <OTH> classified sentiment polarity of reviews at the document level '
'Penn Treebank corpus <CIT> sections 0-20 were used for training , sections 2124 for testing '
'<CIT> introduced five statistical translation models -LRB- IBM Models 1 5 -RRB- '
'In particular , Abney defines a function K that is an upper bound on the negative log-likelihood , and shows his bootstrapping algorithms locally minimize K We now present a generalization of Abneys K function and relate it to another semi-supervised learning technique , entropy regularization <CIT> '
'Second , the word alignment is refined by a grow-diag-final heuristic <CIT> '
'The word alignment is computed using GIZA + +2 for the selected 73,597 sentence pairs in the FBIS corpus in both directions and then combined using union and heuristic diagonal growing <CIT> '
'There has of course been a large amount of work on the more general problem of word-sense disambiguation , eg , <CIT> <OTH> '
'<CIT> estimates a POS tagging error rate of 3 % in the Treebank '
'Word correspondence was further developed in IBM Model-1 <CIT> for statistical machine translation '
'The features that define the constraints on the model are obtained by instantiation of feature templates as in <CIT> '
'Step Description mean stddev % 15 Sample 15s 007s 07 % 16 Extraction 382s 013s 186 % 17 Build tree 1276s 2760s 623 % 18 Percolation 314s 491s 153 % 1911 Leaf updates 62s 175s 30 % 1511 Total 2049s 326s 1000 % 2004 -RRB- ,10 the only one that we were able to train and test under exactly the same experimental conditions -LRB- including the use of POS tags from <CIT> -RRB- '
'Considerations of sentence fluency are also key in sentence simplification <OTH> , sentence compression <OTH> , text re-generation for summarization <CIT> and headline generation <OTH> '
'Movie and product reviews have been the main focus of many of the recent studies in this area <CIT> '
'We compute log-likelihood significance between features and target nouns -LRB- as in <CIT> -RRB- and keep only the most significant 200 features per target word '
'This algorithm and its many variants are widely used in the computational linguistics community <CIT> '
'Consequently , we abstract away from specifying a distribution by allowing the user to assign labels to features -LRB- cf <CIT> , Druck et al '
'We adopted the stop condition suggested in <CIT> et al 1996 the maximization of the likelihood on a cross-validation set of samples which is unseen at the parameter esti ~ _ tion '
'Automatic NE transliteration is an important component in many cross-language applications , such as Cross-Lingual Information Retrieval -LRB- CLIR -RRB- and Machine Translation -LRB- MT -RRB- <CIT> '
'These are the same distributions that are needed by previous POS-based language models -LRB- Equation 5 -RRB- and POS taggers <OTH> '
'73 ment and phrase-extraction heuristics described in <OTH> , minimum-error-rate training <CIT> , a trigram language model with KneserNey smoothing trained with SRILM <OTH> on the English side of the training data , and Moses <OTH> to decode '
'Unlike probabilistic parsing , proposed by <OTH> , \* also a staff member of Matsushita Electric Industrial Co , Ltd , Shinagawa , Tokyo , JAPAN '
'In order to minimize the number of decision errors at the sentence level , we have to choose the sequence of target words eI1 according to the equation <CIT> : eI1 = argmax eI1 n Pr -LRB- eI1jfJ1 -RRB- o = argmax eI1 n Pr -LRB- eI1 -RRB- Pr -LRB- fJ1 jeI1 -RRB- o : Here , the posterior probability Pr -LRB- eI1jfJ1 -RRB- is decomposed into the language model probability Pr -LRB- eJ1 -RRB- and the string translation probability Pr -LRB- fJ1 jeI1 -RRB- '
'Recently , <CIT> presented an unsupervised approach to coreference resolution , which mined the co-referring NP pairs with similar predicatearguments from a large corpus using a bootstrapping method '
'And 20NG is a collection of approximately 20,000 20-category documents 1 In sentiment text classification , we also use two data sets : one is the widely used Cornell movie-review dataset2 <OTH> and one dataset from product reviews of domain DVD3 <CIT> '
'Adaptations to the algorithms in the presence of ngram LMs are discussed in <CIT> '
'By core phrases , we mean the kind of nonrecursive simplifications of the NP and VP that in the literature go by names such as noun\/verb groups <OTH> or chunks , and base NPs <CIT> '
'P -LRB- d -RRB- P L -LRB- d -RRB- -LRB- 4 -RRB- Statistical approaches to language modeling have been used in much NLP research , such as machine translation <CIT> and speech recognition <OTH> '
'The agreement on identifying the boundaries of units , using the kappa statistic discussed in <CIT> , was = 9 -LRB- for two annotators and 500 units -RRB- ; the agreement on features -LRB- two annotators and at least 200 units -RRB- was as follows : utype : = 76 ; verbed : = 9 ; nite : = 81 '
'(Och et al., 1999; Koehn et al., 2003; Liang et al., 2006).'
'The more recent set of techniques includes mult iplicative weightupdate algorithms <OTH> , latent semantic analysis <OTH> , transformation-based learning <OTH> , differential grammars <OTH> , decision lists <CIT> , and a variety of Bayesian classifiers <OTH> '
'Please note that our approach is very different from other approaches to context dependent rule selection such as <OTH> and <CIT> '
'<CIT> proposed a word similarity measure based on the distributio nal pattern of words which allows to construct a thesaurus using a parsed corpus '
'The data was seglnented into baseNP parts and non-lmseNP t -RRB- arts ill a similar fitshion as the data used 1 -RRB- y <CIT> '
'We use maximum entropy model (Berger et al. , 1996) for both the mention-pair model (9) and the entity-mention model (8): a83a84a1a86a85a88a87 a43 a44 a71 a43 a16 a5a13a7 a55a35a34a23a36 a6a35a37 a6a39a38a40a6a42a41 a31a44a43a3a45a31 a6 a45a46a48a47a24a49 a50 a1 a43 a44 a71 a43 a16 a5 a71 (10) a83a84a1a4a85 a87 a55 a81 a71 a43 a16 a5a13a7 a55a35a34 a36 a6 a37 a6a39a38a40a6a42a41 a11a7a32 a45a31 a6 a45a46a48a47 a49 a50 a1 a55a39a81 a71 a43 a16 a5 a71 (11) wherea57 a16 a1a51a8 a71a52a8 a71a90a85a73a5 is a feature and a53 a16 is its weight; a50 a1a33a8 a71a54a8a5 is a normalizing factor to ensure that (10) or (11) is a probability.'
'By core phrases , we mean the kind of nonrecursive simplifications of the NP and VP that in the literature go by names such as noun\/verb groups <OTH> or chunks , and base NPs <CIT> '
'For our contrast submission , we rescore the first-pass translation lattices with a large zero-cutoff stupid-backoff <CIT> language model estimated over approximately five billion words of newswire text '
'However , these unsupervised methodologies show a major drawback by extracting quasi-exact or even exact match pairs of sentences as they rely on classical string similarity measures such as the Edit Distance in the case of <OTH> and Word N-gram Overlap for <CIT> '
'We have adopted the evaluation method of <CIT> : compare the generated hypernyms with hypernyms present in a lexical resource , in our case the Dutch part of EuroWordNet <OTH> '
'F-me. 1 CBC-NER system M 71.67 23.47 35.36CBC-NER system A 70.66 32.86 44.86 2 XIP NER 77.77 56.55 65.48 XIP + CBC M 78.41 60.26 68.15 XIP + CBC A 76.31 60.48 67.48 3 Stanford NER 67.94 68.01 67.97 Stanford + CBC M 69.40 71.07 70.23 Stanford + CBC A 70.09 72.93 71.48 4 GATE NER 63.30 56.88 59.92 GATE + CBC M 66.43 61.79 64.03 GATE + CBC A 66.51 63.10 64.76 5 Stanford + XIP 72.85 75.87 74.33 Stanford + XIP + CBC M 72.94 77.70 75.24 Stanford + XIP + CBC A 73.55 78.93 76.15 6 GATE + XIP 69.38 66.04 67.67 GATE + XIP + CBC M 69.62 67.79 68.69 GATE + XIP + CBC A 69.87 69.10 69.48 7 GATE + Stanford 63.12 69.32 66.07 GATE + Stanford + CBC M 65.09 72.05 68.39 GATE + Stanford + CBC A 65.66 73.25 69.25 Table 1: Results given by different hybrid NER systems and coupled with the CBC-NER system corpora (CoNLL, MUC6, MUC7 and ACE): ner-eng-ie.crf-3-all2008-distsim.ser.gz (Finkel et al., 2005) (line 3 in Table 1),  GATE NER or in short GATE (Cunningham et al., 2002) (line 4 in Table 1),  and several hybrid systems which are given by the combination of pairs taken among the set of the three last-mentioned NER systems (lines 5 to 7 in Table 1).'
'<OTH> and <CIT> and <CIT> <OTH> showed that the MatrixTree Theorem can be used to train edge-factored log-linearmodelsofdependencyparsing '
'These rules can be learned from a parallel corpus using English parsetrees , Chinese strings , and word alignment <CIT> '
'Amount of works have been done on sentimental classification in different levels <CIT> '
'The suffixes C \* and V \* denote the models using incomplete skip-chain edges and vertical sequential edges proposed in <CIT> , as shown in Figures 2 -LRB- a -RRB- and 2 -LRB- c -RRB- '
'The model scaling factors 1 , , 5 and the word and phrase penalties are optimized with respect to some evaluation criterion <CIT> such as BLEU score '
'This makes it suitable for discriminative SMT training , which is still a challenge for large parameter sets <CIT> '
'There have been many statistical measures which estimate co-occurrence and the degree of association in previous researches , such as mutual information <OTH> , t-score <OTH> , dice matrix <CIT> '
'<CIT> 4 '
'45 Consistency of Annotations In order to assess the consistency of annotation , we follow <CIT> in using Cohen ''s ~ , a chancecorrected measure of inter-rater agreement '
'Currently , machine learning methods <CIT> and combinations of classifiers <OTH> have been popular '
'<CIT> used transformation based learning using a large annotated corpus for English '
'In addition , corpus-based stochastic modelling of lexical patterns <OTH> may provide information about word sense frequency of the kind advocated since <OTH> '
'1 Introduction In the community of sentiment analysis <CIT> , transferring a sentiment classifier from one source domain to another target domain is still far from a trivial work , because sentiment expression often behaves with strong domain-specific nature '
'1 Introduction Word compositions have long been a concern in lexicography <OTH> , and now as a specific kind of lexical knowledge , it has been shown that they have an important role in many areas in natural language processing , eg , parsing , generation , lexicon building , word sense disambiguation , and information retrieving , et <CIT> '
'6 Discussion Lack of interannotator agreement presents a significant problem in annotation efforts <CIT> '
'<CIT> has used a few seeds and untagged sentences in a bootstrapping algorithm based on decision lists '
'<CIT> 1990 -RRB- '
'The fact that different authors use different versions of the same gold standard to evaluate similar experiments -LRB- eg Goldwater & Griffiths <OTH> versus <CIT> <OTH> -RRB- supports this claim '
'As is common <CIT> , the treebank is first transformed in various ways , in order to give an accurate PCFG '
'The second type has clear interpretation as a probability model , but no criteria to determine the number of clusters <CIT> '
'It is also related to -LRB- log - -RRB- linear models described in Berger , Della Pietra , and Della Pietra <OTH> , Xue <OTH> ; <CIT> <OTH> , and Peng , Feng , and McCallum -LRB- 2004 -RRB- '
'<CIT> describe a perceptron style algorithm for training millions of features '
'For each co-occurring pair of word types u and v , these likelihoods are initially set proportional to their co-occurrence frequency n -LRB- u , v -RRB- and inversely proportional to their marginal frequencies n -LRB- u -RRB- and n -LRB- v -RRB- z , following <CIT> 2 '
'The optimal weights for the different columns can then be assigned with the help of minimum error rate training <CIT> '
'They have been employed in word sense disambiguation <OTH> , automatic construction of bilingual dictionaries <OTH> , and inducing statistical machine translation models <CIT> '
'53 Experimental setup We used the Stanford Parser <OTH> for both languages , Penn English Treebank <CIT> and Penn Arabic Treebank set <OTH> '
'<OTH> and Chiang <OTH> , in terms of what alignments they induce , has been discussed in Wu <OTH> and <CIT> et al '
'is combined with -LRB- -RRB- E jiT ,1 + to be aligned with -LRB- -RRB- F nmT , , then -LRB- -RRB- -LRB- -RRB- -LRB- -RRB- -LRB- -RRB- ATTCNTATTr E K E i FEF jinmjinm , Pr , P ,1 -RRB- , -LRB- , -RRB- , -LRB- -RRB- ,1 -LRB- + = where K is the degree of EiN Finally , the node translation probability is modeled as -LRB- -RRB- -LRB- -RRB- -LRB- -RRB- tNtNlNlNNN EiFlEiFlEjFl PrPrPr And the text translation probability -LRB- -RRB- EF ttPr is model using IBM model I <CIT> '
'Like <CIT> , the decoder is the same for both the perceptron and the log-linear parsing models ; the only change is the method for setting the weights '
'The reordered sentence is then re-tokenized to be consistent with the baseline system , which uses a different tokenization scheme that is more friendly to the MT system3 We use BLEU scores as the performance measure in our evaluation <CIT> '
'Extensions to Hiero Several authors describe extensions to Hiero , to incorporate additional syntactic information <CIT> , or to combine it with discriminative latent models <OTH> '
'The highest BLEU score <CIT> was chosen as the optimization criterion '
'We found that the deletion of lead parts did not occur very often in our summary , unlike the case of <CIT> '
'The software also required GIZA + + word alignment too <CIT> '
'This result supports the intuition in <CIT> that correlation at segment level is necessary to ensure the reliability of metrics in different situations '
'As the strength of relevance between a target compound noun t and its co-occurring word r , the feature value of r , w -LRB- t ; r -RRB- is deflned by the log likelihood ratio <CIT> 1 as follows '
'Hindle uses the observed frequencies within a specific syntactic pattern -LRB- subject\/verb , and verb\/object -RRB- to derive a cooccu , -RRB- rence score which is an estimate of mutual information <CIT> '
'The kappa statistic <CIT> for identifying question segments is 068 , and for linking question and answer segments given a question segment is 081 '
'As a unified approach , we augment the SDIG by adding all the possible word pairs -LRB- , -RRB- ji fe as a parallel ET pair and using the IBM Model 1 <CIT> word to word translation probability as the ET translation probability '
'Specifically , the following information can be either automatically identified or manually annotated : Syntactic structures automatically identified from a parser <CIT> ; Semantic roles of entities in the question <OTH> ; Discourse roles either manually annotated or identified by rules that map directly from semantic roles to discourse roles '
'In our experiments we use the same definition of structural locality as was proposed for the ISBN dependency parser in <CIT> '
'The labeled corpus is the Penn Wall Street Journal treebank <CIT> '
'1 Introduction Sentiment analysis of text documents has received considerable attention recently <CIT> '
'The phrases in the translations were located using standard phrase extraction techniques <CIT> '
'The production rules in ITGs are of the following form <CIT> , with a notation similar to what is typically used for SDTSs and SCFGs in the right column : A -LRB- BC -RRB- A B1C2 , B1C2 A BC A B1C2 , C2B1 A e f A e , f A e A e , A f A , f It is important to note that RHSs of production rules have at most one source-side and one targetside terminal symbol '
'<OTH> applied the distributional similarity proposed by <CIT> to coordination disambiguation '
'We used the Wall Street Journal -LRB- WSJ -RRB- part of the Penn Treebank <CIT> , where extraction is represented by co-indexing an empty terminal element -LRB- henceforth EE -RRB- to its antecedent '
'Li and Roth demonstrated that their shallow parser , trained to label shallow constituents along the lines of the well-known CoNLL2000 task <OTH> , outperformed the Collins parser in correctly identifying these constituents in the Penn Wall Street Journal -LRB- WSJ -RRB- Treebank <CIT> '
'For every class the weights of the active features are combined and the best scoring class is chosen <CIT> '
'We then built separate directed word alignments for EnglishX andXEnglish -LRB- X -LCB- Indonesian , Spanish -RCB- -RRB- using IBM model 4 <OTH> , combined them using the intersect + grow heuristic <CIT> , and extracted phrase-level translation pairs of maximum length seven using the alignment template approach <CIT> '
'In this paper , we adopt Stanford Maximum Entropy <CIT> implementation in our experiments '
'In the nal step , we score our translations with 4-gram BLEU <CIT> '
'This can be done by smoothing the observed frequencies 7 <OTH> or by class-based methods <CIT> '
'A number of knowledge-rich <OTH> and knowledge-poor <CIT> methods have been proposed for recognizing when words are similar '
'Our approach to statistical machine translation differs from the model proposed in <CIT> in that : We compute the joint model P -LRB- Ws , WT -RRB- from the bilanguage corpus to account for the direct mapping of the source sentence Ws into the target sentence I ? VT that is ordered according to the source language word order '
'For a given choice of q and f , the IIS algorithm <CIT> can be used to find maximum likelihood values for the parameters ~ '
'1 <CIT> shows that Wikipedia can indeed be used as a sense inventory for sense disambiguation '
'Using dictionaries as network of lexical items or senses has been quite popular for word sense disambiguation <OTH> before losing ground to statistical approaches , even though <CIT> tried a revival of such methods '
'Method Number of frames Number of verbs Linguistic resources F-Score -LRB- evaluation based on a gold standard -RRB- Coverage on a corpus C Manning <OTH> 19 200 POS tagger + simple finite state parser 58 T Briscoe & J Carroll <OTH> 161 14 Full parser 55 A Sarkar & D Zeman <OTH> 137 914 Annotated treebank 88 D Kawahara et al '
'<OTH> and <CIT> in merely using binary unigram features , corresponding to the 17,744 unstemmed word or punctuation types with count 4 in the full 2000-document corpus '
'The recurrence property had been utilized to extract keywords or key-phrases from text <CIT> '
'1 Introduction Hyponymy relations can play a crucial role in various NLP systems , and there have been many attempts to develop automatic methods to acquire hyponymy relations from text corpora <CIT> '
'As a result , we can use collocation measures like point-wise mutual information <OTH> or the log-likelihood ratio <CIT> to predict the strong association for a given cue '
'Thus , a lot of alignment techniques have been suggested at ; the sentence <OTH> , phrase <OTH> , nomt t -RRB- hrase <OTH> , word <CIT> , collocation <OTH> and terminology level '
'<CIT> s approach for English resolves three LDD types in parser output trees without traces and coindexation -LRB- Figure 2 -LRB- b -RRB- -RRB- , ie topicalisation -LRB- TOPIC -RRB- , wh-movement in relative clauses -LRB- TOPIC REL -RRB- and interrogatives -LRB- FOCUS -RRB- '
'REALM uses an HMM trained on a large corpus to help determine whether the arguments of a candidate relation are of the appropriate type <CIT> '
'c2007 Association for Computational Linguistics Structural Correspondence Learning for Dependency Parsing Nobuyuki Shimizu Information Technology Center University of Tokyo Tokyo , Japan shimizu @ rdlitcu-tokyoacjp Hiroshi Nakagawa Information Technology Center University of Tokyo Tokyo , Japan nakagawa @ dlitcu-tokyoacjp Abstract Following <CIT> , we present an application of structural correspondence learning to non-projective dependency parsing <OTH> '
'In shift-reduce parsing , further mistakes are often caused by previous ones , so only the first mistake in each sentence -LRB- if there is one -RRB- is easily identifiable ; 7 this is also the argument for early update in applying perceptron learning to these incremental parsing algorithms <CIT> -LRB- see also Section 2 -RRB- '
'We have also used ROUGE evaluation approach <CIT> which is based on n-gram co-occurrences between machine summaries and ideal human summaries '
'Automatically Learning Entailment Rules from the Web Many algorithms for automatically learning paraphrases and entailment rules have been explored in recent years <CIT> '
'In particular , we use the name\/instance lists described by <CIT> and available on Fleischmans web page to generate features between names and nominals -LRB- this list contains a110a111a85 pairs mined from a112a73a96 GBs of news data -RRB- '
'3 Tagging 31 Corpus To facilitate comparison with previous results , we used the UPenn Treebank corpus <CIT> '
'p0 -LRB- t w -RRB- is calculated by ME models as follows <CIT> : p0 -LRB- t w -RRB- = 1Y -LRB- w -RRB- exp braceleftBigg Hsummationdisplay h = 1 hgh -LRB- w , t -RRB- bracerightBigg , -LRB- 20 -RRB- 709 Language Features English Prefixes of 0 up to four characters , suffixes of 0 up to four characters , 0 contains Arabic numerals , 0 contains uppercase characters , 0 contains hyphens '
'a11a29a9 thea13 thea15 a1a4a3a6a5 a11a29a9 thea13 thea15 a11a29a9 thea15 a11a29a9 thea15a1a0 a2 since a11a2a9 thea13 thea15a4a3 a11a29a9 thea15 a11a29a9 thea15 Also note that in the case of phraseness of a bigram , the equation looks similar to pointwise mutual information <CIT> , but they are different '
'<OTH> proposed a summarization system based on the draft and revision <CIT> proposed a system based on extraction and cut-and-paste generation Our abstractors performed the same cut-and-paste operations that Jing and McKeown noted in their work , and we think that our two-step model will be a reasonable starting point for our subsequent research '
'213 Correlation analysis As a correlation measure between terms , we use mutual information <CIT> '
'The f-structure annotation algorithm used for inducing LFG resources from the Penn-II treebank for English <CIT> uses configurational , categorial , function tag and trace information '
'Building on a recent proposal in this direction by <CIT> , we propose a generic method of this sort , and we test it on a set of unrelated tasks , reporting good performance across the board with very little task-specific tweaking '
'Instances of this work include information extraction , ontology induction and resource acquisition <CIT> '
'We further assume that the degree of difficulty of a phrase is directly correlated with the quality of the translation produced by the MT system , which can be approximated using an automatic evaluation metric , such as BLEU <CIT> '
'We adopted IOB -LRB- IOB2 -RRB- labeling <CIT> , where the rst word of an entity of class C is labeled B-C , the words in the entity are labeled I-C , and other words are labeled O '
'If human-aligned data is available , the EMD algorithm provides higher baseline alignments than GIZA + + that have led to better MT performance <CIT> '
'2 Related Work Sentiment Classi cation Traditionally , categorization of opinion texts has been cast as a binary classication task <CIT> '
'<CIT> describe a method of disambiguation , where disambiguation questions are dynamically constructed on the basis of an analysis of the differences among the closest routing destination vectors '
'The unit of utterance corresponds to the unit of segment in the original BLEU and NIST studies <CIT> '
'Another possible comparison could be with a version of <CIT> sentiment classification method applied to Chinese '
'Then the word alignment is refined by performing growdiag-final method <CIT> '
'3 24 Intonation Annotations For our intonation annotation , we have annotated the intonational phrase boundaries , using the ToBI -LRB- Tones and Break Indices -RRB- definition <OTH> '
'For extrinsic evaluation of machine translation , we use the BLEU metric <CIT> '
'Previous work in statistical synchronous grammars has been limited to forms of synchronous context-free grammar <CIT> '
'In this respect it resembles Wus 264 bilingual bracketer <CIT> , but ours uses a different extraction method that allows more than one lexical item in a rule , in keeping with the phrasebased philosophy '
'For this reason there is currently a great deal of interest in methods which incorporate syntactic information within statistical machine translation systems -LRB- eg , see <OTH> -RRB- '
'Using our WSD model to constrain the translation candidates given to the decoder hurts translation quality , as measured by the automated BLEU metric <CIT> '
'In the first step , the scores are initialized according to the G 2 statistic <CIT> '
'-LRB- levelopment of cor1 -RRB- ora with morl -RRB- ho-synta -LRB- : ti -LRB- : and syntacti -LRB- : mmotation <OTH> , <OTH> '
'The translation models and lexical scores were estimated on the training corpus whichwasautomaticallyalignedusingGiza + + <CIT> in both directions between source and target and symmetrised using the growing heuristic <OTH> '
'These 30 questions are determined by growing a classification tree on the word vocabulary as described in <CIT> '
'660 2 Statistical Coreference Resolution Model Our coreference system uses a binary entity-mention model PL( je, m) (henceforth link model ) to score the action of linking a mention m to an entity e. In our implementation, the link model is computed as PL(L = 1je, m) max mprimee PL(L = 1je, mprime, m), (1) where mprime is one mention in entity e, and the basic model building block PL(L = 1je, mprime, m) is an exponential or maximum entropy model (Berger et al. , 1996): PL(Lje, mprime, m) = exp braceleftbig summationtext i igi(e, m prime, m, L)bracerightbig Z(e, mprime, m), (2) where Z(e, mprime, m) is a normalizing factor to ensure that PL( je, mprime, m) is a probability, fgi(e, mprime, m, L)g are features and fig are feature weights.'
'Bitexts also play a role in less automated applications such as concordancing for bilingual lexicography <OTH> , computer-assisted language learning , and tools for translators -LRB- eg '
'To measure the translation quality , we use the BLEU score <CIT> and the NIST score <OTH> '
'Moreover , our approach integrates the abbreviation translation component into the baseline system in a natural way , and thus is able to make use of the minimum-error-rate training <CIT> to automatically adjust the model parameters to reflect the change of the integrated system over the baseline system '
'4 Method-2 : Simple Chunk-based Extraction To overcome the shortcomings of the Brill tagger in identifying particles , we next look to full chunk 2Note , this is the same as the maximum span length of 5 used by <CIT> , and above the maximum attested NP length of 3 from our corpus study -LRB- see Section 22 -RRB- '
'As <CIT> notes , many tasks in computational linguistics are simply more difficult than the content analysis classifications addressed by Krippendorff , and according to Fleiss <OTH> , kappa values between 4 and 75 indicate fair to good agreement anyhow '
'We rerank derivations with cube growing , a lazy beam search algorithm <CIT> '
'Two main extensions from that work that we are making use of are : 1 -RRB- proofs falling below a user defined cost threshold halt the search 2 -RRB- a simple variable typing system reduces the number of axioms written and the size of the search space <CIT> '
'While <CIT> calculate n-gram matches on non-labelled head-modifier sequences derived by head-extraction rules from syntactic trees , we automatically evaluate the quality of translation by calculating an f-score on labeled dependency structures produced by a LexicalFunctional Grammar -LRB- LFG -RRB- parser '
'4 Testing the Four Hypotheses The question of why self-training helps in some cases <CIT> but not others <OTH> has inspired various theories '
'Large treebanks are available for major languages , however these are often based on a speci c text type or genre , eg nancial newspaper text -LRB- the Penn-II Treebank <CIT> -RRB- '
'4 The Experiment For our experiment , we used a tree-bank grammar induced from sections 2-21 of the Penn Wall Street Journal text <CIT> , with section 22 reserved for testing '
'test additional resources JESS-CM -LRB- CRF\/HMM -RRB- 9735 9740 1G-word unlabeled data <CIT> 9728 9733 <OTH> 9715 9724 crude company name detector -LRB- sup '
'We use the GIZA + + implementation of IBM Model 4 <CIT> coupled with the phrase extraction heuristics of Koehn et al '
'4 5 Experiments 51 Evaluation Measures We evaluated the proposed method using four evaluation measures , BLEU <CIT> , NIST <OTH> , WER -LRB- word error rate -RRB- , and PER -LRB- position independent word error rate -RRB- '
'21 Linear Models for NLP We follow the framework outlined in <CIT> '
'This approach has also been used by -LRB- Dagan and Itai , 1994 ; Gale et al , 1992 ; Shiitze , 1992 ; Gale et al , 1993 ; <CIT> , 1995 ; Gale and Church , 1Lunar is not an unknown word in English , Yeltsin finds its translation in the 4-th candidate '
'For phrase-based translation model training , we used the GIZA + + toolkit <CIT> , and 10M bilingual sentences '
'This feature , which is based on the lexical parameters of the IBM Model 1 <CIT> , provides a complementary probability for each tuple in the translation table '
'Some researchers <CIT> have explored the use of Wikipedia information to improve the disambiguation process '
'The trees may be learned directly from parallel corpora <CIT> , or provided by a parser trained on hand-annotated treebanks <OTH> '
'The model is often further restricted so that each source word is assigned to exactly one target word <CIT> '
'are the labeled parsing recall and precision , respectively , as defined in <CIT> -LRB- slightly different from <OTH> -RRB- '
'<CIT> and Hanks 1990 ; Smadja and McKeown 1990 -RRB- '
'The sampler reasons over the infinite space of possible translation units without recourse to arbitrary restrictions -LRB- eg , constraints drawn from a wordalignment <CIT> or a grammar fixed a priori -LRB- Blunsom et al , 1f and e are the input and output sentences respectively '
'The first-sense heuristic can be thought of as striving for maximal specificity at the risk of precluding some admissible senses -LRB- reduced recall -RRB- , 7Allowing for multiple fine-grained senses to be judged as appropriate in a given context goes back at least to Sussna <OTH> ; discussed more recently by , eg , <CIT> '
'5 Experimental Data The sense-tagged text and feature set used in these experiments are the same as in <OTH> '
'The information for semi-supervised sense disambiguation is usually obtained from bilingual corpora -LRB- eg parallel corpora or untagged monolingual corpora in two languages -RRB- <OTH> , or sense-tagged seed examples <CIT> '
'Recently , specific probabilistic tree-based models have been proposed not only for machine translation <CIT> , but also for This work was supported by DARPA contract F49620-001-0337 and ARDA contract MDA904-02-C-0450 '
'On the contrary , a string-to-tree decoder -LRB- eg , <CIT> -RRB- is a parser that applies string-to-tree rules to obtain a target parse for the source string '
'Each element in vectorw gives a weight to its corresponding element in -LRB- y -RRB- , which is the count of a particular feature over the whole sentence y We calculate the vectorw value by supervised learning , using the averaged perceptron algorithm <CIT> , given in Figure 1 '
'<CIT> and Turney and Littman -LRB- 2002 -RRB- exploit the first two generalizations for unsupervised sentiment classification of movie reviews '
'<OTH> , who retrain the <CIT> tagger and reach accuracies of 93 % using CTB-I '
'We chose nouns that occur a minimum of 10 times in the corpus, have no undetermined translations and at least five different translations in the six nonEnglish languages, and have the log likelihood score of at least 18; that is: LL(T T, T S ) =  = 2 1 ij n* j * j*i ij n log  18 where n ij stands for the number of times T T and T S have been seen together in aligned sentences, n i* and n *j stand for the number occurrences of T T and T S, respectively, and n ** represents the total 4 We computed raw percentages only; common measures of annotator agreement such as the Kappa statistic (Carletta, 1996) proved to be inappropriate for our two-category (yesno) classification scheme.'
'It is known that ITGs do not induce the class of inside-out alignments discussed in <CIT> '
'1 Introduction and Previous Research It is by now commonplace knowledge that accurate syntactic parsing is not possible given only a context-free grammar with standard Penn Treebank <CIT> labels -LRB- eg , S , NP , etc -RRB- '
'Their weights are optimized wrt BLEU score using the algorithm described in <CIT> '
'<OTH> describe how to learn hundreds of millions of treetransformation rules from a parsed , aligned Chinese\/English corpus , and <CIT> et al '
'<CIT> found that such smoothing during training gives almost identical results on translation metrics '
'One is how to learn a statistical model to estimate the conditional probability    , and the other is how to generate confusion set C of a given query q 4.1 Maximum Entropy Model for Query Spelling Correction We take a feature-based approach to model the posterior probability     . Specifically we use the maximum entropy model (Berger et al. , 1996) for this task:     = exp     ,   =1 exp(     (,  ) =1 ) (2) where exp(     (, ) =1 ) is the normalization factor;   , is a feature function defined over query q and correction candidate c, while   is the corresponding feature weight.'
'We can mentionhere only part of this work : <CIT> for monolingualextraction , and -LRB- Kupiec , 1993 ; Wu ,1994 ; Smadjaetal '
'Previous workonsentimentanalysishascoveredawiderange of tasks , including polarity classification <CIT> , opinion extraction <CIT> , and opinion source assignment <OTH> '
'The second one is heuristic and tries to use a wordaligned corpus <CIT> '
'Statistical data about these various cooccurrence relations is employed for a variety of applications , such as speech recognition <OTH> , language generation <OTH> , lexicography <CIT> , machine translation <OTH> , information retrieval <OTH> and various disambiguation tasks <OTH> '
'Collins and Koo <CIT> introduced an improved reranking model for parsing which includes a hidden layer of semantic features '
'1 Introduction Many different statistical tests have been proposed to measure the strength of word similarity or word association in natural language texts <CIT> '
'As in most other statistical parsing systems we therefore use the pruning technique described in Goodman <OTH> and <CIT> which assigns a score to each item in the chart equal to the product of the inside probability of the item and its prior probability '
'Some work has been done on adding new terms and relations to WordNet <CIT> and FACTOTUM <OTH> '
'The computation mechanism of GP and LP bears a resemblance to the EM algorithm <CIT> , which iteratively computes maximum likelihood estimates from incomplete data '
'Using BLEU <CIT> as a metric , our method achieves an absolute improvement of 006 -LRB- 2213 % relative -RRB- as compared with the standard model trained with 5,000 L f - L e sentence pairs for French-Spanish translation '
'Due to its popularity for unsupervised POS induction research <OTH> and its often-used tagset , for our initial research , we use the Wall Street Journal -LRB- WSJ -RRB- portion of the Penn Treebank <CIT> , with 36 tags -LRB- plus 9 punctuation tags -RRB- , and we use sections 00-18 , leaving held-out data for future experiments4 Defining frequent frames as those occurring at 4Even if we wanted child-directed speech , the CHILDES database <OTH> uses coarse POS tags '
'We also tested the flat syntactic feature set proposed in <CIT> s work '
'BLEU <CIT> , NIST <OTH> '
'However , as Categorial Grammar formalisms do not usually change the lexical entries of words to deal with movement , but use further rules <OTH> , the lexicons learned here will be valid over corpora with movement '
'For this we aligned 170,863 pairs of Arabic\/English newswire sentences from LDC , trained a state-of-the-art syntax-based statistical machine translation system <OTH> on these sentences and alignments , and measured BLEU scores <CIT> on a separate set of 1298 newswire test sentences '
'The left-to-right parser would likely improve if we were to use a left-corner transform <CIT> '
'Goldwater and Griffiths <OTH> evaluated against the reduced tag set of 17 tags developed by Smith and Eisner <OTH> , while <CIT> <OTH> evaluated against the full Penn Treebank tag set '
'The unknown word tokens are with respect to Training I. Data set Sect''ns Token Unknown Training I 26-270, 600-931 213986 Training II 600-931, 500-527, 1001-1039 204701 Training III 001-270, 301-527, 590-593, 600-1039, 1043-1151 485321 Devset 23839 2849 XH 001-025 7844 381 HKSAR 500-527 8202 1168 SM 590-593, 1001-1002 7793 1300 Test set 23522 2957 XH 271-300 8008 358 HKSAR 528-554 7153 1020 SM 594-596, 1040-1042 8361 1579 5.2 The model Our model builds on research into loglinear models by Ng and Low (2004), Toutanova et al. , (2003) and Ratnaparkhi (1996).'
'1 Introduction A ` pain in the neck '' <OTH> for NLP in languages of the Indo-Aryan family -LRB- eg Hindi-Urdu , Bangla and Kashmiri -RRB- is the fact that most verbs -LRB- nearly half of all instances in Hindi -RRB- occur as complex predicates multi-word complexes which function as a single verbal unit in terms of argument and event structure <OTH> '
'It generates a vector of 5 numeric values for each phrase pair:  phrase translation probability: ( f|e) = count( f, e) count(e),(e| f) = count( f, e) count( f) 2http://www.phramer.org/  Java-based open-source phrase based SMT system 3http://www.isi.edu/licensed-sw/carmel/ 4http://www.speech.sri.com/projects/srilm/ 5http://www.iccs.inf.ed.ac.uk/pkoehn/training.tgz 150  lexical weighting (Koehn et al. , 2003): lex( f|e,a) = nproductdisplay i=1 1 |{j|(i, j)  a}| summationdisplay (i,j)a w(fi|ej) lex(e|f,a) = mproductdisplay j=1 1 |{i|(i, j)  a}| summationdisplay (i,j)a w(ej|fi)  phrase penalty: ( f|e) = e; log(( f|e)) = 1 2.2 Decoding We used the Pharaoh decoder for both the Minimum Error Rate Training (Och, 2003) and test dataset decoding.'
'We can confirm that changing the dimensionality parameter h has rather little effect -LRB- Table 4 -RRB- , which is in line with previous findings <CIT> '
'This includes both the parsers that attach probabilities to parser moves <OTH> , but also those of the lexicalized PCFG variety <CIT> '
'We also test our language model using leave-one-out cross-validation on the Penn Treebank <CIT> -LRB- WSJ -RRB- , giving us 8674 % accuracy -LRB- see Table 1 -RRB- '
'Classes can be induced directly from the corpus <CIT> or taken from a manually crafted taxonomy <OTH> '
'32 System Combination Scheme In our work , we use a sentence-level system combination model to select best translation hypothesis from the candidate pool -LRB- -RRB- This method can also be viewed to be a hypotheses reranking model since we only use the existing translations instead of performing decoding over a confusion network as done in the word-level combination method <CIT> '
'Methods have been proposed for automatic evaluation in MT -LRB- eg , BLEU <CIT> -RRB- '
'Some work identifies inflammatory texts -LRB- eg , <OTH> -RRB- or classifies reviews as positive or negative -LRB- <CIT> -RRB- '
'<CIT> put forward and discussed n-gram models based on classes of words '
'<OTH> develop a bottom-up decoder for BTG <CIT> that uses only phrase pairs '
'We use maximum marginal decoding , which <CIT> reports performs better than Viterbi decoding '
'On the machine-learning side , it would be interesting to generalize the ideas of large-margin classi cation to sequence models , strengthening the results of <CIT> and leading to new optimal training algorithms with stronger guarantees against over tting '
'Therefore , domain adaptation methods have recently been proposed in several NLP areas , eg , word sense disambiguation <OTH> , statistical parsing <OTH> , and lexicalized-grammar parsing <OTH> '
'Baseline Pharaoh with phrases extracted from IBM Model 4 training with maximum phrase length 7 and extraction method diag-growthfinal <CIT> Lex Phrase-decoder simulation : using only the initial lexical rules from the phrase table , all with LHS X , the Glue rule , and a binary reordering rule with its own reordering-feature XCat All nonterminals merged into a single X nonterminal : simulation of the system Hiero <OTH> '
'We believe the benefit to limiting the size of n is connected to <CIT> observation that as n increases , the accuracy of an n-gram model increases , but the reliability of our parameter estimates , drawn as they must be from a limited training text , decreases '
'Several artificial techniques have been used so that classifiers can be developed and tested without having to invest in manually tagging the data : <CIT> and Sch\/itze -LRB- 1995 -RRB- have acquired training and testing materials by creating pseudowords from existing nonhomographic forms '
'<CIT> employ Multiple Sequence Alignment <OTH> to align strings extracted from closely related news articles '
'In contrast , the latter computes four definite probabilities which are included as features within a machine-learning classifier from the Web in an attempt to overcome <CIT> data sparseness problem '
'216 The Maximum Entropy Principle <CIT> is to nd a model p = argmax pC H -LRB- p -RRB- , which means a probability model p -LRB- y x -RRB- that maximizes entropy H -LRB- p -RRB- '
'A similar use of the term phrase exists in machine translation , where phrases are often pairs of word sequences consistent with word-based alignments <CIT> '
'Accurate measurement of semantic similarity between lexical units such as words or phrases is important for numerous tasks in natural language processing such as word sense disambiguation <OTH> , synonym extraction <CIT> , and automatic thesauri generation <OTH> '
'<OTH> -RRB- , better language-specific preprocessing <OTH> and restructuring <OTH> , additional feature functions such as word class language models , and minimum error rate training <CIT> to optimize parameters '
'Most current transliteration systems use a generative model for transliteration such as freely available GIZA + +1 <OTH> , an implementation of the IBM alignment models <CIT> '
'In addition to portability experiments with the parsing model of <CIT> , <OTH> provided a comprehensive analysis of parser portability '
'Since parsing is just an initial stage of natural language understanding , the project was focused not just on obtaining syntactic trees alone -LRB- as is done in many other parsed corpora , for example , Penn TreeBank <CIT> or Tiger <OTH> -RRB- '
'There exists a variety of different metrics , eg , word error rate , position-independent word error rate , BLEU score <CIT> , NIST score <OTH> , METEOR <OTH> , GTM <OTH> '
'In this work , model fit is reported in terms of the likelihood ratio statistic , G 2 , and its significance <CIT> '
'Two main approaches have generally been considered : rule-based <OTH> probabilistic <CIT> '
'<OTH> -RRB- , and distributional methods -LRB- eg , <CIT> et al '
'33 Corpora Our labeled data comes from the Penn Treebank <CIT> and consists of about 40,000 sentences from Wall Street Journal -LRB- WSJ -RRB- articles 153 annotated with syntactic information '
'2 System Description 21 Data Representation In this paper , we change the representation of the original data as follows : Bracketed representation of roles is converted into IOB2 representation <OTH> Word tokens are collapsed into base phrase -LRB- BP -RRB- tokens '
'Models that support non-monotonic decoding generally include a distortion cost , such as aibi11 where ai is the starting position of the foreign phrasefi andbi1 is the ending position of phrase fi1 <CIT> '
'The transcription probabilities can then be easily learned from the alignments induced by GIZA + + , using a scoring function <CIT> '
'Further , we can learn the channel probabilities in an unsupervised manner using a variant of the EM algorithm similar to machine translation <CIT> , and statistical language understanding <OTH> '
'<CIT> and Uryupina -LRB- 2003 -RRB- have already employed a definite probability measure in a similar way , although the way the ratio is computed is slightly different '
'Partitioning 2 : Medium and low frequency words As noted in <CIT> , log-likelihood statistics are able to capture word bi-gram regularities '
'Perhaps this was not observed earlier since <CIT> studied only base NPs , most of which are short '
'Previous work from <OTH> showed improvements in perplexity-oriented measures using mixture-based translation lexicon <CIT> '
'2 Learning algorithm The translation model is a standard linear model <CIT> , which we train using MIRA <OTH> , following Watanabe et al '
'However , in the Grammar Association context , when developing -LRB- using Bayes decomposition -RRB- the basic equations of the system presented in <OTH> , it is said that the reverse model for a28 a13a37a3a38a5a39a32a21a0a35a7 does not seem to admit a simple factorization which is also correct and convenient , so crude heuristics were adopted in the mathematical development of the expression to be maximized '
'The translation and reference files are analyzed by a treebank-based , probabilistic Lexical-Functional Grammar -LRB- LFG -RRB- parser <CIT> , which produces a set of dependency triples for each input '
'The use of such relations -LRB- mainly relations between verbs or nouns and their arguments and modifiers -RRB- for various purposes has received growing attention in recent research <CIT> '
'This amounts to performing binary text categorization under categories Objective and Subjective <CIT> ; 2 '
'In contrast , semi-supervised domain adaptation <CIT> is the scenario in which , in addition to the labeled source data , we only have unlabeled and no labeled target domain data '
'With the success of collaborative sites like Amazons Mechanical Turk 1 , one 1http : \/ \/ wwwmturkcom \/ 59 can provide the task of annotation to multiple oracles on the internet <CIT> '
'Still , a confidence range for BLEU can be estimated by bootstrapping <CIT> '
'Some authors have already designed similar matching techniques , such as the ones described in <OTH> and <CIT> '
'The feature templates in <CIT> that were left out were the ones that look at the previous word , the word two positions before the current , and the word two positions after the current '
'In our case , we computed a likelihood ratio score <CIT> for all pairs of English tokens and Inuktitut substrings of length ranging from 3 to 10 characters '
'More recently , Haffari and Sarkar <OTH> have extended the work of <CIT> and given a better mathematical understanding of self-training algorithms '
'Word alignments are provided by GIZA + + <CIT> with grow-diag-final combination , with infrastructure for alignment combination and phrase extraction provided by the shared task '
'As a result , they are being used in a variety of applications , such as question answering <OTH> , speech recognition <OTH> , language modeling <OTH> , language generation <OTH> and , most notably , machine translation <CIT> '
'By labeling Treeb ~ n ~ nodes with Gr ~ ramar rule names , and not with phrasal and clausal n ~ raes , as in other -LRB- non-gr ~ rarnar-based -RRB- treebanks '' <CIT> , we gain access to all information provided by the Grammar regarding each ~ reebank node '
'The usefulness of likelihood ratios for collocation detection has been made explicit by <CIT> and has been confirmed by an evaluation of various collocation detection methods carried out by Evert and Krenn <OTH> '
'We use maximum entropy modeling <CIT> to directly model the conditional probability a17a19a18a20a2a21a15a23a22a24a26a25 , where each a27a5a15 in a24a29a28a30a18a31a27a32a4a33a6a7a8a9a8a9a8a9a6a23a27a34a11a14a25 is an observation associated with the corresponding speaker a2 a15 a27 a15 is represented here by only one variable for notational ease , but it possibly represents several lexical , durational , structural , and acoustic observations '
'Although we have argued -LRB- section 2 -RRB- that this is unlikely to succeed , to our knowledge , we are the first to investigate the matter empirically11 The best-known MT aligner is undoubtedly GIZA + + <OTH> , which contains implementations of various IBM models <CIT> , as well as the HMM model of Vogel et al '
'Since text planners can not generate either the requisite syntactic variation or quantity of text , <OTH> developed an evaluation strategy for HALOGEN employing a substitute : sentence parses from the Penn TreeBank <CIT> , a corpus that includes texts from newspapers such as the Wall Street Journal , and which have been hand-annotated for syntax by linguists '
'Indeed , only few earlier works reported inter-judge agreement level , and those that did reported rather low Kappa values , such as 054 <CIT> and 055 063 <OTH> '
'The bidirectional word alignmentisusedtoobtainlexicalphrasetranslationpairs using heuristics presented in <CIT> and <OTH> '
'An alternative to linear models is the log-linear models suggested by <CIT> '
'Later taggers have managed to improve Brills figures a little bit , to just above 97 % on the Wall Street Journal corpus using Hidden Markov Models , HMM and Conditional Random Fields , CRF ; eg , <CIT> and Toutanova et al '
'The novel algorithm differs computationally from earlier work in discriminative training algorithms for SMT <CIT> as follows : a90 No computationally expensive a57 - best lists are generated during training : for each input sentence a single block sequence is generated on each iteration over the training data '
'Introduction Since Eric Brill first introduced the method of Transformation-Based Learning -LRB- TBL -RRB- it has been used to learn rules for many natural language processing tasks , such as part-of-speech tagging <OTH> , PPattachment disambiguation <OTH> , text chunking <CIT> , spelling correction <OTH> , dialogue act tagging <OTH> and ellipsis resolution <OTH> '
'For nonprojective parsing , the analogy to the inside algorithm is the O -LRB- n3 -RRB- matrix-tree algorithm , which is dominated asymptotically by a matrix determinant <CIT> '
'GIZA + + <CIT> and the heuristics grow-diag-final-and are used to generate m-ton word alignments '
'The translation probability can also be discriminatively trained such as in <CIT> '
'To help our model learn that it is desirable to copy answer words into the question , we add to each corpus a list of identical dictionary word pairs w iw i For each corpus , we use GIZA <OTH> , a publicly available SMT package that implements the IBM models <CIT> , to train a QA noisy-channel model that maps flattened answer parse trees , obtained using the cut procedure described in Section 31 , into questions '
'4 Data Collection We evaluated out method by running RASP over Brown Corpus and Wall Street Journal , as contained in the Penn Treebank <CIT> '
'translation systems <CIT> and use Moses <OTH> to search for the best target sentence '
'51 Comparison to self-training For completeness , we also compared our results to the self-learning algorithm , which has commonly been referred to as bootstrapping in natural language processing and originally popularized by the work of Yarowsky in word sense disambiguation <CIT> '
'The acquisition of clues is a key technology in these research efforts , as seen in learning methods for document-level SA <OTH> and for phraselevel SA <CIT> '
'Re-decoding <CIT> based regeneration re-decodes the source sentence using original LM as well as new trans105 lation and reordering models that are trained on the source-to-target N-best translations generated in the first pass '
'Following <CIT> , the slot labels are drawn from a set of classes constructed by extending each label by three additional symbols , Beginning\/Inside\/Outside -LRB- B\/I\/O -RRB- '
'The proxy slot denotes a semantic individual which serves the role of an event instance in a partially Davidsonian scheme , as in <CIT> or <OTH> '
'42 Base Model II Using the translation model II <CIT> , where alignments are dependent on word\/entity positions and word\/entity sequence lengths , we have p -LRB- w e -RRB- = mproductdisplay j = 1 lsummationdisplay i = 0 p -LRB- aj = i j , m , l -RRB- p -LRB- wj ei -RRB- -LRB- 2 -RRB- where aj = i means that wj is aligned with ei '
'These were combined using the Grow Diag Final And symmetrization heuristic <CIT> '
'If the target CFG is purely binary branching , then the previous theoretical and linguistic analyses <CIT> suggest that much of the requisite constituent and word order transposition may be accommodated without change to the mirrored ITG '
'First , we considered single sentences as documents , and tokens as sentences -LRB- we define a token as a sequence of characters delimited by 1In our case , the score we seek to globally maximize by dynamic programming is not only taking into account the length criteria described in <OTH> but also a cognate-based one similar to <OTH> '
'For the maximum entropy classifier , we estimate the weights by maximizing the likelihood of a heldout set , using the standard IIS algorithm <CIT> '
'22 The Crossing Constraint According to <CIT> , crossing constraint can be defined in the following '
'<CIT> attain 982 % coverage and a BLEU score of 06652 on the standard WSJ test set -LRB- Section 23 -RRB- '
'Previous publications on Meteor <CIT> have described the details underlying the metric and have extensively compared its performance with Bleu and several other MT evaluation metrics '
'1 Word associations -LRB- co-occurrences , or joint frequencies -RRB- have a wide range of applications including : speech recognition , optical character recognition , and information retrieval -LRB- IR -RRB- <CIT> '
'(Snow et al., 2006; Nakov & Hearst, 2008).'
'Our study is also different from these previous ones in that measuring the agreement among annotators became an issue <CIT> '
'Our approach differs from the corpus-based surface generation approaches of <OTH> and <CIT> '
'In our experiments we use a grammar with a start symbol S , a single preterminal C , and two nonterminals A and B used to ensure that only one parse can generate any given word-level alignment -LRB- ignoring insertions and deletions -RRB- <CIT> '
'There are good reasons for using such a hand-crafted , genre-specific verb lexicon instead of a general resource such as WordNet or Levins <OTH> classes : Many verbs used in the domain of scientific argumentation have assumed a specialized meaning , which our lexicon readily encodes '
'Here , we compare two similarity measures : the familiar BLEU score <CIT> and a score based on string kernels '
'Although this approach can give inaccurate estimates , the counts given to the incorrect senses will disperse randomly throughout the hierarchy as noise , and by accumulating counts up the hierarchy we will tend to gather counts from the correct senses of related words <OTH> '
'Most probabilistic parsing research including , for example , work by by <CIT> , and Charniak -LRB- 1997 -RRB- is based on branching process models <OTH> '
'2 Baseline Coreference Resolution System Our baseline coreference system implements the standard machine learning approach to coreference resolution -LRB- see Ng and Cardie <OTH> , <CIT> , Yang and Su -LRB- 2007 -RRB- , for instance -RRB- , which consists of probabilistic classification and clustering , as described below '
'The training and decoding system of our SMT used the publicly available Pharaoh <CIT> 2 '
'<CIT> propose factored translation models that combine feature functions to handle syntactic , morphological , and other linguistic information in a log-linear model '
'This therefore suggests that better parameters are likely to be learned in the 2 <CIT> generative coreference model mirrors this in the posterior distribution which it assigns to mention types given their salience -LRB- see their Table 1 -RRB- '
'1 Introduction Over the last few years , several automatic metrics for machine translation -LRB- MT -RRB- evaluation have been introduced , largely to reduce the human cost of iterative system evaluation during the development cycle <CIT> '
'Most of the annotation approaches tackling these issues , however , are aimed at performing classifications at either the document level <CIT> , or the sentence or word level <OTH> '
'This logistic regression is also called Maxent as it finds the distribution with maximum entropy that properly estimates the average of each feature over the training data <CIT> '
'The idea of threading EEs to their antecedents in a stochastic parser was proposed by <CIT> , following the GPSG tradition <OTH> '
'The mutual information clustering algorithm <CIT> were used for this '
'1993 ; Chang et al , 1992 ; Collins and Brooks , 1995 ; Fujisaki , 1989 ; Hindle and Rooth , 1991 ; Hindle and Rooth , 1993 ; Jelinek et al , 1990 ; Magerman and <CIT> , 1991 ; Magerman , 1995 ; Ratnaparkhi et al , 1994 ; Resnik , 1993 ; Su and Chang , 1988 -RRB- '
'Building on the annotations from the Wall Street Journal (WSJ) portion of the Penn Treebank (Marcus et al., 1993), the project added several new layers of semantic annotations, such as coreference information, word senses, etc. In its first release (LDC2007T21) through the Linguistic Data Consortium (LDC), the project manually sense-tagged more than 40,000 examples belonging to hundreds of noun and verb types with an ITA of 90%, based on a coarse-grained sense inventory, where each word has an average of only 3.2 senses.'
'Using a variant of the voted perceptron <CIT> , we discriminatively trained our parser in an on-line fashion '
'The approach combines statistical and knowledge-based methods , but unlike many recent corpus-based approaches to sense disambiguation <OTH> , it takes as its starting point the assumption that senseannotated training text is not available '
'Traditionally , maximum-likelihood estimation from relative frequencies is used to obtain conditional probabilities <CIT> , eg , p -LRB- s t -RRB- = c -LRB- s , t -RRB- \/ summationtexts c -LRB- s , t -RRB- -LRB- since the estimation problems for p -LRB- s t -RRB- and p -LRB- t s -RRB- are symmetrical , we will usually refer only to p -LRB- s t -RRB- for brevity -RRB- '
'<OTH> and Lee <OTH> -RRB- can be generally divided into three types : discounting <OTH> , class-based smoothing <CIT> , and distance-weighted averaging <OTH> '
'Many existing systems for statistical machine translation <OTH> implement models presented by <CIT> : The correspondence between the words in the source and the target strings is described by alignments that assign target word positions to each source word position '
'However , they do not elaborate on how the comparisons are done , or on how effective the program is <CIT> describes a heuristic approach to forming unlabeled clusters of closely related senses in an MRD '
'<OTH> , various classification models and linguistic features have been proposed to improve the classification performance <CIT> '
'One of the simplest models in the context of lexical triggers is the IBM model 1 <CIT> which captures lexical dependencies between source and target words '
'grow-diagfinal <CIT> -RRB- '
'<OTH> , <CIT> -RRB- and the exploitation of advanced techniques that involve joint learning -LRB- eg , Daume III and Marcu <OTH> -RRB- and joint inference -LRB- eg , Denis and Baldridge <OTH> -RRB- for coreference resolution and a related extraction task '
'This permits us to make exact comparisons with the parser of Yamada and Matsumoto <OTH> , but also the parsers of <CIT> and Charniak -LRB- 2000 -RRB- , which are evaluated on the same data set in Yamada and Matsumoto <OTH> '
'In fact , the largest source of English dependency trees is automatically generated from the Penn Treebank <CIT> and is by convention exclusively projective '
'Normally , :8 is considered a good agreement <CIT> '
'Recent work <CIT> on this task explored a variety of methodologies to address this issue '
'The maximum entropy models used here are similar in form to those in <CIT> '
'Our baseline is the phrase-based MT system of <CIT> '
'We will provide a more detailed and systematic comparison between MAXIMUM ENTROPY MODELING <OTH> and MEMORY BASED LEARNING <OTH> for morpho-syntactic disambiguation and we investigate whether earlier observed differences in tagging accuracy can be attributed to algorithm bias , information source issues or both '
'So we propose forest reranking , a technique inspired by forest rescoring <CIT> that approximately reranks the packed forest of exponentially many parses '
'We use the standard NIST MTEval data sets for the years 2003 , 2004 and 2005 -LRB- henceforth MT03 , MT04 and MT05 , respectively -RRB- 6 We report results in terms of case-insensitive 4gram BLEU <CIT> scores '
'We adopted the chunk representation proposed by <CIT> and used four different tags : B-NUC and B-SAT for nucleus and satellite-initial tokens , and I-NUC and I-SAT for non-initial tokens , ie , tokens inside a nucleus and satellite span '
'Figures 1 and 2 present best results in the learning experiments for the complete set of patterns used in the collocation approach , over two of our evaluation corpora11 Type Positions Tags\/Words Features Accuracy Precision Recall GIS 1 W 1254 097 096 098 IIS 1 T 136 095 096 094 NB 1 T 136 088 097 084 9 see Rish , 2001 , Ratnaparkhi , 1997 and <CIT> et al , 1996 for a formal description of these algorithms '
'Tag test data using the POS-tagger described in <CIT> '
'In <OTH> , <CIT> the significance of an association -LRB- x , y -RRB- is measured by the mutual information I -LRB- x , y -RRB- , ie the probability of observing x and y together , compared with the probability of observing x and y independently '
'The precision rate using the lexical statistics approach can reach around 60 % if both word bi-gram extraction and n-gram extractions are taking into account <CIT> '
'Many methods exist for clustering , eg , <CIT> '
'The weights for these models are determined using the method described in <CIT> '
'TopSense is tested on 20 words extensively investigated in recent WSD literature <CIT> '
'But there is also extensive research focused on including linguistic knowledge in metrics <CIT> among others '
'<CIT> studied a method for word sense disambiguation using unlabeled data '
'Obtaining a word-aligned corpus usually involves training a word-based translation models <CIT> in each directions and combining the resulting alignments '
'The other utilizes a sort of parallel texts , such as multiple translation of the same text <CIT> , corresponding articles from multiple news sources <CIT> , and bilingual corpus <OTH> '
'For ROUGE-S and ROUGE-SU , we use three variations following <CIT> : the maximum skip distances are 4 , 9 and infinity 7 '
'Finally , other approaches rely on reviews with numeric ratings from websites <CIT> and train -LRB- semi - -RRB- supervised learning algorithms to classify reviews as positive or negative , or in more fine-grained scales <CIT> '
'The first one makes use of the advances in the parsing technology or on the availability of large parsed corpora -LRB- eg Trcebank <CIT> -RRB- to produce algorithms inspired by Hobbs '' baseline method <OTH> '
'The probabilities are ordered according to , at least my , intuition with pronoun being the most likely <OTH> , followed by proper nouns <OTH> , followed by common nouns <OTH> , a fact also noted by <CIT> '
'Therefore , structure divergence and parse errors are two of the major issues that may largely compromise the performance of syntax-based SMT <CIT> '
'Work focusses on analyzing subjective features of text or speech , such as sentiment , opinion , emotion or point of view <CIT> '
'The NIST BLEU-4 is a variant of BLEU <CIT> and is computed as a49a51a50 a2a16a52a53a6 a0a9a8a10a0a12a11a54a13a55a15 a26a57a56a33a58a60a59 a43 a61a63a62 a64 a65a67a66a69a68 a28a71a70a46a72a74a73 a65 a6 a0a9a8a10a0a3a11a54a13a19a75a77a76 a6 a0a9a8a10a0a3a11a54a13 -LRB- 2 -RRB- where a73 a65 a6 a0a78a8a10a0a3a11a54a13 is the precision of a79 - grams in the hypothesis a0 given the reference a0 a11 and a76 a6 a0a78a8a10a0a3a11a54a13a81a80 a43 is a brevity penalty '
'1 Introduction Inversion transduction grammar -LRB- ITG -RRB- constraints <CIT> provide coherent structural constraints on the relationship between a sentence and its translation '
'Examples of this are bilexical grammars -- such as Eisner and Satta <OTH> , Charniak <OTH> , <CIT> <OTH> -- where the lexical heads of each constituent are annotated on both the rightand left-hand sides of the context-free rules , under the constraint that every constituent inherits the lexical head from exactly one of its children , and the lexical head of a POS is its terminal item '
'For a second set of parsing experiments , we used the WSJ portion of the Penn Tree Bank <CIT> and Helmut Schmids enrichment program tmod <OTH> '
'To this end we follow the method introduced by <CIT> , ie by sliding a window of a given size over some texts '
'The MBT POS tagger <OTH> is used to provide POS information '
'In this paper , we modify the method in Albrecht and Hwa <OTH> to only prepare human reference translations for the training examples , and then evaluate the translations produced by the subject systems against the references using BLEU score <CIT> '
'The model weights are trained using the standard ranking perceptron <CIT> '
'Carletta suggests that content analysis researchers consider K -RRB- 8 as good reliability , with67 -LRB- \/ ~ '' -LRB- 8 allowing tentative conclusions to be drawn <CIT> '
'-LRB- 1 -RRB- <CIT> provides evidence that should be chosen by optimizing an objective function basd on the evaluation metric of interest , rather than likelihood '
'2 Related Work Given its potential usefulness in coreference resolution , anaphoricity determination has been studied fairly extensively in the literature and can be classified into three categories : heuristic rule-based <OTH> , statistics-based <CIT> and learning-based <OTH> '
'Proceedings of the 40th Annual Meeting of the Association for cently , semantic resources have also been used in collocation discovery <OTH> , smoothing and model estimation <CIT> and text classi cation <OTH> '
'3TheData For our experiments we used a version of the British National Corpus parsed with the statistical parser of <CIT> '
'1 Introduction During the last decade , statistical machine translation -LRB- SMT -RRB- systems have evolved from the original word-based approach <OTH> into phrase-based translation systems <CIT> '
'In terms of relative performance , Naive Bayes tends to do the worst and SVMs tend to do the best , although the 12http : \/ \/ wwwenglishbhamacuk\/stafi\/oliver \/ software\/tagger\/indexhtm 13 <CIT> unsupervised algorithm uses bigrams containing an adjective or an adverb '
'Thus , we used the five taggers , MBL <OTH> , MXPOST <CIT> , fnTBL <OTH> , TnT , and IceTagger3 , in the same manner as described in <OTH> , but with the following minor changes '
'In this case , one is often required to find the translation -LRB- s -RRB- in the hypergraph that are most similar to the desired translations , with similarity computed via some automatic metric such as BLEU <CIT> '
'4 Maxilnum Entropy The model used here for sentence-boundary detection is based on the maximum entropy model used for POS tagging in <CIT> '
'Using techniques described in <CIT> , Church and Hanks -LRB- 1990 -RRB- , and Hindle and Rooth -LRB- 1991 -RRB- , Figure 4 shows some examples of the most frequent V-O pairs from the AP corpus '
'As mentioned earlier , both of these methods are based on Collinss averaged-perceptron algorithm for sequence labeling <CIT> '
'32 F-Structure Based NLD Recovery <CIT> presented a NLD recovery algorithm operating at LFG f-structure for treebankbased LFG approximations '
'5 SMT Experiments 51 Experimental Setup We used publicly available resources for all our tests : for decoding we used Moses <CIT> and our parallel data was taken from the Spanish-English section of Europarl '
'<OTH> and <CIT> used comparable news articles to obtain sentence level paraphrases '
'Several techniques and results have been reported on learning subcategorization frames -LRB- SFs -RRB- from text corpora <OTH> '
'To find these pairs automatically , wetrainedanon-sequentiallog-linearmodel that achieves a 902 accuracy <OTH> '
'As with the graph-based parser , we use the discriminative perceptron <CIT> to train the transition-based model -LRB- see Figure 5 -RRB- '
'Consequently , semi-supervised learning , which combines both labeled and unlabeled data , has been applied to some NLP tasks such as word sense disambiguation <CIT> , classification <OTH> , clustering <OTH> , named entity classification <OTH> , and parsing <OTH> '
'1 Introduction B (Papineni et al., 2002) was one of the first automatic evaluation metrics for machine translation (MT), and despite being challenged by a number of alternative metrics (Melamed et al., 2003; Banerjee and Lavie, 2005; Snover et al., 2006; Chan and Ng, 2008), it remains the standard in the statistical MTliterature.Callison-Burchetal.(2006)havesubjected B to a searching criticism, with two realworld case studies of significant failures of correlation between B and human adequacy/fluency judgments.Bothcasesinvolvecomparisonsbetween statistical MT systems and other translation methods (human post-editing and a rule-based MT system), and they recommend that the use of B be restrictedtocomparisonsbetweenrelatedsystemsor different versions of the same systems.'
'The supertagger uses a log-linear model to define a distribution over the lexical category set for each word and the previous two categories <CIT> and the forward backward algorithm efficiently sums over all histories to give a distribution for each word '
'The reader is referred to <OTH> and <CIT> for details of MI clustering , but we will first briefly summarize the MI clustering and then describe our hierarchical clustering algorithm '
'However , they can be usefully employed during system development , for example , for quickly assessing modeling ideas or for comparing across different system configurations <CIT> '
'We follow the method used by <CIT> , which encodes the matching with a gazetteer entity using IOB tags , with the modication for Japanese '
'We tuned the parameters of these features with Minimum Error Rate Training -LRB- MERT -RRB- <CIT> on the NIST MT03 Evaluation data set -LRB- 919 sentences -RRB- , and then test the MT performance on NIST MT03 and MT05 Evaluation data <OTH> '
'This source is very important for repairs that do not have initial retracing , and is the mainstay of the ` parser-first '' approach <OTH> -- keep trying alternative corrections until one of them parses '
'glish nouns first appeared in <CIT> '
'For the English experiments , we use the now-standard training and test sets that were introduced in <CIT> 2 '
'Barzilay and Lee <CIT> learned paraphrasing patterns as pairs of word lattices , which are then used to produce sentence level paraphrases '
'Current work has been spurred by two papers , <CIT> and <OTH> '
'Given two sentences X and Y , the WLCS score of X and Y can be computed using the similar dynamic programming procedure as stated in <CIT> '
'For all non-LEAF systems , we take the best performing of the union , refined and intersection symmetrization heuristics <CIT> to combine the 1-to-N and M-to-1 directions resulting in a M-to-N alignment '
'A number of part-of-speech taggers are readily available and widely used , all trained and retrainable on text corpora <OTH> '
'The Xerox experiments <CIT> correspond to something between D1 and D2 , and between TO and T1 , in that there is some initial biasing of the probabilities '
'The traditional framework presented in <CIT> assumes a generative process where the source sentence is passed through a noisy stochastic process to produce the target sentence '
'Experimental results were only reported for the METEOR metric <CIT> '
'1 Introduction Since their appearance , BLEU <CIT> and NIST <OTH> have been the standard tools used for evaluating the quality of machine translation '
'Expansion of the equivalent sentence set can be applied to automatic evaluation of machine translation quality <CIT> , for example '
'The kappa statistic <CIT> has become the de facto standard to assess inter-annotator agreement '
'8 An alternative formula for G 2 is given in <CIT> , but the two are equivalent '
' 00: the current input token and the previous one have the same parent  90: one ancestor of the current input token and the previous input token have the same parent  09: the current input token and one ancestor of the previous input token have the same parent  99 one ancestor of the current input token and one ancestor of the previous input token have the same parent Compared with the B-Chunk and I-Chunk used in Ramshaw and Marcus(1995)~, structural relations 99 and 90 correspond to B-Chunk which represents the first word of the chunk, and structural relations 00 and 09 correspond to I-Chunk which represents each other in the chunk while 90 also means the beginning of the sentence and 09 means the end of the sentence.'
'In <CIT> a set of transformational rules is used for modifying the classification of words '
'A problem mentioned in <CIT> is that the algorithm that computes the compressed representation might need to retain the entire database in memory ; in their paper , they design strategies to work around this problem '
'Decoding with an SCFG -LRB- eg , translating from Chinese to English using the above grammar -RRB- can be cast as a parsing problem -LRB- see Section 3 for details -RRB- , in which case we need to binarize a synchronous rule with more than two nonterminals to achieve polynomial time algorithms <CIT> '
'6 Related Work A pioneering antecedent for our work is <OTH> , who trained a Collins-style generative parser <CIT> over a syntactic structure augmented with the template entity and template relations annotations for the MUC-7 shared task '
'440 respondence learning -LRB- SCL -RRB- domain adaptation algorithm <CIT> for use in sentiment classification '
'A single translation is then selected by finding the candidate that yields the best overall score <CIT> or by cotraining <OTH> '
'The tools used are the Moses toolkit <OTH> for decoding and training , GIZA + + for word alignment <CIT> , and SRILM <OTH> for language models '
'611 Nugget-Based Pyramid Evaluation For our first approach we used a nugget-based evaluation methodology <CIT> '
'Many methods for calculating the similarity have been proposed <CIT> '
'It has been shown that one sense per discourse property can improve the performance of bootstrapping algorithm <CIT> '
'Post-editing of automatic annotation has been pursued in various projects <CIT> '
'It is a fundamental and often a necessary step before linguistic knowledge acquisitions , such as training a phrase translation table in phrasal machine translation -LRB- MT -RRB- system <CIT> , or extracting hierarchial phrase rules or synchronized grammars in syntax-based translation framework '
'Some researchers then tried to automatically extract paraphrase rules <CIT> , which facilitates the rule-based PG methods '
'For such cases , unsupervised approaches have been developed for predicting relations , by using sentences containing discourse connectives as training data <CIT> '
'The model scaling factors 1 , ,5 and the word and phrase penalties are optimized with respect to some evaluation criterion <CIT> , eg BLEU score '
'84 52 Machine translation on Europarl corpus We further tested our WDHMM on a phrase-based machine translation system to see whether our improvement on word alignment can also improve MT accuracy measured by BLEU score <CIT> '
'MT output was evaluated using the standard evaluation metric BLEU <OTH> 2 The parameters of the MT System were optimized for BLEU metric on NIST MTEval2002 test sets using minimum error rate training <CIT> , and the systems were tested on NIST MTEval2003 test sets for both languages '
'Someworkwithintheframework of synchronous grammars <CIT> , while others create a generative story that includes a parse tree provided for one of the sentences <OTH> '
'1 Introduction Most of the current work in statistical machine translation builds on word replacement models developed at IBM in the early 1990s <CIT> '
'1 Introduction Nowadays , statistical machine translation is mainly based on phrases <CIT> '
'<CIT> present a knowledge-lean algorithm that uses multiple-sequence alignment to 177 learn generate sentence-level paraphrases essentially from unannotated corpus data alone '
'6 Related Work In machine translation , the concept of packed forest is first used by <CIT> to characterize the search space of decoding with language models '
'Statistical data about these various cooccurrence relations is employed for a variety of applications , such as speech recognition <OTH> , language generation <OTH> , lexicography <OTH> , machine translation <OTH> , information retrieval <OTH> and various disambiguation tasks <CIT> '
