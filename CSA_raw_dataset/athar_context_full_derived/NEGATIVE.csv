Zhou and Nakagawa 1994 have shown in the experiments of word prediction from the previous word sequence that the HMM is more powerful than the bigram model and is nearly equivalent to the trigram model though the number of parameters of the HMM is less than that in the N ram model
In fact we experienced the underflow problem in preliminary experiments with the EDR corpus 
Since more than half of the symbols in the observations may be noise models estimated in this way are not reliable
This algorithm cannot take advantage of the scaling procedure because it requires the synchronous calculation of all possible sequences in the morpheme network 
However he did not apply this algorithm to the estimation of HMM parameters  
Transformationbased tagging as introduced by Brill 1993 also requires a hand tagged text for training 
In a previous paper Schfitze 1993 we trained a neural network to disambiguate partofspeech using context however no information about the word that is to be categorized was used
Compared to learning rulebased approaches such as the one by Brill 1992 a knn approach provides a uniform approach for all disambiguation tasks more flexibility in the engineering of case representations and a more elegant approach to handling of unknown words see eg Cardie 1994 
A remedy is to aggressively limit the feature space eg to syntactic labels or a small fraction of the bilingual features available as in Chiang et al 2008 Chiang et al 2009 but that reduces the benefit of lexical features 
This method avoids the overfitting problem at the expense of losing the benefit of discriminative training of rich features directly for MT However the feature space problem still exists in these published models 
 In addition the filtering method causes some practical issues First such methods are not suitable for real MT tasks especially for applications with streamed input since the model has to be retrained with each new input sentence or document and training is slow  
However their length features only provided insignificant improvement of 01 BLEU point A crucial difference of our approach is how the length preference is modeled 
We refer to He and Gildea 2006 who tested active learning and cotraining methods but found little or no gain from semisupervised learning and to Swier and Stevenson 2004 who achieved good results using semisupervised methods but tested their methods on a small number of VerbNet roles which have not been used by other SRL Systems
Discriminative models have been found to outperform generative models for many different tasks including SRL Lim et al 2004 For this reason we also employ discriminative models here 
Compared to previous approaches Raghavan and Allan 2007 our method can be used for both classification and structured tasks and the feature query selection methods we propose perform better 
Raghavan and Allan 2007 also propose several methods for learning with labeled features but in a previous comparison GE gave better results Druck et al 2008
Note that Liang et al 2009 only use this method in synthetic experiments and instead use a method similar to total uncertainty for experiments in partofspeech tagging
They find that querying certain features outperforms querying uncertain features but this is likely because their query selection method is similar to the expectation uncertainty method described above and consequently nondiscriminative features may be queried often It is also not clear how this graphbased training method would generalize to structured output spaces 
The 746 final accuracy on apartments is higher than any result obtained by Haghighi and Klein 2006 the highest is 741 higher than the supervised HMM results reported by Grenager et al 2005 744 and matches the results of Mann and Mc Callum 2008 with GE with more accurate sampled label distributions and 10 labeled examples
The conclusions are broadly in agreement with those of Merialdo 1994 but give greater detail about the contributions of different parts of the Model
Many freely available natural language processing tools require their input to be divided into sentences but make no mention of how to accomplish this eg Brill 1994 Collins 1996 
Others perform the division implicitly without discussing performance eg Cutting et al 1992 
There is also a less detailed description of Pahner and Hearst's system SATZ in Pahuer and Hearst 1994 
Liberman and Church suggest in Liberlnan and Church 1992 that a system could be quickly built to divide newswire text into sentences with a nearly negligible error rate but do not actually build such a system 
However their parser was not incremental it used global features such as the number of turn changes 
In this area the work most closely related to ours is that of Barrett and Weld Barrett and Weld 1994 who build an incremental bottomup parser to parse plans Their parser however was not probabilistic or targeted at dialog processing 
Also the chunkbased model is representationally inadequate for centerembedded nestings of subtasks which do occur in our domain although less frequently than the more prevalent “tailrecursive” structures
We presented firstorder expectation semirings and insideoutside computation in more detail than Eisner 2002 and developed extensions to higherorder expectation semirings
We speculate that this contrasts with the disappointing findings of Kehler et al 2004 since SRL provides a more fine grained level of information when compared to predicate argument statistics 
Most empirical work in translation analyzes mod els and algorithms using BLEU Papineni et al 2002 and related metrics Though such metrics are useful as sanity checks in iterative system development they are less useful as analytial tools
In an interesting analysis of phrasebased and hierarchical translation Zollmann et al 2008 forced a phrasebased system to produce the translations generated by a hierarchical system Unfortunately their analysis is incomplete they do not perform the analysis in both directions
 However this remains first of all a timeconsuming task Moreover it is not easy for humans to selec tthe best translation among a set of alternatives let alone assign them probabilities Last but not least the beneficial effect on translation is not guaranteed
[Mani et al 1999] employed rules such as the referencing of pronouns with the most recently mentioned noun phrase However this might be inappropriate in MDS where the use of multiple documents increases the number of possible entities with which an anaphor could be referenced 
In our approach the discourse structure is not fixed but predicted for each particular abstract 
 In cutandpaste summarization Jing and mcKeown 2000 sentence combination operations were implemented manually following the study of a set of professionally written abstracts however the particular “pasting” operation presented here was not implemented 
Previous studies on texttotext abstracting Banko et al 2000 Knight and Marcu 2000 have studied problems such as sentence compression and sentence combination but not the “pasting” procedure presented here 
Recently a number of machine learning approaches have been proposed Zettlemoyer and Collins 2005 Mooney 2007 However they are supervised and providing the target logical form for each sentence is costly and difficult to do consistently and with high quality 
 For ex ample when applying their approach to a different domain with somewhat less rigid syntax Zettlemoyer and Collins 2007 need to introduce new combinators and new forms of candidate lexical Entries 
Even an efficient sampler like MCSAT Poon and Domingos 2006 as used in Poon & Domingos 2008 would have a hard time generating accurate estimates within a reasonable amount of time 
In our work we partially address this issue by enumerating some transformations frequently found in our corpusthat are computationally implementable
Sentence reduction is concerned only with the removal of sentence components so it cannot explain transformations observed in our corpus and in summarization in general such as the reexpression of domain conceptsand verbs
Jing and McKeown 2000 have proposed a rulebasedalgorithm for sentence combination but no results have been reported 
Berger et al 1996 proposed an iterative procedure of adding news features to feature set driven by data 
Although this provides an unlimited freedom for rearranging constituents it also complicates the task of learning the parsing steps which might explain why their evaluation results show marginal improvements at best 
However theseapproaches operate in a limited domain eg terrorist events where information extraction systems can be used to interpret the source text
However they did not report any evaluation of their word extraction method
In other words they count an error only when the system segmentation is not acceptable to human judgement while we count an error whenever the system segmentation does not exactly match the corpus segmentation even if it is inconsistent
In English taggers Weischedel et al 1993 proposed a statistical model to estimate word output probability pwi|tl for an unknown word from spelling information such as inflectional endings derivational endings hyphenation and capitalization Our word model can be thought of a generalization of their statistical model 
 First previous methods for computing lexical chains have either been manual Morris and Hirst 1991 or automated but with exponential efficiency Hirst and StOnge 1997 Barzilay and Elhadad 1997 Because of this computing lexical chains for documents of any reasonable size has been impossible 
Recently many phrase reordering methods have been proposed ranging from simple distance based distortion model Koehn et al 2003 Ochand Ney 2004 flat reordering model Wu 1997Zens et al 2004 lexicalized reordering model Tillmann 2004 Kumar and Byrne 2005 to hierarchical phrasebased model Chiang 2005 Setiawan et al 2007 and classifierbased reordering model with linear features Zens and Ney 2006 Xiong et al 2006 Zhang et al 2007a Xiong et al 2008 However one of the major limitations of these advances is the structured syntactic knowledge which is important to global reordering Li et al 2007 Elming 2008 hasnot been well exploited
Assuming that the number of feature templates in a given set is n the algorithm of Ding and Chang 2008 requires On 2  times of training/test routines it cannot handle a set that consists of hundreds of templates 
However in this paper we clarify a number of details that are omitted in major previous publications concerning tagging with Markov models As two examples Rabiner 1989 and Charniak et al 1993 give good overviews of the techniques and equations used for Markov models and partofspeech tagging but they are not very explicit in the details that are needed for their application 
For the Penn Treebank Ratnaparkhi 1996 reports an accuracy of 966 using the Maximum Entropy approach our much simpler and therefore faster HMM approach delivers 967 T
For example the Markov model tagger used in the comparison of van Halteren et al 1998 yielded worse results than all other taggers 
However the difficulty of such tasks and the fact that they are apparently unrelated has led to the development of largely adhoc solutions tuned to specific challenges 
While the literature suggests that BaumWelch training can degrade performance on the tagging task Elworthy 1994 Merialdo 1994 we have found in early experiments that agreement between a tagger trained in this way and the tagger from the XTag Project consistently increases with each iteration of BaumWelch eventually reaching a plateau but not Decreasing 
Clark 2000 presents a framework which in principle should accommodate lexical ambiguity using mixtures but includes no evidence that it does so 
However their use of “perfect” clusters renders some of their algorithmic suggestions problematic 
A major problem with such methods is that each hypothesis is aligned to the backbone independently leading to sub optimal behavior
While PRW is the first attempt to formalize well known relevance weighting Sparck Jones 1972 Salton and McGill 1983 by probability theory there are several drawbacks in PRW 
A similar argument applies to all other problems in Robertson and Sparck Jones 1976 that are caused by having insufficient training cases 
However this technique needs a corpus for computing IDF score causing the genredependent problem for generic text summarization task 
Each technique brings its own terminology from the cubes of Chiang 2007 to the lazy lists of Pust and Knight 2009 into the mix Often it is not entirely clear why they Work 
Unlike many other methods that directly utilize noun phrase NP coreference Nenkova 2008 Mani et al 1999 we propose a method that employs insertion and substitution of phrases that modify the same chunk in the lead and other sentences
As is well known the extractive summary that has been extensively studied from the early days of summarization history Luhn 1958 suffers from various drawbacks 
Thus we found this step unnecessary and currently did not look at this issue any further 
The empirical results show that our instantiation of SCL to parse disambiguation gives promising initial results even without the many additional extensions on the feature level as done in Blitzer et al 2006 
Although we do agree with RST that the structure of text is hierarchical in many cases it is our belief that the relevance and function of certain text pieces can be determined without analyzing the full hierarchical structure of the text 
In the news domain sentence location is the single most important feature for sentence selection Brandow Mitze and Rau 1995 in our domain location information although less dominant can still give a useful indication  
 We feel that incorporating a robust analysis of discourse structure into a document summarizer is one step along this way 
A general solution to the variable length and depth ofdependency for HMM has been already proposed Tao 1992 but has notbeen implemented in taggers
As this encoding strategy is not wellsuited to a free word order language like German we have focussed on a less surfaceoriented level of description most closely related to the LFG fstructure and representationsused in dependency grammar
In beam search incomplete parses of an utterance are pruned or discarded when on some criterion they are significantly less plausible than other competing parses This pruning is fully interleaved with the parsing process 
McCord interleaved parsing with pruning in the same way as us but only compared constituents over the same span and with the same major category Our comparisons are more global and therefore can result in more effective pruning 
A more elaborate scheme is given in Samuelsson 1994b where the chunking criteria are learned automatically by an entropyminimization method the results however do not appear to improve on the earlier ones In both cases the coverage loss due to grammar specialization was about 10 to 12 using training corpora with about 5000 examples In practice this is still unacceptably high for most Applications 
LEXAS achieves a mean accuracy of 874 on this data set which is higher than the accuracy of 78 reported in Bruce and Wiebe 1994 
Unfortunately in the data set made available in the public domain there is no indication of which sentences are used as test sentences 
Early work on WSD such as Kelly and Stone 1975 Hirst 1987 used handcoding of knowledge to perform WSD The knowledge acquisition process is laborious
The work of Miller et al 1994 Leacock et al 1993 Yarowsky 1992 used only the unordered set of surrounding words to perform WSD and they used statistical classifiers neural networks or IRbased Techniques 
 However the POS used are abbreviated POS and only in a window of b2 words No local collocation knowledge is used A probabilistic classifier is used in Bruce and Wiebe 1994 
 However his work used decision list to perform classification in which only the single best disambiguating evidence that matched a target context is used In contrast we used exemplarbased learning where the contributions of all features are summed up and taken into account in coming up with a classification We also include verbobject syntactic relation as a feature which is not used in Yarowsky 1994 
The work of Miller et al 1994 is the only prior work we know of which attempted to evaluate WSD on a large data set and using the refined sense distinction of WORDNET However their results show no improvement in fact a slight degradation in performance when using surrounding words to perform WSD as compared to the most frequent heuristic 
The work of McRoy 1992 pointed out that a diverse set of knowledge sources are important to achieve WSD but no quantitative evaluation was given on the relative importance of each knowledge source No previous work has reported any such evaluation Either
The more similar conditions reported in previous work are those experiments performed on the WSJ corpus Brill 1992 reports 34 error rate and Daelemans et al 1996 report 967 accuracy We obtained a 9739 accuracy with tri grams plus automatically acquired constraints and 9745 when hand written constraints were added  
Since we are using a larger corpus than Padó et al 2007 who train on the BNC a fairer comparison might be the one with our alternative models that are all outperformed by DM by a large margin 
Unfortunately  this is not the case for such widely used MT evaluation metrics as BLEU  and NIST   applied the parser of  developed for English  to Czech  and found thatthe performance wassubstantially lower when compared to the results for English 
Hanks and  proposed using pointwise mutual information to identify collocations in lexicography  however  the method may result in unacceptable collocations for lowcount pairs Unlike   Smadja  1993  goes beyond the  twoword  limitation and deals with  collocations of arbitrary length  
2This can explain why previous attempts to use WordNet for generating sentencelevel paraphrases  were unsuccessful We have also illustrated that ASIA outperforms three other English systems   even though many of these use more input than just a semantic class name 
For unknown words  SCL gives a relative reduction in error of 195  over   even with 40000 sentences of source domain training data For comparison purposes  we revisit a fullygenerative Bayesian model for unsupervised coreference resolution recently introduced by   discuss its potential weaknesses and consequently propose three modifications to their model  Section 3  
Unfortunately  there is no straightforward generalization of the method of  to the two edge marginal problem Previous literature on GB parsing  Wehrli  1984  Sharp  1985    1986  Kuhns  1986  Abney  1986has not addressed the issue of implementation of the Binding theory  The present paper intends in part to fill this gap 
In general  these authors have found that existing lexicalized parsing models for English  do not straightforwardly generalize to new languages  this typically manifests itself in a severe reduction in parsing performance compared to the results for English 
This method was shown to outperform the class based model proposed in  and can thus be expected to discover better clusters of words Although a rich literature covers bootstrapping methods applied to natural language problems  several questions remain unanswered when these methods are applied to syntactic or semantic pattern acquisition 
It also differs from previous proposals on lexical acquisition using statistical measures such as  which either deny the prior existence of linguistic knowledge or use linguistic knowledge in ad hoc ways In pursuit of better translation  phrasebased models  havesignificantlyimprovedthe quality over classical wordbased models  
Several studies have shown that largemargin methods can be adapted to the special complexities of the task  However  the capacity of these algorithms to improve over stateoftheart baselines is currently limited by their lack of robust dimensionality reduction 
The 746  final accuracy on apartments is higher than any result obtained by   the highest is 741    higher than the supervised HMM results reported by Grenager et al One prominent constraint of the IBM word alignment models  is functional alignment  that is each target word is mapped onto at most one source word 
Although the first three are particular cases where N  1 andor M  1  the distinction is relevant  because most wordbased translation models  eg IBM models   can typically not accommodate general MN alignments 
Insideout alignments   such as the one in Example 13  can not be induced by any of these theories  in fact  there seems to be no useful synchronous grammar formalisms available that handle insideout alignments  with the possible exceptions of synchronous treeadjoining grammars   Bertsch and Nederhof  and generalized multitext grammars   which are all way more complex than ITG  STSG and  22   BRCG 
Bilexical contextfree grammars have been presented in  as an abstraction of language models that have been adopted in several recent realworld parsers  improving stateoftheart parsing accuracy  For the Penn Treebank   reports an accuracy of 966  using the Maximum Entropy approach  our much simpler and therefore faster HMM approach delivers 967  
Although such approaches have been employed effectively   there appears to remain considerable room for improvement In terms of alignment  this wordnumber difference means that multiword connections must be considered  a task which 334 Sue J Ker and Jason S Chang Word Alignment is beyond the reach of methods proposed in recent alignment works based on  Model 1 and 2 
2 Motivation and Prior Work While several authors have looked at the supervised adaptation case  there are less  and especially less successful  studies on semisupervised domain adaptation  
In particular  the model in  failed to generate punctuation  a deficiency of the model 1 Introduction Phrasebased translation models   which go beyond the original IBM translation models  1 by modeling translations of phrases rather than individual words  have been suggested to be the stateoftheart in statistical machine translation by empirical evaluations 
Table 2  Figures about clustering algorithms Algorithm  Sentences   Clusters SHAC 623 CHAC 217 QT 232 EM 416 In fact  table 2 shows that most of the clusters have less than 6 sentences which leads to question the results presented by  who only keep the clusters that contain more than 10 sentences 
These methods go beyond the original IBM machine translation models   by allowing multiword units  phrases  in one language to be translated directly into phrases in another language 1 Introduction Recent works in statistical machine translation  SMT  shows how phrasebased modeling  significantly outperform the historical wordbased modeling  
Our system outperforms competing approaches  including the standard machine translation alignment models  and the stateoftheart Cut and Paste summary alignment technique  For example  we would like to know that if a  JJ  JJ  7We also tried using word clusters  instead of POS but found that POS was more helpful 
This is because their training data  the Penn Treebank   does not fully annotate NP structure Although this Wikipedia gazetteer is much smaller than the English version used by  that has over 2000000 entries  it is the largest gazetteer that can be freely used for Japanese NER 
As the tagger of  can not tag a word lattice  we can not back off to this tagging Pointwise mutual information  PMI  is commonly used for computing the association of two terms   which is defined as  nullnullnull null null  null null nullnullnull nullnullnullnull  nullnull nullnull null null null nullnullnullnullnull However  we argue that PMI is not a suitable measure for our purpose 
Moreover  the parameters of the model must be estimated using averaged perceptron training   which can be unstable This method has the advantage that it is not limited to the model scaling factors as the method described in  They reported that their method is superior to BLEU  in terms of the correlation between human assessment and automatic evaluation 
While the idea of exploiting multiple news reports for paraphrase acquisition is not new  previous efforts  have been restricted to at most two news sources Furthermore  we provide a 638  error reduction compared to IBM Model 4  
With all but two formats IBIIG achieves better FZ  l rates than the best published result in  A maximum entropy approach has been applied to partofspeech tagging before   but the approach s ability to incorporate nonlocal and nonHMMtaggertype evidence has not been fully explored 
If we consider these probabilities as a vector  the similarities of two English words can be obtained by computing the dot product of their corresponding vectors2 The formula is described below  similarity  ei  ej   Nsummationdisplay k  1 p  ei fk  p  ej fk   3  Paraphrasing methods based on monolingual parallel corpora such as  can also be used to compute the similarity ratio of two words  but they dont have as rich training resources as the bilingual methods do 
32 Evaluation Metrics AER  Alignment Error Rate   is the most widely used metric of alignment quality  but requires goldstandard alignments labeled with surepossible annotations to compute  lacking such annotations  we can compute alignment fmeasure instead 
Clustering algorithms have been previously shown to work fairly well for the classification of words into syntactic and semantic classes   but determining the optimum number of classes for a hierarchical cluster tree is an ongoing difficult problem  particularly without prior knowledge of the item classification 
Although  there are various manualautomatic evaluation methods for these systems  eg  BLEU   these methods are basically incapable of dealing with an MTsystem and a wpMTsystem at the same time  as they have different output forms However  reordering models in traditional phrasebased systems are not sufficient to treat such complex cases when we translate long sentences  
Current treebased models that integrate linguistics and statistics  such as GHKM   are not able to generalize well from a single phrase pair Our syntacticrelationbased thesaurus is based on the method proposed by   although Hindle did not apply it to information retrieval 
Both Charniak  and Bikel  were trained using the goldstandard tags  as this produced higher accuracy on the development set than using  s tags The utility of ITG as a reordering constraint for most language pairs  is wellknown both empirically  and analytically   howeverITGsstraight  monotone  andinverted  reverse  rules exhibit strong cohesiveness  which is inadequate to express orientations that require gaps 
This latter point is a critical difference that contrasts to the major weakness of the work of  which uses a topN list of translations to select the maximum BLEU sentence as a target for training  so called local update  
Even the creators of BLEU point out that it may not correlate particularly well with human judgment at the sentence level  1 Introduction In recent years  various phrase translation approaches  have been shown to outperform wordtoword translation models  
Sentencelevel approximations to B exist   but we found it most effective to perform B computations in the context of a setOof previouslytranslated sentences  following Watanabe et al In such a process  original phrasebased decoding  does not take advantage of any linguistic analysis  which  however  is broadly used in rulebased approaches 
However  many of these models are not applicable to parallel treebanks because they assume translation units where either the source text  the target text or both are represented as word sequences without any syntactic structure  2 Statistical Word Alignment Statistical translation models  only allow word to word and multiword to word alignments 
Even the 3 A demo of the parser can be found at httplfgdemocomputingdcuielfgparserhtml creators of BLEU point out that it may not correlate particularly well with human judgment at the sentence level  Our method  extending this line of research with the use of labeled LFG dependencies  partial matching  and nbest parses  allows us to considerably outperform  highest correlations with human judgement  they report 0144 for the correlation with human fluency judgement  0202 for the correlation with human overall judgement   although it has to be kept in mind that such comparison is only tentative  as their correlation is calculated on a different test set 
Unlike wellknown bootstrapping approaches   EM and CE have the possible advantage of maintaining posteriors over hidden labels  or structure  throughout learning  bootstrapping either chooses  for each example  a single label  or remains completely agnostic 
1 Introduction Recent approaches to statistical machine translation  SMT  piggyback on the central concepts of phrasebased SMT  and at the same time attempt to improve some of its shortcomings by incorporating syntactic knowledge in the translation process 
For the results in this paper  we have used Pointwise Mutual Information  PMI  instead of IBM Model 1   since  found it to be as effective on Springer  but faster to compute Numbers in the table correspond to the percentage of experiments in which the condition at the head of the column was true  for example figure in the first row and first column means that for 989 percent of the language pairs the BLEU score for the bidirectional decoder was better than that of the forward decoder  proach   
This provides a compelling advantage over previous dependency language models for MT   whichusea5gramLMonlyduringreranking Experimental results indicate that our model outperforms  coreference model by a large margin on the ACE data sets and compares favorably to a modified version of their model 
This method was preferred against other related methods  like the one introduced in   since it embeds all the available semantic information existing in WordNet  even edges that cross POS  thus offering a richer semantic representation Both the global models  use fairly small training sets  and there is no evidence that their techniques will scale to larger data sets 
Our model improves the baseline provided by    i  accuracy is increased by creating a lexicalised PCFG grammar and enriching conditioning context with parent fstructure features  and  ii  coverage is increased by providing lexical smoothing and fuzzy matching techniques for rule smoothing 
While the model of  significantly outperforms the constrained model of   they both are well below the stateoftheart in constituent parsing By doing so we must emphasize that  as described in the previous section  the BLEU score was not designed to deliver satisfactory results at the sentence level   and this also applies to the closely related NIST score 
 focus on alignment and do not present MT results  while May and Knight  2007  takesthesyntacticrealignmentasaninputtoanEM algorithm where the unaligned target words are insertedintothetemplatesandminimumtemplatesare combinedintobiggertemplates  
But without the global normalization  the maximumlikelihood criterion motivated by the maximum entropy principle  is no longer a feasible option as an optimization criterion   word class   measures polarity using only adjectives  however in our approach we consider the noun  the verb  the adverb and the adjective content words While most parsing methods are currently supervised or semisupervised   they depend on handannotated data which are difficult to come by and which exist only for a few languages 
To analyze our methods on IV and OOV words  we use a detailed evaluation metric than Bakeoff 2006  which includes Foov and Fiv Surprisingly  although JESSCM is a simpler version of the hybrid model in terms of model structure and parameter estimation procedure  JESSCM provides Fscores of 9445 and 8803 for CoNLL00 and 03 data  respectively  which are 015 and 083 points higher than those reported in  for the same configurations 
We presented some theoretical arguments for not limiting extraction to minimal rules  validated them on concrete examples  and presented experiments showing that contextually richer rules provide a 363 BLEU point increase over the minimal rules of  
Our study also shows that the simulatedannealing algorithm  is more effective 1552 than the perceptron algorithm  for feature weight tuning By segmenting words into morphemes  we can improve the performance of natural language systems including machine translation  and information retrieval  
Statistical disambiguation such as  for PPattachment or  for generative parsing greatly improve disambiguation  but as they model by imitation instead of by understanding  complete soundness has to remain elusive 
It can be applied to complicated models such IBM Model4  By 17 0 10 20 30 40 50 60 70 80 90 100 10000 100000 1e06 1e07 Test Set Items with Translations  Training Corpus Size num words unigrams bigrams trigrams 4grams Figure 1 Percent of unique unigrams bigrams trigrams and 4grams from the Europarl Spanish test sentences for which translations were learned in increasingly large training corpora increasing the size of the basic unit of translation phrasebased machine translation does away with many of the problems associated with the original wordbased formulation of statistical machine translation Brown et al  1993
2 21 Word Alignment Adaptation Bidirectional Word Alignment In statistical translation models   only onetoone and moretoone word alignment links can be found The method was intended as a replacement for sentencebased methods  eg     which are very sensitive to noise 
This approach addresses the problematic aspects of both pure knowledgebased generation  where incomplete knowledge is inevitable  and pure statistical bag generation   where the statistical system has no linguistic guidance  In addition  the clustering methods used  such as HMMs and Browns algorithm   seem unable to adequately capture the semantics of MNs since they are based only on the information of adjacent words 
We preferred the loglikelihood ratio to other statistical scores  such as the association ratio  or   2  since it adequately takes into account the frequency of the cooccurring words and is less sensitive to rare events and corpussize  The ubiquitous minimum error rate training  MERT  approach optimizes Viterbi predictions  but does not explicitly boost the aggregated posterior probability of desirable ngrams  
However  work in that direction has so far addressed only parse reranking  
The combination is significantly better than  at a very high level  but more importantly  Shens results  currently representing the replicable stateoftheart in POS tagging  have been significantly surpassed also by the semisupervised Morce  at the 99  confidence level  
a timeconsuming process  Other statistical machine translation systems such as  and  also produce a tree a15 given a sentence a16 Their models are based on mechanisms that generate two languages at the same time  so an English tree a15 is obtained as a subproduct of parsing a16 However  their use of the LM is not mathematically motivated  since their models do not decompose into Pa4a5a2a9a8a3a10a6 and a12a14a4a5a3a7a6 unlike the noisy channel model 
WSD systems have been far more successful in distinguishing coarsegrained senses than finegrained ones   but does that approach neglect necessary meaning differences  Secondly  while most pronoun resolution evaluations simply exclude nonreferential pronouns  recent unsupervised approaches  must deal with all pronouns in unrestricted text  and therefore need robust modules to automatically handle nonreferential instances 
12Poon and Domingos  outperformed  As with similar work   the size of the corpus makes preprocessing such as lemmatization  POS tagging or partial parsing  too costly The size of the development set used to generate 1 and 2  compensates the tendency of the unsmoothed MERT algorithm to overfit  by providing a high ratio between number of variables and number of parameters to be estimated 
 have proposed a rulebased algorithm for sentence combination  but no results have been reported  provides anecdotal evidence that only incorrect alignments are eliminated by ITG constraints 
By segmenting words into morphemes  we can improve the performance of natural language systems including machine translation  and information retrieval  It has been difficult to identify all and only those cases where a token functions as a discourse connective  and in many cases  the syntactic analysis in the Penn TreeBank  provides no help 
For example  10 million words of the American National Corpus  will have manually corrected POS tags  a tenfold increase over the Penn Treebank   currently used for training POS taggers The process of phrase extraction is difficult to optimize in a nondiscriminative setting  many heuristics have been proposed   but it is not obvious which one should be chosen for a given language pair 
While several methods have been proposed to automatically extract compounds   we know of no successful attempt to automatically make classes of compounds Many approaches for POS tagging have been developed in the past  including rulebased tagging   HMM taggers   maximumentropy models   cyclic dependency networks   memorybased learning   etc All of these approaches require either a large amount of annotated training data  for supervised tagging  or a lexicon listing all possible tags for each word  for unsupervised tagging  
Our focus is on the sentence level  unlike  and   we employ a significantly larger set of seed words  and we explore as indicators of orientation words from syntactic classes other than adjectives  nouns  verbs  and adverbs  This additional conditioning has the effect of making the choice of generation rules sensitive to the history of the generation process  and  we argue  provides a simpler  more uniform  general  intuitive and natural probabilistic generation model obviating the need for CFGgrammar transforms in the original proposal of  
1 Introduction The most widely applied training procedure for statistical machine translation IBM model 4  unsupervised training followed by postprocessing with symmetrization heuristics  yields low quality word alignments This is in contrast to purely statistical systems   which are difficult to inspect and modify 
In addition  the semisupervised Morce performs  on single CPU and development data set  77 times faster than the combination and 23 times faster than  In a recent study by   nonlocal information is encoded using an independence model  and the inference is performed by Gibbs sampling  which enables us to use a stateoftheart factored model and carry out training efficiently  but inference still incurs a considerable computational cost  suggests use of an approximation summing over the training data  which does not sum over possible tags   h E f j  2 P    p  ti l hi  f j  hi  ti  i  1 However  we believe this passage is in error  such an estimate is ineffective in the iterative scaling algorithm 
IBM Model1  is a simplistic model which takes no account of the subtler aspects of language translation including the way word order tends to differ across languages A number of studies have investigated sentiment classification at document level  eg    and at sentence level  eg    however  the accuracy is still less than desirable 
Automatic evaluation methods such as BLEU   RED   or the weighted Ngram model proposed here may be more consistent in judging quality as compared to human evaluators  but human judgments remain the only criteria for metaevaluating the automatic methods For comparison purposes  we revisit  fullygenerative Bayesian model for unsupervised coreference resolution  discuss its potential weaknesses and consequently propose three modifications to their model 
The classbased kappa statistic of  can not be applied here  as the classes vary depending on the number of ambiguities per entry in the lexicon While the amount of parallel data required to build such systems is orders of magnitude smaller than corresponding phrase based statistical systems   the variety of linguistic annotation required is greater 
Although this method is comparatively easy to be implemented  it just achieves the same performance as the synchronous binarization method  for syntaxbased SMT systems Among the applications of collocational analysis for lexical acquisition are the derivation of syntactic disambiguation cues Basili et al 1991 1993a Hindle and Rooths 19911993 Sekine 1992 Bogges et al 1992 sense preference Yarowski 1992 acquisition of selectional restrictions Basili et al 1992b 1993b Utsuro et al 1993 lexical preference in generation Smadjia 1991 word clustering Pereira 1993 Hindle 1990 Basili et al 1993c etc In the majority of these papers even though the precedent or subsequent statistical processing reduces the number of accidental associations very large corpora 10000000 words are necessary to obtain reliable data on a large enough number of words
Due to limited variations in the NBest list  the nature of ranking  and more importantly  the nondifferentiable objective functions used for MT  such as BLEU    one often found only local optimal solutions to  with no clue to walk out of the riddles We also compare our performance against  and  and find that we manage higher relative improvement than existing work despite starting from a very competitive baseline CRF Although several methods have already been proposed to incorporate nonlocal features   these present a problem that the types of nonlocal features are somewhat constrained 
The morphological processing in PairClass  is more sophisticated than in  In addition  the performance of the adapted model for Joint ST obviously surpass that of   which achieves an F1 of 9341  for Joint ST  although with more complicated models and features Some are the result of inconsistency in labeling in the training data   which usually reflects a lack of linguistic clarity or determination of the correct part of speech in context 
Therefore  sublanguage techniques such as Sager  and  do not work Mutual information  though potentially of interest as a measure of collocational status  was not tested due to its wellknown property of overemphasising the significance of rare events  
While in traditional wordbased statistical models  the atomic unit that translation operates on is the word  phrasebased methods acknowledge the significant role played in language by multiword expressions  thus incorporating in a statistical framework the insight behind ExampleBased Machine Translation  
Note that the minimum error rate training  uses only the target sentence with the maximum posterior probability whereas  here  the whole probability distribution is taken into account A word order correlation bias  as well as the phrase structure biases in  Models 4 and 5  would be less beneficial with noisier training bitexts or for language pairs with less similar word order 
Although various approaches to SMT system combination have been explored  including enhanced combination model structure   better word alignment between translations  and improved confusion network construction   most previous work simply used the ensemble of SMT systems based on different models and paradigms at hand and did not tackle the issue of how to obtain the ensemble in a principled way 
1 Introduction The field of machine translation has seen many advances in recent years  most notably the shift from wordbased  to phrasebased models which use token ngrams as translation units  Although ITA rates and system performance both significantly improve with coarsegrained senses   the question about what level of granularity is needed remains three models in  are susceptible to the O  n 3  method  cf 
Even with the current incomplete set of semantic templates  the hypertagger brings realizer performance roughly up to stateoftheart levels  as our overall test set BLEU score  slightly exceeds that of   though at a coverage of 96  insteadof98  An alternative method  makes decisions at the end but has a high computational requirement 
Several teams had approaches that relied to varying degrees on an IBM model of statistical machine translation Brown et al  1993 with different improvements brought by different teams consisting of new submodels improvements in the HMM model model combination for optimal alignment etc Several teams used symmetrization metrics as introduced in Och and Ney 2003 union intersection refined most of the times applied on the alignments produced for the two directions sourcetarget and targetsource but also as a way to combine different word alignment systems
In the thriving area of research on automatic analysis and processing of product reviews   little attention has been paid to the important task studied here assessing review helpfulness In such tasks  feature calculation is also very expensive in terms of time required  huge sets of extracted rules must be sorted in two directions for relative frequency calculation of such features as the translation probability p  f e  and reverse translation probability p  e f   
Other statistical systems that address word classification probleans do not emphasize the use of linguistic knowledge and do not deal with a specific word class   or do not exploit as much linguistic knowledge as we do  As one can see in Table 4  the resulting parser ranks among the best lexicalized parsers  beating those of Collins  and Charniak and Johnson  8 Its F1 performance is a 27  reduction in error over  et al 
Our experiments on the Canadian Hansards show that our unsupervised technique is significantly more effective than picking seeds by hand   which in turn is known to rival supervised methods 2 Related Work One of the major problems with the IBM models  and the HMM models  is that they are restricted to the alignment of each sourcelanguage word to at most one targetlanguage word This implies that the complexity of structure divergence between two languages is higher than suggested in literature   report better perplexity results on the Verbmobil Corpus with their HMMbased alignment model in comparison to Model 2 of  
 have implemented a dependency parser with good accuracy  it is almost as good at dependency parsing as Charniak   and very impressive speed  it is about ten times faster than  and four times faster than Charniak   Unlike minimum error rate training   our system is able to exploit large numbers of specific features in the same manner as static reranking systems  
However  to what extent that assumption holds is tested only on a small number of language pairs using hand aligned data  Although evaluated on a different test set  our method also outperforms the correlation with human scores reported in  However  it seems unrealistic to expect a onesizefitsall approach to be achieve uniformly high performance across varied languages  and  in fact  it doesnt Though the system presented in  outperforms the best systems in the 2006 PASCAL challenge for Turkish and Finnish  it still does significantly worse on these languages than English  Fscores of 662 and 665  compared to 794  
For comparison purposes  three additional heuristicallyinduced alignments are generated for each system   1  Intersection of both directions  Aligner  int     2  Union of both directions  Aligner  union    and  3  The previously bestknown heuristic combination approach called growdiagfinal   Aligner  gdf   Brill s results demonstrate that this approach can outperform the Hidden Markov Model approaches that are frequently used for partofspeech tagging   as well as showing promise for other applications 
It has the advantage of naturally capturing local reorderings and is shown to outperform wordbased machine translation  Again the best result was obtained with IOB1  which is an imI  rovement of the best reported F    1 rate for this data set    9203  This is well illustrated by the Collins parser   scrutinized by Bikel  2004   where several transformations are applied in order to improve the analysis of noun phrases  coordination and punctuation 
It is faster and more mnemonic than the one in  1 Introduction Translations tables in Phrasebased Statistical Machine Translation  SMT  are often built on the basis of Maximumlikelihood Estimation  MLE   being one of the major limitations of this approach that the source sentence context in which phrases occur is completely ignored  
In what concerns the evaluation process  although ROUGE  is the most common evaluation metric for the automatic evaluation of summarization  since our approach might introduce in the summary information that it is not present in the original input source  we found that a human evaluation was more adequate to assess the relevance of that additional information 
The program takes the output of char_align   a robust alternative to sentencebased alignment programs  and applies wordlevel constraints using a version of Brown el al s Model 2   modified and extended to deal with robustness issues6 Conclusion Traditional approaches for devising parsing models  smoothing techniques and evaluation metrics are not well suited for MH  as they presuppose 13The lack of head marking  for instance  precludes the use of lexicalized models a la  
The experimental results show that our method outperforms the synchronous binarization method  with over 08 BLEU scores on both NIST 2005 and NIST 2008 ChinesetoEnglish evaluation data sets  produced a corpus of 4000 questions annotated with syntactic trees  and obtained an improvement in parsing accuracy for Bikels reimplementation of the Collins parser  by training a new parser model with a combination of newspaper and question data 
1 Introduction Statistical phrasebased systems  have consistently delivered stateoftheart performance in recent machine translation evaluations  yet these systems remain weak at handling word order changes With these linguistic annotations  we expect the LABTG to address two traditional issues of standard phrasebased SMT  in a more effective manner 
At the same time  we believe our method has advantages over the approach developed initially at IBM  for training translation systems automatically Despite relying on a the same concept  our approach outperforms BE in most comparisons  and it often achieves higher correlations with human judgments than the stringmatching metric ROUGE  
More recent work  has considered methods for speeding up the feature selection methods described in   Ratnaparkhi  1998   and Della Pietra  Della Pietra  and Lafferty  1 Introduction Currently  most of the phrasebased statistical machine translation  PBSMT  models  adopt full matching strategy for phrase translation  which means that a phrase pair  tildewidef  tildewidee  can be used for translating a source phrase f  only if tildewidef  f Due to lack of generalization ability  the full matching strategy has some limitations 
1 Introduction For statistical machine translation  SMT   phrasebased methods  and syntaxbased methods  outperform wordbased methods  To our knowledge no systems directly address Problem 1  instead choosing to ignore the problem by using one or a small handful of reference derivations in an nbest list   or else making local independence assumptions which sidestep the issue  
For example  the statistical word alignment in IBM translation models  can only handle word to word and multiword to word alignments Our graphical representation has two advantages over previous work   unifying sentence relations and incorporating question interactions 
 presented a historybased generation model to overcome some of the inappropriate independence assumptions in the basic generation model of  
Unfortunately  longer sentences  up to 100 tokens  rather than 40   longer phrases  up to 10 tokens  rather than 7   two LMs  rather than just one   higherorder LMs  order 7  rather than 3   multiple higherorder lexicalized reordering models  up to 3   etc all contributed to increased system  s complexity  and  as a result  time limitations prevented us from performing minimumerrorrate training  MERT   for ucb3  ucb4 and ucb5 
Unsupervised methods have been developed for WSD  but despite modest success have not always been well understood statistically  In addition  uniform conditioning on mother grammatical function is more general than the casephenomena specific generation grammar transform of   in that it applies to each and every subpart of a recursive input fstructure driving generation  making available relevant generation history  context  to guide local generation decisions 
Section 5 presents an error analysis for  lexicalized model  which shows that the headhead dependencies used in this model fail to cope well with the flat structures in Negra Even the creators of BLEU point out that it may not correlate particularly well with human judgment at the sentence level   a problem also noted by  and  
Methods like McDonalds  including the wellknown Maximal Marginal Relevance  MMR  algorithm   are subject to another problem  Summarylevel redundancy is not always well modeled by pairwise sentencelevel redundancy 
While other systems  such as   have addressed these tasks to some degree  OPINE is the first to report results Thirdly   deploys the dependency language model to augment the lexical language model probability be1183 tween two head words but never seek a full dependency graph 
METEOR was chosen since  unlike the more commonly used BLEU metric   it provides reasonably reliable scores for individual sentences Our method is a natural extension of those proposed in  and   and overcomes their drawbacks while retaining their advantages 
Another consequence of not generating posthead conjunctions and punctuation as firstclass words is that they 19 In fact  if punctuation occurs before the head  it is not generated at alla deficiency in the parsing model that appears to be a holdover from the deficient punctuation handling in the model of  
One conclusion that we can draw is that at present the additional word features used in  looking at words more than one position away from the current do not appear to be helping the overall performance of the models In comparison  the 2D model in Figure 2  c  used in previous work  can only model the interaction between adjacent questions 
The problem is typically presented in logspace  which simplifies computations  but otherwise does not change the problem due to the monotonicity of the log function  hm  log hprimem  log p  t s   summationdisplay m m hm  t  s   3  Phrasebased models  are limited to the mapping of small contiguous chunks of text Though taggers based on dependency networks   SVM   MaxEnt   CRF   and other methods may reach slightly better results  their traintest cycle is orders of magnitude longer 
Several papers have looked at higherorder representations  but have not examined the equivalence of synpara distributions when formalized as Markov chains  Of the methods we compare against  only the WordNetbased similarity measures    and  provide a method for predicting verb similarities  our learned measure widely outperforms these methods  achieving a 136  Fscore improvement over the LESK similarity measure 
In order to capture the dependency relationship between lexcial heads  breaks down the rules from head outwards  which prevents us from factorizing them in other ways Besides  our model  as being linguistically motivated  is also more expressive than the formally syntaxbased models of Chiang  and  
String alignment with synchronous grammars is quite expensive even for simple synchronous formalisms like ITG  but Duchi et al 1 Introduction Phrasebased systems  flat and hierarchical alike   have achieved a much better translation coverage than wordbased ones   but untranslated words remain a major problem in SMT 
Such a quasisyntactic structure can naturally capture the reordering of phrases that is not directly modeled by a conventional phrasebased approach  
Although generating training examples in advance without a working parser  is much faster than using inference   our training time can probably be decreased further by choosing a parsing strategy with a lower branching factor 
This cost can often be substantial  as with the Penn Treebank  2 Previous work on Sentiment Analysis Some prior studies on sentiment analysis focused on the documentlevel classification of sentiment  where a document is assumed to have only a single sentiment  thus these studies are not applicable to our goal 
Since Czech is a language with relatively high degree of wordorder freedom  and its sentences contain certain syntactic phenomena  such as discontinuous constituents  nonprojective constructions   which can not be straightforwardly handled using the annotation scheme of Penn Treebank   based on phrasestructure trees  we decided to adopt for the PCEDT the dependencybased annotation scheme of the Prague Dependency Treebank PDT  13  give an informal example  but do not elaborate on it 
Like WASP1  the phrase extraction algorithm of PHARAOH is based on the output of a word alignment model such as GIZA     which performs poorly when applied directly to MRLs  Section 32  
This restriction is necessary because the problem of optimizing manytomany alignments 5 Our preliminary experiments with ngrambased overlap measures  such as BLEU  and ROUGE   show that these metrics do not correlate with human judgments on the fusion task  when tested against two reference outputs 
Although the authors of  stated that they would discuss the search problem in a followup arti cle  so far there have no publications devoted to the decoding issue for statistical machine translation This strategy is commonly used in MT evaluation  because of BLEUs wellknown problems with documents of small size  
Our system improves over the latent namedentity tagging in   from 61  to 87  The generalized perceptron proposed by  is closely related to CRFs  but the best CRF training methods seem to have a slight edge over the generalized perceptron 2 Previous Work It is helpful to compare this approach with recent efforts in statistical MT Phrasebased models  are good at learning local translations that are pairs of  consecutive  substrings  but often insufficient in modeling the reorderings of phrases themselves  especially between language pairs with very different wordorder 
When compared to other kernel methods  our approach performs better than those based on the Tree kernel   and is only 02  worse than the best results achieved by a kernel method for parsing  Lexical relationships under the standard IBM models  do not account for manytomany mappings  and phrase extraction relies heavily on the accuracy of the IBM wordtoword alignment 
Despite ME theory and its related training algorithm  do not set restrictions on the range of feature functions1  popular NLP text books  and research papers  seem to limit them to binary features 4 Conclusions Compared with other word alignment algorithms   word_align does not require sentence alignment as input  and was shown to produce useful alignments for small and noisy corpora 
1 Introduction Statistical phrasebased systems  have consistently delivered state of the art performance in recent machine translation evaluations  yet these systems remain weak at handling word order changes 
Our approach not only outperformed a notoriously difficult baseline but also achieved similar performance to the approach of   without requiring their thirdparty data resources While we have shown an increase in performance over a purely syntactic baseline model  the algorithm of    there are a number of avenues to pursue in extending this work 
Presently  there exist methods for learning oppositional terms  and paraphrase learning has been thoroughly studied  but successfully extending these techniques to learn incompatible phrases poses difficulties because of the data distribution Formal complexity analysis has not been carried out  but my algorithm is simpler  at least conceptually  than the variablewordorder parsers of Johnson     and Abramson and Dahl  1989  
These scores are higher than those of several other parsers   but remain behind tim scores of Charniak  2000  who obtains 901  LP and 901  LR for sentences _  40 words In contrast to existing approaches   the context of the whole corpus rather than a single sentence is considered in this iterative  unsupervised procedure  yielding a more reliable alignment 
While minimum error training  has by now become a standard tool for interpolating a small number of aggregate scores  it is not well suited for learning in highdimensional feature spaces 
 and Collins and Duffy  2002  rerank the top N parses from an existing generative parser  but this kind of approach 1Dynamic programming methods  can sometimes be used for both training and decoding  but this requires fairly strong restrictions on the features in the model Ever since its introduction in general  and in computational linguistics   many researchers have pointed out that there are quite some problems in using  eg 
1 Introduction The dominance of traditional phrasebased statistical machine translation  PBSMT  models  has recently been challenged by the development and improvement of a number of new models that explicity take into account the syntax of the sentences being translated 1 Introduction Hierarchical approaches to machine translation have proven increasingly successful in recent years   and often outperform phrasebased systems  on targetlanguage fluency and adequacy 
While we do not have a direct comparison  we note that  performs worse on movie reviews than on his other datasets  the same type of data as the polarity dataset 
At any rate  regularized conditional loglinear models have not previously been applied to the problem of producing a high quality partofspeech tagger  Ratnaparkhi   Toutanova and Manning   and  all present unregularized models 
The most commonly used metric  BLEU  correlates well over large test sets with human judgments   but does not perform as well on sentencelevel evaluation  
We will show that some achieve significantly better results than the standard minimum error rate training of  Allomorphs  eg  deni and deny  are also automatically identified in   but the general problem of recognizing highly irregular forms is examined more extensively in  
Most recently   published their Semisupervised sequential labeling method  whose results on POS tagging seem to be optically better than   but no significance tests were given and the tool is not available for download  ie for repeating the results and significance testing 
By increasing the size of the basic unit of translation  phrasebased machine translation does away with many of the problems associated with the original wordbased formulation of statistical machine translation   in particular  The Brown et al  examine the FS of the weighted loglikelihood ratio  WLLR  on the movie review dataset and achieves an accuracy of 871   which is higher than the result reported by  with the same dataset 
While both  and  propose models which use the parameters of the generative model but train to optimize a discriminative criteria  neither proposes training algorithms which are computationally tractable enough to be used for broad coverage parsing Turneys method did not work well although they reported 80  accuracy in  
 tried a different generative phrase translation model analogous to IBM wordtranslation Model 3   and again found that the standard model outperformed their generative model The automatically generated patterns in PairClass are slightly more general than the patterns of  
Although previous work  has tackled the bootstrapping approach from both the theoretical and practical point of view  many key problems still remain unresolved  such as the selection of initial seed set Unlike   who found optimal performance when was approximately 104  we observed monotonic increases in performance as dropped 
While these approaches have had som e success to date   their usability as parsers in systems for natural language understanding is suspect 
2 Motivation and Prior Work While several authors have looked at the supervised adaptation case  there are less  and especially less successful  studies on semisupervised domain adaptation 
