We generate all the morphologically related forms of the word pair using a lexical transducer for English  Karttunen et al 1992
The Xerox tagger is claimed Cutting el al 1992 to be adaptable and easily trained only a lexicon and suitable amount of untagged text is required  
The disambiguation rules are similar to phonological rewrite rules Kaplan and Kay 1994 and the parsing algorithm is similar to the algorithm for combining the morphological rules with the lexicon Karttunen 1994 
We also tried combining the tuggers using first the rules and then the statistics a similar approach was also used in Tapanainen and Vouti Lainen 1994 
Kupiec 1992 has proposed an estimation method for the Ngram language model using the BaumWelch reestimation algorithm Rabiner et al 1994 from an untagged corpus and Cutting et al 1992 have applied this method to an English tagging systemTakeuchi and Matsumoto 1995 also have developed an extended method for unsegmented languages eg Japanese and applied it to their Japanese tagger  
Juman is a rulebased Japanese tagging system which uses handcoding cost values that represent the implausibility of morpheme connections and word and tagoccurences 
However at higher levels of tagging accuracy the reestimation m e t h o d based on the BaumWelch algorithm is limited by the noise of untagged corpora On this point I agree with Merialdo 1994 and Elworthy 1994 
In the bigram model we can weight each probability of a pair of tags in both models estimated from tagged or untagged corpora A smoothing method such as deleted interpolation Jelinek 1985 can be used for Weighting
A very influential is the work of Brill 1997 who induces more linguistically motivated rules exploiting both a tagged corpus and a lexicon He does not look at the affixes only but also checks their POS class in a lexicon 
In particular in our experiments we used the Large Grammatical Dictionary of Bulgarian Paskaleva2003 created at the Linguistic Modelling Department of the Bulgarian Academy of Sciences CLPPBAS and comprising approximately 995000 wordforms about 65000 lemmas encoded in DELAF format Silberztein1993 
Our approximate rules are similar to the ones proposed by Mikheev 1997 who uses a dictionary to build POS prediction rules with four parts 
Next it looks promising to try to estimate the dictionary word frequencies using a search engine instead of text corpus as proposed by Lapata and Keller 2004 
Finally we would like to explore the machine learning potential offered by morphological dictionaries with application to other related tasks such as stemming Nakov 2003 lemmatisation and POS tagging 
Och 2003 provides evidence that Λ should be chosen by optimizing an objective function based on the evaluation metric of interest rather than likelihood 
 Furthermore Callison Burch et al 2006 point out that it is not always appropriate to use BLEU to compare systems to each other 
Joshua is a hierarchical parsingbased MT system and it can be instructed to produce derivation trees instead of the candidate sentence string Itself 
The MT system we used is Joshua Li et al 2009 a software package that comes complete with a grammar extraction module and a MERT module in addition to the decoder itself 
On the other hand Snow et al 2008 illustrate how AMT can be used to collect data in a fast and cheap fashion for a number of NLP tasks such as word sense disambiguation 
The formalization of this notion and an algorithm for computing the composed transducer are wellknown and are described originally by Elgot and Mezei 1965
We empirically compared our tagger with Eric Brills implementation of his taggerand with our implementation of a trigram tagger adapted from the work of Church1988 that we previously implemented for another purpose
Independently Cutting et aL 1992 quote a performance of 800 words per secondfor their partofspeech tagger based on hidden Markov models
This compression is achievedwhile maintaining random access using a procedure for sparse data tables following the method given by Tarjan and Yao 1979
No pretagged text is necessary for Hidden Markov Models Jelinek 1985 Cutting et al 1991 Kupiec 1992 
Brill and Marcus 1992a have shown that the effort necessary to construct the partofspeech lexicon can be considerably re duced by combining learning procedures and a partial partofspeech categorization elicited from an informant 
We obtained 47025 50dimensional reduced vectors from the SVD and clustered them into 200 classes using the fast clustering algorithm Buckshot Cutting et al 1992
There is both synchronic Ross 1972 and diachronic Tabor 1994 evidence suggesting that words and their uses can inherit properties from several prototypical syntactic categories 
We use the English Slot GrammarESG parser developed at IBM McCord 1990 to analyze the syntactic structure of an input sentence and produce a sentence parse tree 
The same asymptotic complexityis of course found for memory storage in this approach 
The recursive algorithms for tree construction except the final pruning and retrieval are given in Figures 1 and 2 
A windowing approach Sejnowski  Rosenberg 1987 was used to represent the tagging task as a classification problem 
Again we take advantage of the data fusion capabilities of a memorybased approach by combining these two sources of information in the case representation and having the information gain feature relevance weighting technique figure out their relative relevance see Schmid 1994 Samuelsson 1994 for similar solutions 
The experimental methodology was taken from Machine Learning practice eg Weiss  Kulikowski 1991 independent training and test sets were selected from the original corpus the system was trained on the training set and the generalization accuracy percentage of correct category assignments was computed on the independent test set 
The above problems could be partially solved by introducing more resources into collocation extraction such as chunker Wermter and Hahn 2004 parser Lin 1998 Seretan and We hrli 2006 and WordNet Pearce 2001
We adapt the bilingual word alignment model IBM Model 3 Brown et al 1993 to monolingual word alignment 
We take BBNs HierDec a stringtodependency decoder as described in Shen et al 2008 as our baseline for the following two reasons 
Marton and Resnik 2008 introduced features defined on constituent labels to improve the Hiero system Chiang 2005 However due to the limitation of MER training only part of the feature space could used in the system 
A few studies Carpuat and Wu 2007 Ittycheriah and Roukos 2007 He et al 2008 Hasan et al 2008 addressed this defect by selecting the appropriate translation rules for an input span based on its context in the input sentence
The direct translation model in Ittycheriah and Roukos 2007 employed syntactic POS tags and context information neighboring words within a maximum entropy model to predict the correct transfer rules
Although some successful applications have been developed see for instance Chinchor 1998 implementing an automatic text analysis system is still a labour and time intensive task 
Gildea and Jurafsky 2002 were the first to describe a statistical system trained on the data from the FrameNet project to automatically assign semantic roles 
The best system Johansson and Nugues 2008 in CoNLL 2008 achieved an F1measure of 8165 on the workshops evaluation Corpus
To the best of our knowledge no system was able to reproduce the successful results of Swier and Stevenson 2004 on the PropBank Roleset 
Our approach most closely resembles the work of Fürstenau and Lapata 2009 who automatically expand a small training set using an automatic dependency alignment of unlabeled sentences
Fillmore 1968 introduced semantic structures called semantic frames describing abstract actions or common situations frames with common roles and themes semantic roles 
An alternative approach to semantic role labeling is the framework developed by Halliday 1994 and implemented by Mehay et al 2005 
To estimate the parameters of the MEMM+predmodel we turn to the successful Maximum Entropy Berger et al 1996 parameter estimation Method
The Distributional Hypothesis supported by theoretical linguists such as Harris 1954 states that words that occur in the same contexts tend to have similar meanings 
We employ a slightly different clustering method here the fullibmpredict method discussed in Goodman 2001 
Statistical systems have enjoyed considerable success for information retrieval especially using the vector space model Salton et al 1975
Our summarization system relies on semantic predications provided by SemRep Rindflesch and Fiszman 2003 a program that draws on UMLS information to provide underspecified semantic interpretation in the biomedical domain Srinivasan and Rindflesch 2002 Rindflesch et al 2000 
However due to the challenges in providing semantic representation semantic abstraction has not been widely pursued although the TOPIC system Hahn and Reimer 1999 is a notable exception 
Phase 4 saliency is the final transforma tion phase and its operations are adapted from TOPICs Hahn and Reimer 1999 saliency operators 
Subevents Daniel et al 2003 and subtopics Saggion and Lapalme 2002 also contribute to the framework used for comparing documents in multidocument summarization 
For example both Haghighi and Klein 2006 and Mann and McCallum 2008 have demonstrated results better than 661 on the apartments task described above using only a list of 33 highly discriminative features and the labels they indicate 
In traditional active learning Settles 2009 the machine queries the user for only the labels of instances that would be most helpful to the machine 
In this paper we advocate using generalized expectation GE criteria Mann and McCallum 2008 for learning with labeled features  
Computing the first term of the covariance in Equation 2 requires a marginal distribution over three labels two of which will be consecutive but the other of which could appear anywhere in the sequence 
Motivated by the feature query selection method of Tandem Learning Raghavan and Allan 2007 see Section 42 for further discussion we consider a feature selection metric similarity sim that is the maximum similarity to a labeled feature weighted by the log count of the feature 
Liang et al 2009 simultaneously developed a method for learning with and actively selecting measurements or target expectations with associated noise 
Chang et al 2007 only obtain better results than 882 on cora when using 300 labeled examples two hours of estimated annotation time 5000 additional unlabeled examples and extra test time inference constraints 
Concurrent work has used approximate counting schemes based on Morris 1978 to estimate in small space frequencies over a high volume input text stream Van Durme and Lall 2009 Goyal et al 2009
Church et al 2007 looked at Golomb Coding and Brants et al 2007 used tries in a distributed setting These methods are less succinct than randomised approaches 
Much progress in the area of semantic role labeling is due to the creation of resources like FrameNet Fillmore et al 2003 which document the surface realization of semantic roles in real world corpora 
Specifically we view the task of inferring annotations for new verbs as an instance of a structural matching problem and follow a graphbased formulation for pairwise global network alignment Klau 2009 
Previous work has mainly used WordNet Fellbaum 1998 to extend FrameNet For example Burchardt et al 2005 apply a word sense disambiguation system to annotate predicates with a WordNet sense and hyponyms of these predicates are then assumed to evoke the same frame 
In this paper we generalize the proposals of Pennacchiotti et al 2008 and Fürstenau and Lapata 2009 in a unified framework
FrameNet provides definitions for more than 500 frames of which we entertain only a small number 
We solve this optimization problem with a version of the branchandbound algorithm Land and Doig 1960  
Our evaluation assessed the performance of a semantic frame and role labeler with and without the annotations produced by our method 
We extracted features from dependency parses corresponding to those routinely used in the semantic role labeling literature see Baker et al 2007 for an overview 
The algorithm is again described by Cutting et al and by Sharman and a mathematical justification for it can be tbund in Huang et al 1990 
The first major use of HMMs for part of speech tagging was in CLAWS Garside et al 1987 in the 1970s 
One of the most effective taggers based on a pure HMM is that developed at Xerox Cutting et al 1992 
For example CLAWS Garside ct al 1987 normalises the lexical probabilities by the total frequency of the word rather than of the tag 
In Riley 1989 Riley describes a decisiontree based approach to the problem 
There has now been considerable work on discourse parsing using statistical bottomup parsing Soricut and Marcu 2003 hierarchical agglomerative clustering Sporleder and Lascarides 2004 parsing from lexicalized treeadjoining grammars Cristea 2000 and rulebased approaches that use rhetorical relations and discourse cues Forbes et al 2003 Polanyi et al 2004 LeThanh et al 2004 
In addition to the handcrafted models listed above researchers have built stochastic plan recognition models for interaction including ones based on Hidden Markov Models Bui 2003 Blaylock and Allen 2006 and on probabilistic contextfree grammars Alexandersson and Reithinger 1997 Pynadath and Wellman 2000 
This encoding was previously used for incremental sentence parsing by Costa et al 2001 
We use the machine learning toolkit LLAMA Haffner 2006 which encodes multiclass classication problems using binary MaxEnt classifiers to increase the speed of training and to scale the method to large data sets 
We note that these results are competitive with those reported in the literature eg Poesio and Mikheev 1998 Serafin and Eugenio 2004 although the dialog corpus and the label sets are different 
The expectation semiring Eisner 2002 originally proposed for finitestate machines is one such training semiring and can be used to compute feature expectations for the Estep of the EM algorithm or gradients of the likelihood function for gradient descent  
We implement the expectation and variance semirings in Joshua Li et al 2009a and demonstrate their practical benefit by using minimumrisk training to improve Hiero Chiang 2007 
A much more efficient approach usually is the traditional insideoutside algorithm Baker 1979 
If entropy Hp is large eg small γ the Bayes risk Thus Smith and Eisner 2006 try to avoid local minima by starting with large Hp and decreasing it gradually during optimization 
We used a 5gram language model with modified KneserNey smoothing trained on the bitexts English using SRILM Stolcke 2002 
MR with or without DA is scalable to tune a large number of features while MERT is not 
Clearly adding more features improves statistically significant the case with only five features 
Our approach is theoretically elegant like other work in this vein Goodman 1999 Lopez 2009 Gimpel and Smith 2009 
Research on Statistical Machine Translation SMT has shown substantial progress in recent years 
The second aspect motivating our work comes from the subspace learning method in machine learning literature Ho 1998 in which an ensemble of classifiers are trained on subspaces of the full feature space and final classification results are based on the vote of all classifiers in the Ensemble 
Nowadays most of the stateoftheart SMT systems are based on linear models as proposed in Och and Ney 2002 
Since we also adopt a linear scoring function in Equation 3 the feature weights of our combination model can also be tuned on a development a set to optimize the specified evaluation metrics using the standard Minimum Error Rate Training MERT algorithm Och 2003 
Our method is similar to the work proposed by Hildebrand and Vogel 2008  
Statistical significance is computed using the bootstrap resampling method proposed by Koehn 2004  
A number of partofspeech taggers are readilyavailable and widely used all trained and retrainable on text corpora Church 1988Cutting et al 1992 Brill 1992 Weischedel et al 1993
Endemic structural ambiguitywhich can lead to such difficulties as trying to cope with the many thousands of possible parses that a grammar can assign to a sentence can be greatly reduced by addingempirically derived probabilities to grammar rules Fujisaki et al 1989 Sharman Jelinek and Mercer 1990 Black et al 1993 and by computing statistical measures oflexical association Hindle and Rooth 1993
Aneffort has recently been undertaken to create automated machine translation systemsin which the linguistic information needed for translation is extracted automaticallyfrom aligned corpora Brown et al 1990
There are a number of efforts worldwide to manually annotate largecorpora with linguistic information including parts of speech phrase structure andpredicateargument structure eg the Penn Treebank and the British National CorpusMarcus Santorini and Marcinkiewicz 1993 Leech Garside and Bryant 1994
Useful tools such as large aligned corpora eg the aligned Hansards Galeand Church 1991 and semantic word hierarchies eg Wordnet Miller 1990 havealso recently become available
Partofspeech tagging is an activearea of research a great deal of work has been done in this area over the past few yearseg Jelinek 1985 Church 1988 Derose 1988 Hindle 1989 DeMarcken 1990 Merialdo1994 Brill 1992 Black et al 1992 Cutting et al 1992 Kupiec 1992 Charniak et al 1993Weischedel et al 1993 Schutze and Singer 1994
Also it ispossible to cast a number of other useful problems as partofspeech tagging problemssuch as lettertosound translation Huang SonBell and Baggett 1994 and buildingpronunciation networks for speech recognition
Roche and Schabes 1995 show a method for converting a listof tagging transformations into a deterministic finite state transducer with one statetransition taken per word of input the result is a transformationbased tagger whosetagging speed is about ten times that of the fastest Markovmodel tagger
In this respect the present work is closer in spirit to Ji et al 2005 who explore the employment of the ACE 2004 relation ontology as a semanticfilter 
For learning coreference decisions we used a Maximum Entropy Berger et al 1996 model 
First a set of preprocessing components including a chunker and a named entity recognizer is applied to the text in order to identify the noun phrases which are further taken as REs to be used for instance generation
Following Ng  Cardie 2002 our baseline system reimplements the Soon et al 2001 system 
Systems were optimized on the WMT08 French nglish development data 2000 sentences using minimum error rate training Och 2003 and tested on the WMT08 test data 2000 sentences 
All other settings were left at their default values as described by Chiang 2007 and Koehn et al 2007
Zhang et al 2008 and Wellington et al 2006 answer the question what is the minimal grammar that can be induced to completely describe a training set? 
One such problem is sense disambiguationIn the context of machine translation Dagan and Itai Dagan Itai and Schwall 1991Dagan and Itai 1994 used corpora in the target language to resolve ambiguities inthe source language Y
Most successfulmethods have followed speech recognition systems Jelinek Mercer and Roukos 1992and used large corpora to deduce the probability of each part of speech in the currentcontext usually the two previous wordstrigrams 
One good example for this is fulltext retrieval systems Choueka 1980 Suchsystems must handle the morphological ambiguity problem
Ornan 1986 for instance developed a new writing system for Hebrew calledThe Phonemic Script 
Choueka andLusignan 1985 presented a system for the morphological tagging of large texts that isbased on the short context of the word but also depends heavily on human interaction
Methods using the short context of a word in order to resolve ambiguity usually categorical ambiguity are very common in English and other languages DeRose1988 Church 1988 Karlsson 1990 
Such a system that usesthe morpholexical probabilities together with a syntactic knowledge is described inLevinger 1992
The combined system tackles the disambiguation problem by combining two kindsof linguistic information sources MorphoLexical Probabilities and Syntactic Constraints a full description of this system can be found in Levinger [1992]
Previous results had shown a rather satisfying performance for hybrid systems such as the Statistical Phrasebased PostEditing SPE Simard et al 2007 combination in comparison with purely phrasebased statistical models reaching similar BLEU scores and often receiving better human judgement German to English at WMT2007 against the BLEU metric 
They are part of an effort to better integrate a linguistic rulebased system and the statistical correcting layer also illustrated in Ueffing et al 2008 
In order to push further this ruleextraction approach and according to our previous work Dugast et al 2007 Dugast et al 2008 the most promising would probably be the use of alternative meanings and a language model to decode the best translation in such a lattice 
Similarly to classical NLP tasks such as base noun phrase chunking Ramshaw and Marcus 1994 text chunking Ramshaw and Marcus 1995 or named entity recognition Tjong Kim Sang 2002 we formulate the mention detection problem as a classification problem by assigning to each token in the text a label indicating whether it starts a specific mention is inside a specific mention or is outside any mentions  
In contrast in a rule based system the system designer would have to consider how for instance a WordNet Miller 1995 derived information for a particular example interacts with a partofspeechbased information and chunking information 
Instead of a wordbased model we build a characterbased one since word segmentation errors can lead to irrecoverable mention detection errors Jing et al 2003 also observe that characterbased models are better performing than wordbased ones for Chinese named entity recognition 
The core of the approach is a novel decoder based on lattice parsing with quasisynchronous grammar Smith and Eisner 2006 a syntactic formalism that does not require source and target trees to be isomorphic
Lopez 2009 recently argued for a separation between featuresformalisms and the independence assumptions they imply from inference algorithms in MT this separation is widely appreciated in machine learning
We define a single direct loglinear translation model Papineni et al 1997 Och and Ney 2002 that encodes most popular MT features and can be used to encode any features on source and target sentences dependency trees and alignments
We exploit similar approximate inference methods in regularized pseudolikelihood estimation Besag 1975 with hidden variables to discriminatively and efficiently train our model
Phrasebased systems such as Moses Koehn et al 2007 explicitly search for the highestscoring string in which all source words are translated
Recently Chiang 2007 introduced cube pruning as an approximate decoding method that extends a DP decoder with the ability to incorporate features that break the Markovian independence assumptions DP exploits 
We recently proposed cube summing an approximate technique that permits the use of nonlocal features for inside DP algorithms Gimpel and Smith 2009 
For our targetlanguage syntactic features g syn  we  use features similar to lexicalized CFG events collins 1999 specifically following the dependency model of Klein and Manning 2004
Previous research has addressed revision in singledocument summaries [Jing  McKeown 2000] [Mani et al 1999] and has suggested that revising summaries can make them more informative and correct errors
Rhetorical Structure Theory RST [Mann  Thompson 1988] has contributed a great deal to the understanding of the discourse of written Documents 
Inspired by RST [Radev 2000] endeavored to establish a Crossdocument Structure Theory CST that is more appropriate for MDS 
Based on RST [Marcu 2000] established a Rhetorical Parser The parser exploits cue phrases in an algorithm that discovers discourse relationships between phrases in a text 
[Mani et al 1999] focused on the revision of singledocument summaries in order to improve their Informativeness They noted that such revision might also fix ‘coherence errors 
To contrast [Jing  McKeown 2000] concentrated on analyzing humanwritten summaries in order to determine how professionals construct summaries 
[Filatova  Hovy 2001] addressed the issue of resolving temporal references in news stories 
To the best of our knowledge and with the exception of Saggion and Lapalme 2002 indicative generation approach which included operations to add extra linguistic material to generate an indicative abstract the work presented here is the first to investigate this relevant operation in the field of text abstracting and to propose a robust computational method for its simulation 
One could rely on existing trainable sentence selection Kupiec et al 1995 or even phrase selection Banko et al 2000 strategies to pick up appropriate β i s from the document to be abstracted and rely on recent information ordering techniques to sort the β i fragments Lapata 2003 
The features used for the experiments reported here are inspired by previous work in text summarization on content selection Kupiec et al 1995 rhetorical classification Teufel and Moens 2002 and information ordering Lapata 2003 
Note that in this work we have decided to evaluate the predicted structure against the true structure a hard evaluation measure in future work we will assess the abstracts with a set of quality questions similar to those put forward by the Document Understanding Conference Evaluations also in a way similar to Kan and McKeown 2002 who evaluated their abstracts in a retrieval environment
The insertion in the abstract of linguistic material not present in the input document has been addressed in paraphrase generation Barzilay and Lee 2004 and cannedbased summarization Oakes and Paice 2001 in limited domains  
Saggion and Lapalme 2002 have studied and implemented a rulebased verb selection operation in their SumUM system which has been applied to introduce document topics during indicative summary generation 
In this paper we develop the first unsupervised approach to semantic parsing using Markov logic Richardson and Domingos 2006 
Manually encoding all these variations into the grammar is tedious and errorprone Supervised semantic parsing addresses this issue by learning to construct the grammar automatically from sample meaning annotations Mooney 2007 
In many NLP applications there exist rich relations among objects and recent work in statistical relational learning Getoor and Taskar 2007and structured prediction Bakir et al 2007 has shown that leveraging these can greatly improve Accuracy 
One of the most powerful representations for this is Markov logic which is a probabilistic extension of firstorder logic Richardson and Domingos 2006
In particular lexical entries are no longer limited to be adjacent words as in Zettlemoyer and Collins 2005 but can be arbitrary fragments in a dependency tree 
We used the GENIA dataset Kim et al 2003 as the source for knowledge extraction 
The closest available system to USP in aims and capabilities is TextRunner Banko et al 2007 and we compare with it 
The analysis presented here and the idea of the alignments havebeen greatly influenced by the exploration of abstracting manuals Cremmins 1982
We measure coselection between sentences produced by each methodand the sentences selected by the assessors computing recall precision and Fscoreas in Firmin and Chrzanowski 1999 
Maximum Entropy MaxEnt principle has been successfully applied in many classification and tagging tasks Ratnaparkhi 1996 K Nigam and AMcCallum 1999 A McCallum and Pereira 2000 
To discover useful features we exploit the concept of Association Rules AR R Agrawal and Swami 1993 Srikant and Agrawal 1997 which is originally proposed in Data Mining research field to identify frequent itemsets in a large Database
Many researchers Blum and Mitchell 1998 K Nigam and Mitchell 2000 Corduneanu and Jaakkola 2002 have attempted to improve performance with unlabeled data 
There is a growing amount of work on automatic extraction of paraphrases from text corpora Lin and Pantel 2001 Barzilay and Lee 2003 Ibrahim et al 2003 Dolan et al 2004 
For more abstract matching we would need syntacti cally parsed data Lin and Pantel 2001 We ex pect that this would also positively affect the cov Erage
Our generation strategy is reminiscent of Robinand McKeowns 1996 earlier work on revision for summarization although Robin andMcKeown used a threetiered representation of each sentence including its semanticsand its deep and surface syntax all of which were used as triggers for revision
This approach originally proposed by Knight andHatzivassiloglou 1995 and Langkilde and Knight 1998 is a standard method usedin statistical generation
While this approach exploits only syntactic and lexical information Jing andMcKeown 2000 also rely on cohesion information derived from word distribution ina text
In addition to reducing the original sentences Jing andMcKeown 2000 use a number of manually compiled rules to aggregate reducedsentences for example reduced clauses might be conjoined with and
The only other texttotext generation approach able to produce new utterances isthat of Pang Knight and Marcu 2003
Our algorithmperforms local alignment while the algorithm of Pang Knight and Marcu 2003performs global alignment
Second our method is an instance of a multisequence alignment 15 in contrast to thepairwise alignment described in Meyers Yangarber and Grishman 1996
In fact previous applications of multisequence alignment have beenshown to increase the accuracy of the comparison in other NLP tasks Barzilay andLee 2002 Bangalore Murdock and Riccardi 2002 Lacatusu Maiorano and Harabagiu2004 unlike our work these approaches operate on strings not trees and with theexception of Lacatusu Maiorano and Harabagiu 2004 they apply alignment to parallel data not comparable texts
Recent research Daume et al 2002 has show that syntaxbased languagemodels are more suitable for language generation tasks the study of such models isa promising direction to explore
Second we devised a method to obtain the expected word Ngram count in the target texts using an Nbest word segmentation algorithm Nagata 1994 
In English part of speech taggers the maximization of Equation 1 to get the most likely tag sequence is accomplished by the Viterbi algorithm Church 1988 and the maximum likelihood estimates of the parameters of Equation 2 are obtained from untagged corpus by the Forward Backward algorithm Cutting et al 1992 
Nbest word segmentation hypotheses can be obtained by using the ForwardDP Backward A* algorithm Nagata 1994
Chang et al 1995 proposed an automatic dictionary construction method for Chinese from a large unsegmented corpus 311591 sentences with the help of a small segmented seed corpus 1000 Sentences 
This is achieved by introducing an explicit statistical model of unknown words and by using an N best word segmentation algorithm Nagata 1994as an approximation of the generalized Forward Backward algorithm
We trained a 5gram language model from the provided English monolingual training data and the nonEuroparl portions of the parallel training data using modified KneserNey smoothing as implemented in the SRI language modeling toolkit Kneser and Ney 1995 Stolcke 2002
To tune the feature weights of our system we used a variant of the minimum error training algorithm Och 2003 that computes the error statistics from the target sentences from the translation search space represented by a packed forest that are exactly those that are minimally discriminable by changing the feature weights along a single vector in the dimensions of the feature space Macherey et al 2008
To construct the segmentation lattices we define a loginear model of compound word segmentation inspired by Koehn and Knight 2003 making use of features including number of morphemes hypothesized frequency of the segments as freestanding morphemes in a training corpus and letters in each segment 
To address this we explicitly generate beginning and end sentence markers as part of the translation process as sug gested by Xiong et al 2008 The results of doing this are shown in Table 2 
In our experiments we treated the 128 most frequent words in the corpus as function words similar to Setiawan et al 2007
In support of this processing we rely on the linguistic and domain knowledge contained in the National Library of Medicines Unified Medical Language System UMLS ® as well an existing tool the SPECIALIST minimal commitment parser Aronson et al 1994 
In order to identify binding terminology in text we rely on the approach discussed in Rindfiesch et al 1999
ARBITER pursues limited coordination identification in the spirit of Agarwal and Boggess 1992 and Rindflesch 1995 Only binding terms are considered as candidates for coordination
For training in the English experiments we used WSJ Marcus et al 1993 We had to change the format of WSJ to prepare it for our tagging software
A simple rulebased part of speech RBPOS tagger is introduced in Brill 1992 The accuracy of this tagger for English is comparable to a stochastic English POS tagger 
Before trying some completely different approach we would like to improve the current simple approach by some other simple measures adding a morphological analyzer Hajji 1994 as a frontend to the tagger serving as a supplier of possible tags instead of just taking all tags occurring in the training data for a given token simplifying the tagset adding more data 
To select these we use the idea of strong chains introduced by Barzilay and Elhadad 1997 
Hybrid approaches such as extracting phrases instead of sentences and recombining these phrases into salient text have been proposed Barzilay McKeown and Elhadad 1999 
Other recent work looks at summarization as a process of revision in this work the source text is revised until a summary of the desired length is achieved Mani Gates and Bloedorn 1999 Additionally some research has explored cutting and pasting segments of text from the full document to generate a summary Jing and McKeown 2000 
Exploiting the maximum entropy Berger et al 1996 framework the conditional distribution Pre a | f  can be determined through suitable real valued functions called features h  and takes the parametric Form 
This prerocessing step can be accomplished by applying the GIZA++ toolkit Och and Ney 2003 that provides Viterbi alignments based on IBM Model4 
In general phrases are extracted with maximum length in the source and target defined by the parameters J max and I max  All such phrasepairs are efficiently computed by an 2 algorithm with complexity OlI max J max  Cet tolo et al 2005 
Thanks to the nice property of kernelbasedmachine learning method that can implicitly explore structured features in a high dimensional feature space Vapnik 1995 in this paper we propose using convolution tree kernel Haussler 1999 Collins and Duffy 2001 to explore the structured syntactic knowledge for phrase reordering and further combine the tree kernel with other diverse linear features into a composite kernel to strengthen the models predictive ability
We use the MaxEntbased BTG translation system Xiong et al 2006 as our baseline It is a phrasebased SMT system with BTG reordering constraint
Thus it is critical to understand which portion of a parse tree ie structured feature space is the most effective to represent a reordering instance Motivated by the work of Zhang et al 2006 we here examine four cases that contain different substructures as shown in Fig 1 
We generate the boundary word features from the extracted reordering instances in the same way as discussed in Xiong et al 2006 and use Zhangs MaxEnt Tools 2 to train a reordering model for the 2 nd baseline system
Nevertheless since the seminal work of Hobbs et al 1993 it has been possible to conceptualize pragmatic interpretation as a unified reasoning process that selects a representation of the speakers contribution that is most preferred according to a background model of how speakers tend to behave 
We continue a tradition of research that uses simple referential communication tasks to explore the organization and processing of humanc mputer and mediated human–human conversation including recently DeVault and Stone 2007 Gergle et al 2007 Healey and Mills 2006 Schlangen and Fernández 2007 
Our specific task is a two player objectidentification game adapted from the experiments of Clark and WilkesGibbs 1986 and Brennan and Clark 1996 
We give a brief sketch here to highlight the content of COREFs representations the sources of information that COREF uses to construct them and the demands they place on disambiguation See DeVault 2008 for full details 
In the second step the selected features were used to train the model to estimate probabilities We used MALLETs implementation of Limited memory BFGS Nocedal 1980 
To quantify the performance of the learned model in comparison to our baseline we adapt the mean reciprocal rank statistic commonly used for evaluation in information retrieval Vorhees 1999 
The first serious linguistic competitor to datadriven statistical taggers is the English Constraint Grammar parser EngCG cf Voutilainen et al 1992 Karlsson et al eds 1995 
However Voutilainen and Jarvinen 1995 empirically show that an interjudge agreement virtually of 10 is possible at least with the EngCG tag set if not with the original Brown Corpus tag set 
By varying the threshold we can perform a recallprecision or errorrateambiguity tradeoff A similar strategy is adopted in de Mar Cken 1990 
The results from CoNLL shared tasks in 2005 and 2008 Carreras and Marquez 2005 Koomen et al 2005 Surdeanu et al 2008 Johansson and Nugues 2008 further show that SRL pipeline may be one of the standard to achieve a stateoftheart performance in practice 
As for the former hereafter it is referred to synPth we continue to use a dependency version of the pruning algorithm of Xue and Palmer 2004 The pruning algorithm is readdressed as the following 
One is goldstandard syntactic input and other two are based on automatically parsing results of two parsers the stateoftheart syntactic parser described in Johansson and Nugues 2008 7 it is referred to Johansson and an integrated parser described as the following referred to MSTME 
The parser is basically based on the MSTParser 8 using all the features presented by McDonald et al 2006 with projective parsing Moreover we exploit three types of additional features to improve the parser 
For the sake of efficiency we use a fast transition based parser based on maximum entropy as in Zhao and Kit 2008 We still use the similar feature notations of that work 
As mentioned by Pradhan et al 2004 argument identification plays a bottleneck role in improving the performance of a SRL system The effectiveness of the proposed additional pruning techniques may be seen as a significant improvement over the original algorithm of Xue and Palmer 2004 
Titov et al 2009 reported the best result by using joint learning technique up to now The comparison indicates that our integrated system outputs a result quite close to the stateoftheart by the pipeline system of Johansson and Nugues 2008 as the same syntactic structure input is adopted 
Recent comparisons of approaches that can be trained on corpora van Halteren et al 1998 Volk and Schneider 1998 have shown that in most cases statistical aproaches Cutting et al 1992 Schmid 1995 Ratnaparkhi 1996 yield better results than finite state rulebased or memorybased taggers Brill 1993 Daelemans et al 1996
the method of handling unknown words that seems to work best for inflected languages is a suffix analysis as proposed in Samuelsson 1993 Tag probabilities are set according to the words endIng
 uilding on a recent proposal in this direction by Turney 2008 we propose a generic method of this sort and we test it on a set of unrelated tasks reporting good performance across the board with very little taskspecific tweaking 
Turney 2008 is the first to the best of our knowledge to raise the issue of a unified approach In particular he treats synonymy and association as special cases of relational similarity 
Mirkinet al 2006 also integrate information from the lexical patterns in which two words cooccur and similarity of the contexts in which each word occurs on its own to improve performance in lexical entailment Acquisition 
The first task we evaluated our algorithm on is the SAT analogy questions task introduced by Turney et al 2003
We adopt a similar approach to the one used in Turney 2008 and consider each question as a separate binary classification problem with one positive training instance and 5 unknown pairs 
Specifically we test selectional preferences on the dataset constructed by Padó 2007 that collects average plausibility judgments from 20 speakers for nouns as either subjects or objects of verbs 211 nounverb pairs 
Thus corpusbased approaches may have serious difficulties in capturing these relations Havasi et al 2007 but there are reasons to believe that they could still be useful Eslick 2006 uses the assertions of ConceptNet as seeds to parse Web search results and augment ConceptNet by new candidate relations
Fortunately using distributional characteristics of term contexts it is feasible to induce partofspeech categories directly from a corpus of sufficient size as several papers have made clear Brown et al 1992 Schütze 1993 Clark 2000 
There are many applications of computational linguistics particularly those involving shallow processing such as information extraction which can benefit from such automatically derived information especially as research into acquisition of grammar matures eg Clark 2001
Our approach to inducing syntactic clusters is closely related to that described in Brown et al 1992 which is one of the earliest papers on the subject 
We seek to find a partition of the vocabulary that maximizes the mutual information between term categories and their contexts We achieve this in the framework of information theoretic coclustering Dhillon et al 2003 in which a space of entities on the one hand and their contexts on the other are alternately clustered in a way that maximizes mutual information between the two spaces
We conducted experiments with the Reuters21578 Corpus a relatively tiny one for such experiments Clark 2000 reports results on a corpus containing 12 million terms Schütze 1993 on one containing 25 million terms and Brown et al 1992 on one containing 365 million terms In contrast we count approximately 28 million terms in Reuters21578
Schütze  1993 1995proposes two distinct methods by which ambiguity may be resolved 
Most stateoftheart system combination methods are based on constructing a confusion network CN from several input translation hypotheses and choosing the best output from the CN based on several scoring functions eg Rosti et al 2007a He et al 2008 Matusov et al 2008
Prior work has used a number of heuristics to deal with these problems Matusov et al 2006 He et al 08 Some work has made such decisions in a more principled fashion by computing modelbased scores Matusov et al 2008 but still special purpose algorithms and heuristics are needed and a single alignment is fixed
Other than confusionnetworkbased algorithms work most closely related to ours is the method of MT system combination proposed in Jayaraman and Lavie 2005 which we will refer to as JL
If one of the two words is ε the posterior of aligning word ε to state j is computed as suggested by Liang et al 2006
Raw text is processed by a preprocessor which segments the text into sentences using various heuristics about punctuation and then to kenizes and runs it through a widecoverage high performance morphological analyzer developed using twolevel morphology tools by Xerox Kart Tunen 1993 
However the use of regular relations and finite state transducers Kaplan and Kay 1994 provide a very efficient implementation Method 
We have recently completed a prototype implementation of this approach in C for English Brown Corpus and have obtained quite similar results Tiir Of lazer and Ozkan 1997 
The convenience of adding new rules in without worrying about where exactly it goes in terms of rule ordering something that hampered our progress in our earlier work on disambiguating Turkish morphology Oflazer and KuruSz 1994 Oflazer and Tiir 1996 has also been a key positive point 
This has been quite useful for our work on tagging English Tfir Oflazer and 0zkan 1997 where such rules with negative weights were used to fine tune the behavior of the tagger in various prob lematic cases 
While many text categorization models have been proposed so far in this paper we concentrate on the probabilistic models Robertson and Sparck Jones 1976 Kwok 1990 Fuhr 1989 Lewis 1992 Croft 1981 Wong and Yao 1989 Yu et al 1989 because these models have solid formal grounding in probability theory 
Robertson and Sparck Jones 1976 make use of the wellknown logistic or logodds transformation of the probability Pc]d 
In general frequently appearing terms in a document play an important role in information retrieval Salton and McGill 1983 Salton and Yang experimentally verified the importance of withindocument term frequencies in their vector model Salton and Yang 1973 
A wellknown remedy for this problem is to use r + 05R + 1 as the estimate of PT = lie  Robertson and Sparck Jones 1976 While various smoothing methods Church and Gale 1991 Jelinek 1990 are also applicable to these situations and would be expected to work better we used the simple add one remedy in the following experiments
To solve problems 1 and 2 of PRW Kwok 1990 stresses the assumption that a document consists of terms This theory is called the Component Theory CT 
Fuhr 1989 solves problem 2 by assuming that a document is probabilistically indexed by its term vectors This model is called Retrieval with Probabilistie Indexing RPI 
There are several strategies for assigning categories to a document based on the probability Pcld  The simplest one is the kperdoc strategy Field 1975 that assigns the top k categories to each document 
We have to compare our probabilistic model to other non probabilistic models like decision treerule based models one of which has recently been reported to be promising Apt4 et al 1994 
While we used simple document representation in which a document is defined as a set of nouns there could be considered several improvements such as using phrasal information Lewis 1992 clustering terms Sparck Jones 1973 reducing the number of features by using local dictionary Apt4 et al 1994 etc 
If we need to generate summaries that can be used to indicative what topics are addressed in the original document and thus can be used to alert the uses as the source content ie the indicative function Mani et al 1999 extraction approach is capable of handling this kind of tasks 
A comprehensive survey of text summarization approaches can be found in Mani 1999 We briefly review here based on extraction approach Luhn 1959 proposed a simple but effective approach by using term frequencies and their related positions to weight sentences that are extracted to form a summary
The first known supervised learning algorithm was proposed by Kupiec et al 1995 Their approach estimates the probability that a sentence should be included in a summary given its feature values based on the independent assumption of Bayes Rule 
In order to determine word boundaries we employed the longest matching algorithm Sornlertlamvanich 1993 
We further describe an efficient approach to alleviate this problem by using an idea of phrase construction Ohsawa et al 1998 
Our approach is reminiscent of Luhns approach 1959 but uses the other term weighting technique instead of the term frequency Luhn suggested that the frequency of a word occurrence in a document as well as its relative position determines its significance in that document 
More recent works have also employed Luhns approach as a basis component for extracting relevant sentences Buyukkokten et al2001 Lam desina and Jones 2001  
In our work we decide to use TLTF Term Length Term Frequency term weighting technique Banko et al 1999 for scoring words in the document instead of TFIDF 
The recent approach for editing extracted text spans Jing and McKeown 2000 may also produce improvement for our algorithm
This assumption underlies a growing number of recent syntactic theories which give up the contextfree constituent backbone cf McCawley 1987 Dowty 1989 Reape 1993 Kathol and Pollard 1995 
Jing and McKeown 1999 2000 found that human summarization can be traced back to six cutandpaste operations of a text and proposed a revision method consisting of sentence reduction and combination modules with a sentence extraction part Mani and colleagues 1999 proposed a summarization system based on draft and revision together with sentence extraction The revision part is achieved with the sentence aggregation and smoothing modules 
To ameliorate this revision of the extracted sentences is also thought to be effective and many ideas and methods have been proposed so far For example Otterbacher and colleagues 2002 analyzed manually revised extracts and factored out cohesion problems Nenkova 2008 proposed a revision idea that utilizes noun coreference with linguistic quality improvements in mind
Barzilay and McKeown 2005 proposed an idea called sentence fusion that integrates information in overlapping sentences to produce a non overlapping summary sentence 
Their algorithm was further modified and applied to the German biographies by Filippova and Strube 2008
Like the work of Jing and McKeown 2000 and Mani et al 1999 our work was inspired by the summarization method used by human abstractors
Identifying our coreferential chunks is even harder than the conventional coreference resolution and we made a simplifying assumption as in Nenkova 2008 with some additional conditions that were obtained through our preliminary experiments 
The paper presents an application of Structural Correspondence Learning SCL Blitzer et al 2006 for domain adaptation of a stochastic attributevalue grammar SAVG So far SCL has been applied successfully in NLP for PartofSpeech tagging and Sentiment Analysis Blitzer et al 2006 Blitzer et al 2007 
Studies on the supervised task have shown that straightforward baselines eg models based on source only target only or the union of the data achieve a relatively high performance level and are surprisingly difficult to beat Daumé III 2007  
While several authors have looked at the supervised adaptation case there are less and especially less successful studies on semisupervised domain adaptation McClosky et al 2006 Blitzer et al 2006 Dredze et al 2007 
The few studies on adapting disambiguation models Hara et al 2005 Plank and van Noord 2008 have focused exclusively on the supervised scenario
We examine the effectiveness of Structural Correspondence Learning SCL Blitzer et al 2006 for this task a recently proposed adaptation technique shown to be effective for PoS tagging and Sentiment Analysis
Alpino van Noord and Malouf 2005 van Noord 2006 is a robust computational analyzer for Dutch that implements the conceptual twostage parsing Approach 
Pivots are features occurring frequently and behaving similarly in both domains Blitzer et al 2006 They are inspired by auxiliary problems from Ando and Zhang 2005
This allows us to get a possibly noisy but more abstract representation of the underlying data The set of features used in Alpino is further described in van Noord and Malouf 2005
In practice there are more free parameters and model choices Ando and Zhang 2005 Ando 2006 Blitzer et al 2006 Blitzer 2008 besides the ones discussed above
Due to the positive results in Ando 2006 Blitzer et al 2006 include this in their standard setting of SCL and report results using block SVDs only 
If we want to compare the performance of disambiguation models we can employ the φ mesure van Noord and Malouf 2005 van Noord 2007 Intuitively it tells us how much of the disambiguation problem has been solved 
Church 1992 claims that partofspeech taggers depend almost exclusively on lexical probabilities whereas other researchers such as Voutilainen Karlsson et al 1995 argue that word ambiguities vary widely in function of the specific text and genre  
Given the problems created by estimating probabilities on a corpus of restricted size we present in Section 4 a solution for coping with these difficulties
Brill 1995 presents a rulebased partofspeech tagger for unsupervised training corpus  
Summarization of such texts requires a different approach from for example that used in the summarization of news articles  
Metadiscourse is ubiquitous in scientific writing Hyland 1998 found a metadiscourse phrase on average after every 15 words in running text 
Paice 1990 introduces grammars for pattern matching of indicator phrases eg the aimpurpose of this paperarticlestudy and we concludepropose 
This heterogeneity is in stark contrast to the systematic structures Liddy 1991 found to be produced by professional abstractors  
A simpler machine learning approach using only word frequency information and no other features as typically used in tasks like text classification could have been employed and indeed Nanba and Okumura [1999] do so for classifying citation Contexts 
For the task of dialogue act disambiguation Samuel Carberry and VijayShanker 1999 suggest a method of automatically finding cue phrases for disambiguation 
Resnik and Diab 2000 present yet other measures of verb similarity which could be used to arrive at a more datadriven definition of verb classes 
We also plan to consider reasonable applications for semantic tagging One possibility would be to use semantic tagging in the framework of an intelligent on line dictionary lookup such as LocoLex [Bauer et al 1995] 
The learning corpus can consist of plain text but the best results seem achievable with annotated corpora Merialdo 1994 Elworthy 1994 
With respect to computational morphology witness for instance the success of the TwoLevel paradigm introduced by Koskenniemi 1983 
Before proceeding further with the main argument consider three very recent hybrids – systems that employ linguistic rules for resolving some of the ambiguities before using automatically generated corpusbased information collocation matrices Leech Garside and Bryant 1994 Hidden Markov Models Tapanainen and Voutilai nen 1994 or syntactic patterns Tapanainen and Jairvinen 1994   
Functional representation of phrases and clauses has been introduced to facilitate expressing syntactic generMisations The representation is introduced in Voutilainen and Tapanainen 1993 Voutilainen 1994 here only the main characteristics are given 
A small error ratehas been achieved by such systems when a restricted applicationdependent POS setis used eg an error rate of 26 percent has been reported by Marcus Santorini andMarcinkiewicz 1993 using the Penn Treebank corpus
Recently several solutions to the problem of tagging unknown words have beenpresented Charniak et al 1993 Meteer Schwartz and Weischedel 1991
Hypothesesfor unknown words both stochastic Dermatas and Kokkinakis 1993 1994 Malteseand Mancini 1991 Weischedel et al 1993 and connectionist Eineborg and Gamback1993 Elenius 1990 have been applied to unlimited vocabulary taggers
Various interpolation techniques have been proposed for the estimationof the model parameters for unseen events or to smooth the modelparameters Church and Gale 1991 Essen and Steinbiss 1992 Jardinoand Adda 1993 Katz 1987 McInnes 1992
For more details on the linguistic specifications of the annotation scheme see Skut et al 1997
Experience gained from the development of the Penn Treebank Marcus et al 1994 has shown that automatic annotation is useful only if it is absolutely correct while wrong analyses are often difficult to detect and their correction can be timeconsuming 
The work reported here is a logical continuation of two specific strands of research aimed in this general direction The first is the popular idea of statistical tagging eg DeRose 1988 Cutting et al 1992 Church 1988 
In the specific case of partofspeech tagging it is wellknown DeMarcken 1990 that a large proportion of the incorrect tags can be eliminated safely ie with very low risk of eliminating correct tags 
This part of the paper is essentially an extension and generalization of the line of work described in Rayner 1988 Rayner and Samuelsson 1990 Samuelsson and Rayner 1991 Rayner and Samuels son 1994 Samuelsson 1994b 
The result is a specialized grammar this has a larger number of rules but a simpler structure allowing it in practice to be parsed very much more quickly using an LR based method Samuelsson 1994a 
In the second phase the resulting set of chunked rules is converted into LR table form using the method of Samuelsson 1994a 
The experiment was carried out using both the chunking criteria from Rayner and Samuelsson 1994 the Old scheme and the chunking criteria described in Section 3 above the New scheme 
Preliminary experiments we have carried out on the Swedish version of the CLE Gambaick and Rayner 1992 have been encouraging using exactly the same pruning methods and EBL chunking criteria as for English we obtain comparable speedups 
We intend to do so soon and also to repeat the experiments on the French version of the CLE Rayner Carter and Bouillon 1996 
Our approach has been fully implemented in the program LExAs Part of the implementation uses PEBLS Cost and Salzberg 1993 Rachlin and Salzberg 1993 a public domain exemplarbased learning system 
This metric for measuring distance is adopted from Cost and Salzberg 1993 which in turn is adapted from the value difference metric of the earlier work of Stanfill and Waltz 1986 
One line of research focuses on the use of the knowledge contained in a machinereadable dictionary to perform WSD such as Wilks et al 1990 Luk 1995 
Most recently Yarowsky used an unsupervised learning procedure to perform WSD Yarowsky 1995 although this is only tested on disambiguating words into binary coarse sense distinction 
Our point of departure is the work of Lappin and Leass 1994 henceforth LL and Dagan et al 1995 See also Dagan and Itai 1990  
Dagan et al 1995 then developed a postprocessor based on predicateargument statistics that was used to override RAPs decision when it failed to express a clear preference between two or more antecedents which resulted in a modest rise in performance 25 
As previously indicated the weightbased scheme of LL suggests MaxEnt modeling Berger et al 1996 as a particularly natural choice for a machine learning approach 
We took two approaches to smoothing First because Dagan et al used GoodTuring smoothing in their experiments we did likewise so as to replicate their work as closely as possible Second we tried an approach based on the distributional clustering method of Pereira et al 1993  
To address the datasparsity issue we employed the technique used in Keller and Lapata 2003 KL to get a more robust approximation of predicateargument counts 
We describe a POS tagger based on the work described in Padr6 1996that is able to use bitrigram information automatically learned context constraints and linguistically motivated manually written constraints
The usual solutions to this problem are l Prune the tree either during the construction process Quinlan 1993 or afterwards Mingers 1989 2 Smooth the conditional probability distributions using fresh corpus a Magerman 1996 
In a first step the tree is completely expanded and afterwards it is pruned following a minimal costcomplexity criterion Breiman et al 1984 
Consider more complex context features such as nonlimited distance or barrier rules in the style of Samuelsson et al 1996 
This accuracy compares very favourably with results reported in de Marcken 1990 Weisehedel et al 1993 Kempe 1994  for instance to reach the recall of 993  the system by Weischedel et al 1993 has to leave as many as three readings per word in its output 
The tagger is reported Cutting el al 1992 to have a better than 96  accuracy in the analysis of parts of the Brown Corpus The accuracy is similar to other probabilistic taggers  
Only in the analysis of a few words it was agreed that a multiple choice was appropriate because of different meaninglevel interpretations of the utterance these were actually headings where some of the grammatical information was omitted
We could leave the text partly disambiguated and use a syntactic parser that uses both linguistic knowledge and corpusbased heuristics see Tapanainen and Jrvinen 1994 
Turney 2008 recently advocated the need for a uniform approach to corpusbased semantic tasks Turney recasts a number of semantic challenges in terms of relational or analogical similarity
Our approach to selectional preference is nearly identical to the one of Padó et al 2007 We solve SAT analogies with a simplified version of the method of Turney 2006 
Finally our method to detect verb slot similarity is analogous to the slot overlap of Joanis et al 2008 and others 
We use the dataset of Rubenstein and Good enough 1965 consisting of 65 noun pairs rated by 51 subjects on a 04 similarity scale eg car automobile 39 cordsmile 00 
Following Padó and Lapata 2007 we use Pearsons r to evaluate how the distances cosines in the CxLC space between the nouns in each pair correlate with the ratings 
Syntactic alterations Levin 1993 represent a key aspect of the complex constraints that shape the syntaxsemantics interface  
In particular we need to develop a backoff strategy for unseen pairs in the relational similarity tasks that following Turney 2006 could be based on constructing surrogate pairs of taxonomically similar words found in the CxLC space 
We plan to explore how contextual effects can be modeled in our framework focusing in particular on how composition affects word meaning Erk and Padó 2008 
Alternatively DM could be represented as a threemode tensor in the framework of Turney 2007 enabling smoothing operations analogous to singular value decomposition 
In Statistical Machine Translation  SMT   recent work shows that WSD helps translation quality when the WSD system directly uses translation candidates as sense inventories  Most semiautomated approaches have met with limited success  and supervised learning models have tended to outperform dictionarybased classi cation schemes  
While studies have shown that ratings of MT systems by BLEU and similar metrics correlate well with human judgments   
Incremental topdown and leftcorner parsers have been shown to effectively  and efficiently  make use of nonlocal features 
4 Extended Minimum Error Rate Training Minimum error rate training  is widely used to optimize feature weights for a linear model  When we run our classifiers on resourcetight environments such as cellphones 
In recent several years  the system combination methods based on confusion networks developed rapidly   
It is based on Incremental Sigmoid Belief Networks  ISBNs   a class of directed graphical model for structure prediction problems recently proposed in   where they were demonstrated to achieve competitive results on the constituent parsing task 
To solve this problem  we adopt an idea one sense per collocation which was introduced in word sense disambiguation research  The results show that  as compared to BLEU  several recently proposed metrics such as Semanticrole overlap  
For English  after a relatively big jump achieved by   we have seen two significant improvements   and  pushed the results by a significant amount each time 
However  evaluations on the widely used WSJ corpus of the Penn Treebank  show that the accuracy of these parsers still lags behind the stateoftheart 
22 Maximum Entropy Models Maximum entropy  ME  models   also known as 928 loglinear and exponential learning models  provide a general purpose machine learning technique for classification and prediction which has been successfully applied to natural language processing including part of speech tagging  named entity recognition etc 
The use of dependencies in MT evaluation has not been extensively researched before  one exception here would be    and requires more research to improve it  but the method shows potential to become an accurate evaluation metric 
Albeit simple  the algorithm has proven to be very efficient and accurate for the task of parse selection  It is an online training algorithm and has been successfully used in many NLP tasks  such as POS tagging   parsing   Chinese word segmentation   and so on 
We wish to minimize this error function  so we select accordingly  argmin summationdisplay a E  a   a   argmax a p  a  f e     4  Maximizing performance for all of the weights at once is not computationally tractable  but  has described an efficient onedimensional search for a similar problem For FrenchEnglish translation we use a state of the art phrasebased MT system similar to  
In agreement with recent results on parsing with lexicalised probabilistic grammars   our main result is that statistics over lexical features best correspond to independently established truman intuitive preferences and experimental findings 
It is interesting to note that while the study of how the granularity of contextfree grammars CFG affects the performance of a parser 
The former term P  E  is called a language model  representing the likelihood of E The latter term P  J E  is called a translation model  representing the generation probability from E into J As an implementation of P  J E   
The best previous result is an accuracy of 56   Introduction Hierarchical approaches to machine translation have proven increasingly successful in recent years   and often outperform phrasebased systems  on targetlanguage fluency and adequacy 
Throughout  the likelihood ratio  is used as significance measure because of its stable performance in various evaluations  yet many more measures are possible 
However  the study of  provides interesting insights into what makes a good distributional similarity measure in the contexts of semantic similarity prediction and language modeling While  does not discuss distinguishing more than 2 senses of a word  there is no immediate reason to doubt that the  one sense per collocation  rule  would still hold for a larger number of senses 
Similaritybased smoothing  provides an intuitively appealing approach to language modeling which is the classic work on collocation extraction  uses a twostage filtering model in which  in the first step  ngram statistics determine possible collocations and  in the second step  these candidates are submitted to a syntactic valida7Of course  lexical material is always at least partially dependent on the domain in question 
One of the most effective taggers based on a pure HMM is that developed at Xerox  
The success of recent highquality parsers  relies on the availability of such treebank corpora Synchronous binarization  solves this problem by simultaneously binarizing both source and targetsides of a synchronous rule  
 reported very high results  96  on the Brown corpus  for unsupervised POS tagging using Hidden Markov Models  HMMs  by exploiting handbuilt tag dictionaries and equivalence classes 
All the enumerated segment pairs are listed in the following table  We use Dunnings method  because it does not depend on the assumption of normality and it allows comparisons to be made between the signiflcance of the occurrences of both rare and common phenomenon 
3 Extending Bleu and Ter with Flexible Matching Many widely used metrics like Bleu  and Ter  are based on measuring string level similarity between the reference translation and translation hypothesis  just like Meteor Most of them  however  depend on finding exact matches between the words in two strings 
Promising features might include those over source side reordering rules  or source context features  
The BLEU metric  and the closely related NIST metric  along with WER and PER 48 have been widely used by many machine translation researchers 
A later study  found that performance increased to 872  when considering only those portions of the text deemed to be subjective 
Introduction We have seen rapid recent progress in machine translation through the use of rich features and the development of improved decoding algorithms  often based on grammatical formalisms If we view MT as a machine learning problem  features and formalisms imply structural independence assumptions  which are in turn exploited by efficient inference algorithms  including decoders  
Although stateoftheart statistical parsers  are more accurate  the simplicity and efficiency of deterministic parsers make them attractive in a number of situations requiring fast  lightweight parsing  or parsing of large amounts of data 
 has been unable to find real examples of cases where hierarchical alignment would fail under these conditions  at least in fixedwordorder languages that are lightly inflected  such as English and Chinese  p 385  
Introduction Phrasebased method  and syntaxbased method  represent the stateoftheart technologies in statistical machine translation  SMT  Introduction Phrasebased translation  and hierarchical phrasebased translation  are the state of the art in statistical machine translation  SMT  techniques 
Although the Kappa coefficient has a number of advantages over percentage agreement  eg  it takes into account the expected chance interrater agreement  see  for details   we also report percentage agreement as it allows us to compare straightforwardly the human performance and the automatic methods described below  whose performance will also be reported in terms of percentage agreement 
Recent innovations have greatly improved the efficiency of language model integration through multipass techniques  such as forest reranking   local search   and coarsetofine pruning  
In terms of applying nonparametric Bayesian approaches to NLP   evaluated the clustering properties of DPMMs by performing anaphora resolution with good results 
They have been successfully applied in several tasks  such as information retrieval  and harvesting thesauri  showed that the results for FrenchEnglish were competitive to stateoftheart alignment systems 
Another kind of popular approaches to dealing with query translation based on corpusbased techniques uses a parallel corpus containing aligned sentences whose translation pairs are corresponding to each other  
The corpus based statistical parsing community has many fast and accurate automated parsing systems  including systems produced by   Charniak  997  and Ratnaparkhi  997  
Headlexicalized stochastic grammars have recently become increasingly popular  Unsupervised algorit ~ m ~ such as  have reported good accuracy that rivals that of supervised algorithms is one of the most famous work that discussed learning polarity from corpus 
This method  initially proposed by   was successfully evaluated in the context of the SENSEVAL framework  Classifier Training 
 Recently  several successful attempts have been made at using supervised machine learning for word alignment  Loglikelihood ratio The loglikelihood ratio statistic has been found to be accurate for modeling the associations between rare events  
In contrast  the idea of bootstrapping for relation and information extraction was first proposed in   and successfully applied to the construction of semantic lexicons   named entity recognition   extraction of binary relations   and acquisition of structured data for tasks such as Question Answering  Perceptronbased training 
To tune the parameters w of the model  we use the averaged perceptron algorithm  because of its efficiency and past success on various NLP tasks  
But in fact  the issue of editing in text summarization has usually been neglected  notable exceptions being the works by  and Mani  Gates  and Bloedorn  999  We also use Cube Pruning algorithm  to speed up the translation process 
An important contribution to interactive CAT technology was carried out around the TransType  TT  project  
22 ITG Space Inversion Transduction Grammars  or ITGs  provide an efficient formalism to synchronously parse bitext More recently   have proposed methods for automatically extracting from a corpus heads that correlate well with discourse novelty 
A key component of the parsing system is a Maximum Entropy CCG supertagger  which assigns lexical categories to words in a sentence  
The most widely used method for building phrase translation tables  selects  from a word alignment of a parallel bilingual training corpus  all pairs of phrases  up to a given length  that are consistent with the alignment 
In the SMT research community  the second step has been well studied and many methods have been proposed to speed up the decoding process  such as nodebased or spanbased beam search with different pruning strategies  and cube pruning  
A promising approach may be to use aligned bilingual corpora  especially for augmenting existing lexicons with domainspecific terminology  
To facilitate comparisons with previous work   we used the trainingdevelopmenttest partition defined in the corpus and we also used the automaticallyassigned part of speech tags provided in the corpus0 Czech word clusters were derived from the raw text section of the PDT 0  which contains about 39 million words of newswire text 
This kind of corpus has served as an extremely valuable resource for computational linguistics applications such as machine translation and question answering   and has also proved useful in theoretical linguistics research  
For the IBM models defined by a pioneering paper   a decoding algorithm based on a lefttoright search was described in  
In the supervised setting  a recent paper by  shows that  using a very simple feature augmentation method coupled with Support Vector Machines  he is able to effectively use both labeled target and source data to provide the best results in a number of NLP tasks Constraining learning by using document boundaries has been used quite effectively in unsupervised word sense disambiguation  
His results may be improved if more sophisticated methods and larger corpora are used to establish similarity between words  Introduction Statistical parsing models have been shown to be successful in recovering labeled constituencies  and have also been shown to be adequate in recovering dependency relationships  
Introduction Recent works in statistical machine translation  SMT  shows how phrasebased modeling  significantly outperform the historical wordbased modeling    
2 Parsing Model The Berkeley parser  is an efficient and effective parser that introduces latent annotations  to refine syntactic categories to learn better PCFG grammars 
This is a common technique in machine translation for which the IBM translation models are popular methods  
There are only a few successful studies  such as  for chunking and  on constituency parsing 
To reduce the knowledge engineering burden on the user in constructing and porting an IE system  unsupervised learning has been utilized  eg Riloff   Yangarber et al 
Recently socalled reranking techniques  such as maximum entropy models  and gradient methods   have been applied to machine translation  MT   and have provided significant improvements 
Previous work for English  has shown that lexicalization leads to a sizable improvement in parsing performance Tighter integration of semantics into the parsing models  possibly in the form of discriminative reranking models   is a promising way forward in this regard 
The creation of the Penn English Treebank   a syntactically interpreted corpus  played a crucial role in the advances in natural language parsing technology  
For our experiments  we chose GIZA    and the RA approach  the best known alignment combination technique as our initial aligners 42 TBL Templates Our templates consider consecutive words  of size   2 or 3  in both languages 
2 Lexicalized parse trees The first successful work on syntactic disambiguation was based on lexicalized probabilistic contextfree grammar  LPCFG   
 Introduction IBM Model   is a wordalignment model that is widely used in working with parallel bilingual corpora 
Such methods have also been a key driver of progress in statistical machine translation  which depends heavily on unsupervised word alignments  
 improves the F score from 882  to 897   while Charniak and Johnson  2005  improve from 903  to 94  
In   anotherstateoftheartWSDengine  acombination of naive Bayes  maximum entropy  boosting and Kernel PCA models  is used to dynamically determine the score of a phrase pair under consideration and  thus  let the phrase selection adapt to the context of the sentence 
The creation of the Penn English Treebank   a syntactically interpreted corpus  played a crucial role in the advances in natural language parsing technology  for English 
Nowadays  most of the stateoftheart SMT systems are based on bilingual phrases  
Global information is known to be useful in other NLP tasks  especially in the named entity recognition task  and several studies successfully used global features  
All stateoftheart widecoverage parsers relax this assumption in some way  for instance by  i  changing the parser in step  3   such that the application of rules is conditioned on other steps in the derivation process   or by  ii  enriching the nonterminal labels in step    with contextinformation   along with suitable backtransforms in step  4  
Support Vector Machines  SVMs   and Maximum Entropy  ME  method  are powerful learning methods that satisfy such requirements  and are applied successfully to other NLP tasks  
Among these techniques  SCL  Structural Correspondence Learning   is regarded as a promising method to tackle transferlearning problem Several generalpurpose offtheshelf  OTS  parsers have become widely available  
The best example of such an approach is   who proposes a method that automatically identifies collocations that are indicative of the sense of a word  and uses those to iteratively label more examples 
An especially wellfounded framework for doing this is maximum entropy  Stateofart systems for doing word alignment use generative models like GIZA    
Far from full syntactic complexity  we suggest to go back to the simpler alignment methods first described by  
Because it is not feasible here to have humans judge the quality of many sets of translated data  we rely on an array of well known automatic evaluation measures to estimate translation quality  BLEU  is the geometric mean of the ngram precisions in the output with respect to a set of reference translations 
22 Unsupervised Parameter Estimation We can perform maximum likelihood estimation of the parameters of this model in a similar fashion to that of Model 4   described thoroughly in  
To overcome this problem  unsupervised learning methods using huge unlabeled data to boost the performance of rules learned by small labeled data have been proposed recently     
Moreover  the deterministic dependency parser of Yamada and Matsumoto   when trained on the Penn Treebank  gives a dependency accuracy that is almost as good as that of  and Charniak  2000  
David McClosky  Eugene Charniak  and Mark Johnson Brown Laboratory for Linguistic Information Processing  BLLIP  Brown University Providence  RI 0292  dmcc ec mj   csbrownedu Abstract Selftraining has been shown capable of improving on stateoftheart parser performance  despite the conventional wisdom on the matter and several studies to the contrary  
We conclude with some challenges that still remain in applying proactive learning for MT 2 Syntax Based Machine Translation In recent years  corpus based approaches to machine translation have become predominant  with Phrase Based Statistical Machine Translation  PBSMT   being the most actively progressing area  
 demonstrated that semisupervised WSD could be successful 
Second  benefits for sentiment analysis can be realized by decomposing the problem into SO  or neutral versus polar  and polarity classification  
This similarity score is computed as a max over a number of component scoring functions some based on external lexical resources including  various string similarity functions of which most are applied to word lemmas  measures of synonymy hypernymy antonymy and semantic relatedness including a widelyused measure due to Jiang and Conrath 997
The search across a dimension uses the efficient method of  In the hierarchical phrasebased model   and an inversion transduction grammar  ITG    the problem is resolved by restricting to a binarized form where at most two nonterminals are allowed in the righthand side 
In syntactic parse reranking supersenses have been used to build useful latent semantic features  In agreement with recent resuits on parsing with lexicalised probabilistic grammars   
ntroduction In recent years  statistical machine translation have experienced a quantum leap in quality thanks to automatic evaluation  and errorbased optimization 
Probably the most widely used feature weighting function is pointwise Mutual Information MI Church and Patrick 990 Hindle 990 Luk 995 Lin 998 Gauch Wang and Rachakonda 999 Dagan 2000 Baroni and Vegnaduzzo 2004 Chklovski and Pantel 2004 Pantel and Ravichandran 2004 Pantel Ravichandran and Hovy 2004 Weeds Weir and McCarthy 2004 dened by weight MI wflog 2 Pwf PwPf
To some extent  this can probably be explained by the strong tradition of constituent analysis in AngloAmerican linguistics  but this trend has been reinforced by the fact that the major treebank of American English  the Penn Treebank   is annotated primarily with constituent analysis 
As a side product  we find empirical evidence to suggest that the effectiveness of rule lexicalization techniques  and parent annotation techniques  is due to the fact that both lead to a reduction in perplexity in the automata induced from training corpora 
There has been considerable skepticism over whether WSD will actually improve performance of applications  but we are now starting to see improvement in performance due to WSD in crosslingual information retrieval  and machine translation  and we hope that other applications such as questionanswering  text simplication and summarisation might also benet as WSD methods improve 
2 Maximum Entropy Models Maximum entropy  ME  models   also known as loglinear and exponential learning models  provide a general purpose machine learning technique for classification and prediction which has been successfully applied to natural language processing including part of speech tagging  named entity recognition etc Maximum entropy models can integrate features from many heterogeneous information sources for classification 
 Introduction State of the art Statistical Machine Translation  SMT  systems usually adopt a twopass search strategy  as shown in Figure  
5 The statistical parser The parsing model is the one proposed in Merlo and Musillo   which extends the syntactic parser of Henderson  and  with annotations which identify semantic role labels  and has competitive performance 
Study in collocation extraction using lexical statistics has gained some insights to the issues faced in collocation extraction  
Effective training algorithm exists  once the set of features a42 a57 a6 aa33a8 a7a54a8 a7a00a85a68a5 a53 is selected 
There are several distance measures suitable for this purpose  such as the mutual information   the dice coefficient   the phi coefficient   the cosine measure  and the confidence  
 Introduction Cooccurrence statistics extracted from corpora lead to good performance on a wide range of tasks that involve the identification of the semantic relation between two words or concepts  
Maximum entropy models  are a class of exponential models which require no unwarranted independence assumptions and have proven to be very successful in general for integrating information from disparate and possibly overlapping sources 
Support Vector Machines  SVMs   and Maximum Entropy  ME  method  are powerful learning methods that satisfy such requirements  and are applied successfully to other NLP tasks  
42 Support Vector Machines We chose to adopt a tagging perspective for the Simple NP chunking task  in which each word is to be tagged as either B  I or O depending on wether it is in the Beginning  Inside  or Outside of the given chunk  an approach first taken by   and which has become the defacto standard for this task 
In order to overcome this  some unsupervised learning methods and minimallysupervised methods  eg    have been proposed 
For the current work  the Loglikelihood coefficient has been employed   as it is reported to perform well among other scoring methods  
This averaging effect has been shown to help overfitting  
Finally  to estimate the parameters i of the weighted linear model  we adopt the popular minimum error rate training procedure  which directly optimizes translation quality as measured by the BLEU metric 
Indeed  researchers have shown that gigantic language models are key to stateoftheart performance   and the ability of phrasebased decoders to handle largesize  highorder language models with no consequence on asymptotic running time during decoding presents a compelling advantage over CKY decoders  whose time complexity grows prohibitively large with higherorder language models 
Introduction During the last four years  various implementations and extentions to phrasebased statistical models  have led to significant increases in machine translation accuracy  
With the indepth study of opinion mining  researchers committed their efforts for more accurate results  the research of sentiment summarization   domain transfer problem of the sentiment analysis  and finegrained opinion mining  are the main branches of the research of opinion mining 
Finally  the translation model can be formalized as the following optimization problem argmax This optimization problem can be solved by the EM algorithm  
First  we compared our system output to human reference translations using Bleu   a widelyaccepted objective metric for evaluation of machine translations 
 Introduction Phrasebased method  and syntaxbased method  represent the stateoftheart technologies in statistical machine translation  SMT  
According to our experience  the best performance is achieved when the union of the sourcetotarget and targettosource alignment sets  is used for tuple extraction  some experimental results regarding this issue are presented in Section 422  
Nonparametricmodels  may be appropriate 
6 Related Work The popular IBM models for statistical machine translation are described in  
We use five sentiment classification datasets  including the widelyused movie review dataset  MOV   as well as four datasets containing reviews of four different types of products from Amazon  books  BOO   DVDs  DVD   electronics  ELE   and kitchen appliances  KIT    
 shows that baseNP recognition  Fz  I  920  is easier than finding both NP and VP chunks  Fz    88  and that increasing the size of the training data increases the performance on the test set 
Movies Reviews  This is a popular dataset in sentiment analysis literature  
36 Parameter Estimation To estimate parameters and um  we adopt the approach of minimum error rate training  MERT  that is popular in SMT  
 Introduction In recent years  Bracketing Transduction Grammar  BTG  proposed by  has been widely used in statistical machine translation  SMT  
While significant time savings have already been reported on the basis of automatic pretagging  eg  for POS and parse tree taggings in the Penn TreeBank   or named entity taggings for the Genia corpus    this kind of preprocessing does not reduce the number of text tokens actually to be considered 
Veale  used WordNet to answer 374 multiplechoice SAT analogy questions  achieving an accuracy of 43   but the best corpusbased approach attains an accuracy of 56   
The IOB format  introduced in   consistently   ame out as the best format 
They were based on mutual information   conditional probabilities   or on some standard statistical tests  such as the chisquare test or the loglikelihood ratio  
Stateoftheart machine learning techniques including Support Vector Machines   AdaBoost  and Maximum Entropy Models  provide high performance classifiers if one has abundant correctly labeled examples 
2 The BLEU Metric The metric most often used with MERT is BLEU   where the score of a candidate c against a reference translation r is  BLEU  BP  len  c   len  r   exp  4summationdisplay n    4 logpn   where pn is the ngram precision2 and BP is a brevity penalty meant to penalize short outputs  to discourage improving precision at the expense of recall 
Lexicalization can increase parsing performance dramatically for English   and the lexicalized model proposed by Collins  997  has been successfully applied to Czech  and Chinese  
Some NLG researchers are impressed by the success of the BLEU evaluation metric  in Machine Translation  MT   which has transformed the MT field by allowing researchers to quickly and cheaply evaluate the impact of new ideas  algorithms  and data sets 
In addition  the averaged parameters technology  is used to alleviate overfitting and achieve stable performance 
Successful discriminative parsers have used generative models to reduce training time and raise accuracy above generative baselines  
Introduction A hypergraph  as demonstrated by   is a compact datastructure that can encode an exponential number of hypotheses generated by a regular phrasebased machine translation  MT  system  eg  Koehn et al 
compares his method to  and shows that for four words the former performs significantly better in distinguishing between two senses 
So far  SCL has been applied successfully in NLP for PartofSpeech tagging and Sentiment Analysis  
Introduction With the introduction of the BLEU metric for machine translation evaluation   the advantages of doing automatic evaluation for various NLP applications have become increasingly appreciated  they allow for faster implementevaluate cycles  by bypassing the human evaluation bottleneck   less variation in evaluation performance due to errors in human assessor judgment  and  not least  the possibility of hillclimbing on such metrics in order to improve system performance  
SVM has been shown to be useful for text classification tasks   and has previously given good performance in sentiment classification experiments  
Also  in a  stateoftheart English parser  only the words tha  t occur more tha  n d times in training data 
23 The Averaged Perceptron Reranking Model Averaged perceptron  has been successfully applied to several tagging and parsing reranking tasks   and in this paper  we employed it in reranking semantic parses generated by the base semantic parser SCISSOR 
One popular and statistically appealing such measure is LogLikelihood  LL   
Our MT experiments use a reimplementation of Moses  called Phrasal  which provides an easier API for adding features 
Recent several years have witnessed the rapid development of system combination methods based on confusion networks  eg     which show stateoftheart performance in MT benchmarks 
The averaged 555 perceptron has a solid theoretical fundamental and was proved to be effective across a variety of NLP tasks  
23 Classifier Training We chose maximum entropy  as our primary classifier  since it had been successfully applied by the highest performing systems in both the SemEval2007 preposition sense disambiguation task  and the general word sense disambiguation task    
 Introduction Raw parallel data need to be preprocessed in the modern phrasebased SMT before they are aligned by alignment algorithms  one of which is the wellknown tool  GIZA     for training IBM models  4  
3 Language modelling with Bloom filters Recentwork  presenteda scheme for associating static frequency information with a set of ngrams in a BF efficiently 3 Logfrequency Bloom filter 
The efficiency of the scheme for storing ngram statistics within a BF presented in Talbot and Osborne  relies on the Zipflike distribution of ngramfrequencies  mosteventsoccuranextremely small number of times  while a small number are very frequent 
In an experiment on 6800 sentences of ChineseEnglish newswire text with segmentlevel human evaluation from the Linguistic Data Consortium?s LDC Multiple Translation project we compare the LFGbased evaluation method with other popular metrics like BLEU NIST General Text Matcher GTM Turian et al  2003 Translation Error Rate TER Snover et al  2006 and METEOR Banerjee and Lavie 2005 and we show that combining dependency representations with synonyms leads to a more accurate evaluation that correlates better with human judgment
Typical examples of linguistically sophisticated annotation include tagging words with their syntactic category  although this has not been found to be effective for R   lemma of the word  eg  corpus  for  corpora    phrasal information  eg identifying noun groups and phrases    and subjectpredicate identification  
To scale LMs to larger corpora with higherorder dependencies  researchers Work completed while this author was at Google Inc have considered alternative parameterizations such as classbased models   model reduction techniques such as entropybased pruning   novel represention schemes such as suffix arrays   Golomb Coding  and distributed language models that scale more readily  
 Introduction Robust statistical syntactic parsers  made possible by new statistical techniques  and by the availability of large  handannotated training corpora such as WSJ  and Switchboard   have had a major impact on the field of natural language processing 
For the extraction problem  there have been various methods proposed to date  which are quite adequate  whose training corpus for the noun drug was 9 times bigger than that of Karov and Edelman  reports 94  correct performance improved to impressive 939  when using the  one sense per discourse  constraint 
This increase of probabilities is defined as multiplicative change  N  as follows   NPE Tprime   PET   2  The main innovation of the model in  is the possibility of adding at each step the best relation NRij  as well as NIRij  that is Rij with all the relations by the existing taxonomy However  as also pointed out by   this observation does not hold uniformly over all possible cooccurrences of two words 
The MERT module is a highly modular  efficient and customizable implementation of the algorithm described in  Comparison with SSCRFMER When we consider semisupervised SOL methods  SSCRFMER  is the most competitive with HySOL  
 has described an efficient exact one dimensional accuracy maximization technique for a similar search problem in machine translation While these are based on a relatively few number of items and while we have not performed any tests to determine whether the differences in 
A number of part of speech taggers are readily available and widely used  all trained and retrainable on text corpora  To avoid this problem  we adopt crossvalidation training as used in  
The default training set of Penn Treebank  was used for the parser because the domain and style of those texts actually matches fairly well with the domain and style of the texts on which a reading level predictor for second language learners might be used 
This source of overcounting is considered and fixed by  and Zens and Ney  2003   which we briefly review here This further supports the claim by  that loglikelihood ratio is much less sensitive than pmi to low counts Inversion transduction grammar   or ITG  is a wellstudied synchronous grammar formalism 
For example   used cooccurrences between verbs and their subjects and objects  and proposed a similarity metric based on mutual information  but no exploration concerning the effectiveness of other kinds of word relationship is provided  although it is extendable to any kinds of contextual information 
Studies on the supervised task have shown that straightforward baselines  eg models based on source only  target only  or the union of the data  achieve a relatively high performance level and are surprisingly difficult to beat  
Results from  show that under these definitions the following guarantee holds  LogLossUpda  k  BestWtk  a C20 BestLossk  a So it can be seen that the update from a to Upda  k  BestWtk  a is guaranteed to decrease LogLoss by at least W k q C0 W C0 k qC6C7 2 From these results  the algorithms in Figures 3 and 4 could be altered to take the revised definitions of W k and W C0 k into account 
The notion of incrementally merging classes of lexical items is intuitively satisfying and is explored in detail in  Inter and Intra annotator agreement We measured pairwise agreement among annotators using the kappa coefficient  K  which is widely used in computational linguistics for measuring agreement in category judgments  
In the supervised setting  a recent paper by  shows that a simple feature augmentation method for SVM is able to effectively use both labeled target and source data to provide the best domainadaptation results in a number of NLP tasks 
To speed our computations  we use the cube pruning method of  with a fixed beam size informationtheoretic similarity measure is commonly used in lexicon acquisition tasks and has demonstrated good performance in unsupervised WSD  
 Introduction Stateoftheart Statistical Machine Translation  SMT  systems usually adopt a twopass search strategy  as shown in Figure  
Following  we can avoid unnecessary false positives by not querying for the longer ngram in such cases It is explored extensively in  We compare semisupervised LEAF with a previous state of the art semisupervised system  
The main application of these techniques to written input has been in the robust  lexical tagging of corpora with partofspeech labels  Head Lexicalization As previously shown  Charniak      Carroll and Rooth  998   etc   
The most widely used singlewordbased statistical alignment models  SAMs  have been proposed in  Decision lists have already been successfully applied to lexical ambiguity resolution by  
 Introduction Chinese Word Segmentation  CWS  has been witnessed a prominent progress in the last three Bakeoffs      Of particular interest are lexicalized parsing models such as the ones developed by  and Carroll and Rooth  998  
Recently  graphbased methods have proved useful for a number of NLP and IR tasks such as document reranking in ad hoc IR  and analyzing sentiments in text  For English  after a relatively big jump achieved by   we have seen two significant improvements   and  pushed the results by a significant amount each time 
The  algorithm was one of the first bootstrapping algorithms to become widely known in computational linguistics Another widely used discriminative method is the perceptron algorithm   which achieves comparable performance to CRFs with much faster training  so we base this work on the perceptron 
For symmetrization  we found that Och and Neys refined technique described in  produced the best AER for this data set under all experimental conditions Ramshaw and Marcus  successflflly applied Eric Brill s transformationbased learning method to the chunking problem 
METEOR uses the Porter stemmer and synonymmatching via WordNet to calculate recall and precision more accurately  
Recent work  has demonstrated that randomized encodings can be used to represent ngram counts for LMs with signficant spacesavings  circumventing informationtheoretic constraints on lossless data structures by allowing errors with some small probability 
Also  slightly restating the advantages of phrasepairs identified in   these blocks are effective at capturing context including the encoding of noncompositional phrase pairs  and capturing local reordering  but they lack variables  eg embedding between ne pas in French   have sparsity problems  and lack a strategy for global reordering 
The efficient block alignment algorithm in Section 4 is related to the inversion transduction grammar approach to bilingual parsing described in   in both cases the number of alignments is drastically reduced by introducing appropriate reordering restrictions System 
Perceptron Reranking As  observes  perceptron training involves a simple  online algorithm  with few iterations typically required to achieve good performance In order to estimate the conditional distributions shown in Table     
4 Features We used a dependency structure as the context for words because it is the most widely used and one of the best performing contextual information in the past studies  
This corpusbased information typically concerns sequences of 3 tags or words  It has been argued that METEOR correlates better with human judgment due to higher weight on recall than precision  Treebanking 
After a brief period following the introduction of generally accepted and widely used metrics BLEU Papineni et al 2002 and NIST Doddington 2002 when it seemed that this persistent problem has finally been solved the researchers active in the field of machine translation 
However  in   the authors investigate minimum translation units  MTU  which is a refinement over a similar approach by  to eliminate the overlap issue  
 Introduction The field of machine translation has seen many advances in recent years  most notably the shift from wordbased  to phrasebased models which use token ngrams as translation units  
The remaining six entries were all fully automatic machine translation systems  in fact  they were all phrasebased statistical machine translation system that had been trained on the same parallel corpus and most used Bleubased minimum error rate training  to optimize the weights of their log linear models feature functions  
 introduced the averaged perceptron  as a way of reducing overfitting  and it has been shown to perform better than the nonaveraged version on a number of tasks 
This averaging effect has been shown to reduce overfitting and produce much more stable results  Probably the most widely used association weight function is  pointwise  Mutual Information  MI          defined by         log    2 fPwP fwPfwMI  
A known weakness of MI is its tendency to assign high weights for rare features    and Basque   which pose quite different and in the end less severe problems  there have been attempts at solving this problem for some of the highly inflectional European languages  such as     Slovenian       Czech  and   five Central and Eastern European languages   
In order to overcome this problem  we look to the bootstrapping method outlined in  Much later work  relies on the use of extremely large corpora which allow very precise  but sparse features 
The simplest one is the BIO representation scheme   where a B denotes the first item of an element and an I any noninitial item  and a syllable with tag O is not a part of any element 
Conditional Markov models  CMM   have been successfully used in sequence labeling tasks incorporating rich feature sets 
To compare the output of their shallow parser with the output of the wellknown  parser  Li and Roth applied the chunklink conversion script to extract the shallow constituents from the output of the Collins parser on WSJ section 00 while the former is piecewise constant and thus can not be optimized using gradient techniques   
Successful discriminative parsers have relied on generative models to reduce training time and raise accuracy above generative baselines  procedure is the most widelyused version of MERT for SMT  
Some tasks can thrive on a nearly pure diet of unlabeled data  Machine Translation Experiments 4 Experimental Setting For our MT experiments  we used a reimplementation of Moses   a stateoftheart phrasebased system Motivation 
The success of Statistical Machine Translation  SMT  has sparked a successful line of investigation that treats paraphrase acquisition and generation essentially as a monolingual machine translation problem   Phrasebased ChinesetoEnglish MT The MT system used in this paper is Moses  a stateoftheart phrasebased system  eports a success rate of 96  disambiguating twelve words with two clear sense distinctions each one  
It is an online training algorithm and has been successfully used in many NLP tasks  such as POS tagging   parsing   Chinese word segmentation   and so on 
One major resource for corpusbased research is the treebanks available in many research organizations   which carry skeletal syntactic structures or  brackets  that have been manually verified Successful approaches aimed at trying to overcome the sparse data limitation include backoff   
Many previous studies have shown that the loglikelihood ratio is well suited for this purpose  Recent work    has shown that adding many millions of words of machine parsed and reranked LA Times articles does  in fact  improve performance of the parser on the closely related WSJ data 
In addition to the widely used BLEU  and NIST  scores  we also evaluate translation quality with the recently proposed Meteor  and four editdistance style metrics  Word Error Rate  WER   Positionindependent word Error Rate  PER    CDER  which allows block reordering   and Translation Edit Rate  TER   
Most stateoftheart SMT systems treat grammatical elements in exactly the same way as content words  and rely on generalpurpose phrasal translations and target language models to generate these elements  For instance   shows that a simple feature augmentation method for SVM is able to effectively use both labeled target and source data to provide the best domainadaptation results in a number of NLP tasks 
For English  we use three stateoftheart taggers  the taggers of  and  in Step   and the SVM tagger  in Step 3 We used the average perceptron algorithm of  in our experiments  a variation that has been proven to be more effective than the standard algorithm shown in Figure 2 
More recently  phrasebased models  have been proposed as a highly successful alternative to the IBM models Models that can handle nonindependent lexical features have given very good results both for partofspeech and structural disambiguation  
It is often straightforward to obtain large amounts of unlabeled data  making semisupervised approaches appealing  previous work on semisupervised methods for dependency parsing includes  
Recently  an elegant approach to inference in discourse interpretation has been developed at a number of sites   all based on tim notion of abduction  and we have begun to explore its potential application to machine translation 
The state of the art technology for relation extraction primarily relies on patternbased approaches  Many mainstream systems and formalisms would satisfy these criteria  including ones such as the University of Pennsylvania Treebank  which are purely syntactic  though of course  only syntactic properties could then be extracted  
The Penn Treebank  has until recently been the only such corpus  covering 45M words in a single genre of financial reporting 
Some methods which can offer powerful reordering policies have been proposed like syntax based machine translation  and Inversion Transduction Grammar  
Recent work emphasizes corpusbased unsupervised approach  that avoids the need for costly truthed training data We examine the effectiveness of Structural Correspondence Learning  SCL   for this task  a recently proposed adaptation technique shown to be effective for PoS tagging and Sentiment Analysis 
Online votedperceptrons have been reported to work well in a number of NLP tasks  Introduction Large scale annotated corpora  eg  the Penn TreeBank  PTB  project   have played an important role in textmining 
The fluency models hold promise for actual improvements in machine translation output quality  
The notion that nouns have only one sense per discoursecollocation was also exploited by  in his seminal work on bootstrapping for word sense disambiguation Using the components of the rowvector bm as feature function values for the candidate translation em  m a6    M   the system prior weights can easily be trained using the Minimum Error Rate Training described in  
Bootstrapping a PMTG from a lowerdimensional PMTG and a wordtoword translation model is similar in spirit to the way that regular grammars can help to estimate CFGs   and the way that simple translation models can help to bootstrap more sophisticated ones  
Semantic collocations are harder to extract than cooccurrence patternsthe state of the art does not enable us to find semantic collocations automatically t 
Extracting semantic information from word cooccurrence statistics has been effective  particularly for sense disambiguation  High correlation is reported between the BLEU score and human evaluations for translations from Arabic  Chinese  French  and Spanish to English  
The maximum entropy approach  is known to be well suited to solve the classification problem We compared a baseline system  the stateoftheart phrasebased system Pharaoh   against our system 
On the other hand  integrating an additional component into a baseline SMT system is notoriously tricky as evident in the research on integrating word sense disambiguation  WSD  into SMT systems  different ways of integration lead to conflicting conclusions on whether WSD helps MT performance  
This is analogous  and in a certain sense equivalent  to empirical risk minimization  which has been used successfully in related areas  such as speech recognition   language modeling   and machine translation   investigated the use of concurrent parsing of parallel corpora in a transduction inversion framework  helping to resolve attachment ambiguities in one language by the coupled parsing state in the second language 
Sentencelevel subjectivity detection  where training data is easier to obtain than for positive vs negative classification  has been successfully performed using supervised statistical methods alone  or in combination with a knowledgebased approach  
Properly calculated BLEU scores have been shown to correlate reliably with human judgments  
 Introduction During the last few years  SMT systems have evolved from the original wordbased approach  to phrasebased translation systems  
Recently   have successfully constructed high quality and high coverage gazetteers from Wikipedia Introduction The maximum entropy model  has attained great popularity in the NLP field due to its power  robustness  and successful performance in various NLP tasks   
Annotated reference corpora  such as the Brown Corpus   the Penn Treebank   and the BNC   have helped both the development of English computational linguistics tools and English corpus linguistics 
Similarly  Structural Correspondence Learning  has proven to be successful for the two tasks examined  PoS tagging and Sentiment Classification A notable exception is the work of  
Inversion Transduction Grammar  ITG   and SyntaxDirected Translation Schema  SDTS   lack both of these properties 
Introduction Michael  parsing models have been quite influential in the field of natural language processing We do not completely rule out the possibility that some more sophisticated  ontologically promiscuous  firstorder analysis  perhaps along the lines of   might account for these kinds of monotonicity inferences It is promising to optimize the model parameters directly with respect to AER as suggested in statistical machine translation  
In the wellknown socalled IBM word alignment models   reestimating the model parameters depends on the empirical probability P  ek  fk  for each sentence pair  ek  fk  
The classification is performed with a statistical approach  built around the maximum entropy  MaxEnt  principle   that has the advantage of combining arbitrary types of information in making a classification decision 
Furthermore  the BLEU score performance suggests that our model is not very powerful  but some interesting hints can be found in Table 3 when we compare our method with a 5gram language model to a stateoftheart system Moses  based on various evaluation metrics  including BLEU score  NIST score   METEOR   TER   WER and PER 
Disambiguation of a limited number of words is not hard  and necessary context information can be carefully collected and handcrafted to achieve high disambiguation accuracy as shown in  
 and Bikel and Chiang  has demonstrated the applicability of the  model for Czech and Chinese One of the most successful metrics for judging machinegenerated text is BLEU  
Among them  the unsupervised algorithm using decisiontrees  has achieved promising performance Recent projects in semisupervised  and unsupervised  tagging also show significant progress 
Their idea has proven effective for estimating the statistics of unknown words in previous studies   We use the popular online learning algorithm of structured perceptron with parameter averaging   This algorithm is referred to as GHKM  and is widely used in SSMT systems  
Stochastic models  have been widely used in POS tagging for simplicity and language independence of the models solved relational similarity problems using the Web as a corpus Albeit simple  the algorithm has proven to be very efficient and accurate for the task of parse selection  
Two popular techniques that incorporate the error criterion are Minimum Error Rate Training  MERT   and Minimum BayesRisk  MBR  decoding  Automated metrics such as BLEU   RED   Weighted Ngram model  WNM    syntactic relation  semantic vector model  have been shown to correlate closely with scoring or ranking by different human evaluation parameters 
Arguably the most widely used is the mutual information  An especially wellfounded framework is maximum entropy  2 Related Work Supervised machine learning methods including Support Vector Machines  SVM  are often used in sentiment analysis and shown to be very promising  
Turney also reported good result without domain customization  In our experiments  we follow Lowe and McDonald  in using the wellknown loglikelihood ratio G 2  
It is often straightforward to obtain large amounts of unlabeled data  making semisupervised approaches appealing  previous work on semisupervised methods for dependency parsing includes  
In the II  OO  and OI scenarios   succeeded in improving the parser performance only when a reranker was used to reorder the 50best list of the generative parser  with a seed size of 40K sentences Maximum Entropy Maximum entropy classiflcation  MaxEnt  or ME  for short  is an alternative technique which has proven efiective in a number of natural language processing applications  
Yarowsky has proposed an algorithm that requires as little user input as one seed word per sense to start the training process 
Such a method alleviates the problem of creating templates from examples which would be used in an ulterior phase of generation  The variance semiring is essential for many interesting training paradigms such as deterministic 40 annealing   minimum risk   active and semisupervised learning  
Among these methods  CRFs is the most common technique used in NLP and has been successfully applied to PartofSpeech Tagging   NamedEntity Recognition  and shallow parsing  Wikipedia first sentence  WikiFS    used Wikipedia as an external knowledge to improve Named Entity Recognition 
However  this is not unprecedented  discriminatively weighted generative models have been shown to outperform purely discriminative competitors in various NLP classification tasks   and remain the standard approach in statistical translation modeling  A more refined algorithm  
the incremental feature selection algorithm by   allows one feature being added at each selection and at the same time keeps estimated parameter values for the features selected in the previous stages 
We also plan to apply selftraining of nbest tagger which successfully boosted the performance of one of the best existing English syntactic parser  
In machine translation  the rankings from the automatic BLEU method  have been shown to correlate well with human evaluation  and it has been widely used since and has even been adapted for summarization  
Several studies have demonstrated that for instance Statistical Machine Translation  SMT  benefits from incorporating a dedicated WSD module  In our experiments  we have used Averaged Perceptron  and Perceptron with margin  to improve performance In 2004  Conroy  tested Maximal Marginal Relevance  as well as QR decomposition 
Automatically creating or extending taxonomies for specific domains is then a very interesting area of research   
The most notable of these include the trigram HMM tagger   maximum entropy tagger   transformationbased tagger   and cyclic dependency networks  Averaging has been shown to reduce overfitting  as well as reliance on the order of the examples during training 
Synchronous parsing models have been explored with moderate success  Systems based on perceptron have been shown to be competitive in NER and text chunking  
