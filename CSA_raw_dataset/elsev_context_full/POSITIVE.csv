"As such, companies receiving favorable eWOM have a better chance to increase sales [21]. "
"Unlike traditional WOM, eWOM leaves digital records on the Internet, and therefore provides companies with accessible information [20]."
"Although the exact ranking method is a secret, communicativeness and trustworthiness are reported to be important factors [66]."
"However, their main purpose is to save decision time and to make better decisions [40]. For such reasons, companies are interested in providing eWOM as “free sales assistance” [18]."
"The self-report method seems to be most popular due to existing scales such as King and Summers’ [50], although the key informant method has also been used in a recent study [59]. In addition, consumer demographics [1] and loyalty [35] are considered in conjunction with surveys to identify opinion leaders. The main findings of the extant literature are that self-reported and peer-nominated opinion leaders influence the choices of their followers. However, self-reported surveys may capture self-confidence rather than opinion leadership [2,42]."
"As noted by Weimann and colleagues, the network structure method “works best in a closed, self-contained social setting, such as hospitals, prisons, or army bases” [75]. However, a defining feature of eWOM is its potential to reach large numbers of strangers outside a sender’s own social network [26]."
"In the original voting study that introduced the concept of “opinion leader”, Lazarsfeld and colleagues wrote that opinion leaders “were the interested, highly articulate voters who gave political advice or even tried to convert other citizens” [52]. Thus, a key behavior of eWOM opinion leaders is their ability to communicate with other consumers about their product experience [28]."
"Manipulating online user reviews is a known phenomenon [62], which makes it important for consumers to receive eWOM from trustworthy opinion leaders. "
" In contrast, Feick and Price [34] found influential consumers, or market mavens, who have broad product category knowledge."
"Identifying opinion leaders from observed behaviors such as WOM is the most expensive method, although highly accurate [75]. Fortunately, online user reviews are now available to companies and can serve as a proxy for overall WOM [79]. This approach is consistent with recent research findings that link online consumer behavior with product sales [51]."
"Opinion leader WOM has become so important to the pharmaceutical industry that the top 15 drug companies spent a third of their marketing expenditures on opinion leaders in 2004 [32], though industry practitioners observe that the practice of identifying opinion leaders is ad hoc [49]."
" In this sense, advances in WMSNs over the next few years will be addressed to the design, development and integration of all these capabilities into a single standard mesh solution which, in turn, will ensure a strong boost of this technology in a wide range of areas, such as automation and control (home and industrial), environmental surveillance, precision agriculture, traffic monitoring, or health servicesÂ  [1]."
"Therefore, as a first contribution of this research work, we introduce a multi-objective (MO) optimization problemÂ  [7], which allows us to formulate multiple objectives in a single problem definition, in order to later apply the mathematical tool denoted as Goal Programming (GP)Â  [8,9] to simultaneously satisfy the objectives pursued. "
"In order to obtain the best performance of the WMSN, we numerically estimated the goals for the aggregate message collision time and network lifetime metrics which are consistent with the operation of some real scenarios related to agriculture applications where our group has wide experienceÂ  [10,11]. The numerical results obtained by GP clearly satisfy the deviations of these goals, thus setting an optimal design in order to reach an efficient and complete sensing monitoring during, at least, the period specified by the end users for the WMSN applications under consideration. "
"To this end, we first take into consideration the studies in [12-14], which allow us to learn about the strong points and limitations of the different techniques that cope with the HT effects in WSNs and WMSNs."
"we propose a solution which follows some of the rules related to channel multiplexing [15] and time-slot scheduling  [16,17] techniques, and does not require additional and costly hardware, as it is the case of proposals that employ CDMA-based or directional antenna methodologies."
" Among them, we highlight those relevant surveys  [15-20] which overview the current state of the art regarding technological solutions that offer some hints to alleviate this issue. "
"In particular, results inÂ  [21] demonstrate that network performance is significantly degraded by the HT phenomenon. A notable limitation of the proposed model lies in the computation time required by the simulation framework, especially, as the number of network nodes increases. "
"To finish with the related analytical studies, the work inÂ  [23] measures quantitatively the effect of hidden nodes in a WSN for several scenarios (single-hop, grid, random). This fact is accomplished by analyzing a straightforward mathematical model satisfying, among others, the next two assertions: (i) each node is characterized by an M/M/1/K queue, and (ii) the traffic rates follow the same criteria as those defined by studiesÂ  [21,22]. "
" In this paper, we take our inspiration from the investigation inÂ  [23] to devise our multi-objective optimization problem extended to the WMSN field.As regards solutions that alleviate the hidden terminal phenomenon, we highlight the recent studies inÂ  [12,13]. "
" Furthermore, as stated inÂ  [29], the RTS/CTS mechanism does not resolve completely the HT problems in multi-hop networks. Another limitation of Queen-MAC is that broadcast traffic can only flow from the sink node to the remaining network nodes."
"Finally, inÂ  [13], network nodes implement a learning mechanism at the network formation phase in which nodes determine the number and the position of the slots required for transmitting/receiving information. This learning mechanism leads to a decision making process by all network nodes which must decide, at each slot, whether they transmit their data, listen for ongoing transmissions from neighbors, or switch to the sleep state to save energy."
"  To deal with this concern, the IEEE 802.15.5 LR-WPAN mesh standard incorporates the Asynchronous Energy Saving (ASES) modeÂ  [2,34]. In ASES, sensor nodes usually remain in a state of inactivity operating with extremely low power consumption (OFF) most of the time, and they switch periodically to a state of full activity (ON) during a short period only to transmit/receive data."
"On the other hand, to obtain a multi-channel coordination, nodes run a pseudorandom algorithm which resolves, by means of the senderâs address, the channel to tune in each slot. This technique is also employed in other multi-channel proposals such as the ones inÂ  [32,33], and it has been adopted in our design as well. "
"Nevertheless, ASES does not have any mechanism to solve other significant concerns as the hidden terminals problem. This phenomenon may provoke a large amount of collisions which, in turn, increase the number of retransmissions and, as a consequence, the messages lost. Collisions due to hidden terminals can be classified into two groupsÂ  [35]: primary and secondary collisions."
"To this end, we take advantage of the local synchronization approaches proposed by studies asÂ  [12,40], where nodes share their clock with neighbors in coverage area. "
"This assumption, together with the regular grid mesh topology selected in this paper for our numerical and simulation campaigns, allows us to isolate HT problems from other interference sources. In this regard, it is worth remarking that this same consideration is adopted in the work of Cano etÂ al.Â  [23] with identical objective: to eliminate the influence of interference sources other than hidden terminals in order to obtain an appropriate quantification of the HT impact."
"This queuing model could be applied because, in a mesh network, the possibility to finally send/receive a message depends on multiple factors, such as the access to the medium and the communications available in the vicinity. In addition, we consider that nodes access the physical medium by making use of the unslotted CSMA-CA mechanism as defined by the IEEE 802.15.5 standardÂ  [41]. "
" The rest of nodes forward data messages by selecting their nearest one-hop neighbor to the sink. Regarding the power-consumption evaluation, we have opted for the energy model of TelosB devicesÂ  [49]. "
"Regarding our evaluation, we start from Zheng development for the simulation of the IEEE 802.15.5 LR-WPAN mesh on ns-2Â  [53], which implements the main mandatory functions related to the mesh topology formation and routingÂ  [2,34]. In particular, the routing protocol proposed inÂ  [53] satisfies our design assumption regarding that each network node selects its nearest neighbor toward the destination/sink to forward data (Assumption (iii))."
"Finally, concerning MULTI-HIT proposal, we could also improve its performance as follows. On the one hand, nodes would periodically collect information referred to, for instance, the Receiver Signal Strength Indicator (RSSI) and/or the Link Quality Indicator (LQI), and using strategies such as machine-learning techniquesÂ  [55], each node would be able to determine whether a given channel is reliable enough to conduct the communications."
"Agile software development is a set of iterative and incremental software engineering methods that are advocated based on an “agile philosophy” captured in the Agile Manifesto (Fowler and Highsmith, 2001). While mostly repackaging and re-branding pre-viously well-known good software development practices, the agile movement can be considered as an alternative to so called tradi-tional software development methods."
"Systematic literature reviews are a means of identifying, evalu-ating and interpreting all available research relevant to a particular research question, or topic area, or phenomenon of interest. They are appropriate for summarizing existing research, for identifying gaps in the existing literature, as well as for providing background for positioning new research (Kitchenham, 2007). In this paper, we present the results of a systematic literature review on the topic of large-scale agile transformations."
"In some cases, the source presented very vague indicators on size. For instance, one case (Cloke, 2007) was included, as there were indications of large-scale considerations, although the size remained unclear. If there were no indications of size, the paper was excluded, e.g. (Miller and Haddad, 2012)."
"The closest to our study is the State of Agile survey, as large part of the respondents of their latest survey (VersionOne, Inc, 2016) were from large organizations that had at least partially adopted agile. The results of that survey have similarities with ours."
"Consultants and practitioners have put for-ward several frameworks for scaling agile. For example, agile con-sultants have put together an “Agile Scaling Knowledgebase Deci-sion Matrix” (Mat, 2016), where they briefly compare different ag-ile scaling approaches in one big excel sheet. Currently, they have listed nine different approaches. While going through the articles for this review, we noticed that experiences reports and research papers very seldom mention any frameworks. To our knowledge, there exists only a handful of papers on actual usage experiences related to a particular scaling framework. "
"Surveys on challenges and success factors for agile projects in general have been con-ducted, e.g.(Chow and Cao, 2008). However, specifically large-scale agile projects have not been scientifically studied. Non-scientific surveys exist, the most famous one being the State of Agile Survey that Version One has been conducting annually since 2007. "
This paper describes a computationally efficient method to estimate probability distributions based on the recent work by Bernacchia and Pigolotti (2011). 
"For these reasons, the method of Bernacchia and Pigolotti (2011) for estimating PDF distributions should in principle be well suited for such an application because it provides an objective PDF estimate that requires no prior assumptions regarding the underlying distribution. Bernacchia and Pigolotti (2011) derive an expression for a data-derived, optimal kernel (Watson and Leadbetter, 1963) and the resulting and self-consistent kernel density estimate; their kernel derivation method is even optimal for multi-modal data."
"Since the direct calculation of the discrete Fourier transform is notoriously slow, it would be preferable to evaluate this discrete Fourier transform using the fast Fourier transform (FFT) method of Cooley and Tukey (1965). However, the FFT is not directly applicable since it requires that the Fourier coefficients are specified on an evenly-spaced grid. "
"In this paper, we show how to accelerate the computational performance of the BP11 density estimation method using the nonuniform FFT (nuFFT) method of Greengard and Lee (2004) to approximate the empirical characteristic function (SectionÂ  2)."
"Bernacchia and Pigolotti (2011) recently derived a method for objectively estimating the probability distribution function (PDF) of a univariate dataset. They show that the dataset itself can be used to derive a kernel (both its shape and width) in an objective, data-driven way. We summarize the essential details of the derivation and the method here"
"Bernacchia and Pigolotti (2011) use this relationship and the result of Watson and Leadbetter (1963), which states that the mean squared error of a kernel density estimate is minimized if the kernel satisfies the equation:"
"Since the Ïj values are assumed to be randomly distributed, they presumably are not regularly spaced, which excludes the possibility of using a standard FFT method to evaluate the DFT. However, the nonuniform FFT (nuFFT) method described by Greengard and Lee (2004) is specifically designed to reduce the computational cost of DFTs on irregularly-spaced data. The nuFFT method can be summarized as follows."
"To this end, Dutt and Rokhlin (1993) provide an expression for specifying the width of the Gaussian h and the point-width q of the convolution such that the resulting FFT is the same as the direct DFT within a specifiable accuracy."
"since the nonuniform FFT method is only guaranteed to provide a good approximation over this range (Dutt and Rokhlin, 1993). Following Dutt and Rokhlin (1993), we specify the width of the convolution kernel as h=1.5629, and we apply the convolution to the q=28Ïk nearest points surrounding each Ïj data value."
"Bernacchia and Pigolotti (2011) note that the selection of the subset of frequencies is arbitrary and corresponds to the arbitrary choice of initial density estimate in the iterative procedure that they use to derive the expression for ÏË. As long as the subset is bounded and the bound grows with N, a self-consistent estimate will converge. Our filter choice satisfies these criteria for integrable characteristic functions, since the stability threshold decreases with increasing N and therefore higher frequencies are permitted as N increases. "
"Because atmospheric velocities are known to exhibit statistical self-similarity in reality and in models (Nastrom and Gage, 1985; Skamarock, 2004; Rauscher etÂ al., 2013), we apply the analysis to a realization of a fractional Brownian motion, which is a type of self-similar field (Mandelbrot, 1983). "
"We use the method of Wood and Chan (1994) to generate an fBm field with H=0.6 and 217 points. We apply the fast, self-consistent density estimation method described in SectionÂ  2 to estimate the PDF of increments at distances of 21 to 29 grid points, with distance intervals that are integer powers of two. "
"The structure functions are well-described by power laws as expected from Eq. (4). We estimate the exponents of the structure functions using the York etÂ al. (2004) maximum likelihood method in logâlog space, and we show in Fig.Â 1(d) that the exponents vary as Hm=0.6âm as expected for an fBm field with H=0.6 (Davis etÂ al., 1996)."
"We use output from the Community Atmosphere ModelÂ 4 (CAM4) (Neale etÂ al., 2010), which is a modular hydrostatic atmospheric model with a variety of parameterizations that simulate various processes important for atmospheric dynamics (e.g., radiative transfer, convection, precipitation, etc.). "
"To characterize the distribution of horizontal velocity increments at the modelâs highest resolution, we use the uniform-resolution 30Â km simulation described by Rauscher etÂ al. (2013). We use one year of model output that is recorded for every 6 model hours. "
"The model is configured in accord with the aquaplanet protocol specified by Neale and Hoskins (2000), in which the surface of the simulated planet is covered with water, and all boundary conditions are specified with rotational (in the direction of planetary rotation) and hemispheric symmetry. "
"The dashed gray lines in the figures show a power-law fit, using the York etÂ al. (2004) maximum likelihood method, to the structure functions for increment distances ranging between approximately 100 and 500Â km. We choose these bounds for two separate reasons. "
"For the lower bound, it is well known that the diffusive properties of atmospheric models tend to dampen variability for length scales ranging from one grid cell to ten grid cells (Skamarock, 2004). This effect manifests as a steepening of the first order structure functions for the two smallest increment distances (distances corresponding to 1 and 2 grid cells), so we restrict the fit to increment distances that are greater than or equal to 4 grid cells, which is approximately 100Â km."
"Additionally, since it is hypothesized that there should be a scale-break for distances greater than approximately 500Â km (e.g., Nastrom and Gage, 1985), we restrict our fit to increment distances less than or equal to this value.We perform a similar power-law fitting procedure for the 1st through 9th order structure functions."
This is consistent with the first-order structure function of the water vapor field reported by Pressel (2012) for a similar model configuration.
" It shows that the (modeled) atmosphere is not well-characterized by a single scaling exponent, as suggested by Nastrom and Gage (1985), but that the fractal behavior of the atmosphere ranges from anti-persistent (H1<0.5) to persistent (H1>0.5) depending on location."
"While we could have used other methods of density estimation, such as binning or traditional kernel density estimation, the Bernacchia and Pigolotti (2011) method avoids the complication of having to choose either bin width or kernel bandwidth, which is a subjective choice when faced with data from an unknown distribution. The Bernacchia and Pigolotti (2011) method simultaneously and objectively determines both the optimal shape and optimal bandwidth for a kernel density estimate. However, because the Bernacchia and Pigolotti (2011) method involves a transformation from data-space to Fourier-space (i.e.,Â calculation of the empirical characteristic function), the method is quite slow if the empirical characteristic function is calculated using a direct Fourier transform. "
"Our approach is designed to be metamodel-agnostic. As the implementation of our approach is based on the Eclipse Modeling Framework (EMF) (Steinberg et al., 2008), it is applicable to any Ecore-based modeling language, such as UML, any domain-specific modeling language (Gray et al., 2007), and even to Ecore itself which allows to apply the approach not only to models but also to metamodels."
"Current model comparison tools apply a two-phase process: (i) correspondences between model elements are computed by model matching algorithms (Kolovos et al., 2009), and (ii) a model diffing phase computes the differences between two models from the established correspondences. For instance, EMF Compare (Brun and Pierantonio, 2008) â a prominent representative of model comparison tools in the Eclipse ecosystem â is capable of detecting the following types of atomic operations"
"Model transformations (cf. Czarnecki and Helsen, 2006 for an overview) are the current technique of choice for specifying executable composite operations. In particular, composite operations are specified by rules stating the operation's preconditions, postconditions, and actions that have to be executed for applying the operation. An example operation specification is depicted using graph transformation syntax (Heckel, 2006) in the lower part of Fig. 2 for the refactoring Extract Superclass."
" For the specification and execution of composite operations, we integrated our prototype with our in-place transformation tool EMF Modeling Operations11http://www.modelversioning.org/emf-modeling-operations. (EMO) (Brosch et al., 2009), which basically implements the concepts of graph transformations. In EMO, the pre- and postconditions are expressed using the Object Constraint Language Object Management Group (2010) (OCL)."
"Such approaches are often referred to as operation-based versioning/merging (Lippe and Oosterom, 1992). Refactoring tracking is realized by Dig et al. (2008), Ekman and Asklund (2004) and Robbes (2007). All these approaches highly depend on the used development environment. Furthermore, manually performed refactorings are not detectable and refactorings which have been made obsolete by successive changes might be wrongly indicated. State-based refactoring detection mechanisms aim to reveal refactorings a posteriori.  For instance, Dig et al. (2006) propose an approach to detect applied refactorings in Java code. They first perform a fast syntactic analysis followed by a semantical analysis. A similar approach is followed by Weissgerber and Diehl (2006)."
" A heuristic-based approach is presented in Demeyer et al. (2000) in which a combination of various software measures as indicator for a certain refactoring is used. For instance, a decrease in a method's size, among other measures, is used to indicate that the refactoring Split Method has been applied.Refactoring detection in code artifacts is in general more challenging than in model artifacts."
"There is widely related work in the field of ontology engineering. Hartung et al. (2010) present an approach for generating so called semantically enriched evolution mappings between two versions of an ontology. Evolution mappings can be seen as diff models which comprise atomic as well as composite operations. Their goal is to produce a minimal diff model by using a rule-based system for minimizing the atomic operations by aggregating them to composite operations. The approach is tailored to an ontology language as well as to a small set of composite operations such as moving, splitting, and merging concepts by providing specific detection rules. Finally, they apply aggregation functions to further shrink down the size of the diff model by combining composite operations, which is in our approach supported by using iterations in the transformation rules.In summary, the presented approach of this paper is the first generic solution that allows the reuse of specifications for execution composite operations also for detecting applications of them. All other approaches are either based on operation tracking or they are restricted to a dedicated language and pre-defined composite operations, which have to be re-formulated as detection rules"
"To realize remote service functionality, data logging and analysis techniques are required as well as the possibility to remotely access operational data (Vogel-Heuser et al., 2014c)."
"Requirements engineering in general includes the activities of identifying, documenting, verifying and validating, coordinating, and managing requirements (Pohl & Rupp, 2011). Requirements play a crucial role in the engineering of production plants, as they describe the stakeholder's needs and therefore the intention of the plant as well as demanded properties to be competitive and economic. "
"The functional requirements can usually be derived from the product that has to be produced (Vyatkin, 2013) and are the minimum set of requirements to be specified completely describing the design problem (Braha and Maimon, 1997). They refer to the main intention and are the “nonnegotiable characteristics of the desired solution” (Braha and Maimon, 1997). Therefore, the production development phase is often performed subsequently to the product development phase and is part of the product realization (Bellgran and Säfsten, 2010)."
 All other requirements mentioned in ISO/IEC 25010 (2011) do not have intensified influence on aPS but have to be considered as well. 
"There are a lot of techniques proposed in order to identify requirements. Common techniques are conducting surveys and interviews, performing brainstorming, and using checklists (Blanchard, 2004). Further methods, like e.g. the quality function deployment (Chan and Wu, 2002), are proposed by research. However, which practices are really performed is project and company specific (Bellgran and Säfsten, 2010)."
"This approach cannot provide a complete specification of each possible action of the production machine, but can satisfy the need of an evolution support with a minimal human effort in order to tackle the practical problems of tight time and cost restrictions during evolution. This helps keeping specifications up to date even after undocumented evolution steps (Haubeck et al., 2013)."
" Getir et al. (2013) show, also on the PPU case study, that the evolution between system architecture and fault frees cannot be automated and that instead expertise from engineers is required to correctly evolve both the system architecture and the system's fault trees to ensure consistency."
"Recently, the term “technical debt” has been coined (Kruchten et al., 2012) for the effects when sub-optimal solutions are chosen to meet short-term goals similar to financial debt. "
"Based on the identification of these changes, Rieke presents an approach to synchronize changes between the high-level specification of CONSENS and the discipline-specific specifications to ensure consistency between them using Triple Graph Grammars (Rieke, 2015)."
" For modeling these viewpoints, the modeling framework relies on the Focus theory (Broy and Stølen, 2001) which provides strict formal semantics."
"The approaches of Filieri et al. (2012), Goševa-Popstojanova and Trivedi (2001), Zheng et al. (2008), and Filieri et al. (2015) address this problem by inferring the characteristics at runtime from the running system in order to increase the accuracy of the quality predictions. "
"With respect to incremental analysis approaches for non-functional properties, Kwiatkowska et al. (2011) present an incremental technique for quantitative verification of Markov decision processes, which is able to reuse results from previous verification runs and exploiting a decomposition of the model. "
"According to ARC (2011), in most manufacturing systems the use of IEC 61131-3 (IEC, 2009, 2013) compliant runtime environments currently is and will be the state of industrial practice in the next 5–10 years."
"Similarly, Sjoberg et al. (2013) conducted a large-scale study and concluded that code smells are good indicators for assessing maintainability on file level. "
"Efforts towards evaluating different methods of implementing logic control algorithms within IEC 61131-3 were conducted (Hajarnavis and Young, 2008), but specific patterns have not been derived yet.  However, design patterns within control engineering would address a multitude of issues such as controller design, architectural design as well as implementation aspects (Sanz and Zalewski, 2003)."
" In Preschern et al. (2012), patterns for improving system flexibility and maintainability are introduced. In Fay et al. (2015), an approach is presented which integrates the use of function-oriented design patterns into the engineering workflow of aPS to assist the designer regarding the fulfillment of NFRs. It could be shown that the application of these design patterns has a positive impact on the correct design of the software functions and on the appropriate deployment to automation hardware (Fay et al., 2015)."
"Besides the aforementioned shortcomings, best practices have been proposed that overcome code anomalies and, thus, improve and retain evolvability of software systems. The most popular means is refactoring, which provides systematic techniques for restructuring the internal system (i.e., the source code) while the external visible behavior is preserved (Fowler, 1999)."
"Model Checkers for embedded software for hybrid systems are investigated for nearly two decades now, debuted in 1997 (Henziger et al., 1997). Ortmeier et al. (2004)investigated verification of embedded software focusing on safety aspects, but did not take automation and PLC software into account. The aforementioned MechatronicUML method (Becker et al. 2012; Heinzemann et al., 2013) supports links between engineering disciplines. It supports a compositional approach for the verification of discrete real-time behavior (Giese et al., 2003) to improve scalability. "
"It is also possible to mine commonalities and differences in models for the creation of a family model, which is a so called 150% model (Schaefer et al., 2012) containing the complete variability of the system. This vastly improves the commonly used clone-and-own practice in mechanical engineering, which introduces lots of redundancies and is error-prone for debugging and maintenance."
"With respect to ensuring consistency of models with manually changed generated code, the approach by Bork et al. (2008) exploits the templates used for code generation to re-parse the generated code with manual changes."
"The existing results are mainly concerned with the existence theory of (2) and with the question of convergence, which asks whether solutions of (2) converge to a solution of (1) as â¥yâyâ â¥â©½Î²â0. These problems have been treated in very general settings in [38,51] (see also [34,54,55]). Convergence rates have been derived in [6] for linear equations in Hilbert spaces and later generalized in [34] to non-linear equations in Banach spaces. Convergence rates have also been derived in [7,9,32] for the reconstruction of sparse sequences."
"But even for the linear case, where we indeed prove stability, so far stability theorems are non-existent in the literature. Though some results have been derived in [34], they only cover a very weak form of stability, which states that the solutions of (2) with perturbed data stay close to the solution with unperturbed data, if one additionally increases the regularization parameter Î² in the perturbed problem by a sufficient amount."
"The lower semi-continuity of Wp has, for instance, been shown in [27]. In order to show the inequality (25), let Î¾1,Î¾2âP(Î©ÃÎ©) be two measures that realize the infimum in the definition of Wp(Î¼1,Î½) and Wp(Î¼2,Î½), respectively."
"In addition, it has been shown in [59] that the weak convergence of a sequence (uk)kâNâL1(Î©) to uâL1(Î©) together with the convergence R(uk)âR(u) imply that â¥ukâuâ¥1â0. Thus the topology ÏR coincides with the strong topology on X"
Exact values for the constant K in (37) (and thus for the constant c in (35)) can be derived from [60]. Bregman distances satisfying (37) are called r-coercive in [35]. This r-coercivity has already been applied in [2] for the minimization of Tikhonov functionals in Banach spaces.
"Under some assumptions, the solution of (40) with y=Fxâ  and Î²=0 has been shown to recover xâ  exactly provided the set {Î»âÎ:xÎ»â â 0} has sufficiently small cardinality (that is, it is sufficiently sparse). Results for p<1 can be found in [11,15,25,48]."
"For p>=1, the same type of results (Propositions 6.5, 6.7) has also been obtained for âp-Tikhonov regularization in [31,50]. The results for the non-convex case, pâ(0,1), are based on [30], where the same rate for non-convex Tikhonov regularization with a priori parameter choice has been derived (see also [29]). Similar, but weaker, results have been already been derived in [5,28,61] in the context of Tikhonov regularization. In [61], the conditions for the convergence rates result for non-convex regularization are basically the same as in Proposition 6.7, but only a rate of order O(Î²1/2) has been obtained. In [5,28], a linear convergence rate O(Î²) is proven, but with a considerably stronger range condition: each standard basis vector eÎ», Î»âÎ, has to satisfy eÎ»âranFâ."
"In Section 5 we have derived quantitative estimates (convergence rates) for the difference between xâ  and minimizers xÎ²âÎ£(F,y,Î²) in terms of a (generalized) Bregman distance. All these estimates hold provided S(F(xâ ),y)â©½Î² and a source inequality introduced in [36] is satisfied."
" For linear operators, the required source inequality follows from a source wise representation of a subgradient of R at xâ . This carries on the result of [6] for constrained regularization. "
"High-dimensional time-series data are becoming increasingly abundant across a wide variety of domains, spanning economics [13], neuroscience [10], and cosmology [28]."
"Linear dynamical system (LDS) models are amongst the most popular and powerful, because of their intuitive nature and ease of implementation [15]."
"The famous Kalman Filter-Smoother is one of the most popular and powerful tools for time-series prediction with an LDS, given known parameters [14]."
"a generalization of the now classic Baum-Welch expectation maximization algorithm, commonly used for system identification in much lower dimensional linear dynamical systems [20]."
"A static LDS model with a diagonal R is equivalent to Factor Analysis, while one with multiples of the identity R matrix leads to Principal Component Analysis (PCA) [21]."
one way to search for the maximum likelihood estimation (MLE) is through iterative methods such as Expectation-Maximization (EM) [22].
Determining an initial solution with subspace identification and then refining it with EM is an effective approach [6].
"The Kirby 21 data were acquired from the FM Kirby Research Center at the Kennedy Krieger Institute, an affiliate of Johns Hopkins University [16]."
"The data are preprocessed with FSL, a comprehensive library of analysis tools for fMRI, MRI, and DTI brain imaging data [23]."
"The Human Connectome Project (HCP) is a systematic effort to map macroscopic human brain circuits and their relationship to behavior in a large population of healthy adults [9,18,26]."
"To determine the number of latent states, d, the profile likelihood method proposed by Zhu et al. [30] is utilized."
"The covariance structures in the observation equation, R, should be generalized and prior knowledge could be incorporated into it [1]."
"“a scientific tool is not only considered to be something that strengthens our senses or is useful in taking measurements, but also as an aid to our understanding”[21, p. 51].Dyson points out that new tools (created by diversifiers) enable observation of new phenomena, which possibly conflict with previous theories—pushing the field into the ‘anomalies’ and then ‘crisis’ phase."
"To address these questions, it is in fact easier to first address the more abstract question—what is the value of philosophy in general? A powerful answer to this is presented by Russell [23, chapter XV]."
"It is likely however that we need both the forces of unification and diversification to move forward, summarised eloquently by Langley [17], referencing Dyson"
Some authors reported improved target delineation with the use of PET CT [1]. Aminoacid PET CT remains and expensive imaging technology and it is not available to the majority of radiotherapy departments.
One of the most popular DWI segmentation methods assumes that ADC distribution can be represented by a mixture of Gaussian distributions (GMM) where each tissue type is modelled by at least one component [21â25].
The mixture model parameters were estimated with the use of Expectation-Maximization algorithm (EM) [34]. The initial conditions for the EM algorithm were set through a method based on dynamic programming partitions [35].
GMM based algorithms have been successfully used for automatic segmentation of NMR imaging since 2005 [42]. 
"Admittedly, there are no detailed studies on ADC distribution for brain tumours at different stages, but it has been shown that in squamous cell carcinoma ADC in viable tumour remained constant independent of tumour stage, while areas with an increased ADC correlated well with areas of necrosis (reduced cell density) [43]."
"The ADC, which quantifies overall diffusion occurring within each voxel and is affected not only by the volume of the extravascular extracellular space but also by its spatial configuration, is able to detect early microstructural tissue changes associated with cell death [46]."
"Diffusion weighted imaging and the quantitative parameter of this examination, the ADC, can be used to characterize highly cellular versus acellular regions. In tumors, the ADC is usually highest in cystic or necrotic areas then in solid tumor components [47]."
The results obtained by MiMSeg are as good as the results of semi automated algorithm based on active contours [49].
"The observed deterioration of SOM performance, with respect to the result published in the original paper, might result from a smaller training dataset compared to the one used in original work of Vijayakumar etÂ al. [39] and demonstrates its sensitivity toward the size of the training dataset, which seems to be of lesser significance for MiMSeg"
"Even if the Google Bouncer (Google, 2012) security service scrutinises apps before allowing them to be published in Google Play, there is evidence (Miners, 2014) showing that malicious software (malware) can be found among legitimate apps as well. In most of the cases, the main goal of these malware apps is to access sensitive phone resources, e.g., personal data, the phone billing system, geo-location information, home banking info, etc. "
" Even though in this work we focus on Android, it is worth to note that similar flaws have been reported also for other well known mobile platforms (e.g., iOS) in the literature (Damopoulos etÂ al., 2013; Egele etÂ al., 2011)."
"Considering these drawbacks, in this paper we propose an expressive and fine-grain policy enforcement approach for Android that is able to selectively prevent privacy invasive app behaviour. The approach we present builds upon the Model-based Security Toolkit (SecKit) (Neisse etÂ al., 2015), leveraging on the policy language and Policy Decision Point (PDP) component, and shows how policy refinement and policy enforcement can be achieved in the context of the Android mobile operating system."
"Existing approaches for enforcement of Android security policies are either hard-coded interfaces with a limited set of enforcement options (Beresford etÂ al., 2011; Zhou etÂ al., 2011), or flexible and fine-grain approaches using a security policy specification language focusing on low level actions (e.g. API invocations or system calls) (Rasthofer etÂ al., 2014). The first type of approach lacks in flexibility since the set of enforcement options is limited, while the second one is too low level in order to be understandable and usable considering the complexity of policies by users."
" Many papers in the literature (Enck etÂ al., 2010; Gibler etÂ al., 2012; Stirparo and Kounelis, 2012; Zhou and Jiang, 2013) have shown apps with high invasion and manipulation on users' personal data. This exploitation is enabled by Android design vulnerabilities (Shabtai etÂ al., 2010), and is triggered by the increasing value of users' personal information in digital businesses."
"Furthermore, it is almost impossible to guarantee the fairness of any given app, as it has been showed that centralised security checks (e.g., Google Bouncer, 2012) can be bypassed (Ducklin, 2012; Miller and Oberheide, 2012), while legitimate overprivileged apps (Geneiatakis etÂ al., 2015) can be manipulated in order to provide access to personal data as shown in Xing etÂ al. (2014). "
"This is due to the fact that permissions are not a one-to-one mapping scheme with the corresponding API method calls that implement the actual functionalities. Indeed, their granularity is quite coarse, and, considering the 197 permissions of Android SDK version 17 associated to the 1259 methods with permission checks published in (Felt etÂ al., 2011), on average a permission is associated to 7 API methods."
"Threats to users' privacy may be posed not only by malware apps but also by legitimate apps. Many legitimate apps are characterised by a certain degree of privacy invasiveness, which is related to the permissions they request and to which use they make out of the protected methods. In this direction, TaintDroid (Enck etÂ al., 2010) as well as other research works (Gibler etÂ al., 2012; Stirparo and Kounelis, 2012; Zhou and Jiang, 2013) demonstrate the type of end-users' personal data manipulation performed by mobile apps."
"Our analysis consists in:1.extracting static features (i.e., permissions and respective invoked methods of the Android API) from apps using the Dexpler (Bartel etÂ al., 2012) and Soot framework (Vallee-Rai etÂ al., 1999);"
identifying the sensitive method invocations incorporated in a given application using the permission map published in Felt etÂ al. (2011).
 Other orthogonal approaches such as Nan etÂ al. (2015) and Zhou etÂ al. (2013) reveal that runtime information gathering could disclose users' different states (inside/outside of their house) and impose a real threat even for their safety.
"Fig.Â 4 presents a high level overview of the solution we propose. Our framework starts with the decompilation of the Android app using the ApkTool decompiler (Tumbleson and Winiewski, 2010) that reads the App .apk file (step 1) and produces the original bytecode (step 2)."
We refer the reader to Neisse etÂ al. (2015) for a complete description of all operators and semantics of the language.
"Consequently, similar to pure Java applications, Android apps can be reverse-engineered using the appropriate tools, i.e., smali/baksmali (Gruver, 2009), ApkTool (Tumbleson and Winiewski, 2010), Androguard (Desnos, 2012), Dexpler (Bartel etÂ al., 2012) and Dex2Jar (Pan, 2012).Using this reverse engineering tools it is possible to access an intermediate (human) readable representation of an app compiled bytecode, without having access to the original source code itself."
"Also Apex, introduced by Nauman etÂ al. (2010), focuses on policy enforcement for regulating ICC flows. In contrast to Saint, Apex supports more complex policy conditions using dynamic attributes including cardinality and time dimension constrains, i.e., restricting the maximum number of SMS messages sent by an app. Policy rules must be defined to manage the initialisation, updating, and resetting of dynamic attributes. Both Saint and Apex support authorisation actions to allow or deny an ICC flow, without the possibility of modifying or obfuscating a flow which is supported in our framework."
"CRePE, introduced by Conti etÂ al. (2011)., is also a customised Android OS system able to enforce fine-grained security policies considering time and location features. Policies in this system intercept authorisation requests before the standard Android permission checks, so that if the request is allowed by CRePE the standard permission check may still deny it. In addition to the standard permission checks, it also intercepts and enforces policies when activities are started. Policies in CRePE consist of propositional conditions of allow or deny actions, which are less expressive than our policy rules that also support modifications and delays as enforcement actions"
"Shabtai etÂ al. (2010) first proposed the use of SELinux in Android to implement low-level Mandatory Access Control (MAC) policies. From Android 4.3 on, SELinux is used by default to further define apps in permissive mode, only logging permission denials. From Android 5.0, SELinux is used in full enforcement mode, in which permission denials are logged and enforced according to the specified policies"
"Batyuk etÂ al. (2011) introduced Androlyzer, a server based solution that focuses mainly on informing users about apps potential security and privacy risks. To do this, Androlyzer first does the reverse engineering of the app, and then a static analysis to determine possible flaws. In addition, Androlyzer provides an approach to mitigate the identified flaws by modifying the examined app based on users' preferences. However, Androlyzer does not use an expressive policy language to support users in enforcing their security requirements into an app."
"Papamartzivanos etÂ al. (2014) propose a cloud-based crowd-sourcing architecture where users share any locally logged information about the app of interest. The authors' goal is to use the exchanged logs to calculate the app's privacy exposure level considering the exchanged information between the various participants in the system. The authors use the Cydia Substrate, which can only be installed in rooted devices to hook code in method invocations and object creations. A user may decide to always allow, deny, or be asked about what to do every time a hooked method is invoked by the running app."
"TISSA, proposed by Zhou etÂ al. (2011), introduces a privacy mode functionality in Android with coarse-grained control over the behaviour of an app. Using TISSA users can have more fine-grained control over private information like location, phone identity, contacts, call log, etc. TISSA is implemented as a modified OS with proxy content providers for each controlled information that are responsible for retrieving and enforcing the corresponding policies. TISSA's policies are hard-coded and restricted to a static set of authorisation options without support for complex conditions. A very similar approach with slightly less control on private information is introduced by Beresford etÂ al. (2011) in their solution named MockDroid. Complementary, AppFence proposed by Hornyack etÂ al. (2011)., also implemented as a modified OS on the basis of TaintDroid, shadows and ex-filtrates users' private data according to their preferences."
"Schreckling etÂ al. (2013) introduce Kynoid, a solution that extends Taintdroid with security policies at the variable level. Kynoid retains the taint propagation performed by Taintdroid and maintains a dependency graph where a direct edge represents a security requirement (a.k.a. policy) between two objects. In this way, Kynoid provides a fine grained control to sensitive flows. The focus of Kynoid is on information flow policies, which are not explicitly supported in our framework and are part of our future work."
"Zhauniarovich etÂ al. (2014) propose MOSES, which enforces context-based policy specification at the kernel level, meaning that MOSES requires a modification to the underlying OS. In this approach users can define a security profile that could be applied in a specific context, i.e., at a specific time and location for a given app. Note that if a security profile is not linked to an app, then MOSES does not allow access to any âsensitiveâ resource since by default employs a negative authorisation policy. MOSES security profile consists of allow or deny rules according to user's requirement."
"IdentiDroid, proposed by Shebaro etÂ al. (2014), is a customised version of Android which gives to the user the possibility to switch in an anonymous modality that shadows sensitive data and block permissions at runtime. Even though there is no a complex policy definition and refinement, the IdentiDroid Profile Manager allows users to define different profiles specifying which applications can access or not sensitive data and resources."
"Bagheri etÂ al. (2015) in DroidGuard introduce a framework for modelling inter-app vulnerabilities and employing the appropriate protection mechanism to enhance user's privacy and security. Briefly, DroidGuard analyses statically a set of given apps to foresee security flaws realising through apps intercommunication. The generated model is used as a policy to be employed as a proactive countermeasure."
" Hence, an algorithm that performs generally well on all these problems is usually desirable. Motivated by this consideration, a general framework Population-based Algorithm Portfolios (PAP) has been proposed [17]."
" Although some analyses have been conducted in [17] to give guidelines along this direction, no approach has been developed."
" For example, statistical racing [4,29] is a general-purpose tool to find an algorithm that performs as well as possible on a problem class. "
"A representative method in this category is the so-called âracing multiple algorithms on a single problemâ approach proposed by Yuan and Gallagher [29], which is an extension of statistical racing. For a given problem, this approach first executes all the candidate algorithms on the problem and compares the different algorithms with a pre-defined statistical test. Candidate algorithms that perform significantly poorly (in statistical sense) will be eliminated. Then, the remaining candidate algorithms are applied to the problem again to further eliminate some candidate algorithms. This procedure is repeated until only one candidate is left or the time budget is used up."
" Another intra-problem method that is worthy of mention is the intra-problem Adaptive Online Time Allocation (intra-AOTA) approach [8]. As shown by its name, intra-AOTA does not directly select the best algorithm, but iteratively allocates the time budget to a set of candidate algorithms. To be specific, intra-AOTA divides the given time budget into several slots. Each slot corresponds to an iteration. At the first iteration, the time slot is allocated to all the candidate algorithms according to some prior distributions or rules. Then, the average fitness of solutions obtained by each algorithm on the problem is recorded. Then, a linear model that maps the time allocation scheme to expected performance improvements is built and employed to determine the time allocation scheme for the next iteration. This procedure is repeated until the total time budget is used up. In the extreme case, intra-AOTA can also be used for algorithm selection by allocating all the time to a single candidate algorithm at each iteration. AMALGAM [25] and EEAs [16,30] also adopted a similar adaptation strategy as intra-AOTA. "
"To evaluate the effectiveness of EPM-PAP, experimental studies have been carried out. Four existing EAs, including self-adaptive differential evolution with neighborhood search (SaNSDE) [27], particle swarm optimizer with inertia weight (wPSO) [19], generalized generation gap (G3) model with generic parent-centric recombination (PCX) operator (G3PCX) [6] and covariance matrix adaptation evolution strategy (CMA-ES) [2], were chosen as the candidate EAs. "
"Concretely, we used all the parameter settings suggested in [27] when implementing SaNSDE."
"According to [19], a linearly decreasing inertia weight over the course of the search is employed in wPSO. The two coefficients of wPSO were both set to 1.49445. For G3PCX and CMA-ES, we simply used the source code of G3PCX and CMA-ES provided by their inventors (the codes are available online) and the parameters were set according to [2,3,6]. There exist a few variants of PCX operator. As suggested in [19], the variant that employs the best individual in the population as the main parent for generating offspring was used."
"Non-parametric multiple-comparison statistical test described in [7] has been conducted to analyze the performance of all the compared algorithms. Specifically, two sets of tests have been carried out. "
"Following [17], U(A ,F,T) is defined based on the pair-wise comparison of PAP instantiations, as given in Eq (2): (2)P(A >A |F)=1n k=1nP(A k>A k |fk);fk Fwhere A and A are different subsets of A and represent the corresponding PAP instantiations"
"To make statistical inferences from the observed difference in AUC, we followed the recommendations given in a recent article (DemÅ¡ar, 2006) that looked at the problem of benchmarking classifiers on multiple data sets. The recommendations given were for a set of simple robust non-parametric tests for the statistical comparison of the classifiers (DemÅ¡ar, 2006)."
"In Weiss and Provost (2003) it was found that the naturally occurring class distributions in the 25 data sets looked at, often did not produce the best-performing classifiers. More specifically, based on the AUC measure (which was preferred over the use of the error rate), it was shown that the optimal class distribution should contain between 50% and 90% minority class examples within the training set."
" Alternatively, a progressive adaptive sampling strategy for selecting the optimal class distribution is proposed in Provost, Jensen, and Oates (1999). Whilst this method of class adjustment can be very effective for large data sets, with adequate observations in the minority class of defaulters, in some low default portfolios there are only a very small number of loan defaults to begin with."
"Various kinds of techniques have been compared in the literature to try and ascertain the most effective way of overcoming a large class imbalance. Chawla, Bowyer, Hall, and Kegelmeyer (2002) proposed a synthetic minority over-sampling technique (SMOTE) which was applied to example data sets in fraud, telecommunications management, and detection of oil spills in satellite images."
" In Japkowicz (2000), over-sampling and downsizing were compared to the authorâs own method of âlearning by recognitionâ in order to determine the most effective technique. The findings, however, were inconclusive but demonstrated that both over-sampling the minority class and downsizing the majority class can be very effective."
"Subsequently, Batista (2004) identified ten alternative techniques in dealing with class imbalances and trialed them on thirteen data sets. The techniques chosen included a variety of under-sampling and over-sampling methods. Findings suggested that generally over-sampling methods provide more accurate results than under-sampling methods. Also, a combination of either SMOTE (Chawla et al., 2002) and Tomek links or SMOTE and ENN (a nearest-neighbour cleaning rule), were proposed."
"The least square support vector machine (LS-SVM) proposed by Suykens, Van Gestel, De Brabanter, De Moor, and Vandewalle (2002) is a further adaptation of Vapnikâs original SVM formulation which leads to solving linear KKT (KarushâKuhnâTucker) systems (rather than a more complex quadratic programing problem)."
A more detailed explanation of how to train a random forest can be found in Breiman (2001). For the Random Forests classification technique two parameters require tuning. These are the number of trees and the number of attributes used to grow each tree.
"A more detailed explanation of gradient boosting can be found in Friedman (2001, 2002). The gradient boosting classifier requires tuning of the number of iterations and the maximum branch size used in the splitting rule."
The performance criterion chosen to measure this effect is the area under the receiver operator characteristic curve (AUC) statistic as proposed by Baesens et al. (2003).
" The AUC statistic was computed using the ROC macro by DeLong, DeLong, and Clarke-Pearson (1988), which is available from the SAS website (http://.support.sas.com/kb/25/017.html)."
"For the LS-SVM classifier, a linear kernel was chosen and a grid search mechanism was used to tune the hyper-parameters. For the LS-SVM, the LS-SVMlab Matlab toolbox developed by Suykens et al. (2002) was used."
"The k-Nearest Neighbours technique was applied for both k=10 and k=100, using the Weka (Witten & Frank, 2005) IBk classifier. For the gradient boosting classifier a partitioning algorithm was used as proposed by Friedman (2001). "
"We used Friedmanâs test (Friedman, 1940) to compare the AUCs of the different classifiers. The Friedman test statistic is based on the average ranked (AR) performances of the classification techniques on each data set, and is calculated as follows:"
"The post hoc Nemenyi test (Nemenyi, 1963) is applied to report any significant differences between individual classifiers. The Nemenyi post hoc test states that the performances of two or more classifiers are significantly different if their average ranks differ by at least the critical difference (CD), given by(12)"
" This finding seems to confirm the suggestion made in Baesens et al. (2003) that most credit scoring data sets are only weakly non-linear. However, techniques such as QDA, C4.5 and k-NN10 perform significantly worse than the best performing classifiers at each percentage reduction. The majority of classification techniques yielded classification performances that are quite competitive with each other."
" Three data sets commonly used in the literature were employed for this purpose, namely, UIUC [9], Outex [13], and KTH-TIPS2b [2]. "
"To evaluate the classification performance, the proposed descriptors were also compared to other state-of-the-art and classical texture descriptors: Grey-Level Co-occurrence Matrix (GLCM) [8], Fourier [7], multifractals [19], Local Binary Patterns (LBP) [14], LBP+VAR [14], and MR8 [17]."
The ability of the connected fractal dimension to differentiate between images of normal and abnormal retinal vessels in [10] suggests that concept of connectivity might also be useful in other applications where the local regularity of the binary image needs to be assessed. 
"To extend the notion of connectivity to grey-level images, a strategy was proposed in [5] based on a pseudo-three-dimensional representation of grey-level images. "
"Even though a similar definition of adjacency can be used in three-dimensional spaces (26-adjacency, for instance), this is not directly applicable to sparse sets of points such as those in the grey-level mapping. Therefore, a new definition for adjacency was established in [5] based on Euclidean distances between points"
More details and a pseudo-code is provided in [5] for the interested reader.
The version of Outex database used here is the suite Outex_TC_00013 in [13] and contains 1360 colour images (here converted to a grey-scale) captured under controlled conditions of illumination and imaging geometry. 
"The classification of the databases according to the compared descriptors was carried out using a linear discriminant classifier (LDA) [4] after a principal component analysis (PCA) [4] to reduce the correlation among features in all compared approaches. Such approach for the classification prevents redundant information from being considered in the segmentation of the feature space. The simultaneous dimensionality reduction and decorrelation is achieved by a combination of PCA with the canonical analysis of LDA, which takes into account the distribution of features among the classes in the training set."
Particle swarm optimization (PSO) is a metaheuristic method for global optimization which is inspired by the behavior of a swarm of birds or fish [7].
"In this paper, we focus on the chaotic PSO exploiting a virtual quartic objective function based on the personal and global best solutions(CPSO-VQO)[17]"
"In this paper, the system (1)-(4) is called standard updating system (SP). This extremely simple approach is so effective that PSO have been applied to many optimization problems arising in various fields of science and engineering [7]"
This extremely simple approach is  so effective that PSO have been applied to many optimization problems arising invarious fields of science and engineering[7].
"the ability of this method to explore other areas for better solutions is  crucial to find high-quality solutions, and thus various improvements have been investigated[4,19]."
"One of the most popular of them is called the inertia weight approach (PSO-IWA) [5],which,as the search progresses, linearly reduces win(1)of(SP)in order to strengthen the diversification in the early stages and its intensification in the final stages of the search."
"On the other hand, metaheuristic methods exploiting a chaotic system based on the steep estdescent method have been investigated [15,18,20]; these methods normally search for a solution along the steepest descent direction of the objective function, but they can also execute an extensive search by exploiting the chaotic nature of generated sequences."
The GP method works better than do methods with the transformation and a larger step-size [16].
"Moreover,in[17],authors theoretically showed a sufficient condition under which the updating system used in CPSO-VQO is chaotic, and through computational experiments, demonstrated that CPSO-VQO perform well when applied to some global optimization problems[17]."
"Nevertheless, since wd is selected to be nonzero in CPSO-VQO from the reason mentioned above, almost all particles are able to search for solutions without becoming trapped, and such a behavior was not observed in the numerical experiments [17]."
"Theorem2 is an improved version of the original theorem by Marotto [11], which was proved by Li and Chen [9]"
"We set w=0 in (SP), and if the absolute value of jth component of the velocity of a particle i is sufficiently small,|vij(t)|<εR, then vij(t) is reset by a randomized number uniformly selected from (−Vmax,Vmax) to avoid being trapped at undesirable local minimum similarly to HPSO-TVAC [13]."
"We selected (ws,wf,c1,c2)=(0.7,0.2,2.0,2.0) for PSO-IWA, which were more appropriate parameter values for high-dimensional problems than those recommended in papers[5], and we selected (wc,cc1,cc2)= (0.5,2.0,2.0)for CEPSOA."
"We applied the self-adaptive differential evolution (SADE) [2] to the six benchmark problems, which is one of the differential evolution (DE)[14], a popular metaheuristic method for the continuous global optimization."
"In order to achieve the best possible results, we followed the paradigm of problem-oriented research, i.e., collaborating with real users to solve their tasks (Sedlmair et al., 2012). Therefore, we worked in accordance with the nested model for visualization design and validation by Munzner (Munzner, 2009), which divides visualization design into four levels combined with ap-propriate validation methods (see Section 2). Specifically, we focused on the third and fourth level of Munzner’s model (Munzner, 2009) (third level: visual encoding and interaction design, fourth level:"
"Our problem-oriented ap-proach to the study of knowledge-assisted visualization systems is based on our prior work (Wagner et al., 2014) which ana-lyzed the needs of malware analysts in relation to their work on behavior-based malware pattern analysis. For this, we started with a problem characterization (Sedlmair et al., 2012) corre-sponding to the first level of the nested model, whereby we followed a three-step qualitative research approach includ-ing systematic literature research, focus group meetings and semi-structured interviews."
"Based on the gained insights, we analyzed the data, the users and the tasks in the problem domain using the data-users-tasks analysis framework introduced by Miksch and Aigner (Miksch and Aigner, 2014). This corresponds to the second level of the nested model (operation and data type abstraction)."
" These data providers are described in detail in Section 3 of a survey by Wagner et al. (2015). Both approaches (static and dynamic analy-sis) yield patterns and rules, which are later used for malicious software detection and classification."
"For the visualization of patterns which are in-cluded in the represented execution order, arc-diagrams (Wattenberg, 2002) are used (see Fig. 4.2.d). In this way, the analyst receives a visual representation of recurrence pat-terns up to the five largest patterns in a rule."
"In addition to the design decision in relation to a programming IDE, we used Gestalt principles (Johnson, 2014) to improve interface clarity. Each exploration area (Rule Ex-plorer and Call Explorer) contains its own filtering area below the data visualization (based on the Gestalt principles of prox-imity and similarity)."
"For a better understanding of its func-tionality, we describe KAMAS according to five steps based on the visual information seeking mantra by Shneiderman (1992): overview first, rearrange and filter, details-on-demand, then extract and analyze further."
"By monitoring all activities, these systems generate a report (i.e. trace) of system and API calls sequentially invoked by the sample. In the second step, the traces are clustered with Malheur, an automated behavior analy-sis and classification tool developed by Rieck et al. (Rieck, 2016). In the third step, all traces within a cluster are concatenated and processed using the Sequitur algorithm (Nevill-Manning and Witten, 1997). In relation to this step, the system and API"
"To increase the perfor-mance of our prototype, we decided to use a data-oriented design (Fabian, 2013) (e.g., used in game development and real time rendering) to organize and perform transformations on the loaded data. "
"To support the malware analysts during their work, we integrated a KDB related to the malware behavior schema by Dornhackl et al. (2014)., which is included on the left side of the interface (see Fig. 4.1) as an indented list (tree structure)."
"Basically, the dynamic query features (Ahlberg et al., 1992) were described as being very useful. If the user left the focus of a filtering input box, the in-terface had to automatically apply the entered parameter."
"The results show a SUS value of 75.83 points out of 100, which can be interpreted as good without signifi-cant usability issues according to the SUS description by Bangor et al. (2009). They described the SUS questionnaires result from different perspectives: "
"Based on the SUS description and average evaluation of the system by the par-ticipants, the result of the usability assessment was very positive. Sauro (2011) compared 500 SUS scores and identi-fied the average score as 68 points. Additionally, he showed that only 36% of the compared tests reached an SUS score higher than 74, and only 10% of the systems reached an SUS score greater than 80, which shows us that our system receives a grade of ‘B” at this implementation state."
"In contrast to other malware analysis systems which build their visual metaphors directly on the output of the data providers, KAMAS uses an input grammar generated by a combination of Malheur (Rieck, 2016) and Se-quitur (Nevill-Manning and Witten, 1997) for cluster and data classification. Therefore, we use analytical and visual represen-tation methods to provide a scalable and problem-tailored visualization solution following the visual analytics agenda (Keim et al., 2010; Thomas and Cook, 2005)."
"In many applications, it is common to consider sequences with circular structure: for instance, the orientation of two images or the leftmost position of two linearised circular DNA sequences may be irrelevant. To this end, an algorithm to compute the cyclic edit distance in time O(mn log m ) was proposed (Maes, 2003 [18]) and several heuristics have been proposed to speed up this computation."
"Re- cently, a new algorithm based on q -grams was proposed for circular sequence comparison (Grossi et al., 2016 [13]). We extend this algorithm for cyclic edit distance computation and show that this new heuris- tic is faster and more accurate than the state of the art."
"The contours of a shape may be represented through a cyclic sequence which can be used in the computation of the cyclic edit distance. This can identify similarities in shapes which appear to be distinct from one another [20,26] ."
"Circular molecular structures are abundant in all domains of life: bacteria, archaea, and eukaryotes, and in viruses. Exhaustive reviews of circular molecular structures can be found in [7] and [14] ."
"Due to this arbitrariness, a suitable rotation of one sequence would give much better results for a pairwise alignment. This motivates the design of efficient algorithms that are specifically devoted to the comparison of circular sequences [1,4,5,13] ."
"An exact branch and bound algorithm based on Maes’s algorithm, which runs in time O(mn log m ) , was proposed by Barrachina and Marzal [2] . This method explores only the nodes on the edit graph that could lead to an optimal path, resulting in a much faster algorithm on average ."
"The weighted Bunke and Buhler algorithm ( WeBBA ) combines the lower and upper bound estimations, computed by the BBA and EBBA algorithms, to produce an approximation of the cyclic edit distance in time O(mn ) [23] . It is perhaps the best performing heuristic currently."
Palazon-Gonzalez and Marzal [27] studied the same problem but from the indexing point of view for classification and re- trieval.
Grossi et al. [13] presented an exact algorithm to compute the β-blockwise q -gram distance between x and y .
Each rotation of x   is then compared to y   excluding when a letter of x i 1 (letter $ ) is found at index 0 of the rotation of x   . Notice that the notion of edit distance is not appropriate here due to the existence of letter $ which denotes a don’t care letter.To this end we make use of the Needleman–Wunsch algorithm [25] to compute a similarity score for each rotation of string x   and string y   .
"The standard edit distance al- gorithm is used when computing the edit distance with non-unit costs. It runs in time O(mn ) [8] . Hence, notice that, compared to the other heuristics, hCED offers an additional advantage."
The space complexity is O(βm + n ) ; the edit distance and Needleman–Wunsch algorithms can both be implemented in O(m + n ) space [8] .
Algorithm hCED can now be directly used for computing the cyclic edit distance between all pairs of sequences for progressive multiple cir- cular sequence alignment [3] .
"The most prominent approach is to apply a sliding window technique (e.g., Dalal and Triggs, 2005; Nair and Clark, 2004; Felzenszwalb et al., 2008; Viola et al., 2003)"
"Typically, the goal of such methods is to build a generic model that is applicable for all possible scenarios and tasks (e.g., Leibe et al., 2008; Felzenszwalb et al., 2008; Dalal and Triggs, 2005)."
"In fact, to train such classifiers less training data is required and for the particular task they are usually better in terms of accuracy and efficiency (Levin et al., 2003; Wu and Nevatia, 2007a; Roth et al., 2005)."
"More specific and thus more efficient classifiers avoiding these problems can be trained using classifier grids (e.g., Grabner et al., 2007; Stalder et al., 2009; Roth et al., 2009)"
"Adaptive approaches, in general, suffer from the drifting problem, i.e., due to wrong updates the system starts to learn something completely different degrading the classification performance. To avoid drifting in classifier grids (Roth et al., 2009) applied fixed update strategies."
"Even though for most object detection scenarios a stationary camera can be assumed, this constraint, which could help to drastically improve the classification performance, has been only of limited interest (e.g., Hoiem et al., 2006; Roth and Bischof, 2008; Wu and Nevatia, 2007b; Nair and Clark, 2004)."
"The first problem was addressed in Roth et al. (2009), where the main idea was to further increase the stability and to speed up the computation by a combination of two generative models in parallel: a pre-trained model for the positive class and an adaptive model for the negative class."
"The authors showed that the recall can be drastically increased, but on the expense of the precision. In contrast, in Sternig et al. (2010b) we proposed to use a co-training approach (Classifier Co-Grids) in combination with a robust on-line learner"
"For calculating the recall–precision curves (RPC) a detection is counted as true positive if it fulfills the overlap criterion (Agarwal et al., 2004), where a minimal overlap of 50% is required."
"As we already showed in Roth et al. (2009) that the approach is robust even when running in a real-world 24/7 setup, the goal of this paper was to address the problem of short-time drifting if objects are not moving for a longer period of time."
"However, since in our case the ambiguity concerns the negative samples, we modified the original multiple-instance learning idea (inverse MIL). We adapted on-line MILBoost (Babenko et al., 2009) to fit to our problem"
"However, several common characteristics can be identified. First of all, most works consider the total distance traveled or the routing costs of the nurses in the objective function (see e.g. Akjiratikarl, Yenradee, & Drake, 2007; Begur, Miller, & Weaver, 1997; Eveborn, Flisberg, & Rönnqvist, 2006; Eveborn, Rönnqvist, Einarsdóttir, Eklund, Líden, & Almroth, 2009; Hiermann, Prandtstetter, Rendl, Puchinger, & Raidl, 2015; Mankowska, Meisel, & Bierwirth, 2014; Rasmussen, Justesen, Dohn, & Larsen, 2012; Trautsamwieser, Gronalt, & Hirsch, 2011), often in addition to a number of other terms."
"Since the latter functions are piecewise linear, they can be optimized efficiently (despite being non-convex) using dynamic programming (Vidal et al., 2015)"
"The idea behind this method is partially based on existing methods for non-convex piecewise linear cost functions (for an overview we refer to Vidal et al. (2015) and Hashimoto, Yagiura, Imahori, and Ibaraki (2013))"
A successful general-purpose LNS algorithm for a variety of vehicle routing problems was proposed by Pisinger and Ropke (2007).
"Several simple removal and insertion operators, selected randomly in each iteration, are applied. An adaptive version of LNS is proposed by Ropke and Pisinger (2006) in which the selection of the operators is biased using their success in previous iterations"
"Tricoire (2012) shows that, using LNS as a subheuristic, the MDLS framework produces results which are competitive to those of the best known solution method for three general multi-objective optimization problems (multi-dimensional multi-objective knapsack problem, bi-objective set packing problem, bi-objective orienteering problem)."
"To assess the quality of the proposed metaheuristic, Pareto-optimal solutions for small problem instances are generated by embedding the model described in Section 2.1 into the well-known -constraint scheme (Laumanns, Thiele, & Zitzler, 2006)."
"TD and TDM receive attention currently both in the academia and the industry (Li et al., 2015a). Researchers and practitioners are becoming more interested in the concept of TD and the reasons why it should be an essential part of decision-making in software development (Falessi et al., 2014)."
"When TD starts to accumulate, it is often a safer and faster choice to take more TD with a quick and dirty solution, because there is a risk of breaking the product even more by modifying a complex part of the code base (Yli-Huumo et al., 2015a).Thus, code base complexity can force the company to take more TD intentionally, because the fixing of current TD would take too much time and money, while quick and dirty solutions are easier and faster to implement (Yli-Huumo et al., 2014)."
"A portfolio approach for TDM has been suggested by Guo and Seaman (2011). The approach is widely used in the finance domain as a risk reduction strategy for investors, to determine the types and amounts of assets to be invested or divested. "
"Code reviews, where another developer checks your code can be used to prevent bad solutions from getting to the code base (Baker, 1997; Kemerer and Paulk, 2009), while setting up coding standards/guidelines for the development team to ensure as much cohesion as possible during the development (Green and Ledgard, 2011) can improve understandability and learnability."
"Documentation is a valuable practice that improves understandability and communication (Das et al., 2007; Forward and Lethbridge, 2002). Therefore, adopting even a simple documentation practice for TD representation/documentation improves other TDM activities and the overall TDM strategy."
" Based on the referenced literature [2-5], we modified (B)â(Bâ²), which represents the âfuzzy-stateâ assumption. This modified assumption asserts that at any given time, the system has only two states, namely, the fuzzy success state, and the fuzzy failure state. "
"Cheng and Mon [6] used the Î±-cut of Level-1 fuzzy numbers to obtain the intervals and determine the fuzzy reliability of the serial system. In addition, they successfully identified the fuzzy reliability of the parallel system. "
" Furthermore, to ensure easy defuzzification, the signed distance proposed by Abbasbandy and Asady [1] must be considered and modified into the signed distance of an interval-valued fuzzy number."
"Using a method similar to that of Yao and Wu [13], we considered the signed distance and ranking on FIV(Î»,Ï). "
"Using the same arguments as those of Yao and Wu [13], we obtained the following properties"
"By applying the method of Kaufmann and Gupta [10] and Zimmermann [14], we derived the following property:"
"Using the method proposed by Zimmermann [14], we used Fig. 4 approximate to Fig. 3"
Example 1 is based on the example presented by Chen [7] and Singer [12].Two grinding machines are working next to each other. 
"Thus, the interaction cannot be modeled using the statistical mean properties of turbulence e.g. turbulent kinetic energy and dissipation rate [3-5]. Instead, the interaction might be better described by the distribution of the properties of single turbulent vortices, such as vortex size, lifetime, number density (the number of turbulent vortices per unit fluid volume), growth and dissipation rate, and the turbulent kinetic energy for vortices of different sizes at different locations.Moreover the detailed description of turbulence spectra helps to improve understanding of turbulence. "
"The understanding of turbulence is a part of wish list, suggested at Turbulence Colloquium in Marseille 2011 for current and future studies [6]. The intention of this research work is to improve the understanding of turbulence."
"Here, the wave number, Îº, is 2Ï/Î». uÂ¯Î» is the mean fluctuating velocity of turbulent vortices of size Î» and it is theoretically given by [10]"
The experimental data shows that the constant Î± is 1.5 approximately [11].
Risso and Fabre [12] have used the same value as given by Pope [13] and Lasheras [14] has pointed out that there is a range for C from about 2â8.2.
The coefficient CS is determined using the dynamic model [13].
The numerical procedure was based on an implicit iterative technique with a pressure based solver. It is important that the LES simulations are run for at least a few mean flow residence times to become statistically steady [16]. 
"When 80% of the turbulent kinetic energy is resolved, the LES simulation can be considered well-resolved [17].Another measure of resolution quality is the ratio of instantaneous subgrid turbulent viscosity to the molecular viscosity. "
"Two-point correlations of velocities, is another important method to determine the resolution of resolved-scale flow field in large eddy simulation. Su et al. [19] and Davidson [20] used two point correlations of velocities to quantify how many cells are resolving the large structures."
"At least five to ten cells are required to ensure that the largest scales are well resolved in LES [20]. For both Reynolds numbers studied, the two-point correlation analysis showed that at least fifteen grid cells were resolving the large scales."
" Chakraborty and et al. [22] showed that the Q-criterion and Î»2 almost give the same flow structures.When there is no imposed non-uniform strain field in the turbulent flow, the Q-criterion can be used to identify the core location of the turbulent vortices. The Q-criterion represents the local balance between vorticity and strain rate. In order to visualize a wide range of vortices even the weakest one in the bulk, the Q-criterion can be normalized with respect to the vorticity [18]."
"Recently, an analysis of the turbulent kinetic energy on a 2D plane of a 3D LES simulation revealed that less than 40% of the turbulent kinetic energy (TKE) on the plane would be captured within the structures identified by the lowest possible cut off Q-criterion [18]."
"Since the cut off used in the algorithm (0.1) is located very close to the peak in turbulent kinetic energy (Fig. 6), an extension of the volume identified with the normalized Q-criterion will allow more turbulent kinetic energy to be captured within the vortex. To extend the vortex volume, the BiotâSavart law was implemented.A distribution of vorticity in a vortex induces the relative velocity field based on the BiotâSavart law [23]. "
"By increasing the Reynolds number, the inertial subrange in a turbulent flow is increased [24]. As shown in Fig. 9, the range over which the number densities are following the modeled line, is larger in the case of higher Reynolds number i.e. at Re=50,000, which is expected."
"Here, we introduce the discriminative skin-presence features (DSPFs), derived from the discriminative textural features (DTFs) described in our earlier works on skin detection (Kawulok, 2012) and image colorization (Kawulok et al., 2012). The differences between the DTFs and the DSPFs, discussed later in Section 4, allow for the computation time reduction and efficacy improvement. "
"Existing color-based skin segmentation techniques take advantage from the observation that skin-tone color has common properties which can be defined in various color spaces. In general, skin color detectors rely on rule-based or statistical skin modeling. A thorough survey comparing various color-based skin detection approaches was presented by Kakumanu et al. (2007)."
" Skin-tone color was modeled in the HSV color space by Tsekeridou and Pitas (1998). Kovac et al. (2003) proposed a model defined in the RGB color space. An approach introduced by Hsu et al. (2002) takes advantage of common skin color properties in nonlinearly transformed YCbCr color space using an elliptical skin color model. A technique operating in multiple color spaces to increase the stability was described by Kukharev and Nowosielski (2004). Cheddad et al. (2009) proposed reducing the RGB color space to a single dimension, in which the decision rules are defined."
Lee et al. (2007) proposed a method based on a multi-layer perceptron extracting lighting features from an analyzed image to adjust the skin detector. An approach for adapting the segmentation threshold in the probability map based on the assumption that a skin region is coherent and should have homogenous textural features was introduced by Phung et al. (2003). This method was further extended by Zhang et al. (2004) by involving the artificial neural network (ANN) for estimating an optimal acceptance threshold. The ANN was also used for adaptation by Yang et al. (2010). A method for a dynamic model adaptation based on observed changes in the histogram extracted from a tracked skin region was proposed by Soriano et al. (2000). Motion detectors for the skin color model adaptation were explored by Dadgostar and Sarrafzadeh (2006). 
"An interesting algorithm incorporating color, texture and space analysis was given by Jiang et al. (2007). Initially, skin probability map color filter with a low acceptance threshold is applied in the RGB color space. "
"Although the color-based skin models can be efficiently adapted to a given image, it was proved by Zhu et al. (2004) that it is hardly possible to separate skin from non-skin pixels using such approaches. It is easy to see that skin pixels are usually grouped into blobs whereas the non-skin false positives are scattered around the spatial domain. A number of skin segmentation techniques emerged based on this observation: Kruppa et al. (2002) assumed that the skin blobs are of an elliptical shape, a threshold hysteresis was applied by Argyros and Lourakis (2004) and recently by Baltzakis et al. (2012). "
"In the presented study, the skin probability maps were obtained using Bayesian skin modeling introduced by Jones and Rehg (2002). The method consists in analyzing the color histograms of the skin and non-skin pixels, and the skin probability, given a certain color value, is determined using the Bayes rule."
"where Ï is a local skin dissimilarity measure between two neighboring pixels, p0 is a pixel that lies at the seed boundary, pl=x, and l is the total path length. The minimization is performed using the Dijkstraâs algorithm as proposed by Ikonen and Toivanen (2007). In addition, PÎ² threshold is used as proposed by del Solar and Verschae (2004), which prevents the propagation to the regions of very low skin probability."
"We have implemented the approach as an extension to the EvoSuite tool (Fraser and Arcuri, 2013b), and analyze the effects of the different parameters involved in the local search, and determine the best configuration."
"Different types of local search algorithms exist, including simulated annealing, tabu search, iterated local search and variable neighborhood search (see Gendreau and Potvin, 2010, for example, for further details). A popular version of local search often used in test data generation is Korel's Alternating Variable Method (Korel, 1990; Ferguson and Korel, 1996). The Alternating Variable Method (AVM) is a local search technique similar to hill climbing, and was introduced by Korel (1990). "
"In contrast to local search algorithms, global search algorithms try to overcome local optima in order to find more globally optimal solutions. Harman and McMinn (2010) recently determined that global search is more effective than local search, but less efficient, as it is more costly."
"The use of MAs for test generation was originally proposed by Wang and Jeng (2006) in the context of test generation for procedural code, and has since then been applied in different domains, such as combinatorial testingÂ (Rodriguez-Tello and Torres-Jimenez, 2010). Harman and McMinn (2010) analyzed the effects of global and local search, and concluded that MAs achieve better performance than global search and local search. In the context of generating unit tests for object-oriented code, Arcuri and Yao (2007) combined a GA with hill climbing to form an MA when generating unit tests for container classes. "
"In the context of generating unit tests for object-oriented code, Arcuri and Yao (2007) combined a GA with hill climbing to form an MA when generating unit tests for container classes. Liaskos and Roper (2008) also confirmed that the combination of global and local search algorithms leads to improved coverage when generating test cases for classes."
"Floating point datatypes: for floating point variables (float, double) we use the same approach as originally defined by Harman and McMinn (2007) for handling floating point numbers with the AVM. "
"Considering the high costs of fitness evaluations in the test generation scenario, a generally preferred choiceÂ (El-Mihoub etÂ al., 2006) is Lamarckian learning, i.e., the local search changes the genotype and its fitness value, rather than just the fitness value. A common implementation of MAs applies this learning immediately after reproductionÂ (Moscato, 1989). However, there remain several different parametersÂ (El-Mihoub etÂ al., 2006):"
"Therefore, we chose classes already used in previous experimentsÂ (Arcuri and Fraser, 2013), but excluded those on which EvoSuite trivially already achieves 100% coverage, as there was no scope for improvement with local search. In order to ensure that the set of classes for experimentation was variegated, we tried to strike a balance among different kinds of classes. To this end, beside classes taken from the case study in Arcuri and Fraser (2013), we also included four benchmark classes on integer and floating point calculations from the Roops22http://code.google.com/p/roops/. benchmark suite for object-oriented testing, This results in a total of 16 classes, of which some characteristics are given in Table 1. "
"On the other hand, there are several research questions in unit test generation that are still open and may influence the degree of achievable improvement, like handling of files, network connections, databases, GUI events, etc. Therefore, we used the case study of the Carfast (Park etÂ al., 2012) test generation tool33Available at: http://www.carfast.org, accessed June 2013. "
"During all these runs, EvoSuite was configured using the optimal configuration determined in our previous experiments on tuningÂ (Arcuri and Fraser, 2013). To evaluate the statistical and practical differences among the different settings, we followed the guidelines by Arcuri and Briand (2014). "
"RQ4 showed how different classes influence the effectiveness of local search. Consequently, instead of applying local search with a fixed configuration, we next consider how doing so in an adaptive way influences results. As described in SectionÂ 4.3, we use the adaptive methods introduced by Galeotti etÂ al. (2013). "
" In contrast, the CarfastÂ (Park etÂ al., 2012) case study is devoid of such environmental dependencies, but still consists of a set of automatically generated software projects that are intended to be realistic. "
" The nearest approximation we are aware of is Scheelen etÂ al. (2012), who investigated a single company by connecting with followers on LinkedIn, where the social media structure is based around employment."
"Obvious features often work well: Perito etÂ al. (2011) focused on the identifiability of usernames. As well as contributing a Markov modelling approach for estimating uniqueness of usernames which suggested that they are on average highly unique identifiers, they build and evaluate a classifier which links profiles based on username pairings, achieving good classification accuracy, and suggesting that usernames are an ideal feature for connecting profiles.Combining features can also prove effective. "
"Our method relies on linking profiles, a practice which specifically ties to certain vulnerabilities. Linked profiles can be particularly vulnerable to certain social engineering attacks. Chen etÂ al. (2012) detail some of these vulnerabilities, and demonstrate that additional details such as phone numbers can be better retrieved when multiple profiles of the target can be linked."
"As a complement to this, the absence of a profile on a certain social media network can be a vulnerability in itself. Kontaxis etÂ al. (2011) describes the profile cloning attack which lets social engineers use existing information on one person to imitate them on a service on which they do not have an account, along with a detection strategy for this.More generally, there is a wide body of literature regarding specific social engineering vulnerabilities. "
"From the roster pages we can extract the names of employees (using the Stanford NER tool (Finkel etÂ al., 2005)). This gives us a list of known employee names OE."
"The Google+ social network includes an âother profilesâ attribute which highlights profiles of the account holder on other online social networks. Using a method adopted from Gonzalez etÂ al. (2013), we randomly sampled 1161 Google+ profiles from the network.For those profiles from this sample which included an âother profilesâ attribute, the referenced profile was downloaded tothe extent permitted by that social network's API â acting only as an application or developer account, with no effort made at invasive methods such as issuing friend requests. "
"This sampling process is designed to avoid biases in the dataset being used for evaluating this component of the system, and is more fully described by Edwards etÂ al. (2016)."
"To date this focus has largely been attributed to technical shortcomings, demonstrated in the sharp rise of disclosed vulnerabilities (Kaspersky Lab, 2015), and neglecting the importance of social and organisational factors. An initial step towards understanding the potential impact of social engineering on ICS was discussed by Green etÂ al. (2015); however this work focuses solely on malicious emails, with vulnerability assessments achieved through the use of interview data."
" This could be combined with more active countermeasures, such as phishing email susceptibility tests as described by Finn and Jakobsson (2007), or by creating honeypot social media accounts in a similar manner to that described by Lee etÂ al. for uncovering social spammers (Lee etÂ al., 2010). "
"we can take advantage of the well-developed theory and algorithms of the latter, see [2,4,5,8,14,24,25], to mention only a few possible sources"
"We also remark that max-Łukasiewicz semiring can be seen as a special case of incline algebras of Cao, Kim and Roush [9], see also, e.g., Han–Li [28] and Tan [40]."
"Powers of matrices over distributive lattices are studied, e.g., by Cechlárová [11]."
"This work uses a terminology based on the Event Processing Technical Society (EPTS) glossary  [20], which originated from the CEP literature. This terminology has been chosen because its terms are broadly defined and encompass most of the SP concepts"
"Declarative: the expected results of the computation are declared, often using a language similar to SQL. The Continuous Query Language (CQL)  [21] is the most prominent representative of this category."
Imperative: the computations to be performed are directly specified using operators that transform event streams. The Aurora Stream Query Algebra (SQuAl)  [17] inspired most languages in this category.
"Similarly, the discussion around Big Data, and the rise of the MapReduce platform  [28], have also had a great impact on CEP."
"Other frameworks, such as Twitter’s Storm   [31] and Yahoo’s S4   [32], propose a more radical departure from the MapReduce programming model, but maintain runtime platforms inspired by MapReduce implementations."
"CloudSim   [12] is a well-known cloud computing simulator that can represent various types of clouds, including private, public, hybrid, and multi-cloud environments. In CloudSim, users define workloads by creating instances of cloudlets, which are submitted and processed by virtual machines (VMs) deployed in the cloud."
"Garg and Buyya  [9] created NetworkCloudSim, which extends CloudSim with a three-tier network model and an application model that can represent communicating processes. Guérout et al."
GreenCloud   [13] is a cloud simulator developed as an extension of the NS-2 network simulator  [36].
"Apache Storm is an open-source distributed stream processing system that has been adopted by many enterprises, despite limitations regarding QoS maintenance, privacy, and security  [42]."
"Note, however, that Storm is still a young product and many researchers are working to overcome its problems. For instance, Aniello et al.  [43] proposed a scheduler that can be used to improve the system performance, and Chang et al.  [44] introduced CCAF, a security framework that can be used to secure Storm deployments."
" Ideally, such optimization should be managed autonomously by the application itself, which should be able to self-optimize its functioning. For this purpose, widely accepted by the distributed systems community, is the use of the Autonomic Computing (AC) paradigm [2] to endow distributed systems with self-management capacities, such as self-adaptation and self-optimizing."
" In our approach, we model variability at the architectural level using the Common Variability Language  [7] (CVL). The reason for choosing CVL is twofold. First, it is an MOF-based variability language and this means that any MOF-based application model can be easily extended with variability information using CVL; second, it has been submitted to the OMG for its standardization and it is expected to be accepted soon as the standard for modelling and resolving variability."
"For the rest of steps, we follow the widely known MAPE-K loop  [8] of the AC paradigm, where MAPE stands for Monitoring, Analysis, Plan and Execution and K stands for Knowledge. "
"For this we use a GA called DAGAMEÂ  [22], optimized to be executed at runtime with scarce resources. As our DRS is installed inside a mobile device, we present some evaluation results showing that our approach is feasible and efficient enough to be executed with the fairly limited resources of a mobile device, resulting in good response times and nearly-optimal architectural configurations.The rest of the paper is organized as follows. "
" Using GAs it is possible to find nearly-optimal solutions for optimization problems without having to explore the whole solutions space. As stated by Guo etÂ al.Â  [23], applying GAs can be highly appropriate when the solutions space is very wide and it is not affordable to evaluate all of them due to a lack of resources and time.In genetic algorithms, candidates to be returned as the solution to the optimization problem are known as chromosomes, making up a population. "
"For instance, the criterion used to determine the utility of a component could be the precision and the measuring rate in the case of a component that provides location information or the quality in the case of a component for video streaming. Because of its ability to fit well with optimization problems based on variability, the concept of utility function has been applied before in other proposals, such as MUSICÂ  [13,12]."
" In this paper, we use the DAGAME algorithm that focuses on optimizing feature models configurations, to optimize the VSpecs tree, as it has proven to be efficient and produces nearly-optimal resultsÂ  [22]. Concretely, this algorithm is able to generate configurations with about 90% optimality, which means that the utility of the solutions obtained using this algorithm is approximately 90% of the utility of the optimal configuration that would be obtained using an exact algorithm."
" According to the definition provided inÂ  [31], an architectural element is quiescent if the following criteria are satisfied:1.It is in a passive state.2.It is not currently executing a transaction.3.No transactions initiated by other elements will require it.Therefore, when a component is quiescent, we can safely remove or modify it. Typically, the transitions from each one of these states is described using a state machineÂ  [31,32], as shown in Fig.Â 7.A weaker condition than the quiescence property is the tranquillity Â  [33] property. Tranquillity is less disruptive in the application running, but is still a sufficient condition to place an application in a consistent state before and after the runtime changes. Note that although both concepts can be considered in our approach, our current implementation is based on the quiescence property."
"It can be seen that the solutions obtained by DAGAME are very close to the optimal ones. If we apply the concept of optimality Â  [23], which is defined by Guo etÂ al. as the ratio between the utility of the solution obtained using DAGAME and the utility of the optimal solution, obtained using the exact method, we can see that, in the worst case, the optimality of the solutions obtained using DAGAME is higher than 87.4%. This is a very high degree of optimality, particularly taking into account that the optimization problem is NP-hard."
"This limitation is not strictly related to our approach, but to the optimization algorithm. However, as shown inÂ  [22], it is very straightforward to modify DAGAME in order to include the ability to take into account the usage of distinct resources. In fact, this extension of DAGAME is part of our future work."
"Vassev etÂ al.Â  [34] propose ASCENS, a framework for the representation and reasoning of knowledge, which is defined as a specific interpretation of the context data. In this framework, which enables awareness and self-adaptation, knowledge is specified using KnowLang, a language based on ontologies and Bayesian networks."
"InÂ  [26], an FM is transformed into a Multi-dimensional Multiple-choice Knapsack Problem that allows nearly-optimal FM configurations in polynomial-time to be found. This is also the objective ofÂ  [23], but using genetic algorithms, being even faster than the previous one."
"Pnueli and Rosner have shown that the synthesis problem is undecidable for the architecture shown in Fig. 1[12] and hence in general, and Finkbeiner and Schewe [7] have identified the class of architectures, in which synthesis is decidable."
"The undecidability of safety and reachability languages has been established in [8], using a reduction to tiling languages."
"One can naturally extend them to matrices and vectors leading to the maxâmin (fuzzy) linear algebra of [2,4,12,13,15]. Note that in [15] the authors developed a more general version of maxâmin algebra over arbitrary linearly ordered set, but we will not follow this generalization here."
"The development of maxâmin convexity has been mostly inspired by new geometric techniques in maxâplus (tropical) linear algebra, like those developed in [1,6,7,16]. The development of tropical (maxâplus) convexity was started by K. Zimmermann [28], and it gained new impetus after the works of Cohen, Gaubert, Quadrat and Singer [5], and Develin and Sturmfels [6]. This development has led to many theoretical and algorithmic results, and in particular, to new methods describing the solution set of maxâplus linear systems of equations [1,16]."
"K. Zimmermann [29] also suggested to develop the convex geometry over wider classes of semirings with idempotent addition, including the maxâmin semiring. To the authors' knowledge, the case of maxâmin semiring did not receive much interest in the past. Some recent developments in maxâmin convexity include the description of maxâmin segments [22,26], maxâmin semispaces [23] and hyperplanes [17], separation and non-separation results [18,19]."
"Although our interest here is mostly theoretical, it is also motivated by the theory of fuzzy sets [27], which has numerous applications in computer science and decision theory. For example, in [8] Dubois and Prade developed an axiomatic approach to quantitative utility theory. The utility function introduced there relies on the notion of possibilistic mixture, where the possibilistic mixture (which under some natural conditions is also a possibilistic measure [10]) of the possibilistic measures Ï1,Ï2 with possibilities Î±,Î²,maxâ¡(Î±,Î²)=1, is defined as maxâ¡(minâ¡(Î±,Ï1),minâ¡(Î²,Ï2)), that is, as a point on the maxâmin segment [Ï1,Ï2]. "
"Our approach is inspired by a geometric idea behind the notion the tropical rank [7], that is, a tropically convex polytope can be represented as a union of conventionally convex sets, and its dimension can be defined as the greatest dimension of these convex sets."
"In this section we describe general segments in Bd, following [22,26], where complete proofs can be found. Note that the description of the segments in [22,26] is done for the equivalent case where B=[ââ,+â].Let x=(x1,...,xd), y=(y1,...,yd)âBd, and assume that we are in the case of comparable endpoints, say xâ¤y in the natural order of Bd."
"In the remaining part of the paper, following the parallel with the tropical rank considered by Develin, Santos and Sturmfels [7] in the maxâplus algebra, we investigate how our notion of dimension relates with the notion of strong regularity in maxâmin algebra. For AâB(d,m+1), the ith column will be denoted by Aâ¢i."
"Note that the definition of strong regularity is introduced here for kÃ(k+1) rectangular matrices. A more usual âsquareâ version of this definition will appear in the next section, and we will show that it is equivalent to the one studied in [2,12]."
"We now show that for AâB(k,k) our notion of strong regularity is equivalent to the trapezoidal property, and hence it coincides with the strong regularity in maxâmin algebra introduced in [2]."
"In this paper we introduce the notion of dimension of a maxâmin convex set and show that it is equivalent to a notion of matrix rank based on the strong regularity for the matrices in maxâmin algebra [2].Since the maxâmin convex combinations also appear as mixtures of possibilistic measures in the framework of Dubois and Prade's possibility theory [8], this paper can be seen as a contribution towards the geometry of such mixtures."
"Furthermore, it is plausible that the results of this paper might be generalizable to the setting of [15], and also to the L-convexities and biconvexities of [24]."
"For instance, VavreÄka and FarkaÅ¡ (2014) presented a connectionist architecture that learns to bind visual properties of objects (spatial location, shape and color) to proper lexical features. These unimodal representations are bound together based on the co-occurrence of audiovisual inputs using a self-organizing neural network. Similarly, Morse, Benitez, Belpaeme, Cangelosi, and Smith (2015) investigated how infants may map a name to an object and how body posture may affect these mappings. "
"For this purpose, we extended our recently proposed spatiotemporal hierarchy for the integration of pose-motion action cues (Parisi et al., 2015) to include an associative network layer where actionâword mappings develop from co-occurring audiovisual inputs using asymmetric inter-layer connectivity."
"Experience-driven development plays a crucial role in the brain (Nelson, 2000), with topographic maps being a common feature of the cortex for processing sensory inputs (Willshaw & von der Malsburg, 1976). Different models of neural self-organization have been proposed to resemble the dynamics of basic biological findings on Hebbian-like learning and map plasticity (e.g., Fritzke, 1995; Kohonen, 1988)."
"Our learning model consists of hierarchically-arranged GWR networks (Marsland et al., 2002) that obtain progressively generalized representations of sensory inputs and learn inherent spatiotemporal dependencies. "
"To evaluate our system, we compared newly obtained results with recently reported results using hierarchical GWR-based recognition (Parisi et al., 2015). We conducted additional experiments with different percentages of available labeled samples during the training, ranging between 100% (all samples are labeled) and 0%.3.1Audiovisual inputsOur action dataset is composed of 10 full-body actions performed by 13 subjects (Parisi et al., 2014). "
" Similar to VavreÄka and FarkaÅ¡ (2014) and Morse et al. (2015), we argue that the co-occurrence of sensory inputs is a sufficient source of information to create robust multimodal representations with the use of associative links between unimodal representations that can be incrementally learned in an unsupervised fashion. "
"Specifically for the visual cortex, Hasson et al. (2008) showed that while early visual areas such as the primary visual cortex (V1) and the motion-sensitive area (MT+) yield higher responses to instantaneous sensory input, high-level areas such as the STS were more affected by information accumulated over longer timescales (â¼12s). This kind of hierarchical aggregation is a fundamental organizational principle of cortical networks for dealing with perceptual and cognitive processes that unfold over time (Fonlupt, 2003)"
"On the other hand, several developmental studies have shown that human infants are able to learn actionâword mappings also in the presence of missing, ambiguous or sometimes contradictory referents using cross-situational statistics (Smith & Yu, 2008). Thus, it would be interesting to evaluate the robustness of the system if the available labels are sometimes inaccurate or in contradiction with previously learned labels. "
"More recently, behavioural (Wexler etÂ al., 1998; WohlschlÃ¤ger, 2001) and neuroscientific experiments (Georgopoulos, Lurito, Petrides, Schwartz, & Massey, 1989; Lamm etÂ al., 2007) have suggested the idea that mental rotation relies on a mentally simulated action (Michelon, Vettel, & Zacks, 2006) rather than on a purely visual and spatial imagery skill."
"We (Seepanomwan, Caligiore, Baldassarre, & Cangelosi, 2013a, 2013b) recently proposed a neural-network model whose macro architecture was linked to brain macro areas. This model was able to solve a simple mental rotation task of 2D visually-perceived objects in a simulated humanoid robot, the iCub (Tikhanoff etÂ al., 2008). This model, a predecessor to the model proposed here, was developed within an âembodied cognitionâ theoretical framework for which high-level cognition processes rely on the same areas of the brain used to process analogous sensorimotor information (Borghi & Cimatti, 2010). "
"This model (together with other analogous models, e.g. Bogacz etÂ al., 2006) is very important, as it allows the reproduction of the reaction times often recorded in psychological experiments (Caligiore etÂ al., 2010, 2008; Erlhagen & SchÃ¶ner, 2002). It is one of the most accredited models of decision making processes taking place in the human brain (Bogacz, 2007)."
"Fig.Â 3 shows the three sets of 2D abstract objects, broadly similar to those employed by Hochberg and Gellman (1977), used as stimuli during the mental rotation tasks. The stimuli were coloured in red to improve their detection by the iCubâs camera. They were designed to create different levels of difficulty in the mental rotation task."
"To this purpose, the model incorporated the mutual inhibition model (Bogacz etÂ al., 2006; Usher & McClelland, 2001) that allows a more accurate and biologically-plausible reproduction of the decision making process of the participants of target psychological experiments. This allowed the model to reproduce the key findings of experiments on mental rotation showing increasing reaction times and error rates in relation to increasing disparities of the orientation angles of the rotated and the target objects, whereas previous robotic models on mental rotation reproduced less consistent reaction times and could not reproduce error rates (see Seepanomwan etÂ al., 2013a, 2013b)."
"To discover hidden variables and learn their dynamics, Li et al. [30] built a probabilistic model to estimate the expectation of missing values conditioned on the observed parts. Their next work [31] imposed bone-length constraints in a linear dynamical system (LDS) to boost the performance. The method [31], however, has to rely on the existence of other markers on the same segment to make inter-marker distance measurement possible [4].Recently, data-driven methods have attracted a lot of attention. "
Xiao et al. [58] adopted sparse representation to predict the missing values and Baumann et al. [4] fixed missing data via searching poses with a similar marker set from a prior-database and then optimizing an energy minimization function to synthesize the positional data of the missing markers. 
"Lee and Shin [26] formulated rotation smoothing as a nonlinear optimization problem and iteratively minimized the energy function to smooth the motion. In their later work [27], they proposed a linear time-invariant filtering framework for filtering the orientation data by transforming the orientation data into their analogues in a vector space; applying a time-domain filter on them; and transforming the results back to the orientation space."
"Meanwhile, CandÃ¨s et al. [8] proved that it is possible to recover most low-rank matrices from what appears to be an incomplete set of entries. If an imperfect motion sequence is refined by matrix completion methods, the result also should be both low-rank and temporally stable."
"And inspired by the success of the penalized least squares regression model [14], we seek to minimize the following objective function"
"The main drawback of the penalized least squares regression models is their sensitivity to the outliers. We should choose robust weighting functions to minimize or cancel the side effects of the outliers. Several weighting functions are available to achieve this goal, such as the bisquare weight function [14,16] and the Welsch robust function [37] and so on. "
"As indicated by Eq. (14), the output xË relies on the smoothing regularization parameter Î¼. In order to set it correctly and automatically, we resort to the method of generalized cross-validation (GCV) [10,28]."
"Indeed, the above low-rank matrix completion problem Eq. (21) can be efficiently solved via many algorithms such as the accelerated proximal gradient (APG) algorithm [34,55], the SVT algorithm [7] and the augmented Lagrange multiplier (ALM) algorithm [33]. In [33], the authors have compared all of these algorithms and demonstrated that the ALM algorithm is much faster than the other methods and its precision is also higher [33]. More importantly, the ALM algorithm has a pleasing Q-linear convergence speed.  Therefore, we adopt the ALM algorithm to solve the above objective functions."
"Since it is at most about 30-40% of the data is missing or noisy in practice [29], we fix the ratio of the missing or noisy data to 30-40% to evaluate all the algorithms. "
"SVT [23] and our method are much more suitable to handle long sequences (e.g., boxing and tai chi sequences in Fig. 7(k) and (i)) than short sequences (e.g., run and walk sequences in Fig. 7(a) and (b)).â¢Linear and Spline methods are suitable for handling short time randomly missing data as Fig. 6. "
"Dynammo [30] works very well in handling periodic actions such as walking and running. However, it will corrupt when the noise increases or some data are randomly missing as Figs. 6 and 8.4."
"Successful applications have been made to fields such as vehicle suspension [19–20], X-Y table motion control [21], automated driving [22], brushless DC motors [23], robot control [24,25], and dual-stage actuators [26,27]. The effectiveness of preview control in wind turbine control has also been demonstrated [28–30]"
"In [31–33], multi-model adaptive control using multiple models to identify the unknown plant is considered; as higher level adaptive control, this improves the transient response of control systems with large uncertainties in a stable fashion."
"Several countries around the world see nuclear power as an important route for cutting carbon emissions while meeting their energy demands in the future (IPCC, 2014), even though the viability of the nuclear option in a sustainable energy mix is being debated constantly (Kaygusuz, 2012; Mari, 2014; Mez, 2012; Verbruggen, 2008)."
" However, these sorts of optimal control problems are commonly solved in Mathematical Finance and, more generally, in Operations Research (Sahebjamnia, Torabi, & Mansouri, 2015), and we are going to follow that framework here to identify the best post-accident recovery strategies. Such a framework will provide a superstructure to COCO-2."
"Considering the joint cost-minimal strategy across a number of measures is particularly important in the situations such as large accidents where a combination of individually justified actions may be deemed unacceptable as a whole due to high levels of disruption to society (ICRP, 2009)."
"To accounting for these multiple factors in the context of nuclear emergencies, the so-called Multi-Criteria Decision Analysis (MCDA) methodology has been applied successfully (French, 1996; Hämäläinen et al., 2000). This methodology is more generic than the purely economic valuation considered in this paper, and therefore it could be used to extend the main insights gained in the present study with regards to the effect of temporal and spatial flexibilities on cost-minimal strategies. "
"There is some discussion in the literature about what the true costs of exposure to radiation are, and (Cuttler & Pollycove, 2009) is just one example of the relatively recent point of view that low levels of exposure to radiation can in fact provide some benefit to the recipient. "
"COCO-2 implements WTP approach, which values life in terms of the amounts that people are prepared to pay to reduce risk of death/illness (NHS and private medical insurance are good examples Higgins et al., 2008)."
"We believe that at the current state of knowledge there is no absolutely compelling evidence in favour of any particular approach for putting monetary values on heath, including the effects of radiation, and coming up with the best possible valuation for economic consequences of receiving a dose remains of large significance (Thomas & Vaughan, 2015)."
"However, owing to the fact that the vast majority of nuclear installations are surrounded by rural or, at most, semi-urban areas (Grimston et al., 2014), we can safely assume that all the productivity figures per person defined in this section are constants and are determined by the economic make up of the given locations regardless of their population sizes."
"Chen [3] presented an approach for reconciling existing work-flows to bring about compatibility. A software collaboration agent extracts the interface processes from two workflows that are intending to work together and gives an offer to a candidate pro-vider, which evaluates the offer and creates a counter-offer. The partner then either accepts or rejects the offer. The process of offer generation, counter-offer generation, acceptance and rejection goes on recursively till the negotiation is terminated or reconcilia-tion is achieved."
[4] presented a bottom-up cross organisational workflow enact-ment approach. The approach is workflow management system (WfMS) independent and the enactment is done via progressive linking enabled by runtime agents.
"With the increase in demand for reusability and interoperabil-ity, research has considered composing web services into compos-ite services to automatically generate business processes. Sirin et al. [25] created a semi-automatic web services composition sys-tem, which allows users to select from a list of web services at each step of composition. "
"Later, Sirin et al. [27] extended their semi-automatic web ser-vice composition system to a fully automatic system. They imple-mented an OWLStoSHOP2 translator to translate collections of OWLS process definitions into SHOP2 domain. The SHOP2 planner then uses the created domain to produce a valid plan according to the constraints entered by the user and imposed by the relevant web services. The generated SHOP2 plan is converted to OWLS for-mat by a plan converter called SHOP2toOWL, and executed by the Execution System. A limitation with this system is that it plans for a single organisation only and does not take collaboration among multiple business organisations into account."
"Problem Solving Methods (PSMs) [9] is another area that has a conceptual resemblance to web service composition, due to its fo-cus on reusable domain-independent reasoning about ontologies [11]. In PSMs, the properties of a method can be specified as a method ontology. With the help of mapping ontologies, the inputs and outputs of the PSM can be connected to the entities in the ontologies of different domains. The use of the idea of PSM and mapping ontologies can be interesting in web services composition domain."
The framework presented in the paper is closely related to the system proposed by Sirin et al. [27]. The work presented in this paper extends the appli-cation of AI planning to workflow generation as well as workflow collaboration. Below are some of the major extensions and improvements the proposed framework makes to the approach ta-ken by Sirin et al. for workflow generation.
"The Translate-Atomic-Process(Q) algorithm translates OWLS atomic processes into SHOP2 operators. It extends the translation algorithm put forward in [27], to translate atomic processes with both outputs and post-conditions. It takes a definition Q of an atomic process A as input and outputs a SHOP2 operator O."
"We will consider a Vendor/Customer example scenario. This is a modification of the example presented by Chen [3]. The vendor in this example is an overseas exporter. The vendor waits for the ad-vance payment from a customer, checks the received payment and then starts the manufacturing process."
"Unfortunately, large, standardized datasets for validation are yet missing for mission impact assessment and in the following presented work. de Barros Barreto etÂ al. (2013) introduce a well-understood modeling technique and use BPMN models to acquire knowledge."
"This modeling approach is well accepted and can be found, e.g., in Albanese etÂ al. (2013), de Barros Barreto etÂ al. (2013), Musman etÂ al. (2011). Fig.Â 1 shows a sketch of a BPMN model used throughout this paper.Designing such BPMN models is handled manually by an expert from a company or by an external business consultant having a precise expertise in the understanding of business analysis. "
"Therefore, we extend a model by Jakobson (2011) and model mission dependencies as shown in Fig.Â 2 as a graph of mission nodes. We model a company as being dependent on its business processes."
"With Definition 3, a mission dependency model represents a probabilistic graphical model, and, in particular, a Bayesian network, as, e.g., defined by Pearl and Russell (2003). A key feature of Bayesian networks is the ability to locally interpret individual parameters, i.e., to locally interpret individual probabilities of CPDs. This feature allows Bayesian networks to be a direct representation of the world, as stated by Pearl and Russell (2003). "
" Sommestad and Hunstad (2013) conducted an experiment at the information warfare lab of the Swedish defense research agency, which gives us the opportunity to demonstrate Example 4 for vulnerability impact assessment. "
In our work by Granadillo etÂ al. (2016) we demonstrate an approach to unify these three-dimensional assessments with further multi-dimensional assessments of response plans and propose a selection of optimal response plans based on an unweighted best compromise in all dimensions. In Granadillo etÂ al. (2016) we evaluate the suitability of the presented mission impact assessment for operational impact assessment to obtain adequate responses to cyber-attacks in the here-discussed ARETi environment. 
 We discuss and evaluate the benefits of a combination of both toward a well-defined probabilistic mission defense and assurance approach in Motzek and Moller (2016).
"Future work is dedicated to adapt existing approximation techniques, e.g., Rao-Blackwellised particle filtering as presented in Doucet etÂ al. (2000) for DBNs toward novel ADBNs. "
"Prior studies have examined winner-takes-all competition (Eisenmann, Parker and Van Alstyne 2006), i.e., a situation where one platform ultimately wins the platform race. Econometric mod-eling studies, such as Tse (2006) and Sun and Tse (2009) have created models of platform competition that emphasize the role of single- or multi-homing"
"Prior research has focused on software vendors’ multi-homing in console games marketplaces (Landsman and Stremersch, 2011), Soft-ware as a Service (SaaS) marketplaces (Burkard, Draisbach, Widjaja, and Buxmann, 2011; Burkard, Widjaja, and Buxmann, 2012), and also within Apple’s ecosystem (Idu et al., 2011). In their study on the gam-ing console market, Landsman and Stremersch (2011) found that the multi-homing of games has a negative effect on sales at the mar-ketplace level, although the negative effect decreases when a plat-form matures or gains market share."
"Idu et al. (2011) investigated the iPhone, iPad, and Mac software marketplaces, and found that, out of the top 1,800 applications, 17.2% were multi-homed in two market-places and 2.1% in all three marketplaces."
"In their theoretical analysis of competitive advantage in two-sided markets, Sun and Tse (2009) highlighted the importance of the dis-tinction between multi-homing and single-homing in determining the winner among competing platforms. Drawing on dynamic sys-tems models, Sun and Tse (2009) argued that, in the context of single-homing, only the largest network will survive and that network size is the critical factor in determining the winner among competing plat-forms. This is due to the fact that in a two-sided market, network participants become a critical resource for the platform orchestrator (Sun and Tse, 2009). By drawing on two dynamic systems models, Sun and Tse (2009) concluded that a multi-homing market is able to sustain several platforms, whereas a single-homing market is prone to becoming dominated by a single platform. However, Sun and Tse (2009) pointed out that their analysis of platform competition focused on the quantity of network partici-pants but did not address the quality of participants. This issue is particularly important in the context of mobile application ecosys-tems, since most of the installations in Google Play were generated from a small set of applications (Hyrynsalmi, Suominen, Mäkilä, and Knuutila 2012)."
"Following Landsman and Stremersch (2011), we investigated multi-homing by dividing it into two levels: seller- and platform-level multi-homing. "
"We used Levenshtein distance (Levenshtein, 1966) to measure the similarity of two names and employed Python’s diﬄib library3 for comparisons."
"As pointed out by Hyrynsalmi et al. (2012), most of the installa-tions in Google Play were generated by a small set of applications. Therefore, we pay special attention to this subset of applications, also referred to as ‘superstars’ (Landsman and Stremersch, 2011), and we consider developers of these superstar applications as the ‘nu-cleus developers’ of the respective ecosystems."
"This ob-servation contrasts with prior studies that suggested that the level of multi-homing is, at most, small (Boudreau, 2007, 2012). Altogether, our results have several implications for both research and practice that are discussed in the following section."
"According to Sun and Tse’s (2009) theory of platform competition, a multi-homing market can sustain several competing ecosystems; however, a single-homing market eventually evolves into only one prevailing ecosystem. When examining multi-homing at the level of the whole content of the three ecosystems, our findings support Sun and Tse’s (2009) asser-tion that the market would evolve into one dominant ecosystem."
"With regard to the second area of future research, a business ecosystem should, among other success factors, support niche and opportunity creation (Iansiti and Levien 2004). However, we did not address different aspects of niche creation inside an ecosystem when discussing the numbers of developers and applications. As a result, we encourage further research to create measures for niche and op-portunity creation, and to investigate whether the size of an ecosys-tem affects niche creation and the success of these niches."
"However, since the prioneering work of Lord etÂ al. [72], the proposal of similarity measures for genomics and proteomics based on the Gene Ontology (GO) [5] have attracted a lot of attention, as detailed in a recent survey on the topic [76]."
"Many GO-based semantic similarity measures have been proposed for protein functional similarity [28,29,101,132], giving rise to applications in protein classification and protein-protein interactions [41,129], gene proritization [117] and many others reported in [76, p.2]."
"In [57], Lastra-DÃ­az and GarcÃ­a-Serrano introduce a new family of similarity measures based on an Information Content (IC) model, whose pioneering work is introduced by Resnik [108]. Their new familiy of semantic similarity measures is based on two unexplored notions: a non-linear normalization of the classic Jiang-Conrath distance [52], and a generalization of this latter distance on non tree-like taxonomies defined as the length of the shortest path within an IC-weigthed taxonomy."
"One of the similarity measures introduced in [57], called coswJ&Csim, obtains the best results on the RG65 dataset. In another subsequent work [56], the same aforementioned authors introduce a new family of intrinsic and corpus-based IC models and a new algebraic framework for their derivation, which is based on the estimation of the conditional probabilities between child and parent concepts within a taxonomy. This latter family of IC models is refined in another subsequent paper [58], which also sets out the new state of the art and confirms the outperformance of the coswJ&Csim similarity measure in a statistically significant manner among the family of ontology-based semantic similarity measures based on WordNet."
"In addition, this work also introduces a new replication framework and the WNSimRep v1 dataset for the first time provided as supplementary material in [63], whose aim is to provide a gold standard to assist in the exact replication of ontology-based similarity measures and IC models. "
"This work is part of a novel innitiative on computational reproducibility recently introduced by Chirigati etÂ al. [26], whose pioneering work is introduced by Wolke etÂ al. [127] with the aim of leading to the exact replication of several dynamic resource allocation strategies in cloud data centers evaluated in a companion paper [128]."
"Another significant example of caching is the approach adopted by the WNetSS semantic measures library introduced recently by Aouicha etÂ al. [15]. Unlike SML, which computes the topological features on-the-fly by storing them in an in-memory cache, WNetSS carries-out a time-consuming off-line pre-processing of all WordNet-based topological information which is stored in a MySQL server. This latter caching strategy based on MySQL could be appropiate for supporting a large Web-based experimental platform, such as the SISR system proposed in [15]. "
"Pedersen [94], and subsequently Fokkens etÂ al. [37], warn of the need to reproduce and validate previous methods and results reported in the literature, a suggestion that we subscribe to in our aforementioned works [56â58], where we also refuted some previous conclusions and warn of finding some contradictory results. "
"A recent study [6,33] on the perception of this reproducibility âcrisisâ in science shows that the aforementioned reproducibility problems in our area are not the exception but the rule. Precisely, this latter fact has encouraged the recent manifesto for reproducible science [90], which we also subscribe."
"For example, Zhou etÂ al. [134] define the root depth as 1, whilst the standard convention in graph theory is 0. Most authors define the hyponym set as the descendant node set without including the base node itself. However, in [43], the hyponym set also includes the base concept. In addition, we find works that do not detail the IC models used in their experiments, or how these IC models were built. "
"In a recent work [57], we find some contradictory results and difficulties in replicating previous methods and experiments reported in the literature. These reproducibility problems were confirmed in another subsequent work, such as [56], whilst new contradictory results are reported in [58]. Several replication problems were solved with the kind support of most authors. "
"First, edge-counting measures, the so-called path-based measures, whose core idea is the use of the length of the shortest path between concepts as an estimation of their degree of similarity, such as the pioneering work of Rada etÂ al. [107]. Second, the family of IC-based similarity measures, whose core idea is the use of an Information Content (IC) model, such as the pioneering work of Resnik [108], and the subsequent measures introduced by Jiang and Conrath [52] and Lin [70]. Third, the familiy of feature-based similarity measures, whose core idea is the use of set-theory operators between the feature sets of the concepts, such as the pioneering work of Tversky [124]. And fourth, other similarity measures that cannot be directly categorized into any previous family, which are based on similarity graphs derived from WordNet [122], novel contributions of the hyponym set [43], or aggregations of other measures [75]."
"Finally, we mention five significant further lines of research into ontology-based similarity measures. Stanchev [122] introduces an assymmetric similarity weighted graph derived from WordNet, whilst MartÃ­nez-Gil [75] proposes an aggregated similarity measure based on a combination of multiple ontology-based similarity measures and Van Miltenburg [125] proposes a method to compute the semantic similarity between adjectives based on the use of the similarity between their sets of derivational source names in WordNet. More recently, Meymandpour etÂ al. [85] propose several semantic similarity measures for Linked Open Data (LOD) based on IC models, whilst Batet and SÃ¡nchez [13] propose a semantic relatedness measure based on the combination of highly-accurate ontology-based semantic similarity measures with a resemblance measure derived from corpus statistics."
"To bridge this gap, Seco etÂ al. [119] introduce the first intrinsic IC model in the literature, whose core hypothesis is that the IC models can be directly computed from intrinsic taxonomical features."
"HESML V1R2 currently supports the WordNet taxonomy, most ontology-based similarity measures and all the IC models for concept similarity reported in the literature with the only exception of the IC models introduced by Harispe etÂ al. [46], although the latter IC model could be included in future versions. In addition to the aforementioned IC models [46], Seddiqui and Aono [120] and PirrÃ³ and Euzenat [104] propose two further intrinsic IC models not implemented by HESML which are based on the integration of all types of taxonomical relationships, and thus especially designed for semantic relatedness measures. In addition, we plan to provide ongoing support for further ontologies such as Wikidata [126] and the Gene Ontology (GO) [5] among others, as well as further similarity and relatedness measures. On the other hand, the HESML architecture allows further similarity measures, IC models and ontology readers to be developed easily. We also urge potential users to propose further functionality. In order to remain up to date on new HESML versions, as well as asking for technical support, we invite the readers to subscribe to the HESML forum detailed in tableÂ 8."
"We follow the same experimental setup as that detailed in [56] and [58], including the same datasets, preprocessing steps, evaluation metrics, baselines, management of polysemic words and reporting of the results. All the experiments compute the Pearson and Spearman correlation metrics for a set of ontology-based similarity measures on each word similarity benchmark shown in tableÂ 22, as detailed in [56]. "
"All the corpus-based IC models are derived from the family of â*add1.datâ WordNet-based frequency files included in the [95] dataset, which is a dataset of corpus-based files created for a series of papers on similarity measures in WordNet, such as [93] and [96]. "
"All benchmarks detailed in tableÂ 17 are implemented on a single Java console program called HESML_VS_SML_test.jar, which is publicly available at [61]."
"In addition, we have introduced a set of reproducible experiments based on ReproZip [64] and HESML, which corresponds to the experimental surveys introduced by Lastra-DÃ­az and GarcÃ­ a-Serrano in [57], [56] and [58], as well as the WNSimRep v1 replication framework and dataset [63] and a benchmark of semantic measures libraries [61]."
"As forthcoming activities, we plan to extend HESML in order to support Wikidata [126] and non âis-aâ relationships in the short term, whilst in the mid term, we expect to support the Gene Ontology (GO), MeSH and SNOMED-CT ontologies. "
"Among the various solutions proposed to that end, the adoption of Model-Driven Engineering (MDE) Schmidt (2006) has fared rather well by measure of interest and success. Evidence collected in domain-specific initiatives (cf. e.g., (Bordin and Vardanega, 2007; Panunzio and Vardanega, 2007; Bordin et al., 2008)) shows that the higher level of abstraction in the design process facilitated by MDE allows addressing non-functional concerns earlier in the development, thereby enabling proactive analysis, maturation and consolidation of the software design."
That joint initiative proved the component model (initially captured in Panunzio and Vardanega (2010)) to be an essential facilitator to the industrial adoption of the proposed approach.
"Two decades later the Correctness by Construction (C-by-C) manifesto (Chapman, 2006) promoted a software production method fostering the early detection and removal of development errors for safer, cheaper and more reliable software."
"Our approach aims at achieving the properties of composability and compositionality. When composability and compositionality can be assured by static analysis, guaranteed throughout implementation, and actively preserved at run time, we may speak of composition with guarantees (Vardanega, 2009), which is our grander goal here."
"The connector (Mehta et al., 2000) is the software entity responsible for the interaction between components, which actually is a mediated communication between containers. Connectors allow separating interaction concerns from functional concerns. "
"The PROGRESS component model (ProCom) (Carlson et al., 2010) extends SaveCCM to address high-level concerns typical of early design stages of a large-scale distributed embedded system: high-level early analysis and deployment to processing units"
"Second, we propose a more generic model of transient synchronization based on a Weakly Coupled Oscillator (WCO) framework (Hoppensteadt & Izhikevich, 1997). WCOs are a standard approach for studying synchronization dynamics (Hoppensteadt & Izhikevich, 1997) and can be derived by applying a phase reduction approach to neurophysiologically realistic neural (Gutkin, Ermentrout, & Reyes, 2005) or neural network (Brown, Moehlis, & Holmes, 2004) models."
This section describes a spectral analysis applied to the ECOG data presented in Canolty etÂ al. (2007). ECOG data was recorded from an 8-by-8 grid of electrodes placed over fronto-temporal cortex. 
"Each nonword matched one of the words (action verbs) in duration, intensity and power spectrum, but was rendered unintelligible by removing components of the modulation power spectrum using the Modulation Transfer Function (MTF) algorithm described in Elliott and Theunissen (2009). This is based on a two-dimensional Fourier transform of the log spectrogram, after which slower timeâfrequency modulations are removed. This results in spectrograms which are, for example, less smooth in the time and frequency domain, as shown for example in the third row of Fig.Â 3. Further details of the MTF processing are given in Canolty etÂ al. (2007). Overall, our database comprised 96 speech utterances (â.wavâ files) and 96 matched nonwords."
"Human speech is characterized by a four-fold variation in the speed at which words are spoken (Miller, Grosjean, & Lomanto, 1984), and any speech recognition system whether artificial or natural, will have to deal with this range of âtime-warpâ. In the above coding scheme time-warp invariance is achieved because the timing of the recognition event (gamma burst) depends on the speed at which the word is spoken. This is discussed at length in Hopfield and Brody (2001) and illustrated in Fig.Â 5. Time-warp invariance occurs rather naturally with OT features and WCOâTS/HB models but is more complicated to add to other representations. "
"Much research examines the robustness of stable synchronized states as a function of variations in oscillator frequencies. Kuramoto (1984), for example, has derived the following result."
"A quantitative relationship can be derived, for example, by assuming that the instantaneous phases are Gaussian distributed with phase variance Ït2. The instantaneous field power is then given by (Roweis, 2009)"
"We also compare augmented and minimal models using the model evidence, as computed using the Posterior Harmonic Mean (PHM) (Gelman etÂ al., 1995). This approximates the evidence for a model using samples from the posterior density (15)"
" The ECoG data we have analysed has recordings of activity from 64 electrodes placed over fronto-temporal cortex, yet we have modelled data from only a single electrode. Extension of our modelling to include multiple regions, as inÂ Corchs and Deco (2004); Husain etÂ al. (2004), is therefore an important direction for future work."
"Researchers have designed algorithms to solve many interesting problems for these devices, such as GPU sorting or hashing [1-4], linear algebra  [5-7], dynamic programming  [8,9], graph algorithms  [10â-13], and many other classic algorithms  [14,15]. These projects generally report impressive gains in performance. These devices appear to be here to stay. "
"Aggarwal and Vitter proposed the Disk Access Machine (DAM) model [22] which counts the number of memory transfers from slow to fast memory instead of simply counting the number of memory accesses by the program. Therefore, it better captures the fact that modern machines have memory hierarchies and exploiting spatial and temporal locality on these machines can lead to better performance. There are also a number of other models that consider the memory access costs of sequential algorithms in different ways  [23-29]."
"However, we are interested in the interplay between the farthest level, since the latencies are the largest at that level, and therefore have the biggest impact on the performance. We expect that the model can be extended to also model other levels of the memory hierarchy.We analyze 4 classic algorithms for the problem of computing All Pairs Shortest Paths (APSP) on a weighted graph in the TMM model  [43]. We compare the analysis from this model with the PRAM analysis of these 4 algorithms to gain intuition about the usefulness of both our model and the PRAM model for analyzing performance of algorithms on highly-threaded, many-core machines. Our results validate the intuition that this model can provide more information than the PRAM model for the large latency, finite thread case."
"The Block Transfer model (BT)Â  [27] addresses this deficiency by defining that a block of consecutive locations can be copied from memory to memory, taking one unit of time per element after the initial access time. "
"Thus, they simplified and reduced the MH parameters by putting forward a new Uniform Memory Hierarchy (UMH) modelÂ  [28,29]."
"Quite different to PRAM, the Bulk-Synchronous Parallel (BSP) model  [34] attempts to bridge theory and practice by allowing processors to work asynchronously, and it models latency and limited bandwidth for distributed memory machines without shared memory."
" We carefully analyze how state-compatible automata relate to quasi-deterministic automata, which were introduced by Korp and Middeldorp [13] to overcome problems with tree automata completion for non-left-linear term rewrite systems. "
"First of all, tree automata come with a set of final states that is different from the set of states; this, crucially, allows them to accept languages that are not closed under the subterm relation. (This is not a problem in [4], where the languages of interest are terminating terms. These languages can be closed under subterms without losing the termination property.)"
"Tree automata have an obvious application for disproving (local) confluence, as pointed out in [19]."
"To handle this problem, the notion of raise-consistency was introduced in [13]. The basic idea is to ensure that whenever an automaton accepts terms s1,s2 with base(s1)=base(s2), it also accepts s1âs2 in a related state, cf."
"We have formalized all results from Section 3 and significant parts of Sections 5 and 6 within IsaFoR, our Isabelle Formalization of Rewriting, in combination with executable algorithms which check state-compatibility, state-coherence, and state-raise-consistency. These are used in CeTA [18], a certifier for several properties related to term rewriting."
"For example, we group the transitions of an automaton by their root symbols and store these groups in ordered trees using Isabelle's collection framework [15]."
"For c=0 in [2,3] it is suggested to apply the previous transformation (4) with h=a and is claimed that the Chen and the Lu systems with c=0 are âa particular case of the T-systemâ [26,77] (which was published later)(7)"
"Recently a very interesting discussion on the equivalence of the Lorenz, the Chen and the Lu systems was initiated in [2,3,11].Below a few remarks, concerning the discussion and being important, are given."
"Generally speaking, for quadratic systems the existence of a trajectory on tâ[t0,+â) does not imply its existence on tâ(-â,t0] (see, e.g., examples from the paper [21] on the completeness of quadratic polynomial systems or the classical example xÌ=x2)."
"Recently interesting examples, that were previously unexplored, where reported in the Lorenz system and the Chen system [74,75].Recall that in the case c=0 in [2,3,11] there is remarked that the Chen and the Lu systems are only a particular case of the T-system, which was published in 2004 [77] (i.e. later than the Chen and the Lu systems were published)."
"The sufficiency of condition (10) was first obtained in [40,41]. The hypothesis that inequality (10) is a necessary condition was accepted in [40,41] and was first proved in [13]."
"Recently in the papers [37,46] it is proposed a new effective analyticalânumerical procedure for localization of homoclinic trajectories (Fishing principle). For applying this method to three-dimensional systems it is of very importance the existence of the two-dimensional stable manifold of a saddle point, on which the trajectories are attracted to the saddle point from which a homoclinic trajectory is outgoing (see Fig. 1)."
"Nevertheless in [37] it is shown that a small change of all these systems in a neighborhood of a saddle can lead to the satisfaction of all conditions of the Shilnikov theorem and, consequently, to Shilnikov chaos in the Lorenz, the Chen, and the Lu systems. Such a construction requires also to make the use of an analog of Fishing principle and the existence of a two-dimensional stable manifold."
"The mathematical tools, developed independently by Douady Oesterle [20] and Ilyashenko [25], permitted one to obtain the following extension of the Liouville theorem and to estimate the Hausdorff dimension of K"
In the work [43] it is introduced Lyapunov functions in the estimates of the form (18) and proved the following result.
"The estimate from above of the Lyapunov dimension by Lyapunov functions [36] and its comparison with the local Lyapunov dimension in zero stationary point permit one to obtain the exact formula of dimension for a generalized Lorenz system (9) with a certain parameter d. For example, for the Lorenz system (where d=1) the following result is known:"
"In 2012 G. Leonov in his work [35] demonstrated by the transformation xâhx,yâhy,zâhz,tâh-1t with h=a that without loss of generality nonlinear Chen and the Lu systems can be considered as two-parametric systems (for a=0 the Chen and the Lu systems become linear and their dynamics have minor interest). Later, in 2013, in the works [2,3] the transformation with h=-c was applied to transform the Chen and Lu systems for c>0 to the form of the Lorenz system (here for c=0 the Chen and the Lu systems do not become linear and can be transformed by the transformation with h=a to the T-system which was published later)."
"We note that in the constant viscosity case, the heuristic statement of incompressible Reynolds equation from Stokes model is obtained in [2] while the more rigorous one based on asymptotic developments is proved in [3]."
"Barus law has been also used to model pressure dependence of viscosity in the original Stokes equation in [9] arguing also that Reynolds equation is only valid when the shear stress is much smaller than the reciprocal of the pressureâviscosity coefficient, while later on in [10] a corrected Reynolds equation is obtained from NavierâStokes ones with pressure dependence of viscosity and in [11] a simpler derivation is obtained."
"More recently, by assuming that the viscosity depends on pressure in Stokes equation according to Barus law, in [12] a more careful derivation of the limit Reynolds equation is carried out. The authors find additional terms to those ones appearing in the classical Reynolds equations used for elastohydrodynamic computations. Furthermore, they evaluate the consequences of these additional terms in the hydrodynamic regimes leading to high enough pressure values."
"Note that the viscosity depends on pressure. After some simplifying assumptions detailed in [12], including that âp/ây=0, Eqs. (2) and (3) are reduced to(5)dpdx=Î¼â2uây2+2dÎ¼dpâuâxdpdx."
"As indicated in the review paper [13] and throughout the literature, cavitation is one of the most relevant features of lubrication problems. "
" This phenomenon is defined in [14] as the rupture of the continuous fluid film due to the formation of air bubbles inside. The presence of two different regions, the first with a complete fluid film and the second with a partial film (partial lubrication in cavitated area), has been experimentally observed in many lubricated devices such as journal-bearings, ball-bearings, etc."
A review concerning the mathematical and physical analysis of the different models is presented in [15]. 
"In order to compare the results with those ones appearing in [12] for the case of a rigid long rolling-cylinder, we introduce the angular coordinate, t, defined by the change of variablex=Rsin(t)"
" As the present study is mainly devoted to numerical aspects, neither the theoretical results related to existence and uniqueness, nor the statement of the qualitative properties of the solution of these models are considered here. However, let us mention some related partial results in the literature. In the variational inequality formulation in the forthcoming Section 5.1, existence and uniqueness is well known for the usual model with Î±=0 (see [16], for example). "
"Concerning the alternative first order ODE model, it is based on some classical results in [18] for the Reynolds solution in the isoviscous regime. More precisely, in [18] it is proved that cavitation can only occur in the divergent part of the gap (i.e. the region where dh/dt>0) and that the cavitated area is a connected one. "
Numerical solution of the alternative Reynolds equationThe numerical solutions for the problem (25)â(27) may be obtained by the combination of finite element techniques with a classical projection method [19] or a more complex duality type algorithm proposed in [20]. 
" The application of these techniques to classical piezoviscous formulations can be found in [6-8,21], for example. "
"At this point we propose two alternatives to solve the discretized problem (38): a relaxation algorithm with projection on the convex set Kh first described in [19] and the duality type algorithm proposed in [20] for the numerical solution partial differential equations involving maximal monotone operators. As the first one is classical we describe the application of the second one.In order to apply the duality method in [20], "
"One significant approach to software size prediction is the Use Case Points (UCP) method, which is a prediction model based on the work of Karner (1993). Azevedo et al. (2011) brings a discussion about influence of extends association in use cases, which helps to count UCP more precisely. Software size prediction through use case analysis addresses object-oriented design; thus, this method is now widely used. "
"The use case size points method was evaluated in Braz and Vergilio (2006). The authors emphasised the internal structure of the use case scenario in their method, where the primary actors take on roles and are classified based on an adjustment factor. This approach can lead to better evaluations of actors and use cases."
"Wang et al., (2009) proposed an extended UCP in that employed fuzzy sets and a Bayesian belief network used to set unadjusted UCP. The result of this approach was a probabilistic effort estimation model."
" Classification methods are often used to recognize the motion activities based on various classification methods [13] or classifiers for a specific action class [14]. Different classification methods have been successfully used in the literature, including feature-reduced Gaussian [15], image-based reconstruction [16], support vector machine [17,18], time frequency analysis [19], kernel-based representation [20], RBF neural network [21] and random decision forests [22]."
"It was found that the stick figure model was the simplest representation of the human body, whereby joints are connected by a line segment [23,24]. However, the processes used by previous scholars to develop estimation algorithms were rather complicated and difficult to interpret."
"Meanwhile, Shen et al. [9] proposed unconstrained motion estimation using a single frame. This method was found to be suitable for long sequences of motions and different types of movements. "
"The independent method to shape models is reported in Hofmann and Gavrila [10], who combined probabilistic single-frame pose recovery, temporal integration and texture model adaptation to estimate 3D upper body movements. This method has proven successful in complex environments without a specific initial pose. "
"Despite the low-level feature discussed by Kim and Park [42], Benický and Jurišica [43] and Long and Wu [44], the middle-level features that consider points and strokes are more informative compared to the edge feature from the low-level feature [45]. "
"Middle-level features are usually connected local features and global features that represent complex motion activity [46]. For instance, Campos et al. [47] and Fotiadou and Nikolaidis [48] used a popular middle-level feature in action recognition referred to as Bag of Words (BoW). "
" Meanwhile, Josinski et al. [49] used the features extracted from spatial trajectories for gait motion recognition. In their study, it was proven that the feature selection methods influence the accuracy of the gait based on the identification process."
"In addition, a discriminative feature was proposed by Wang et al. [50] to reduce the computational expense in action recognition."
"However, recent works [4,5,37,39,40] have shown that human body posture coordinate data analysis is worth further investigation, particularly from a data mining perspective"
"For instance, Zhou et al. [15] performed feature-reduced Gaussian process classification to recognize articulated and deformable human actions. The classification approach was applied to a space-time human silhouette to learn and predict the action categories. The Gaussian process was used by Raskin et al. [52] for dimensionality reduction and to improve the ability to track an object in high-dimensional space. On the other hand, Bodor et al. [16] classified human motions using image-based reconstruction. The advantage of their approach was the ability to automatically construct a proper view of an image to match the training view, which leads to improvements in classification accuracy. "
"Orović et al. [19]classified arm movements based on time frequency analysis of radar data. Other studies reported human motion classification using Kernel-based representation [20], ANN [53], RBF neural networks [21], random decision forests [22] and pyroelectric infrared (PIR) sensor [54]. Clearly, human motion classification has become an active field of study."
"As stated by Yoo et al. [57], 2D motion data are based on intuition and able to define the posture faster for computer animation. Thus, we have chosen to analyze our motion data as a 2D system instead of a 3D system. "
"Each piece of raw video data is transformed into snapshot images of 0.5 equal time steps. Following Hoshino et al. [59], Souvenir and Parrigan [60] and Eichner et al. [61], the image files were transformed based on the most fundamental assumption that human motions can be ideally modeled as 2D rigid body segments. For the sake of simplicity and convenience, this is merely an approximation."
"Chen, Chiang, and Storey (2012) report the publication of 126 academic articles in business journals in 2011 containing the phrase “business analytics” in the title or abstract, equal to the total pub-lished in such journals in the ten years prior (252 articles in total be-tween 2000 and 2011). Similarly, research from various practitioner sources forecasts growing importance of utilising new sources of data and substantial growth in demand for staff with analytical skills (e.g. Manyika et al., 2011)."
"Until the end of 2013 only one pa-per had been published in this journal, which was primarily focused on a very specific application of financial modelling (Gosh & Troutt, 2012). "
"Compared to the 252 found by Chen et al. (2012) across all business journals, in the OR/MS literature only 13 were found across the same time period (the years up to and including 2011). We are not able to determine whether these 13 articles are also reported in the study by Chen et al. (2012); we suspect that at least some are."
"Decision Analytics in 2014, a journal that features both analytics and OR/MS content. However, considering that the first academic articles discussing analytics were published in the early 2000s (e.g. Kohavi, Rothleder, & Simoudis (2002)), the tardiness of the OR/MS academic community’s response is surprising enough to warrant further explo-ration of the underlying causes."
"Perhaps the most cited definition of analytics is that provided by Davenport and Harris (2007, p. 7):"
"The claim that analytics is a subset of business intelligence (BI) is a view supported by others such as Bartlett (2013, p. 4) who ar-gues “Business Intelligence = Business Analytics + Information Tech-nology”. However, this is contradicted in other research: Vesset, Mc-Donough, Morris, and Woodward (2009) and SAP (2012) state the opposite view, describing BI as the subset of analytics. The work of Chen et al. (2012), Chiang, Goes, and Stohr (2012) and Lim, Chen, and Chen (2012) sidesteps this by considering the two as a com-posite, using the acronym “BI&A”. "
"This ambiguity is not confined to the differences between analytics and BI; there are other examples where definitions of analytics can be seen to be very similar to other supposedly separate fields. For example, Laursen and Thorlund (2010, p. XII) define analytics as “delivering the right decision support to the right people at the right time”. This definition is very similar to that given by Shim et al. (2002) to the field of decision support systems (DSS):"
"Another example is INFORMS’ definitions of analytics as “the scientific process of transforming data into insight for making better decisions” (Liberatore & Luo, 2011, p. 582); which bears close relation to their definition of OR/MS as “the application of advanced analytical methods to make better decisions” (INFORMS, 2013). A clear argument can be made that the definitions are somewhat interchangeable, ergo that partitions between each are ill-defined."
"An alternative approach, popular in practitioner literature, is to define analytics not as a concept but as a practice. The most preva-lent of such definitions is proposed in Lustig, Dietrich, Johnson, and Dziekan (2010), who argue that analytics comprises of three distinct aspects:"
"Predictive analytics, whilst regarded by many to be an evolution of the approaches of data mining and machine learning (Agosta, 2004; Shmueli & Koppius, 2011), still has suﬃcient com-monality with these disciplines so as to make a complete distinction problematic."
"Varshney and Mojsilovic (2011, p. 84), however, propose “applied mathematics, applied prob-ability, applied statistics, computer science, and signal processing” whereas Evans (2012) argues for BI/information systems, statistics and OR/MS."
"In Decision Support Systems, arguably the area’s most influential book of its era, Keen and Scott Morton (1978, pp. 33–34) propose that four disciplines are integral: “computer science, information economics, management science and behavioral science”. "
"This movement, using the concepts introduced by Kuhn (1962), can therefore be described as the dominant paradigm in the ‘science’ of business manage-ment."
"Whilst the move-ment’s momentum eventually waned, it had significant impact at the time, as well as leaving a clear legacy on management practice (Taksa, 1992). Accordingly it would seem appropriate to consider this new approach to management as the start of a new manage-ment paradigm. Not only is there the notion of “inconsummerabil-ity” with the practices of proceeding periods, but also that there has been the progression of “normal science” in the years since (Kuhn, 1962)."
"This is supported by the work of Locke (1989) into what he re-gards as the start of a new academic paradigm at a similar time. He argues this brought a new approach of management training through education, opposing the tradition of coming up the ranks from “apprentice” to “master-craftsmen”, a practice he argues as being without “applied science” (Locke, 1989, p. 4)."
"The advances and applications of the paradigm have sought to make available data, tools and analyses to provide the evidence to allow decision makers access to discursive evidence that can supple-ment their use of intuition and experience for more effective decision making (see Shah, Horne, & Capellá (2012) for further discussion on this area)."
"This is particularly evident in ‘soft OR’ (Rosenhead & Mingers, 2001) and ‘behavioural OR’ (Hämälläinen, Luoma, & Saarinen, 2013), but also, in approaches such as multi-criteria decision analysis (MCDA), the use of more subjective expert or decision maker judgement as a data input (see Köksalan, Wallenius, & Zionts (2011) for further discussion of the development of these methods). "
"Elsewhere, attempts were made to cre-ate solutions to “wicked” problems (Churchman, 1967); problems which are harder to structure and define due to conflicting perspec-tives amongst relevant stakeholders. This led to the development of the soft systems methodology (Checkland, 1981) and strategic options development and analysis (Ackerman & Eden, 2010). "
"Whilst these were still essentially GUIs, in contrast with the first DSS, these dashboards were pre-populated with key performance indicators (KPIs) designed to speedily convey the crit-ical measures of business performance (Few, 2006). The use of such metrics as management tools had become popularised by Kaplan and Norton’s (1992) balanced scorecard."
"This data is of such scale as to limit the application of BI archi-tecture and relational databases (Stonebraker et al., 2007), creating a demand for new technologies and architectures. Most notable is perhaps Hadoop, a distributed file system (DFS), designed to store, process and analyse such data, but also includes NoSQL and NewSQL databases (Cattell, 2010); the proliferation of cloud computing; and API-streams from data-rich sites such as Facebook and Twitter. In short, there has been a completely new ecosystem of businesses, technologies and cottage industries built to tackle the challenges of big data (see Feinleib (2012) for a visual representation of this ecosystem)."
"However, the challenges and opportunities presented by working with the extremely large datasets of the pe-riod has led to new approaches, which led Anderson (2008) to claim that the ‘scientific method’ is “obsolete”. He argues that as opposed to the deductive approach of hypothesis testing, the new big datasets require an inductive approach where correlations are the key to the process:"
"Secondly, there have been many efforts to provide decision support and automation in ‘real-time’ (e.g. Davenport & Harris, 2007; Niedermann, Radeschütz, & Mitschang, 2011). "
"There is a demand for reliable and robust methods for detecting PCD, especially for a quantitative measure of texture abnormalities for the purpose of predicting the stages of PCD [1,3]. Motivated by the problem of detecting PCD, this study focuses on providing a computer-aided detection framework for PCD, specifically for Oculopharyngeal Muscular Dystrophy (OPMD), one variety of PCD, and may also assist physicians in identifying other difficult cases."
"Genetic Programming (GP) employs tree structure representations to solve problems [22]. With inductive learning algorithms of varying power, the GP has been successfully applied to various learning algorithms, including the feature synthesis approach which has demonstrated the superiority for diverse classification problems [20,23,24]."
" The method commonly used for image background extraction is to select thresholds based on image color spaces [16]. To extract cell objects from the background, selection of the optimal threshold was introduced in [5] using the zeroth- and first-order cumulative moments of the gray-level histogram."
"Combined with probability theory, the graph-based method offered advantages for the classification of subcellular protein patterns in fluorescence microscope images [7,8]. Alternatively, the active contour-based method was feasible and effective for leukocyte tracking problems, when tracking cells in vitro, using the information of the region [9] and the edge [10]."
The details of the GP can be found in [22]
"When the transformed data X=xii=1m, the input of the EM steps, with the k known classes are assumed under the mixture of the k Gaussians, the multivariate Gaussian mixture with a d-dimensional mean vector Î¼j and a dÃd covariance matrix âj for the data X is given by [29,30]:(8)"
"The main reason is that the change due to optical distortion errors (associated with microscopy illumination effects) would be far less significant in the cumulative histogram, as compared to that change in standard histograms, because the continuous non-decreasing image representation is inherently noise-resistant [39]. Our experimental results support this."
"The CD approach is relatively new. It started gaining wide at-tention only in 2010, when Humble and Farley published the book titled “Continuous Delivery: Reliable Software Releases through Build, Test, and Deployment Automation” (Humble and Farley, 2010). However, the CD approach has become increasingly popu-lar, as shown by Google search trends (Fig. 1)."
"Infrastructure has been mentioned as a challenge by some studies. For example, Claps et al. (2015) reported that the need for adding additional computing power, network bandwidth, and memory to CI servers is a challenge. They consider adding more resources as a strategy (Claps et al., 2015)."
"To support CD, ideally we need the capability to automatically provision the required infrastructure. In other words, we need to change to Infrastructure as a Service (IaaS) (Mell and Grance, 2011)."
"Nevertheless, I believe these strategies are useful because we found that putting some of the existing strategies into action is challenging in its own right. For example, Claps et al. (2015) re-ported that it is important to “ensure that top management im-plements a strategy to push the need to implement the CD pro-cess” as a strategy to adopt CD. However, putting this strategy into"
"As another example, Claps et al. (2015) also reported provid-ing more resources to a product’s CI servers as a strategy to adopt CD. However, acquiring more resources was a true challenge for us. Without buy-in by the related stakeholders and their sustained support, we were unable to obtain the required resources."
"This includes, among others, a need for economically sound mechanisms to incentivize user participation, which is reportedly low in some of the existing systems [124] and is recognized as a significant obstacle factor in the others [50]. "
"To tackle the negative impact of peers churn on the performance of peer-assisted content delivery systems, the authors in [20] proposed a crowdsourcing-based content distribution system called Thunder Crystal. "
"The impact of the limited upload bandwidth on peer-assisted content distribution has been evaluated in  [42]. The authors demonstrated that although the demand for a high resolution video content is growing over time, its growth rate is in fact smaller than that of the average upload bandwidth among Internet users. This indicates a very positive trend for the future of peer-assisted content delivery, suggesting that the upload capacities of the end-user Internet connections in future would allow for pervasive peer-to-peer video sharing."
The authors of [41] suggested that a lower startup delay and continuous transmission could be achieved by utilizing P2P resources more efficiently. 
"A centralized architecture to minimize the inter-ISP traffic has been proposed in [67]. The approach relies on the Super Nodes (SN) deployed within each ISP and acting as central coordination units between the groups of peers and the CDN. In contrast, the authors of [78] proposed to employ the existing ISP-aware P2P protocols for PA-CDNs as a solution to handle the tussle between the ISPs and P2P networks. Thus, an ISP-aware PA-CDN in small ISPs can be built on top of a Tracker system, whereas DHT-based indexing such as Kadmelia can be more suitable for large ISPs. "
"Zhang et al. in  [122] reported that, peer-assistance enables significant benefits for Kankan in distributing popular videos, i.e., up to 98.0% of the video content in Kankan are distributed in a peer-to-peer fashion, whereas edge nodes are responsible for handling a long-tail of unpopular videos."
"To tackle the problem of peers’ inaccessibility behind the middle boxes, LiveSky and NetSession have exploited the modified version of STUN [92] and UDP protocols. "
"It has been shown, in various recent papers, that by employing efficient topology-aware and ISP-friendly peer selection policies [67,123] the deleterious influences of inter-ISP traffic can be minimized."
"Indeed, a study on the two leading CDN operators, i.e. Limelight and Akamai, demonstrated that, significant traffic savings (i.e., up to 66.53% for Akamai and 65.55% for Limlight) can be achieved, even if peer-to-peer traffic is localized within the ISP domains [44]. "
" Some recent studies have shown that, cache-enabled Device-to-Device (D2D) communication can be helpful in improving spectral efficiency and reducing communication delay [36,77,107]. "
The straight skeleton S(P) of a simple polygon P was introduced by Aichholzer et al. [1] and is defined by considering the propagation of a so-called wavefront.
"The weighted version of the straight skeleton, where edges no longer move at unit speed, was first mentioned by Eppstein and Erickson [2] and studied in detail by Biedl et al. [4,5]. "
"Recently Biedl et al. [4] showed that many of the seemingly obvious properties of straight skeletons no longer hold when weights are not unit weights. Therefore, diligent consideration is required when extending existing proofs to weighted straight skeletons."
"Variable length Markov chains (Bühlmann and Wyner, 1999) relax the assumption that the conditional distribution involves  prior responses, instead allowing the number of previous variables that enter the conditioning to vary according to the values of these variables"
" Variable length Markov chains provide a rich model space and allow the choice of structured models that incorporate longer range dependences. Efficient model selection methods are available (Bühlmann, 2000). "
"The rationale is that the variability of (5) depends on the node counts  and , and is greatest when these are small. Consequently when a constant threshold is used, nodes with small counts tend to be judged dissimilar by chance. The proposed threshold is twice the maximum of the asymptotic standard deviation under the null hypothesis. In Browning and Browning (2007a) the threshold was further modified to take the form
where  and  are scale and shift tuning parameters, respectively. Increasing the threshold results in simpler partitions: since the number of nodes at higher stages is reduced accordingly this also increases the efficiency of the selection process. The values  and  were recommended in Browning and Browning (2007a), based on unpublished simulation studies."
"We compare the model selection algorithm described in the previous section with that described by Browning and Browning (2007a) and implemented in Beagle in two ways: firstly, by comparing their rate of convergence as the sample size increases using simulation, and secondly by assessing the goodness-of-fit of the selected model using ten-fold cross-validation. In both cases we use the three data sets described in Section  5"
We compare the performance of the proposed model selection algorithm based on penalized likelihood criteria (both AIC and BIC) to that implemented in Beagle. Here we use both the settings suggested in Browning and Browning (2007a) ( and ) and the settings implicit in Browning (2006) ( and ). We compare the algorithms in respect to the rate at which the selected model converges to the true model as the sample size increases
"Table 4 shows the goodness-of-fit, as assessed by the mean edge probability , of the selected model obtained using the proposed method and using Beagle with different tuning parameter settings. We observe that for all three data sets the goodness-of-fit of the proposed method is comparable to that of Beagle, when the settings implicit in Browning (2006) ( and ) are used, and superior to Beagle, when the settings suggested in Browning and Browning (2007a) ( and ) are used. For all three data sets, setting  and  gives the best fit. The performance of AIC is observed to be better than that of BIC for all data sets, and as good as or better than that of Beagle."
"In this paper we have given a brief introduction to the use of APFA to model discrete longitudinal data, and adapted an algorithm proposed by Ron et al. (1998) and implemented (with modifications) by Browning and Browning (2007a), so that this seeks to minimize a penalized likelihood criterion, for example AIC or BIC. We also compared the algorithms by assessing the rate of convergence to the true model as  as well as their goodness-of-fit. The algorithm proposed here using the AIC performs at least as well as that implemented in Beagle in both respects. The improvement over Beagle is substantial when the settings suggested in Browning and Browning (2007a) ( and ) are used, but when the settings implicit in Browning (2006) ( and ) are chosen the performance of the two algorithms was very similar"
"According to data mining practitioners [20], data preparation1 can take up to 80% of the modelling efforts and is crucial for development of well-performing models."
"In modelling of the Blood Oxygen Level Dependent (BOLD) signal, this effect is additionally magnified by the haemodynamic response delay [13] and relatively low resolution of the images [27]. "
"The intersection of the 10 sets obtained in this way resulted in 92 voxels common for all sessions (Fig. 6), which is a slightly different result when compared to [17], where the authors have reported the intersection to contain 93 voxels."
"The ratio of the average intra-class distance to the average inter-class distance. This is a measure inspired by clustering algorithms [4], designed to encourage formation of groups of samples, which are similar to each other while dissimilar to the samples in other groups."
" Unfortunately, such a relation is not computable (Weiser, 1984). A well known approximation is based on Weiser's program slice (Weiser, 1984): a slice is the set of program statements that affect the values computed at a particular statement of interest (referred to as a slicing criterion)."
"Two kinds of SDG slices are used in this paper: backward slices and forward slices (Horwitz et al., 1990; Ottenstein and Ottenstein, 1984)."
"Same-slice clustersAn alternative definition uses the same-slice relation in place of slice inclusion (Binkley and Harman, 2005). This relation replaces the need to check if two vertices are in each others slice with checking if two vertices have the same slice. The result is captured in the following definitions for same-slice cluster. "
"The section then quantitatively considers the existence of coherent dependence clusters and identifies patterns of clustering within the programs. This is followed by a series of four case studies, where qualitative analysis, aided by the decluvi cluster visualization tool (Islam et al., 2010a), highlight how knowledge of clusters can aid a software engineer. "
"The slices along with the mapping between the SDG vertices and the actual source code are extracted from the mature and widely used slicing tool CodeSurfer (Anderson and Teitelbaum, 2001) (version 2.1). The cluster visualizations were generated by decluvi (Islam et al., 2010a) using data extracted from CodeSurfer. The"
"At the Medium and High settings, CodeSurfer performs extensive pointer analysis using the algorithm proposed by Fahndrich et al. (1998), which implements a variant of Andersen's pointer analysis algorithm (Andersen, 1994) (this includes parameter aliasing). At the medium setting, fields of a structure are not distinguished while the High level distinguishes structure fields. "
"There is no automatic way to determine whether the slices are correct and precise. Weiser (1984) considers smaller slices to be better. Slice size is often used to measure the impact of the analysis' precision (Shapiro and Horwitz, 1997), similarly we also use slice size as a measure of precision."
"Considering the precision of individual programs, five of the programs have a precision greater than 97%, while the lowest precision, for findutils, is 92.37%. This is, however, a significant improvement over previous use of slice size as the hash value, which is only 78.3% accurate in the strict case of zero tolerance for variation in slice contents (Binkley and Harman, 2005)."
"To assess if a program includes a large coherent cluster, requires making a judgement concerning what threshold constitutes large. Following prior empirical work (Binkley and Harman, 2005; Harman et al., 2009; Islam et al., 2010a,b), a threshold of 10% is used. In other words, a program is said to contain a large coherent cluster if 10% of the program's SDG vertices produce the same backward slice as well as the same forward slice"
"Furthermore, groups of inter-dependent coherent clusters form larger dependence structures than same-slice clusters and provides a better approximation for slice-based clusters. This indicates that the sizes of dependence clusters reported by previous studies (Binkley et al., 2008; Binkley and Harman, 2005, 2009; Harman et al., 2009; Islam et al., 2010b) maybe conservative and mutual dependence clusters are larger and more prevalent than previously reported"
"Initial work on dependence clusters advised that they might cause problems in software maintenance, and thus even be considered harmful, because they represent an intricate interweaving of mutual dependencies between program elements. Thus a large dependence cluster might be thought of as a bad code smell (Elssamadisy and Schalliol, 2002) or a anti-pattern (Binkley et al., 2008). Black et al. (2006) suggested that dependence clusters are potentially where bugs may be located and suggested the possibility of a link between clusters and program faults."
" Each update was manually checked using CVSAnaly (Robles et al., 2004) to determine whether the update was a bug fix or simply an enhancement or upgrade to the system. Those commits that were identified as bug fixes were isolated and mapped to the release that contained the update."
"This can be achieved by measuring the impact of the proposed change (Black, 2001) or by attempting to identify portions of code for which a change can be safely performed free from side effects (Gallagher and Lyle, 1991; Tonella, 2003)."
"A recently proposed impact analysis framework (Acharya and Robinson, 2011) reports that impact sets are often part of large dependence clusters when using time consuming but high precision slicing. When low precision slicing is used, the study reports smaller dependence clusters. "
"This paper uses the most precise static slicing available. There has also been recent work on finding dependence communities in software (Hamilton and Danicic, 2012) where social network community structure detection algorithms are applied to slice-inclusion graphs to identify communities."
"This paper extends our previous work which introduced coherent dependence clusters (Islam et al., 2010b) and decluvi (Islam et al., 2010a). Previous work established the existence of coherent dependence clusters and detailed the functionalities of the visualization tool. "
" Finally, we also present studies which show the lack of correlation between coherent clusters and bug fixes and show that coherent clusters remain surprisingly stable during system evolution.In some ways our work follows the evolutionary development of the study of software clones (Bellon et al., 2007), which were thought to be harmful and problematic when first observed."
"While engineers needed to be aware of them, it remains a subject of much debate as to whether or not they should be refactored, tolerated or even nurtured (Bouktif et al., 2006; Kapser and Godfrey, 2008).We believe the same kind of discussion may apply to dependence clusters."
"Qualitative models have traditionally been produced on flip charts using marker pens, but computer-mediated modelling is increasing in popularity, and this can facilitate remotely distributed and/or anonymous stakeholder participation, bringing advantages compared with face-to-face, pen and paper modelling (Er and Ng, 1995; Fjermestad, 2004; Fan et al., 2007)."
"Some PSMs are explicitly systemic (Jackson, 2000; Midgley, 2000, 2003). They not only seek to enhance mutual understanding between stakeholders, but they also support participants in undertaking ‘bigger picture’ analyses, which may cast new light on the issue and potential solutions."
"First, claiming universality for knowledge about systemic PSMs would suggest that this knowledge will remain stable over time. However, it is clear from the literature (e.g., Rosenhead and Mingers, 2004; Shaw et al., 2006; Franco et al., 2007) that new problem structuring methods are being produced on a regular basis, indicating that people are learning from previous practice and are also having to respond to an ever increasing number of unique practical situations."
"Our second methodological argument, following Eden (1995) and others, is that only seeking knowledge about the supposedly generic strengths and weaknesses of methods ignores legitimate questions that can be asked about the effectiveness of those methods in particular local circumstances. "
"No doubt the list of possible aspects of context could be extended indefinitely (Gopal and Prasad, 2000), and different issues will be relevant in different situations, so we argue that it is more useful to give some methodological guidelines for exploring context in local situations than it is to provide a generic inventory of variables."
"Establishing a boundary for analysis involves making a value judgement on what issues and stakeholders are important or peripheral (Ulrich, 1994). Therefore, undertaking an exploration of different stakeholders’ values and priorities can be helpful. "
"Identifying the presence of influential institutional or organisational systems may be important. Any such system can have its own agenda, rationality and momentum that may come to dominate an intervention (Douglas, 1986; Luhmann, 1986; Brocklesby, 2009), yet organisational systems still have to interact with others, and tensions can result (Paterson and Teubner, 1998). Thus, an institutional analysis can be a useful aspect of boundary critique."
"Pettigrew (1987) notes that wider systemic (e.g., socio-economic and ecological) contexts not only influence perceptions of methods and processes, but also the content of participants’ deliberations."
"We note that our questions align well with five high level criteria suggested by Hjortsø (2004) for the evaluation of PSMs, except that our questions go into much more detail. Hjortsø’s criteria are the extent to which the method is a good fit for the context, and whether it supports (i) mutual understanding; (ii) stakeholder involvement in decision making; (iii) the acceptance, transparency and accountability of decision making; and (iv) the collaborative management of complexity and conflict."
"This method was first introduced in [9] and extended to the case of positive-dimensional solution sets in [10]. In practice, regeneration is typically quite efficient, often producing the nonsingular isolated solutions of a polynomial system much faster than standard homotopy methods, in part by automatically taking advantage of any symmetries or structure in f(z) (see Section 9.3 of [9])."
This section outlines in broad strokes the regeneration homotopy method for computing the isolated nonsingular solutions of a polynomial system as first developed in [9] and stated succinctly for the nonsingular case in [10]
"A more general statement than Lemma 3.3 is given as an exercise in [7] and a related result for a pure d-dimensional algebraic subset is presented in [8], for d>0"
"Using precisely the Bertini settings described on the examples page for [3] for all runs, we find that regeneration is fastest, followed by perturbed regeneration. All other homotopy types (perturbed or not) were cost prohibitive, taking at least twice as long as perturbed regeneration."
"As outlined in [9], regeneration techniques can be combined with deflation to find singular solutions. Deflation is a technique that replaces a polynomial system f on CN and an isolated singular solution xâ with a new polynomial system f^(x,Î¾) on CNÃCM with an isolated nonsingular solution (xâ,Î¾â). Deflation was first considered in a general context in [21,22]. Deflation for polynomial systems, including a proof that the method will ultimately yield a nonsingular solution, was further considered in [15,14]. A more recent, more general version of deflation, strong deflation, was developed in [11]. "
"If the user expects positive-dimensional solution sets (or cannot preclude their presence), regenerative cascade [10] is typically the best bet"
"Regeneration (perturbed or not) seems to be a good option if the problem has some special structure, whereas a multihomogeneous homotopy may be a good option if the variables naturally fall into multiple groupings. For sparse problems, polyhedral methods [12,26,16] are an especially good option"
"An area-level linear mixed model with random area effects was first proposed by Fay and Herriot (1979) to estimate average per-capita income in small places of the US. Since then, the FayâHerriot model has been one of the most widely used models in small area estimation.In recent years, many researchers have investigated applications of the FayâHerriot model to small area estimation problems. "
"Without being exhaustive, we cite some papers dealing with the FayâHerriot model. Prasad and Rao (1990); Datta and Lahiri (2000); Das etÂ al. (2004), GonzÃ¡lez-Manteiga etÂ al. (2010), Jiang etÂ al. (2011), Datta etÂ al. (2011a) and Kubokawa (2011) gave tools for measuring the uncertainty of model-based small area estimators."
"Datta etÂ al. (2011b), Bell etÂ al. (2013) and Pfeffermann etÂ al. (2014) studied the problem of benchmarking."
 Ybarra and Lohr (2008) proposed a new small area estimator that accounts for sampling variability in the auxiliary information. 
 Choudry and Rao (1989) introduced a model including several time instants and considering an autocorrelated structure for sampling errors. 
 Rao and Yu (1994) proposed a model that borrows information across areas and over time.
Ghosh etÂ al. (1996) proposed a time correlated area level model to estimate the median income of four-person families for American states. 
"Datta etÂ al. (1999), You and Rao (2000), Datta etÂ al. (2002), Esteban etÂ al. (2011, 2012), Marhuenda etÂ al. (2013) and Morales etÂ al. (2015) gave some extensions of the RaoâYu model with applications to the estimation of labor or poverty indicators. "
" This paper applies multivariate FayâHerriot models to time correlated data. In this setup, the introduced multivariate models contain the models proposed by Esteban etÂ al. (2011) as particular cases."
Prasad and Rao (1990) gave an approximation to the MSE of the EBLUP of Î¼dr under the univariate FayâHerriot model when their proposed moment-based estimator of the variance Ïur2 is employed. Datta and Lahiri (2000) extended the results of Prasad and Rao (1990) to the case of the general longitudinal model. 
"Although, the multivariate FayâHerriot model (4) can be written in the form of the general linear mixed model considered by Das etÂ al. (2004), the approximation of the matrix of mean squared crossed errors is not covered by this paper."
"Under the assumption of normality on u and e and for unbiased and translation invariant estimators of Î¸, Kackar and Harville (1981) proved that the expectations of the last two terms in (12) are null. Therefore, by taking expectations, we get (13)"
The target of Simulation 2 is to investigate the behavior of the MSE estimator (15) and the three bootstrap alternatives considered by GonzÃ¡lez-Manteiga etÂ al. (2008b).
"Esteban etÂ al. (2012) and Marhuenda etÂ al. (2013) gave estimates of province poverty proportions and gaps by using data from the 2006 Spanish Living Condition Survey (SLCS). They calculated EBLUPs based on univariate temporal area-level linear mixed models. This section uses the same data as in the above cited papers for estimating poverty proportions and gaps, but calculates EBLUPs based on multivariate FayâHerriot models."
Esteban etÂ al. (2012) studies several univariate extensions of the FayâHerriot model to temporal data. These authors recommended using their model 3 with random effects taking into account for AR(1) time correlation within each domain. They use past data from 2004 and 2005 for giving estimates of 2006.
"Given such detailed correspondences with experienced qualia and multiple types of data, it can be argued that these dynamical resonant states are not just “neural correlates of consciousness” (Chalmers, 2000; Mormann & Koch, 2007). Rather, they are mechanistic representations of the qualia that embody individual conscious experiences on the psychological level. "
"For example, Koch and Tsuchiya (2007) note that “subjects can attend to a location for many seconds and yet fail to see one or more attributes of an object at that location…In lateral masking (visual crowding), the orientation of a peripherally-presented grating is hidden from conscious sight but remains sufficiently potent to induce an orientation-dependent aftereffect…” "
"One reason for this inter-modality unity is that a small number of equations suffice to model all modalities. These include equations for short-term memory, or STM; medium-term memory, or MTM; and long-term memory, or LTM, that were introduced in Grossberg (1968c, 1969); see Grossberg (2013b) for a review. These equations are used to define a somewhat larger number of modules, or microcircuits, that are also used in multiple modalities where they can carry out different functions within each modality."
"These adaptive weights are pruned as learning proceeds, leading to learning of an attended critical feature pattern. It has been mathematically proved that such match learning can solve the stability–plasticity dilemma by creating stable categories in response to arbitrary sequences of events presented in any order; e.g.,  Carpenter and Grossberg (1987, 1991)."
"Taken together, these signals form a recurrent on-center off-surround network that is capable of contrast-enhancing and normalizing its activities (Grossberg, 1973, 1980). This recurrent exchange of excitatory signals, combined with competitive inhibitory signals, helps to choose a winning focus of spatial attention in PPC that configures itself to the shape of the attended object surface in V4."
"Fortunately, basic properties of bipole cells for perceptual grouping, and simple feedback interactions between the boundary and surface streams, go a long way to accomplish figure-ground separation. (Grossberg, 1994, 2016a)."
"The Where stream is also called the How stream because of its important role in controlling actions in space (Goodale & Milner, 1992). This proposed link of space to action gains new significance in terms of my proposal of how the conscious qualia of unoccluded surfaces help to determine reachable target positions in space."
"As noted by Bonneh, Cooperman, and Sagi (2001, p. 798), motion-induced blindness may be influenced by “perceptual grouping effects, object rivalry, and visual field anisotropy…Disappearance might reflect a disruption of attentional processing…uncovering the dynamics of competition between object representations within the human visual system”."
"How this is predicted to occur was first proposed in Grossberg (1994, 1997), where it was also shown that, remarkably, the process that assures perceptual consistency also initiates figure-ground separation."
The Li and DiCarlo (2008) and Chiu and Yantis (2009) experimental paradigms may be combined to further test the model prediction of how spatial attention may modulate learning of invariant object categories
"A helpful way to understand auditory–visual homologs is to consider the perception–action circular reactions that occur during auditory and visual development (Piaget, 1945, 1951, 1952). "
"Grossberg (2003b, 2013b), Grossberg and Pearson (2008), and Grossberg and Kazerounian (2016) review the hypothesis, first proposed in Grossberg (1978a), that Item-and-Order working memories satisfy two postulates which ensure that speech and language can be learned in a stable way through time: the LTM Invariance Principle and the Normalization Rule. "
"Agam, Galperin, Gold, and Sekuler (2007) reported data consistent with the formation of list chunks as movement sequences are practiced, thereby supporting the prediction that working memory networks are designed to interact closely with list chunking networks."
NormNet was tested by speaker-normalizing and learning steady-state vowel categories from the Peterson and Barney (1952) database with an accuracy similar to that of human listeners
"Several experiments have reported such asymmetric vocalic context effects from T to S, but not conversely (Kunisaki & Fujisaki, 1977; Mann & Repp, 1980), and psychophysical experiments support the importance of consonant/vowel ratio as a cue for voicing in English (e.g.,  Port and Dalby (1982))."
" The hippocampus in this model includes a circuit for adaptively timed learning, called a spectral timing circuit in the earlier START model (Grossberg & Merrill, 1992, 1996; Grossberg & Schmajuk, 1989). Such a spectral timing circuit shows how a population of cells with differently timed responses (the “spectrum”), none of which can individually time a response over a long time interval (Fig. 41, bottom row), can together bridge the temporal gap between CS offset and US onset during trace conditioning (Fig. 41, top row), as well as during other learning paradigms wherein temporal gaps occur. "
"This inhibition would not, however, occur in response to an unexpected non-occurrence because the spectral timing circuit would not be active then (Grossberg & Merrill, 1992, 1996; Grossberg & Schmajuk, 1989). As a result, an unexpected non-occurrence could lead to the usual sequence of cognitive, emotional, and motor responses to correct the predictive error."
"Simply trying to find the smallest group of experts exhibiting all required skills is no longer a valid approach. A major factor of team success, for instance, is whether a set of experts can work together effectively [2,3]. The frequency of previous interactions is one possible indicator of successful collaborations. "
Recent efforts introduce social network information to enhance the skill profile of individual members. Hyeongon et al. [3] measure the familarity between experts to derive a person's know-who
"Cheatham and Cleereman [5] apply social network analysis to detect common interests and collaborations. The extracted information, however, is again applied independently from the overall team structure."
"The authors, however, remain silent on specific details about the actual algorithm to find an optimal team. An alternative approach is first finding tightly connected communities [12] and then analyzing the available skills to generate desirable team configurations."
Papers on existing online expert communities such as Slashdot [20] and Yahoo! answers [21] yield specific knowledge about the social network structure and expertise distribution that need to be supported by a team composition mechanism.
"Composition is driven by the client's preferences [30], environment context [31,32], or service context (i.e., current expert context) [33]. We can take inspiration from such research to refine the properties and requirements of teams to include context such as expert's organization or location."
"In contrast, the network structure has gained significant impact for determining the most important network element. A prominent example of a graph-based global importance metric is Google's page rank [34]."
Social network-based expert finding and subsequently team formation will soon become central business concerns [39].
"Most social networks yield a power-law degree distribution [57,58]. In such networks most nodes exhibit only a few neighbors while a few nodes are extremely well connected. For simulation purposes such a degree structure emerges from preferential attachment of edges [59]."
"The important work of Lee, Padmanabhan, and Whang (1997) not only brought the term bullwhip effect to widespread academic attention, but also proposed an additional four causes to the problem where players are assumed to behave completely rationally. "
"his effect is considered to be an important cause of business and economic cycles (Clark, 1917; Mitchell, 1913; Samuelson, 1939). Vaile, Grether, and Cox (1952) reported that the production of consumer goods is estimated to have fallen from an index of 100–80 during the 1929–1932 depression, while that of all capital equipment fell from 100 to 35. Such an effect is sufficient to generate demand amplification. "
"The experimental approach was pioneered by the seminal paper of Sterman (1989), who documented a role-playing game for inventory management called the ‘Beer Game’. This later became the standard experimental framework to study supply chain dynamics"
"Chen, Dresner, Ryan, and Simchi-Levi (2000a) identified that bullwhip is, at least partly, due to the unpredictability of demand, lead-times and the need to forecast future demand. Since then these causes have received a large amount of research attention."
"The first order auto-regressive demand model, AR(1), has perhaps been the most frequently adopted (Chen et al., 2000a; Lee et al., 1997, 2000, amongst others). This demand model has only one parameter, so it is easy to observe the impact of autocorrelation without sacrificing too much tractability. "
"There is a rising concern over the environmental impact of production and supply chain systems and increased regulations have made reverse logistics a hot topic for research (Govindan et al., 2014). There have been some attempts in the literature to quantify the bullwhip effect in reverse logistics systems. Tang and Naim (2004) incorporated a remanufacturing process into the APIOBPCS model (John et al., 1994). "
"Current technological advances allow decision makers to access information more easily by using wireless networks, data warehouses and similar tools [42,52]. The vast amount of information is not necessarily linked to more accurate and efficient decisions, but rather sometimes to “information overload” for a decision maker (e.g. [41,72]). Scientific interest also focuses on handling large amounts of information and on overcoming mental resource limitations and cognitive biases (e.g. [23,46]). These developments have led to the advancement of stylized decision aids that “represent the problem in a stylized way that capitalizes on some special human cognitive processing ability” [86, p. 46]. Traditional stylized decision aids are tables and graphs in the form of lines, scatter plots, bar charts, and animations [45]. These display formats have been used successfully to extend human processing abilities in decision making [34,78,79]. Nevertheless, the potential of stylized decision aids has not yet fully been explored in eNS research. Thus far, scholars have focused on the improvement of tool-functionalities which aid bargainers in the negotiation process (e.g. [11,37,53]). In that sense, graphical support implemented in a system would be used to improve process and outcome (e.g. [7,12,82]). In electronic negotiation systems, information to be represented in a graphical manner would include message threads, preferences and utility values [62]"
"Complementing the cognitive fit theory, Paivio [48–50] proposes the dual coding theory. This suggests that human working memory encodes, organizes, stores and retrieves imagery and verbal information in two different ways. When retrieving, processing and reproducing information, cognitive activities are mediated by two independent yet interconnected cognitive subsystems in the human mind: An imagery system (specialized in the representation and processing of nonverbal objects in a sequential manner) and a verbal system (specialized in handling linguistic propositions using a parallel processing system). Both methods are functionally interconnected at the referential levels, so that an activity in one system can cause an activity in the other system. The visual argument approach asserts that graphical displays make less demands on human cognitive resources [34,59]. According to this theory, graphs enable users to extract information without engaging in deep processing by providing guidance, constraints and facilitations in cognitive processes."
"The cognitive fit theory and its complementary models (dual coding theory, visual argument approach and conjoint retention hypothesis) have received significant attention in empirical research. Several studies confirm the basic assumptions of the cognitive fit theory and propose further extensions. Speier and Morris [71] provide a study associating literature on graphical support and cognitive fit theory. They investigate the characteristics of query interfaces and show that visual interfaces provide a holistic perspective of the presented data. Along with Smelcer and Carmel [68], they extend the view of comparative advantages of graphical display formats by showing that the performance difference in terms of time and accuracy increases even with task complexity. The relationship between the level of information processing and environmental complexity has the shape of an inverted “U” [65], demonstrating that graphical aids allow users to gather more information prior to reaching the critical point of information overload. Free cognitive resources can be used elsewhere. A more recent Speier study [70] illustrates that subjects supported with graphs perform as well as subjects supported with tables, when facing complex symbolic tasks involving decision accuracy. Furthermore, they outperform the latter when facing spatial tasks. Graphs help subjects find solutions faster regardless of task complexity in spatial tasks, while subjects supported with tables are only equally efficient in complex symbolic tasks. Concerning the characteristics of spatial language, Hubona et al. [21] provide support for the cognitive fit theory in terms of decision accuracy but not in terms of time. Recently, Khatri et al. [28] extended the perspective of cognitive fit for external problem presentations and internal task representations. They find subjects to perform more accurately but not faster when both presentation formats match. The fit of both presentations facilitates an understanding of the presented information."
"Other studies suggest a trade-off between the benefits of minimizing errors and the cognitive effort or time needed in a particular task environment [14]. When facing complex situations, decision makers use cognitive simplification strategies [15,61] and pursue a strategy of swapping effort in terms of time invested in the problem solution for accuracy [24]. The graphical organization of information influences the equation of this cost–benefit tradeoff by allowing the user to pursue an adequate strategy more easily than others. Jarvenpaa [22] introduces the term “incongruence” to describe a situation in which the processing required for a decision strategy and the process encouraged by the graphical tool are in conflict. Thus, the cost–benefit principle assumes that this incongruence results in additional costs for the user, increased effort or time or higher likelihood of mistakes. Dilla and Steibart [13] confirm that additional mental calculations increase the potential of making mistakes."
"Tables represent information that is symbolic in nature. They present data in separable formats, which introduce single point values more accurately than other representation aids [12,31,67]. Results from various studies indicate that tables should be used to present information when decision makers are required to recall specific amounts, handle values accurately (e.g. [5,12,45]) or compare data [44]. Therefore, in conflict situations with high sensitivity to small deviations from the optimum, tabular reports can provide exact values that are more resistant to distortion in comparison with other forms of information representation [4,5]. Tables provide little integrative information. Any links between the single values displayed must be made by the decision makers since tables neither provide support for integrating the effects of a number of variables in one period of time, as schematic faces do, nor for showing the effects of one variable over more periods of time, as graphs do [67,79]. The general assumption is that symbolic representation facilitates extracting and acting on discrete data values, and analytical processes provide the most appropriate access for decision makers to data presented in tables [78–80]."
"Alternatively, literature proposes the use of the negotiation dance graph [56]. In contrast to the history graph, the negotiation dance graph rates and visualizes each exchanged offer according to the real preferences of both negotiators, i.e. the history of offers is presented in the joint utility space (see Fig. 3). The negotiation dance graph presents preference information about the counterpart to the negotiators, thus providing significantly more information than the negotiation history graph. Operatively, each offer is rated on the ordinate according to the preferences of the focal negotiator, while on the horizontal axis the offer is rated according to the preferences of the counterpart. While in a single attribute negotiation, preference information can directly be inferred from the dance graph, this information is much more difficult to read in a multi-attribute negotiation. Nevertheless, by comparing several offers made by the negotiation partner, the negotiators can identify the counterpart's major trade-offs between attributes. Within an integrative negotiation approach, the knowledge of the counterpart's true preferences facilitates Pareto-improving negotiation moves and consequently efficient agreements [56]. Within a distributive negotiation setting, however, it bears the danger of being exploited by opportunistic and competitive negotiators. In the negotiation dance, the factor “time” is considered to be more implicit as all offers are numbered in chronological order and linked by spatial lines. The main difference between the history graph and the negotiation dance graph is that in the history graph calculations are made only on the basis of the focal user's preferences, whereas in the negotiation dance each rating is a function of utilities of both users."
"Swaab et al. [75] propose that negotiators provided with graphical decision aids develop a better understanding of the negotiation problem. Through the display of the utilities of previous offers and counteroffers during the negotiation, negotiators can more easily identify tendencies and trends, conflicting issues and topics less exposed to conflict. Since negotiators refer to salient information [61], we assume that negotiators with graph support will be more focused on the task at hand, with knowledge of the entire process and the ability to discuss issues in terms of utility values. Furthermore, negotiators supported with graphs should be better able to create a shared cognition of the conflict situation and consequently facilitate communication about needs and interests rather than positions (e.g. [43,61,75]). Additionally, graphs could also enhance the process of idea generation [7]. Altogether, we assume:"
"Social interaction is closely related to the issue of fairness. It is assumed that there are several reasons why people act in a fair manner [9]. Apart from altruistic motives, people behave justly hoping for reciprocity from the other party or to avoid being punished for unfair behavior (e.g. [54,87]). The dynamic representation of behavior in the history graph makes both concessions and resistance to concede visible for negotiation partners. We assume that this will evoke more discussions about fairness"
"In any conflict situation, both parties have to converge in order to reach an agreement, i.e. at least one has to make a concession. Concessions seem to be crucial, especially when parties are trapped in a deadlock, or when conflict spirals occur and the situation escalates [29,55]. People often view bargaining situations negatively and perceive concessions as losses. Negotiators supported with the history graph can easily assess the effects of concessionary steps since they are displayed dynamically. We, therefore, hypothesize:"
"Negotiators often base their decisions on heuristic strategies or on oversimplifying rules, which allow them to generate leverage effects within the decision accuracy-benefit trade-offs [24]. This behavior reduces cognitive effort and negative effect [18]. Negotiators trust their own judgments to be correct. However, if conflicts become more difficult, the result is often overconfidence (e.g. [23,46]) and less concessionary behavior from the involved parties [2]. To convince or persuade the counterpart of a biased opinion, they use hard tactics (threats, intimidation and demanding commitments) [69]. When negotiators are provided with the negotiation history graph, the risk to succumb to overconfidence is reduced. As discussed above, negotiators can more easily analyze previous concession behavior and infer how much effort is required to reach an agreement. We hypothesize:
H 1(e)"
"In summary, negotiators provided with the history graph are expected to share priority information and stress social relationships and fairness. They will use fewer hard tactics and make more concessions. In negotiation theory, this behavior is classified as “integrative negotiation behavior” [84,85] and has been shown to have a positive effect on agreement. We hypothesize:"
"When negotiations have closed and parties leave the virtual bargaining table, they feel like either winners or losers [38]. Their mood and feelings depend on various factors. The process by which agreement was reached must be considered. The provision of the history graph will lead to integrative negotiation behavior resulting in a better bargaining climate [25]. According to the hypotheses stated above, we expect higher joint and more balanced outcomes to have a positive impact on the level of satisfaction (e.g. [17,37,77]). All of these factors contribute to the following hypothesis:
H 3(c)"
"We applied content analysis to the 60 negotiation transcripts following the five stage model suggested by Srnka and Koeszegi [73]. Each negotiation transcript was unitized by two coders. At the end of unitization, two quality checks were performed. When assessing intercoder reliability of unitization, we reached a Guetzkow's U = 0.17% and the textual conformance of unitization of 91.36% of all coded units. Both results can be considered very satisfying [30,73,84]. Differing unitizations were eliminated through discussion. In total, the 60 negotiation transcripts were divided into 10,161 codable units. For categorization, a category scheme was developed including 64 subcategories summarized in nine main categories (see Appendix A). Each negotiation transcript was coded by two coders. The inter-coder reliability, Cohen's κ, reached 0.94 which can be considered an excellent result [39]. Again, discrepancies between coders were discussed, and all differences were eliminated."
"The SetNet software is available from [1] along with all of the study material and a video preview of our work. This includes the diagrams, questions and the performance data collected. The studyâs diagrams are also available as supplementary material."
 Gurr proposed that visualizations which are well-matched to their semantics are effective [19]
"We summarize these results, presented as a series of layout guides by Blake etÂ al. [10]:"
"Well-formed Draw Euler diagrams that are well-formed [37]. The well-formedness properties include: only simple curves, sets represented by unique curves, no concurrency between curves, no 3-points, and curves that cross and do not brush."
"The authors of [10] further observe, using similarity theory [13], that the reduced ability to discriminate implies the use of rectangles leads to worse performance as compared to circles."
"The primary purpose of this section is to introduce and theoretically analyze the four techniques that we evaluated: EulerView [41], Bubble Sets [12], LineSets [3], and KelpFusion [28]; they are illustrated in Fig.Â 1."
An Euler diagram is well-formed if it possess all six properties.Smooth curves Draw Euler diagrams with smooth curves [7].Diverging lines Draw Euler diagrams with diverging lines which means that the crossing angle at points of curve intersection approaches 90 degrees [7].Shape Draw Euler diagrams with circles [10]. Symmetry Draw Euler diagrams with highly symmetrical curves [10].Shape discrimination Draw Euler diagrams such that the regions are discernable from the curves via their shape but not at the expense of symmetry [10].
"Lastly, the boundaries of the regions are similar in shape to the curves themselves so the shape discrimination guide is not met.3.3LineSets (LS)LineSets were introduced to overcome problems associated with visual clutter that is seen in Euler diagrams [3]. "
"An early attempt to combine sets and networks in a single visualization relied on first drawing an Euler diagram then placing a graph inside it [30], however the sets were often visualized with convoluted, difficult to follow curves. "
"Graph clusters are visualized with transparent hulls by Santamaria and Theron [39]. However, the technique removes edges from the graph and it is not sufficiently sophisticated for arbitrary overlapping sets."
It is possible to visualize grouped network data using ComEd and DupEd by drawing edges between the nodes [35]. 
"An incremental constrained graph layout, IPSep-CoLa [14], has been employed to create such a visualization by defining the grouping of the nodes into sets as constraints on the graph layout. This results in a balanced layout with respect to sets and to the network structure. "
" Moreover, it uses circles to represent the sets, instead of rectangles connected with concave curves. Further techniques for visualizing similar data are described in [18,40,44] and a recent survey on set visualizations [5]"
"Alper etÂ al. [3] evaluate LineSets as compared to Bubble Sets with three categories of grouped network data: 50 items, three sets and five set intersections; 100 items, four sets and 10 set intersections; and 200 items, five sets and 30 set intersections. They did not indicate how many connections existed between the items (or, therefore, how many edges were in the corresponding graphs). The tasks that users were asked to perform focused on set-theoretic properties and were of the form: how many sets are there?; which of these two sets is largest?; which sets do both x and y like?; and how many elements of this set are also in these other sets? LineSets were shown to yield significantly more correct answers and to permit significantly faster responses than Bubble Sets. However, the tasks did not utilize both the network and the set information in conjunction, which perhaps explains why there was no mention of the number of edges in the graphs."
"Meulemans etÂ al. evaluated KelpFusion against both LineSets and Bubble Sets [28]. Their study used visualizations of grouped data (with no network information) with the following characteristics: four sets with 15 to 49 items; five sets with 12 to 29 items. As this study focused on data items that were locations on a map, connection information through a graph-based visualization was not present. As such, the tasks again were focused on set-theoretic concepts, such as cardinality; e.g. How many elements are there in these two sets? (paraphrased here). The study found that KelpFusion and LineSets both yielded significantly better accuracy than Bubble Sets. With regards to completion time for the tasks, KelpFusion performed fastest, followed by LineSets and, lastly, Bubble Sets. Thus, overall, KelpFusion was the most effective visualization technique."
"To visualize the sets, SetNet builds on an Euler diagram drawing technique [42], called iCircles. The iCircles method draws a single or multiple circles to visualize each set. Since iCircles is capable of visualizing any finite collection of sets, it enables the representation of complex overlapping groups whilst preserving the simplicity of the region shapes."
"An extension of iCircles to include graphs has already been given in [36], which we further extend in this paper.Firstly, SetNet draws the circles using iCircles, giving the sets primary spatial rights. See [42] for details of how the iCircles tool works. "
 The technique improves on [36] by adding a processing step. The aim is to adjust the layout of the Euler diagram to account for the inclusion of the nodes.
"To prevent the topological arrangement from changing, the system performs structure checking each time a circle is moved. This test establishes whether the moved circle still intersects with all and only the same circles as before, for instance. A more detailed description on how structure checking is performed is in [46]."
" Once an improved layout has been determined, the final processing step is a force-directed layout of nodes. This is a standard spring embedder [15], with linear attractive forces and the addition of a repulsive node-circle force that pushes nodes away from circle borders."
"We judge ‘most effective’ in terms of task performance measuring accuracy and time; the tasks we use are given in Section 5.3. Of these two performance measures we view accuracy as more important than completion time, consistent with other researchers, such as [3]."
"We derived eight data sets from Twitter ego-networks, obtained from the SNAP network data set collection [27]. This collection contains ego-networks of 1000 Twitter users. The nodes of each network are users connected to the ego user."
"Using the eight data sets, we generated visualizations using each of the five techniques; all of the diagrams are available from [1] as well as in the supplementary material."
"For the techniques that assign primary spatial rights to the network, the graphs were laid out using Gephi ForceAtlas 2 layout algorithm [6]; the same graph layout was used for these three techniques."
"Sets The colour assigned to sets was the same across visualizations, except for EulerView which has a colour selector over which we had no control. For the remaining techniques, a palette of colours was generated using a qualitative colour scale from colorbrewer2.org [20]."
" Moreover, we also wanted our choice of tasks to be practically relevant. With this in mind, we appealed to [47], which presents tasks that people need to perform when analyzing social networks."
"We adopted a crowdsourcing approach, using Amazon Mechanical Turk (MTurk) [32] to automatically out-source tasks. There is evidence that crowdsourcing is a valid approach for collecting data because this method has now gained recognition within the scientific community [21,32]."
"The tasks, called HITs (Human Intelligence Tasks), are completed by anonymous participants who are paid on completing the HIT. The HITs were based on the templates provided by Micallef etÂ al. [29]."
"Consistent with Meulemans etÂ al. [28], we only analyze the correct answers. Removing incorrect responses left 3738 observations (Bubble Sets: 742, EulerView: 805, KelpFusion: 777, LineSets: 649, and SetNet: 765). "
"It is interesting to recall that these previous studies were inconsistent with their findings. One found Bubble Sets outperformed LineSets [24] whereas another found that KelpFusion outperformed LineSets which, in turn, outperformed Bubble Sets [28]. "
"We deduce that, whilst visualization design should respect perceptual theories, the importance of graphical properties should not be underestimated.In the case of [25], the set visualization was simpler to the one we used as there were no group overlaps. "
"It would also be interesting to perform further empirical studies to explore the use of multiple unjoined circles, as with SetNet, as compared to the use of multiple joined convex shapes as in [35]."
" Beebee (1994) states that genre is primarily the “precondition for the creation and the reading of texts” and literary learning or academic research is secondary. Extending Beebee’s idea, Frow believes that interpretation is led by genre due to the constriction of the process of signs (semiotics) and the “production of meaning” (Frow, 2006, chap. 5). Lorch (1989) believes that the text provides signals (writing devices) which emphasize the text’s content and structure. In this and many other contexts, genre postulates the kinds of meaning that are suitable and pertinent in a specific situation or context."
"the perceiver uses sensory information, and builds or constructs this incomplete information to make sense of it (Braisby & Gellatly, 2005). Ecologists (Gibson and others) believe in an alternative and direct (bottom-up) framework for perception and Gibson, heavily influenced by Gestalt theories, not only challenged the stages but also introduced the notion of ‘affordance’ "
As Cole et al. (2010) found different tasks during reading comportment enabled the switching between skimming and reading behavior and are inherent indicators of the present task. Scanning and skimming are two separate processes that substantially benefit such areas of research as reading and searching. 
Toms & Campbell (1999a) contended that the ‘attributes’ of a document’s genre enable it to be specifically identified and showed that genre features play a significant role in recognizing documents.
"This paper led me to classical papers by Peter Whittle [9], Yu
Rozanov [7], JohnWoods [10] and Julian Besag [1], as well as the paper
by Besag and Moran [2] on parameter estimation in Gaussian Markov
random field models."
"A seismic
shift occurred in 1984 thanks to the seminal paper on simulated
annealing, stochastic relaxation and MAP restoration of images by
Geman and Geman [5] that appeared in PAMI. This paper demonstrated
that foundational methods from mathematical statistics can and will
make a significant impact on computer vision."
"A good example
of the impact of MRF-driven methods is the well cited paper on
the comparison of energy minimization methods for MRFs that
appeared in PAMI in 2008 [8]."
"InÂ [7,8] we have shown that approaches using Case Based Reasoning (CBR) and rules as knowledge management techniques succeed in autonomically enacting SLAs and governing important parts of Cloud computing infrastructures."
"Using rulesÂ [8] we managed to improve not only SLA adherence and resource allocation efficiency as discussed inÂ [7], but also attained an efficient use of reallocation actions and high scalability."
"Borgetto etÂ al.Â [21] tackle the trade-off between consolidating VMs on PMs and turning off PMs on the one hand, and attaining SLOs for CPU and memory on the other. "
" Therefore, a mechanism is required for the automatic adaptation between different templates without changing the templates themselves. A possible solution for this is the so called SLA mapping approach presented inÂ [33]. "
"Detailed information on the adaptation phase including the SLA mapping approach are found inÂ [33,34]."
This monitoring framework has proven to be highly scalable and is presented in more detail inÂ [3].
" In order to give the KB some knowledge about what to do in specific situations, several initial cases are stored in the KB as described inÂ [7] in more detail."
 A successful self-adaptation has been presented inÂ [42]
"More recently, a method focused on both big and class imbalanced data classification was proposed [29]. It is a cost-sensitive support vector machine using randomized dual coordinate descent method (CSVM-RDCD) and it belongs to the class of embedded methods, i.e. the examples selection is integrated in the learning algorithm and classifier dependent. This method was tested on three data sets with relative class imbalance and three data sets with severe class imbalance, of which only one of them with a large number of examples. "
"The PSS method can be used to preprocess very large training data with significant skew between classes. It is an undersampling method because it acts by reducing examples belonging only to the majority class. It is based on the computation of Tomek links [31], defined as a pair of nearest neighbors of opposite classes."
"Our methodology was combined with SVM, a powerful technique for data classification with many applications in literature. For details on the SVM algorithm we refer to [33], here we discuss its limitations on large and imbalanced training sets. In fact, despite its good theoretic foundations and high classification accuracy, it is not suitable for classification of huge data, because the training complexity of SVM is highly dependent on the size of data set."
"The three real data sets have been extracted from the Forest Cover Type data set of the UCI repository [5] having 7 classes and 581,012 samples. This is for the prediction of forest cover type based on 54 cartographic variables. "
" To be an effective practical addition to the design process, computational simulation time must be shortened and reduced order modelling has been shown to be an effective method for attaining this goal [2,3]"
The fundamental aim of reduced order modelling is to reduce the number of degrees of freedom (DoF) necessary to produce the required information to an acceptable level of accuracy [4]. The vast majority of reduced order models (ROMs) achieve this result by projecting the governing equations onto a series of spatial modes [5].
 The vast majority of reduced order models (ROMs) achieve this result by projecting the governing equations onto a series of spatial modes [5].
" In equation driven methods, such as nonlinear normal modes [6], the modes are constructed by applying mathematical reduction techniques directly to the equations themselves. Data driven methods, such as proper orthogonal decomposition (POD), use sample stored calculated solutions or experimental data to generate the spatial modes [1]. It is the data driven approach that is adopted here."
"To overcome the issue of interpolation cost, we employ radial basis function (RBF) interpolation [21], where the interpolation expense does not increase significantly with the number of dimensions."
"RBF interpolation can be readily implemented, as it does not require a triangulation to define the connectivity between the data points [22]. "
" It has been applied to many fields, including the solution of partial differential equations [23]"
"Stabilisation and discontinuity capturing is achieved by the explicit addition of artificial viscosity. A fully implicit three level second order method is adopted for the time discretisation. At each time step, the implicit equation system is solved by explicit iteration, with multigrid acceleration [24,25]. "
"For regions with discontinuities a first-order harmonic term is added, where a pressure switch is used to ensure the term is only significant in regions of strong pressure gradients [26]. "
"The JST scheme [27] is applied for stabilization, more details on the solver can be found in [26]. "
" An initial mesh with 44573 nodes was generated and, to allow for the complete movement of the wing, a further 31 meshes were obtained from this mesh by mesh movement. This was accomplished using the Delaunay graph concept [28], resulting in the same nodal connectivities on each mesh."
 This was accomplished using the MATLAB [29] function interp1
"This family of techniques is discussed elsewhere [30,31] where it is stated that the applicability of POD interpolation techniques to unsteady problems is unclear. The results presented here show that they are clearly applicable."
"The same finding was discussed by Bouhoubeiny and Druault [32], who applied a similar technique to interpolate experimental data in time. This proof can be generalised to higher numbers of dimensions and other interpolation techniques. It equally applies to the unsteady version of the technique presented here and to the steady versions presented and discussed elsewhere. "
When compared with 31 competing interpolation techniques on a range of functions it was found that RBFs performed best in terms of accuracy for a variety of functions [33]. Interpolation techniques based on triangulations have the disadvantages of a large auxiliary storage required for the triangulation [33]. 
" It is difficult to find comparisons of this nature, however in an early POD/ROM paper Pettit and Beran [34] speculate that using a ROM should be better than interpolation, but stated that this needed to be tested."
Degroote et al. [30] provide a comparison between Kriging interpolation of the solution field and ROMs projected onto basis functions interpolated with respect to the parameter space. They found that direct interpolation of the solution field gives errors of similar order to that achieved by the projection ROMs.
"Results were also recently reported by Wang et al. [31] who compared POD interpolation techniques to POD projection techniques. In the first example presented, with a four dimensional parameter space, they found interpolation errors which were orders of magnitude lower than projection techniques."
" Part of this problem has been recently addressed by Sivaraman et al. (2016) with their programmable packet scheduling techniques in switches and by Mittal et al. (2016) introduction of packet scheduling algorithms that roughly meet the requirements of a universal packet sched-uler. These approaches can affect the order and timing of packet departures from a queue in a switch or NF, however we sug-gest a promising alternative direction that is inline with Ama-zon’s attempts to integrate custom schedulers in their cloud ser-vices (Amazon, 2016)."
"We use (Perf, 2016) to access the performance counters of the Linux kernel. The SCC Profiler passes the process IDs (PIDs) of the NFV service chain to Perf asking for a variety of events (labeled as “Perf+Linux Kernel” in Fig. 2).3"
"Moreover, our processor takes advantage of Intel’s Direct Data I/O (Intel, 2016a) (DDIO) technology, allowing the Ethernet con-troller to use a portion of the processor’s last-level cache (LLC) as a primary source and destination of data rather than the main mem-ory, thus achieving lower latency. This portion can be up to 10% of the LLC’s capacity, which in our system6 results in 2 MB (Intel, 2016a). Details about how DDIO works in our testbed are given in Section 6.2.1.2 in Katsikas (2016b)"
"The complete list of available events is available at Intel (2016b) and a detailed explanation of how these counters are col-lected and combined is provided in the Appendix A.1 in Katsikas (2016b). Specifically, during the execution of an NFV service chain we acquire CPU core, L1, and DRAM events using Perf, while L2 and L3 events are fetched using Intel’s PCM tool. "
"As for the ability of the SCC Profiler to time the functions of the NFV stack, we exploit Intel’s high-precision event timers (HPET) (Intel, 2016c) via Perf to acquire the entire list of functions together with their contribution to the total latency."
"We implemented an NF using the fastest Click (Kohler et al., 2000) variant to date, called FastClick (Barbette et al., 2015). We focus on a basic NF, a router, using a slightly modified version of the router implemented by Kohler et al. in Kohler et al. (2000)."
" Finally, although we target NFV applications that use the native Linux driver for I/O, we also deployed the same router using FastClick’s DPDK (DPDK, 2016) I/O elements (using the DPDK network driver) to examine the highest achievable performance. Fig. 4 shows the latency of the router in these four different environments, using a single CPU core. "
"The results shown in Table 4 were interpreted in detail in Sec-tion 6.3.1.1 of the licentiate thesis by Katsikas (2016b). This anal-ysis showed how the four different I/O approaches of the same software router interact with the underlying hardware (via the re-spective network driver) and which of these approaches exhibits performance bottlenecks. To summarize the study made in Katsikas (2016b), we quan-tified the performance difference between a state of the art NFV router using the DPDK network driver and the kernel-based router"
"The main source of data about a system’s performance can be collected by tools that reveal the hardware’s performance. For example, lmbench (McVoy and Staelin, 1996) and Intel’s memory latency checker (MLC) (Intel, 2013) quantify the la-tency when transferring data of variable size across different hard-ware components (i.e., registers, caches, main memory). "
"Modern code profilers, such as OProfile (Levon, 2016) and Perf (Perf, 2016), access low-level performance counters at run-time, providing statistics about applications or the entire OS. Such tools can draw a developer’s attention to those functions that exhibit high utilization of system resources, hence these func-tions offer the greatest potential for acceleration. "
"Moreover, likwid (likwid, 2016) pro-vides a broader performance monitoring suite. One can either wrap an entire application to measure its performance with respect to key hardware counters, or enclose a particular piece of code within an application between likwid start and stop functions."
"DProf (Pesterev et al., 2010) helps programmers understand cache miss costs by associating these misses with the data types instead of the code. DProf provides clear insights into which ob-jects of an application’s data structures incur expensive cache loads; however, DProf mostly focuses on the LLCs and in partic-ular how data moves in and out of LLCs. This focus results from the tool being designed to optimize cross-CPU data exchanges. "
"Sivaraman et al. (2016) envisioned future switches with programmable boards that allow network administrators to deploy custom packet scheduling schemes. They introduced a Push-In First-Out abstraction model that controls the order and de-parture of packets, capturing the needs of several packet schedul-ing schemes. Mittal et al. (2016) explored the possibility of design-ing a universal packet scheduler that can match the results of any scheduling algorithm, concluding that the Least Slack Time First al-gorithm is the one that best approximates the universal scheduler."
"Netmap (Rizzo, 2012), DPDK (DPDK, 2016), PFQ (Bonelli et al., 2012), and PF_RING (Deri, 2011) are network I/O mechanisms that boost NFV performance by providing direct ac-cess to the ring buffers of a NIC, using custom network drivers. The time required for these tools to be widely adopted in the market motivated a solution that could be immediately adopted by cloud providers."
"We found that the vectorized I/O technique (Bhattacharya et al., 2003) introduced in version 2.5 of the Linux kernel permits read-ing/writing frames from/to multiple buffers using a single trans-action. This technique is supported by the standard ixgbe driver and can be exploited by activating the scatter/gather feature of the NIC. Our implementation is released as an open source project (Katsikas, 2016a) on top of FastClick."
