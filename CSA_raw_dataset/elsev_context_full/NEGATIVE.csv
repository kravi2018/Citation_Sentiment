"Information on WOM content (i.e., message) has generally been unavailable to companies in the past because interpersonal communication such as a chat between friends leaves no record for analysis [20]. As a result, researchers turn to senders and ask whether they really are opinion leaders by a questionnaire survey [50]. "
"The network structure approach also requires knowledge on consumers’ social networks that are often private information [5,43]. Moreover, even when companies acquire information about consumers’ social networks, consumers’ influence over strangers outside their social networks in an online setting is difficult to determine. Therefore, the network structure approach is not suitable for identifying eWOM opinion leadership."
"The self-report method seems to be most popular due to existing scales such as King and Summers’ [50], although the key informant method has also been used in a recent study [59]. In addition, consumer demographics [1] and loyalty [35] are considered in conjunction with surveys to identify opinion leaders. The main findings of the extant literature are that self-reported and peer-nominated opinion leaders influence the choices of their followers. However, self-reported surveys may capture self-confidence rather than opinion leadership [2,42]."
"While individuals can arguably expand their social network to include the strangers, Dunbar’s number (150) suggests a cognitive limitation in the number of social relationships that people can maintain [30]. Existing evidence suggests that the Internet does not remove the cognitive/biological constraints on human communication [36]."
"Consumers do not know all opinion leaders, as consumers only know a limited number of peers [30], and companies cannot directly compare different opinion leaders reported in either the self-report or key informant approaches. However, our approach allows us to identify all opinion leaders among a large number of consumers and compare their relative strengths in opinion leadership. "
"Identifying opinion leaders from observed behaviors such as WOM is the most expensive method, although highly accurate [75]. Fortunately, online user reviews are now available to companies and can serve as a proxy for overall WOM [79]. This approach is consistent with recent research findings that link online consumer behavior with product sales [51]."
"Although some researchers treat all reviewers as opinion leaders [25], we are interested in examining a much smaller set of reviewers because it is costly for a company to recruit all available reviewers [75]. The theoretical basis for considering a subset of reviewers is that opinion leadership is not a dichotomy; rather, it varies in a continuous fashion [22,67]. "
"The extant literature seems to imply that marketers only need to focus on two product effects [21,23]. However, our results, based on eWOM from opinion leaders, suggest the importance of improving all three product effects at the same time. "
Many executives have little idea of how to orchestrate a marketing campaign that exploits the full power of opinion leader eWOM [31]. Part of the challenge is the lack of a proper approach to identify eWOM opinion leaders
Our paper fills the gap by studying opinion leader and eWOM together as the original interpersonal communication theory intends [47].
"By using this new approach, we identify communicative, buzz-generating, and trustworthy opinion leaders and find their eWOM positively associated with product sales, contrary to a prior study that has raised doubts about the influence of opinion leaders [74]. Furthermore"
"Our method is both more accurate than traditional survey methods [35,59] in measuring opinion leadership, and more comprehensive than network structure methods [41,43]. "
" However, unlike the study carried out by Di Marco etÂ al., authors inÂ  [22] did not report results referred to the computational burden required by the analytical model resolution, especially, in the case in which the number of nodes is big. "
"Furthermore, the procedure proposed by Samaras etÂ al.Â inÂ  [22] is restricted to multi-tier single-hop hierarchical networks (i.e., cluster-tree topology), while the work of Di Marco etÂ al.Â  [21] can be extended to other topologies as the mesh one."
" In addition, the Request to Send/Clear to Send (RTS/CTS) mechanism, commonly used in IEEE 802.11-based networksÂ  [27], is also employed right before exchanging data/acknowledgment (ack) messages. Queen-MAC is evaluated by means of computer simulation, showing a good performance in terms of network lifetime and message delivery ratio. Nevertheless, Queen-MAC exhibits some significant shortcomings. One of its main disadvantages lies in the use of the RTS/CTS mechanism."
"Throughput per linkTo calculate the throughput under HT conditions, we take inspiration from the work of Cano etÂ al.Â  [23], where the HT problem in a WSN is undertaken. Unlike the work of Cano etÂ al., we include diverse design premises of the IEEE 802.15.5 standard in the throughput characterization, which provide a more realistic study of the WMSN requirements."
" To this end, the decision makers, through a preliminary study, estimate the expected result for that goal.The idea behind distinguishing GP from other more conventional methods of optimization (i.e., single-objective, sequential-objectives or other MO techniquesÂ  [42-45]) is to introduce flexibility into the objective functions (as opposed to the rigid constraints of the conventional techniques). "
"These kinds of journeys are included in our study. However, we excluded cases where an ini-tially small agile organization grew organically (Maranzato et al., 2012), and discussions focusing on processes or tools without describing organizational change (Lyon and Evans, 2008)."
"To complicate matters, some papers talked about “the team” in singular when referring to the organization (Hodgkins and Hohmann, 2007), making it nontrivial to judge whether the orga-nization met the large scale criteria based on the choice of words of the author."
"Further examples on exclusion by the large scale facet were cases with large organizations but only a single team adopt-ing agile (Fulgham et al., 2011). We considered cases of single teams (although as part of a larger organization) unrelated to this research as our focus is on transformation of the entire (development) organization."
"Also piloting cases that reported only single teams using agile, even though considering the whole organization would meet the large scale criteria, were excluded, e.g. (Scott et al., 2008). Finally, cases where the organization was growing to large scale, but did not meet the size criteria at the start of the transformation, were excluded."
"Instead of using the most complete paper as suggested in the guidelines for systematic literature reviews (Kitchenham, 2007), we combined the results presented in each paper and considered the case as a single unit in our analysis. Keeping in mind the potential bias caused by duplicate publications, we think that including all papers enabled us to get a more in-depth understanding of the individual cases."
"There does not exist similar literature studies on large-scale ag-ile as ours, nor does there exist any surveys specifically on large-scale agile. The studies and surveys that do exist have studied agile in general, not specifically as large-scale nor agile transformations, e.g. Chow and Cao (2008) studied success factors in agile software projects in general. The only studies that touch the topic of this paper are not scientific but done by agile consulting or tool com-panies, e.g. Version One’s State of Agile surveys (VersionOne, Inc, 2016) or Forrester’s surveys (For, 2012; Giudice et al., 2014)."
"This self-consistent estimate converges on the true distribution at a faster rate than traditional binning or kernel density estimation methods (Bernacchia and Pigolotti, 2011).However, during our initial attempts to apply the PDF estimation method of Bernacchia and Pigolotti (2011) (hereafter BP11), we discovered that its computational performance is not practicable. "
"Though this manuscript focuses specifically on the case of using the nonuniform FFT to improve the ECF calculation stage of the Bernacchia and Pigolotti (2011) estimation method, this method should be applicable to other ECF-based methods. We posit that the nonuniform FFT would especially reduce the computational cost of multidimensional ECF calculations: potentially by a factor of íª(100d) for d-dimensional data.If the BP11 method can be extended to multidimensional data, then a nonuniform FFT method could be used to dramatically decrease the computational time of the method. "
"To address this issue, a more concise view of model differences is required that aggregates the atomic operations into composite operation applications such that the intent of the change is becoming explicit. Existing solutions (Hartung et al., 2010; KÃ¼ster et al., 2008; Xing and Stroulia, 2006) only provide language-specific operation detection algorithms. However, due to the plethora of existing modeling languages, this is an unfavorable solution."
"First, there is the approach by Xing and Stroulia (2006) for detecting refactorings in evolving software models which is integrated in UMLDiff. Refactorings are expressed by change pattern queries used to search a diff model obtained by a state-based model comparison. Although, the general goal of UMLDiff is comparable to ours, there are several major differences. First, UMLDiff is tailored to a fixed modeling language, namely a subset of structural UML diagrams while our approach is applicable for any modeling language."
"Second, the approach by Vermolen et al. (2011) copes with the detection of complex evolution steps between different versions of a metamodel. They use a diff model comprising primitive operations as input and calculate on these basis complex operations. The approach is tailored to the core of current metamodeling languages, but follows a similar methodology as UMLDiff. However, a specific feature is the detection of masked operations, i.e., operations hidden by other operations, by defining additional detection rules. Nevertheless, the approach is again dedicated to one single modeling language and does not allow to reuse the operation specifications used for execution for the detection process."
"Third, KÃuster et al. (2008) calculate hierarchical change logs for process models. The authors apply the concept of Single-EntryâSingle-Exit fragments to calculate the hierarchical change logs after computing the correspondences between two process models. Thereby, several atomic changes are hidden behind one compound change. The difference to our work is twofold. First, we consider the detection of composite operations comprising changes cross-cutting the whole model, i.e., we are not restricted to reason only about one hierarchy branch, and second, our approach is language independent, thus we are not restricted to process models.Ontology engineering. "
"Buckley et al. (2005) introduced four aspects of software changes as the basis for software evolution: temporal properties (when), object of change (where does the change occur), system properties (what), and change support (how). The authors neglect the stake-holders (who) and the reason for change (why), which is essential to address the “nature of [the] evolution phenomenon, its drivers and impact” (Lehman and Ramil, 2002). "
"Managing (evolving) functional requirements is a well-studied topic in the literature (Ramesh and Jarke, 2001). In contrast, managing non-functional requirements – especially dependability requirements – for aPS is rarely addressed (Fay et al., 2015; Ladiges et al., 2013). One reason is that the relationship between evolution and dependability of a system is vaguely understood until now because both are very complex challenges (Felici, 2003; Machado et al., 2006; Vogel-Heuser et al., 2014c)."
" The restructuring of a production unit versus a continuous improvement process of these units is another topic in this field (Schuh et. al 2013). Furthermore, continuous integration of the software and the hardware of an aPS is more complicated than in a pure software setting as the changes in hardware are much slower to realize than in software as well as that simulation models of the hardware system are required for automated integration tests."
"A variety of norms, guidelines, recommendations and approaches is available for performing a structured and systematic requirements engineering. However, in industrial practice these approaches are not strictly followed. This was, for example, shown in a survey regarding requirements engineering practice for software projects in industry by Neill and Laplante (2003). Morris et al. (1998), identified the following reasons for missing requirements engineering practices in research and development projects in industry"
"Choices are often made as a result of project team discussions and not by comparing design possibilities with requirements specifications to find the best solution. Decisions are made based on employer experience as well as reusable existing solutions and done intuitively. Reasons named by Bellgran and Säfsten (2010) are mainly high time-pressure and low priority despite of the assumption that a well performed requirements engineering helps reducing time for fixing design and implementation errors. The evolutionary changes usually have to be carried out under even tighter schedules and higher time pressure, because production standstill must be minimized, thus resulting in even worse boundary conditions for a systematic requirements engineering."
"For many aPS in operation, a formal specification has never been provided (Frey and Litz, 2000). This often includes that no assessment of the quality of changes is done because of lack of time, or cost constraints, or missing measurements (Bellgran and Säfsten, 2010). Consequently, a lack of explicit knowledge about the process behavior and its actual occurring properties unfolds."
"While engineering approaches exist in the different disciplines to address these problems (e.g., architectural description languages in software engineering Medvidovic and Taylor (2000), the problems become more difficult to identify and solve if a change in one discipline's parts results in problems in another discipline."
"A formal semantics for automatic verification of structural compatibility has been proposed (Feldmann et al., 2014a), but verifying functional conformance is not considered yet."
"With respect to the repair of inconsistencies, approaches have been proposed to use OCL constraints and the part of the constraint, which has not been satisfied, to create an appropriate repair action (Nentwich et al., 2003)."
"Most approaches addressing safety employ this kind of additional annotations (Giese and Tichy, 2006; Grunske et al., 2005; Papadopoulos et al., 2001). Tribastone and Gilmore (2008) and Becker et al. (2009) propose similar approaches addressing performance.Those approaches are restricted to be used during the design of the system as they only use assumptions of the behavior of the system with respect to its characteristics affecting probabilistic quality attributes, e.g., failure rates, performance of certain parts."
"Despite efforts toward including object-oriented programming aspects within IEC 61131-3 (IEC, 2013), the standard in its current version has not yet been fully established in the industry. Thus, we focus on IEC 61131-3 without object-oriented extension (IEC, 2009), which is mostly used within state of the art industrial applications (Thramboulidis and Frey, 2011)."
" The evolution of aPS especially during operation as discussed in scenario category “V” (Table 1) is performed on code level by suppliers' start-up personnel or customers' operation staff with limited software education, but huge knowledge about the process. This may lead to inconsistencies between implementation and design artifacts as well as to unclear code structure (Katzke et al. 2004; Vogel-Heuser et al. 2014a)"
" In Kormann et al. (2012), the semantics of sequence diagrams are adapted in order to make direct IEC 61131-3 code generation possible. In this way, the modeled test scenarios can be executed directly. In recent years, SysML is increasingly established for supporting the development process of real-time systems by Detommasi et al. (2013). However, investigations on the possibilities to derive test cases from these models or adapting these models are still missing."
"Lochau et al. (2014) propose model-based testing for variant-rich aPS. Based on a 150% UML state chart test model incorporating all variant-specific test models with explicit specification of differences by means of feature annotations, test case generation for the complete system family is applicable in order to test the corresponding PLC control software. In contrast to test case generation for each variant in isolation, their approach exploits the specification of commonality and variability between variants to reuse generated test cases also for other variants. "
"Static code analysis is successfully applied for several programming languages and environments, e.g. Lint for C (Johnson, 1988) and FindBugs for Java (Ayewah et al., 2008). However, static code analysis for IEC 61131-3 is not yet supported sufficiently (Angerer et al., 2013)."
"Finally, the MechatronicUML provides specifically support for systems which self-adapt their behavior at runtime by modeling adaptations by architectural reconfiguration based on graph transformations (Eckardt et al., 2013; Tichy et al., 2008). Verification of the adaptation behavior is based on graph transformation verification approaches (Becker et al., 2006; Ghamaraian et al., 2012). However, the approach does not address the specifics of aPS, e.g., the employed languages."
"Verification of PLC programs – written mainly in the programming languages Sequential Function Charts (Bauer et al., 2004) and Instruction List (Huuck, 2005) – was investigated by means of the model checker UPAAL, but lack in analyzing industrial PLC programs due to size and structure. "
"Arcade supporting model checking to of PLC programs – written for the programming languages ST, IL as well as vendor-specific programming language and their combinations as often applied in IEC 61131 environments – is presented in Biallas et al. (2012). It provides a counterexample-guided abstraction refinement mechanism, which is already applied successfully for verification of embedded systems in Stursberg et al. (2004) and hierarchical predicate abstraction (Biallas et al., 2013) to cope with the challenge of state explosion. Nevertheless, the approach was not applied to industrial PLC software to identify its applicability in practice."
 Greifeneder and Frey (2007) proposed the modeling language called DesLaNAS which can be applied in combination with a probabilistic model checking but an automatic translation of the description model into the form needed for model checking is not available. 
" Variant annotations, e.g., using stereotypes in UML models (Gomaa, 2006) or presence conditions (Czarnecki and Antkiewicz, 2005), define which parts of the model have to be removed to derive a concrete product variant. Annotative variability models become easily very complex and unmanageable for large SPLs with many variants."
" Thüm et al. (2009) present an algorithm computing the differences between two feature models after changes to a feature model. In general, the inputs are the original and the evolved feature models. The algorithm computes deleted or added product variants and scales up to large feature models with thousands of features. However, it only focuses on a homogeneous feature model and does not cover multidisciplinary aspects and features of different granularity."
"Tool support is fully available and is already tested with an Eclipse as a large product line example (Pleuss et al., 2012). But, again, multidisciplinary feature modeling is not considered."
" Wille et al. (2013) apply this idea to block-based diagrams, e.g., as available in Matlab/Simulink. Holthusen et al. (2014)apply it in a prototypical manner to the IEC 61131-3- language FBD. However, an extension to the full language scope of IEC61131 is still missing."
"This cycle of generating code from models and propagating changes on the source code back to the models is known as round-trip engineering (Hettel et al., 2008) in contrast to the one-way forward engineering of source code from models and the one-way reverse engineering from models from source code. A particular issue, which complicates this challenge, is that manual changes in source code often do not have a corresponding equivalent in the model, i.e., the modeling language is not able to express the changed behavior."
"Sim and Vogel-Heuser (2010) proved the benefit of active learning comparing mechatronic engineering students with students of computer science, but still an appropriate education for MDE with a focus on aPS is missing."
"The modeling approaches used in these and other works (Bassi et al., 2011; Hackenberg et al. 2014; Bonfè et al., 2013) enable an integration of software models and physical models into a single consistent syntax. In contrast to these integrated approaches, there are many research works that combine control code (or an executable model thereof) with an object-oriented simulation model of the physical parts of the aPS. Such approaches require to include a separate simulation tool, which is possible for the designer but not for the technician on-site, and are therefore not considered here in more detail."
"Aside from works addressing IEC 61131-3 implementations, event-driven implementations conforming to the IEC 61499 standard (Bianchi et al., 2003; Chhabra and Emami, 2011; Hirsch, 2010; Hirsch et al., 2008; Vyatkin et al., 2009) were proposed. A systems engineering framework based on SysML and IEC 61499 is considered in Hirsch (2010) and Hirsch et al. (2008). However, with the approach of Hirsch (2010) and Hirsch et al. (2008), debugging of automation software directly inside the SysML model is not provided."
"Another approach (Angyal et al., 2008) proposes to use the Abstract Syntax Tree of the generated source code as an intermediate model, which represents the source code as a model, and uses bi-directional incremental model merges to keep the abstract syntax tree consistent with the changed code. However, the approach does not address the challenge to ensure that the abstract syntax tree is consistent with the model. Furthermore, it does not allow the use of template-based code generation."
"Several recent efforts to invert large sparse matrices using a series of computational tricks show promise, though they are still extremely computationally expensive [4,12]"
"it would lead to additional costs and set-up burden, as it would require a Spark cluster [29]"
"“Historians of science are accustomed to call these two traditions in science Cartesian and Baconian, since Descartes was the great unifier and Bacon the great diversifier at the birth of modern science in the seventeenth century.”[1, p. 40].However it is doubtful that Dyson believes these are exact synonyms for his terminology."
" In an adult brain, white and grey matter, despite of their anisotropic (WM) and isotropic (GM) properties, are characterized by similar ADC values [26,27] and cannot be efficiently distinguished."
"In this context, various solutions have been proposed to identify possible malicious apps and behaviours (Aafer etÂ al., 2013; Arp etÂ al., 2014; Google, 2012; Wu etÂ al., 2012). However, most of these mechanisms are based on analysis of permissions granted to apps, i.e., the set of API methods they are allowed to invoke. This type of analysis is not sufficient, because it can result in an over-approximation that rates legitimate apps as malicious (false positives) and it is not able to detect collusion attacks66Attacks that allow apps to indirectly execute operations for which they do not have specific permission."
"In contrast to the refinement rules proposed in Neisse and Doerr (2013), our extension considers also the modification of events in addition to only allowing or denying the execution of activities."
"Approaches like Damopoulos etÂ al. (2014) and Grace etÂ al. (2012) that focus specifically on malware detection is out of the scope of this paper. We present these solutions in chronological order, briefly analysing the functionality proposed, the implementation details, limitations, and contrast to our proposal."
"Kirin, proposed by Enck etÂ al. (2009), is a security service running on the phone which analyses the requested permissions of an app and detects potential security flaws. When an app is about to be installed, Kirin evaluates, using a rule based engine, if there is a match between the set of requested permissions and the signatures defined in the rule engine. Note that the signatures defined in the rule engine represent possible attack vectors, for example, RECORD_AUDIO and INTERNET permissions define a rule in the signature set. Kirin results in a high number of false positives since legitimate apps that follow the defined signature pattern are characterised as malicious. In contrast to our proposal, Kirin does not provide enforcement capabilities, it is only a solution to inform users about possible risks."
"Ongtang et al. (2009) propose Saint as an extension of Kirin. In addition to the analysis performed by Kirin at install time, Saint monitors apps also at the runtime Inter-Component Communication (ICC) flows, e.g., activities initialisation, components binding to services, access to content providers, etc. The policies defined in Saint are static and similarly to our approach define conditions to control the runtime behaviour. For example, when a specific activity can bind with a specific content provider considering the allowed permissions, signature, or package name. Saint is implemented as a modified Android middleware, while our proposal relies on app instrumentation for policy enforcement. Furthermore, we propose a more flexible architecture and expressive policy language with the possibility of deploying and changing policies at runtime without requiring changes to the middleware or instrumented app."
"Orthogonally to our proposal, Dietz etÂ al., 2011) propose QUIRE as a solution to protect android apps manipulation by other malicious apps or services. Their proposal is to enable apps to reason about access to sensitive data through call chain validation. To achieve this goal the authors propose the modification of the underlying OS IPCs mechanism in order to pass the appropriate information between IPCs.In the same direction, Porscha, proposed by Ongtang etÂ al. (2010), introduces a Digital Rights Management (DRM) framework for Android phones that mediates the access to protected content between different Android components. For example, it can regulate the access of an app to the content of an SMS message. The Porscha mediator supports constraints on devices, apps, and on the use (e.g., cardinality) of the protected data. Porscha mediates ICC flows, with extensions including a policy filed, and it has been implemented as a modified Android firmware that is considered to be trusted. Our proposal does not require changes to the Android firmware, therefore, our solution could be adopted straightforwardly in all different firmware versions."
"Bai etÂ al. (2010) propose a context-aware usage control that focus on a user basis mechanism for granting and revoking permissions, similar to the approach introduced in the latest android OS version. To do so, authors enable users to define policies related to application permission grants in order to protect access to users' sensitive resources. However, the use of this approach requires the modification of underlying Android OS services. In a similar direction, Sun etÂ al. (2012) introduce a design that requires the modification of Android sandbox as well, in order to monitor access to sensitive information. In this approach the hook points are installed before the actual permission check occurs by Android OS. On the contrary, SecureDroid (Arena etÂ al., 2013) extends Android OS security manager service to control access also for user defined sensitive URIs."
"Feth and Pretschner (2012) employ information flow tracking as well. Their framework uses an expressive policy language to describe users' preferences to content providers, intents, and certain data sinks like the network, file system and IPC in order to eliminate access to private data. Jung etÂ al. (2013) extend the work of Feth and Pretschner with context-aware policy rules. In this direction, Andriatsimandefitra etÂ al. (2012) introduce an approach for determining data flows, which is an important aspect for realising a policy enforcement tool. In contrast to all these approaches for policy enforcement in Android we are the only ones to propose the use of policy refinement techniques to simplify the management of the security policies by end-users."
"Constroid, introduced by Schreckling etÂ al. (2012), also defines a management framework for employing data-centric security policies of fine granularity. To do this, Constroid adopts the UCONABC model. However, in contrast to our contribution in this paper only the abstract model is detailed and no concrete example of policy is provided."
"Zefferer and Teufl (2013) propose a solution for device security assessment based on user defined preferences. The use of security policies guarantees that each application that integrates the developed service can define and assess its own critical aspects. In order for such a service to be employed in a given device is required by a third party app to integrate the appropriate controls to the app through the corresponding API. However, researchers have shown that programmers are not taking into consideration in most of the cases security features."
"AppGuard, introduced by Backes etÂ al. (2013), is an app instrumentation framework that runs directly in users' device and allows user-centric security policies customisations. AppGuard computes a risk score for each app considering the number of dangerous permissions and provides the option of instrumenting the app to control the access to âdangerousâ calls. For example, in an app with NETWORK permission, a user can choose to enable/disable the corresponding functionality. The solution presented by Bartel etÂ al. (2012) follows the same direction. In contrast to our proposal these solutions do not support context-based policy specification and policy refinement in order to simplify the policy management by end-users."
"DroidForce proposed by Rasthofer etÂ al. (2014) relies on the Soot framework for analysing and instrumenting an app to enforce a security policy. This approach considers PEPs injected in multiple applications with a single PDP running as an app in the phone, addresses information flow intra-app statically and inter-app at runtime, and uses an expressive policy language with cardinality and temporal constraints. Their policies allow or deny an activity, while do not support modification/obfuscation of values. Complementary, Jing etÂ al. (2015) propose DeepDroid, which in contrast to DroidForce performs instrumentation at the native level with the possibility of intercepting system calls in addition to methods invocation to regulate the access to sensitive resources. However, DeepDroid does not consider information flow tracking nor uses any expressive policy language for enforcement."
"More similar to our approach Cotterell etÂ al. (2015) introduce a solution to enable users to install policies for controlling sensitive data; however, the policies are not based on access to sensitive resources. Instead, the authors focus on known malicious activities for defining a policy with a more explicit focus on malware apps."
"Empirical studies showed that, by integrating the advantages of different EAs into one framework, PAP not only provides practitioners a unified approach for solving his/her problem set, but also may lead to better performance than a single EA [17]."
"1. In addition, some benchmarking studies have been undertaken to empirically compare the performance of these various techniques (e.g., Baesens et al., 2003), but they did not focus specifically on how these techniques compare on heavily imbalanced samples, or to what extent any such comparison is affected by the issue of class imbalance. For example, in Baesens et al. (2003) seventeen techniques including both well-known techniques such as logistic regression and discriminant analysis and more advanced techniques such as least square support vector machines were compared on eight real-life credit scoring data sets."
"Our proposed method has some particularities that distinguish it from previous textural analyses in the literature. First, unlike the method in [11] that focuses on particular affine regions, our method considers all pixels to have the same importance. "
"In all cases, the connectivity descriptors outperformed all the other approaches by at least 5% (UIUC), 2% (Outex), and 13% (KTH-TIPS2b) in relative percentages. In KTH-TIPS2b the classification performance was also better than that reported in [16] (76%) using a similar database and protocol."
" It has been reported that these PSO variants have a more diverse search than does the standard PSO [1,3,10,17]"
"Most models derive a chaotic system by transforming the original constraint problem into an unconstrained one with a diffeomorphic function, such as a sigmoid function, and then applying the steepest descent method with a large step-size to the unconstrained problem, for which it is well known that the derived system is chaotic if the step-size is sufficiently large [6]."
"As a countermeasure, a large inertia weight wd of(C1) was selected in [17]; this can reduce the amount of change of the critical value β that is caused by the variation in r.However, this is not an essential solution because the change remains."
"Since in [17] it was reported that the diversity of the search was more significant during the early stages in the numerical experiments for CPSO-VQO, in this model, the initial value of σ(t) is set to be large and is decreased exponentially"
"“What does the user want to see?” and “What do the data want to be?” as well as how these two points mutually enhance one another. In the problem domain of cyber security, Fink et al. (2009) developed a set of visualization design principles with a focus on high-resolution displays and presented proto-types according to these design principles. Goodall et al. (2004) conducted contextual interviews to gain a better understand-ing of the intrusion detection workflow and proposed a three-phased model in which tasks could be decoupled by necessary know-how to provide more flexibility for organizations in train-ing new analysts. However, none of these user-centered studies tackled behavior-based malware pattern analysis."
"On the other hand, there are systems for “Malware Classifica-tion” which support the comparison of many samples to identify the common behavior (e.g., Gove et al., 2014; Han et al., 2014; Long et al., 2014). For example, Wüchner et al. (2014) pro-posed the interactive “DAVAST” system for the detection and analysis of email worm attacks. Additionally, Gove et al. (2014) introduced “SEEM”, which allows the behavioral comparison of a large set of malware samples in relation to the imported DLLs and callback domains. However, none of these approaches covered the full set of requirements as specified by Wagner et al. (2014) during their problem characterization and abstraction. "
"when sequencing molecules, the position where a circular sequence starts can be totally arbitrary. For instance, the linearised human (NC 001807) and chimpanzee (NC 001643) mito- chondrial DNA (mtDNA) sequences do not start in the same region [13] ."
They propose two algorithms. The first one modifies the branch and bound algorithm of Barrachina and Marzal [2] by avoiding exploring ranges known to be lower than the lower bound in the branch and bound computation. The second one modifies the BBA algorithm by preventing searching for distances when it is known that the final result will not improve the current external bound.
"Adaptive systems, however, have one main disadvantage: new unlabeled data has to be robustly included into an already built model. Typical approaches are self-training (e.g., Rosenberg et al., 2005; Li et al., 2007), co-training (e.g., Blum and Mitchell, 1998; Levin et al., 2003), semi-supervised learning (e.g., Goldberg et al., 2008), or the application of oracles1 (e.g., Nair and Clark, 2004; Wu and Nevatia, 2005)."
"Having a large stack assures that the assumption for the negative bag containing at least one negative sample is mostly valid, since the probability that an object stays at one specific location over a longer period of time is very low (Sternig et al., 2010a)."
"This causes short-term drifting in existing classifier grid approaches (e.g., Roth et al., 2009), which is in particular the problem addressed within this paper. From all experiments the benefits of the proposed methods are clearly visible."
"Creating temporary solutions to the code base increases complexity, which makes further development hard and time-consuming (Yli-Huumo et al., 2015a; Yli-Huumo et al., 2014)"
"The metaphor technical debt (TD) has been introduced by Ward Cunningham (Cunningham, 1992). He describes the metaphor as ‘Shipping first time code is like going into debt. A little debt speeds development so long as it is paid back promptly with a rewrite. Objects make the cost of this transaction tolerable. The danger occurs when the debt is not repaid. Every minute spent on not-quite-right code counts as interest on that debt.” (op.cit., p. 29-30). Even though the metaphor was first introduced over twenty years ago, a recent mapping study shows that it has received the attention of researchers and practitioners only in the past few years (Li et al., 2015a)."
"TD is often seen only as a negative concept in software development (Lim et al., 2012;Yli-Huumo et al., 2014). Software developers think that creating shortcuts and non-scalable solutions will increase the complexity within the code base (Yli-Huumo et al., 2014). When the code base starts to accumulate with too much TD that is not fixed afterwards, the development becomes more challenging, because the shortcuts are not designed to work well with other parts of the code base. Complexities in the code base start to reduce the overall quality and productivity goes down when new solutions and features must be implemented to the code base in debt (Yli-Huumo et al., 2015a)."
" The current literature related to TDM has identified and developed some processes and tools (Li et al., 2015a). Managing technical debt (MTD) workshops have gathered multiple studies related to TD and TDM in the past years (Seaman et al., 2015). However, TDM is challenging to implement, and it is hard for managers and developers to estimate and identify what and how much TD the current system has, how it will change, and what effects it will have in the future (Li et al., 2015a)."
"The reduction and repayment of TD are done by refactoring or rewriting the bad solutions (Codabux and Williams, 2013). Refactoring or rewriting can be seen as processes for “changing a software system in such a way that it does not alter the external behavior of the code yet improves its internal structure. It is a disciplined way to clean up code that minimizes the chances of introducing bugs. In essence when you refactor you are improving the design of the code after it has been written” (Fowler et al., 1999, p. 9). However, changing old solutions in the code is not easy, because improving the code base requires a significantly competent developer, and the company cannot just use all development time on refactoring or rewriting the solutions. "
"Even though the current literature has started to tackle and identify the concept and solutions of TDM, the problem is that there is a need for more empirical evidence from real-life software development (Li et al., 2015a)."
"TD was an important discussion topic in most of the development teams. This is not a surprise, considering the popularity of TD research in the past few years (Li et al., 2015a). The biggest issue with TD communication has been the gap between technical and non-technical stakeholders (Klinger et al., 2011). "
"Communication related to TD issues does not often transfer from the development team to the business stakeholders, which leads to TD issues not receiving the required time to get fixed (Yli-Huumo et al., 2014)."
"Without proper tracking and documentation of architectural changes and issues, it is also extremely challenging to quantify TD (Klinger et al., 2011). The inability to quantify TD also creates more challenges to other TDM activities, such as communication, repayment, monitoring, and measurement, due to the lack of TD data."
"However, the challenge in identification is that TD is not just related to simple errors, but especially to the architectural and design issues of software. It is challenging to identify this type of TD with tools. The challenge is how the tools tackle architectural or structural issues and technology gaps (Kruchten et al., 2012a)"
"Ramasubbu et al. (2015) describe TD prioritization with three dimensions: customer satisfaction needs, reliability demanded by the business, and probability of technology disruption. These dimensions are essential for decisions, but quantifying these with exact numbers is extremely difficult."
"A case study does not provide statistical generalizability (Yin, 2003), i.e. a case study with a limited number of cases cannot be generalized over a population. We, however, consider generalization as theoretical (Lee and Baskerville, 2003), i.e. abstraction from concrete events and actions to theoretical constructs. "
"[B] A comparison of the fuzzy and estimated reliabilities in this study and those obtained by Chen [7].Chen [7] used a fuzzy number that was used as a triangular fuzzy number in this study. As stated in [A], the triangular fuzzy number is a special case of the Level (Î»,1)iâv fuzzy number. Therefore, the consideration of fuzziness is superior to that of Chen [7]. Chen did not consider the estimated reliability of the system in the fuzzy sense."
The cumulative distribution of Eq. (11) is shown as the black line in Fig. 12. It can be concluded that the zero parameter model by Angelidou et al. [25] predicts a reasonable but not perfect distribution of enstrophy
"First, we transform the skin probability map using the DSPF space, and then apply the spatial analysis in the transformed skin map. For the spatial analysis we adapted an algorithm based on the distance transform in a combined domain (DTCD) of hue, luminance and skin probability, developed during our earlier study (Kawulok, 2013). Originally, the DTCD algorithm operates in raw skin probability maps, but here it has been adapted to perform the propagation in the DSPF space. The proposed method was compared with another approach proposed by Jiang et al. (2007), who also exploit the textural features, which is followed by the spatial analysis. Not only do the results indicate that our method outperforms the alternative algorithms, but also the gain attributed to the spatial analysis is larger than in case of processing raw skin probability maps."
"In our earlier research (Kawulok, 2010) we introduced an energy-based technique for skin blobs analysis. The pixels are adjoined to the skin regions depending on the amount of the energy which is spread over the image according to the local skin probability..Recently, we proposed to use the distance transform in a combined domain (DTCD) of hue, luminance and skin probability (Kawulok, 2013). The algorithm was proved to be very competitive and outperformed our energy-based method and the method proposed by del Solar and Verschae (2004). We overcame the most significant shortcoming of the latter approach, i.e. misbehaving in case of smooth transitions between skin and non-skin regions, by taking advantage of the cumulative character of the distance transform. "
"The proposed algorithm was compared with the Gabor wavelet-based texture analysis method proposed by Jiang et al. (2007). As it was mentioned in Section 2, the texture map threshold ÎT is set individually for every image presented in the original work. We tried to determine an optimal value of the threshold investigating the range 5â©½ÎTâ©½100, which covers all of the values quoted by the authors, but the overall results were quite poor. We overcame this problem by multiplying the skin probability map by the normalized textural filter response (0 for the maximal value, and 1 for the minimal), instead of applying the threshold ÎT."
"The table also includes the comparison with the DTF method (Kawulok, 2012). Although this earlier approach performs better if the distance transform is not applied (Î´min=20.39% compared with 21.14% for DSPF), the improved BIF extraction scheme offers lower error than DTF with DTCD. Moreover, the computation time is also reduced here due to a smaller number of the basic image features."
"Existing research has demonstrated the social engineering risks posed by such OSINT data (Ball etÂ al., 2012). However, this normally relies on labour intensive manual analysis (Creese etÂ al., 2012), which is impractical and poses a high cost to a potential attacker. Alternatively, such techniques utilise automated conversational agents (Huber etÂ al., 2009), which do not scale and are not very effective due to the challenges of imitating human conversational behaviour. "
"Huber etÂ al. (2009) make use of an organisation's Facebook presence to automatically identify and target its employees. Their tool gathers public information on members from Facebook, then attempts to expand that information through mechanisms like friend requests. Theoretically, their tool then uses Facebook chat to act as a chat-bot, building a rapport before executing a predefined attack (e.g., sending a link). Their evaluation shows that this scheme is impractical due to the overhead associated with imitating a human conversational partner."
"Ball etÂ al. (2012) detail how open source information can be used to construct spear-phishing attacks on an organisation's employees. They manually mine employee information from an organisation's website and gather additional information using the Maltego toolkit, before then using the Simple Phishing Toolkit to create phishing emails based on each employee's interests.The approach of Ball etÂ al. demonstrates the value of OSINT in this domain, but their method still relies on significant manual workload, whereas we focus on methods which can be deployed as part of a completely automated scanner."
"Scheelen etÂ al. (2012) attempt to map out a company's structure from online sources, including gathering information for social engineering. In their method, they first connect to the company on LinkedIn and then crawl LinkedIn for a list of employees, then search Facebook for those employees, matching on name, profile picture and location. They prune multiple matches by sending friend requests from âzombieâ profiles which are designed so as to look relevant to the targeted organisation. Their organisational mapping is based on heuristic processing of self-reported roles in LinkedIn profiles.In contrast to the connections and friend requests utilised by Scheelen etÂ al., our interaction with the target organisation is entirely passive, leaving the target organisation unaware of this stage of information-gathering. While we also resolve the identities of employees, we do this through a more flexible process using a larger and richer set of potential features, as described in Section 4.2.2.2.2"
"Clearly, the sheer amount of data created by mobile devices, the Internet of Things (IoT)  [1], and a myriad of other sources cannot be handled by traditional data processing approaches  [2]."
"More recently, the usage of simulators in the Cloud Computing field has also become widespread, which motivated the development of a number of simulators such as CloudSim   [12], GreenCloud   [13], and iCanCloud   [14]. None of these, however, can effectively model CEP applications."
" Existing approaches  [3,9-14] mainly consist of analysing, at design time, the contextual changes and the generation of the reconfiguration plans to fit the new environmental conditions. Then, the set of valid configurations are pre-calculated, as well as the differences between pairs of configurations and the conditions to adapt the system from one configuration to another one. All this previously calculated information is loaded into the device as part of the knowledge base of the MAPE-K loop. This is a shortcoming which limits the number of possible configurations and prevents the generation of the optimal ones."
 Other existing approaches that generate the configurations at  runtime [15-20] also have limitations in mobile environments as usually most of them demand high computing resources.
"Those DSPL approaches that perform the analysis and derivation of reconfiguration plans at design time are usually based on the definition of a set of eventâconditionâaction (ECA) rulesÂ  [29,20]. An ECA rule includes the event that will trigger a reconfiguration, a condition about the system state that must be evaluated as true, and the reconfiguration plan or actions that have to be executed. The main problem with this approach is that the number of rules could become untreatable, especially if the number of potential configurations is very high."
"4.1Generation of the reconfiguration planAs Brataas etÂ al. show inÂ  [30], the reconfiguration time is divided into three different tasks: (1) analyse the context data; (2) plan (decide) the new configuration and (3) execute the plan in order to deploy the new configuration. They prove that the cost of the first and third tasks can be considered fixed, while it is critical to make the plan task as efficient as possible because it depends on the number of configuration variants. Therefore, the challenge is to find the set of choices for the VSpecs tree (i.e.Â the resolution model) that defines the optimal configuration (the one that provides the highest utility while not exceeding the resources limitations) in a highly efficient way. However, this is an NP-hard problemÂ  [26] and, therefore, it is impossible to use exact techniques to solve this optimization problem for our purpose."
"Shen etÂ al.Â  [29] propose a dynamic reconfiguration approach based on dynamic aspect weaving where the set of valid configurations is also generated at design time and the reconfiguration process is triggered by ECA rules. The variability is specified using FMs, and they propose a meta-model for specifying role models, which are used to bind features to elements of the software architecture. However, their approach relies on the JBoss-AOP framework, which is not available in mobile devices. "
"Although the MUSIC middleware does not focus on mobile devices in particular, it is possible to execute it on mobile devices supporting an implementation of the OSGi platform (e.g.Â Android devices).Brataas etÂ al.Â  [35] propose a mechanism for extending MUSIC with support for specifying the requirements and the utility of the components of a software architecture. They estimate how many hardware operations are generated by each user action and the application response time. To this end, a structure and performance (SP) model is defined, which allows them to evaluate, at runtime, the resource usage of each configuration and therefore decide whether a configuration is appropriate for the current context. We have identified several drawbacks to this approach:"
"Cheng etÂ al.Â  [17] propose a predictive, instead of reactive, adaptation approach, trying to foresee changes in the availability of resources and lower the disruption to the quality of service provided to the user. To this end, they extend the Rainbow frameworkÂ  [37] with a mechanism for predicting resource availability based on data gathered in the past. The adaptation process is based on predefined strategies, which specify the changes that need to be applied to the system under certain conditions and contexts in particular. Furthermore, this approach has been evaluated by applying it to a web service and, therefore, it is not possible to assess the suitability of their approach in the case of mobile applications."
"InÂ  [18], Gomaa etÂ al. propose a DSPL approach which enables the dynamic adaptation of software architectures, but it is exclusively focused on service-oriented architectures. The variability is modelled using FMs, the features of which are mapped to the artefacts of the software architecture. However, it does not provide details on how a configuration of the FM is selected at runtime, although it states that this process is usually human assisted."
"The most similar approach to ours is the work presented inÂ  [19], where an optimization algorithm is also used to improve user interface adaptation at runtime. An important difference is that their work is specific to a user interface architectural model, while our approach is more general because it can be applied to the architectural model of any kind of applications. In their approach, the dynamic variation points are modelled by performing a mapping between the context and the different reconfiguration actions that can be executed at runtime. They use a different optimization algorithm (NSGA-II) although, as in our case, their approach does not depend on a particular optimization algorithm and is designed to work with other algorithms. Finally, the average adaptation time of our approach is considerably lower than the reported inÂ  [19]."
" On the other hand, the proposal of Benavides etÂ al.Â  [38] always finds the optimal configuration using Constraint Satisfaction Problems with exponential-time complexity, making it unsuitable for runtime optimization."
"Church's solvability problem was first raised for specifications in S1S (monadic second order logic of one successor). It inspired the great works of Buchi and Landweber on finite games of infinite duration [2-4] and of Rabin on finite automata over infinite structures [13,14]."
"Pnueli and Rosner extended the question to a setting, where the processes have access to incomplete information [12,10,9,7,1]. They introduced architectures, where the communication from an external environment to working processes and the communication between working processes is through boolean variables. "
"In this article, we show that distributed synthesis is undecidable even for the syntactic safety and reachability fragments of LTL [11,6], of ACTL [6], and of their semantic intersection."
"Different from other incremental models of self-organization that create new neurons at a fixed growth rate (e.g. Fritzke, 1995, 1997), GWR learning creates new neurons whenever the activity of well-trained neurons is smaller than a given threshold. This mechanism creates a larger number of neurons at early stages of the training and then tune the weights through subsequent training epochs. While the process of neural growth of the GWR algorithm does not resemble biologically plausible mechanisms of neurogenesis (e.g., Eriksson et al., 1998; Gould, 2007; Ming & Song, 2011), it is an efficient learning model exhibiting a computationally convenient trade-off between adaptation to dynamic input and learning convergence. For instance, it has been shown that GWR learning is particularly suitable for novelty detection and cumulative learning in robot scenarios (Marsland, Nehmzow, & Shapiro, 2005)."
"For example, Sasama etÂ al. (2009) proposed an error back-propagation neural network model that takes two images as input and produces one binary vector as an output that encodes the angular disparity between the two input images together with a response answer (match/mismatch). Similarly, Inui and Ashizawa (2011) proposed a radial basis function neural network to mentally rotate 3D objects. These models use neural networks but these networks have not been designed to reproduce brain mechanisms suggested to underlie mental rotation in humans. "
"These aspects have indeed been neglected by prior computational models of mental rotation (e.g., Inui & Ashizawa, 2011; Sasama etÂ al., 2009). Although cognitive robotics models of mental simulation have been recently proposed (Di Nuovo, De La Cruz, & Marocco, 2013), these do not directly address mental rotation capabilities, but rather mental simulation for motor planning tasks. They also do not propose hypotheses on the brain mechanisms that may underlie them (e.g., Di Nuovo, Marocco, Cangelosi, De La Cruz, & Di Nuovo, 2012). "
"In this study we propose a new neuro-robotic model of mental rotation that builds upon the prior model proposed in Seepanomwan etÂ al. (2013a, 2013b) and overcomes its limitations discussed above. Specifically, the new model has generalisation capabilities to transfer the mental rotation processes acquired with a small set of 2D visual training stimuli to novel 2D visual objects. "
"The architecture we proposed and its functioning mechanisms represent a further step with respect to previous computational models (e.g., Inui & Ashizawa, 2011; Sasama etÂ al., 2009) as these focused on mental rotation mechanisms without relating them to the other supporting processes such as matching processes and decision making processes (Lamm etÂ al., 2007)."
"The architecture also represents an innovation with respect to previous neuro-robotic models (Seepanomwan etÂ al., 2013a, 2013b) that did not distinguish between the brain areas possibly performing visual and proprioceptive processes and also used abstract monitoring and decision making mechanisms."
"To overcome these drawbacks, Herda et al. [17] introduced a real-time method using an anatomical human model to predict the position of the markers. It is unfortunately very difficult and time consuming to setup such a model."
"Lai et al. [23] noticed that the low-rank property of mocap data has not been explicitly exploited in the previous work, and they proposed to handle the mocap data refinement problem based on low-rank matrix completion theory and algorithm. The key point of their method is that it does not need any training data. Inspired by [23], our model also takes the low-rank structure property into account. Besides, we include two other properties into our design: the temporal stability and the noise effect. Compared with [23], our model does not only take the low-rank structure property into account but also the temporal stability property of motion data and noise effect. Our proposed model can handle both sub-problems of mocap data refinement at the same time, while in [23] they used two separate models to achieve the same goal. Another significant difference between our model and its counterpart in [23] is that we do not need to guess the standard deviation of the noise which is used for solving the de-noising model in [23]. More importantly, the optimization method for solving our model is much faster and robust than SVT [23], which has been proven both in theory and in experiment [33]. "
" In Section 4, we also have observed that our method is not only faster than the work [23] but also outperforms it in the experiments on both synthetic and real data. Meanwhile, we also notice that two low-rank matrix based methods [19,47] have been proposed almost at the same time as ours following the work [23]. "
" From the last two columns of each sub-image in Fig. 11, the shortcoming of SVT [23] is shown that it is unable to correctly predict some missing values. Relatively speaking, we find that Dynammo [30] offers the best performance for all the three motion sequences. However, when some markers are missing for a long period of time, Dynammo [30] also fails to correctly predict the missing values."
"Conventional robust preview control, which has been studied in [5–9], is restricted to small ranges of variations."
"It is clear that choosing the location of each nuclear installation is a balancing act between various factors, most importantly the proximity of the sites to urban areas with significant population (Grimston, Nuttall, & Vaughan, 2014). Whilst the so-called semi-urban installations (for example, at Hartlepool and Heysham in the UK) could reduce operational and transmission costs, they pose a greater risk to the nearby population, and therefore require detailed emergency planning."
"The dose conversion factors for the individual elements could vary significantly depending on their decay energy: the factor for Cs-134 is around 9 times greater than that for Cs-137 (Yoo, Jang, Lee, Noh, & Cho, 2013)."
"These radiation levels are well below the known thresholds for the deterministic effects, and tend to cause stochastic effects on human health (Choi, Costes, & Abergel, 2015), including cancers. "
"Van-der-Aalst and Weske [31] applied a three step Public-to-Pri-vate approach to inter-organisational workflows. In the first step, the partner organisations agree on a common public workflow; in the second step, the common public workflow is divided be-tween the interacting organisations; and in the third step, the organisations create their private workflows autonomously. This approach requires manual negotiations to reach an agreement, which can be very time consuming especially if there are many partners."
"Krukkert [13] proposed a solution in the openXchange project. Two activity diagrams are taken as input and compared to find out all common execution sequences. If any common sequence is found then a common activity diagram is constructed for collabo-ration. For the solution to work, there must be a common activity sequence in the workflows or activity diagrams of the participating organisations. If a common sequence is not found, then collabora-tion cannot proceed, which is a limitation of the system."
"Okutan and Cicekli [15] proposed an event calculus based web service composition and execution (WSCE) system. The system has two phases, namely composition phase and execution phase. In the composition phase, the OWLS process definitions are translated to axioms in event calculus domain. Web services are encoded as ac-tions, web service inputs and outputs as action’s knowledge pre-conditions and knowledge effects, and web service preconditions and effects as action preconditions and effects. The user inputs are substituted as initial condition axioms and the outputs as goals. Based on the domain knowledge, the Abductive Event Calculus Planner generates plans to reach the given goal state. The plans are presented to the user in the form of visual graphs, which can be sorted according to user’s quality of service parameter among execution duration, price, reliability and availability. In the execution phase, the selected graph is transformed to OWLS descriptions and passed to the execution engine. The user enters the actual input values, and the actual web services modelled by the OWLS pro-cesses are invoked.WSCE is a good effort to use event calculus for web service com-position. The main benefit of this system is that it supports concur-rent plans and so it is better suited for solving real world business scenarios. The main issue with this system is that it can compose workflows for a single organsisation only and does not take the generation of collaborative workflows for multiple organizations into account. The work, if extended for solving multi-organisation scenarios, can be a good addition to research."
"Recently, there has been some work on composing workflows for multiple organisations. [1] suggested a Pi-Calculus based ap-proach to compose web services into cross organisational business processes. A cross organisational business processes is modelled as a set of concurrent local processes, which has a global start and a global end activity. The activities in the local processes can receive external start messages. A cross organisational con-troller controls the flow of control and data in the cross organisa-tional process. The limitation with this work is that it uses a manual modelling approach and the web services composition is not automatic."
"Correˆa da Silva et al. [8] presented a lightweight, flexible and user-friendly platform for cross organisational workflow interac-tions. The platform is named JamSession and it can be considered as a meeting point for already existing software components to form new and innovative service systems. JamSession is a user-friendly and light-weight platform, providing an appropriate framework to specify and implement cross organisational work-flow interactions. It uses knowledge-based interaction protocols and predicates to models cross organisational workflows and activ-ities in such away that the workflow definitions are local to the respective workflow management systems, and only the interac-tion protocols are made public. It makes the workflows highly decoupled. While the paper claims that the interaction protocols can be used to specify and execute cross organisational workflows, it only shows examples for the execution of cross organisational workflows. So, it is not possible to deduce whether the interaction protocols can be used for bringing about collaboration among cross organisational workflows at build time."
"Unlike the translation algorithm described in Sirin et al. [27] which translates only the preconditions of atomic processes into the preconditions of SHOP2 operators, Translate-Atomic-Pro-cess(Q) translates both the preconditions and inputs of atomic pro-cesses into the preconditions of SHOP2 operators. This enables the developed framework to use web services that have both inputs and preconditions in workflow generation. Similarly, unlike the translation algorithm described in Sirin et al. [27] which translates only the effects of atomic processes into the post-conditions of SHOP2 operators, Translate-Atomic-Process(Q) translates both the effects and outputs of atomic processes into the post-condi-tions of SHOP2 operators. This enables the presented framework to use web services that have both outputs and effects in workflow generation."
"Unlike the discussed approaches [27,33], the implemented framework is not focussed on finding an execution path for already defined composite processes. We be-lieve that forming an execution path for an already built composite process limits the strength of workflow generation by limiting the automation. Therefore, the composite processes are decomposed to atomic processes and then the atomic processes are used to create a single SHOP2 if-then-else method to guide the composition process."
"Unlike the proposed approach described above, the approach by Sirin et al. [27] does not combine operators to form a method. This means that their system can generate workflows only if the user manually defines the composite processes and passes them to the system. "
"As the system proposed by Sirin et al. [27] targets the creation of workflows for a single organisation only, it has a single SHOP2 domain to begin with. Therefore, they do not present any algo-rithm for collapsing the domains of multiple collaborating organi-sations into a single domain."
"Albanese etÂ al. (2013) present a well-modeled formalism for complex inter-dependencies of missions as a set of tasks. Using numerical scores and tolerances in a holistic approach Albanese etÂ al. focus on cost minimization. Their approach can solely be validated holistically, as involved parameters do not bear local semantics and do not provide bias-free and context-free understandable results. Buckshaw etÂ al. (2005) propose a quantitative risk management by involving various experts and present a score-based assessment based on individual values and a standardization using a weighted sum. Unfortunately, a mathematical foundation is missing and obtained results are only interpretable after deep training of experts in the characteristics of this approach. Buckshaw etÂ al. themselves note that a validation of the proposed model requires large amounts of actual data and ground truth, which both are not available."
"Jakobson (2011) presents a well-understood conceptual framework using interdependencies based on operational capacity at different abstraction layers. In this dependency model, impacts are propagated and reduce the operational capacity, which has a similar intention to our approach. However Jakobson (2011) uses self-defined metrics for propagating impacts through Boolean gates, which cannot provide context- and bias-free understandable results or parametrization. Moreover, an explicit representation of âintra-assetâ dependencies is required, i.e., all individual critical, and non-critical resources must be identified. "
Musman etÂ al. (2011) proposes the use of BPMN models and describes a process for evaluating impacts of cyber-attacks. However Musman etÂ al. (2011) fails to get across any mathematical approaches or formal definitions for impact assessment.
"Further works focuses solely on modeling. For example, Goodall etÂ al. (2009) focus on modeling and available data integration using ontologies but do not address an impact assessment. Another ontology-based approach is presented by Amico etÂ al. (2010), which identifies multiple experts while noting that, e.g., system administrators are not capable of understanding an organization's missions."
"In terms of (probabilistic) approaches toward assessments of impacts caused by vulnerabilities and attacks, probabilistic models have been researched by Wang etÂ al. (2008), Liu and Man (2005), or Xie etÂ al. (2010). However, Wang etÂ al. base their work on attack graphs and do not consider imperfect knowledge, e.g., unknown extents of damage causable by vulnerabilities, uncertainty of specific events and potentially disagreeing information sources as we do. Xiep etÂ al. and Liu etÂ al. are significantly limited by the lack of supporting cyclic dependencies and do not consider any mission impact relations. Chung etÂ al. (2013) consider a probabilistic approach as well to determine the likelihoods of explicit attack paths. However, presented probability theory in Chung etÂ al. (2013) is not sound and voids fundamental principles of probabilistic inference in multiply connected graphs. Other impact propagation approaches, e.g., by Kheir etÂ al. (2009) or Jahnke etÂ al. (2007), claiming to handle details such as disagreeing information sources and cycles, are not probabilistic based and degrade to a handcrafted propagation algorithm with arbitrary scores, where parameters are only assessable by deeply trained experts and obtained results can only be used in a holistic way, as they provide no directly interpretable meaning. "
"To obtain well-defined results, i.e., to obtain a solid and consistent business dependency model from multiple sources and experts, a semantic normalization and merging is required for business dependency model. We deeply discuss and propose a solution to the semantic normalization and merging problem of mission dependency models in Motzek etÂ al. (2016)."
"To date, the managerial and scholarly debate on two-sided mar-kets has followed the logic of the credit card business, where the ab-solute number of merchants accepting a credit card — or the number of applications available in the marketplace — determines the value of the credit card for the end user (see e.g., Chen, 2010; Reuters,2012; Lee, 2015; Smith, 2015). However, this approach considers all appli-cations equal and thus ignores the qualitative aspects of the market dynamics. "
"According to Sun and Tse (2009), this implies that the mar-ket would be able to sustain more than one ecosystem. Overall, our research advances Sun and Tse’s (2009) model of platform compe-tition by emphasizing that multi-homing can manifest differently within a single group of actors in the market."
"As a result, we depart from Sun and Tse (2009) who emphasized the sheer size of the two sides of the market as a decisive factor in platform competition. In addition, our findings differ from the extant research (e.g., Yamakami, 2010; Holzer and Ondrus, 2011; Schultz et al., 2011) that, grounded on network externalities (Katz and Shapiro, 1985), somewhat simplistically argues that a large base of develop-ers leads to a large number of applications that, in turn, leads to an increasing number of end-users, and vice versa. "
"Second, our findings imply that application marketplaces are not used to differentiating their ecosystem from that of competitors. The results of the content analysis show that the content of the most installed applications are similar in the three leading mobile appli-cations ecosystems. This supports the findings by Hyrynsalmi et al. (2013) who did not find differentiation between the consumers nor the application offerings of the ecosystems."
"For example, prior survey research indicates that mobile gaming is not of interest to customers (see e.g., Economides and Grousopoulou, 2009; Bouwman, Carlsson, Castillo, Giaglis, and Walden, 2010; Suominen, Hyrynsalmi, and Knuutila, 2014). However, our results show that games form the majority of the most installed applications. This might be a result of a pattern whereby a user downloads several games, tries them all once, and then removes uninteresting ones from the device."
"A second motivation is related to several software architecture issues that lead to practical difficulties for the functional extension of current software libraries. For instance, WordNet::Similarity [99] and WS4J [121] were designed before the emergence of the intrinsic IC models described in sectionÂ 2.1, thus, these libraries maintain in-memory tables with the concept frequency counts which are interrogated in order to compute the IC values required in a similarity evaluation step; however, their data structures does not provide any proper abstraction layer or software architecture to integrate new intrinsic IC models easily."
"Many works introducing similarity measures or IC models during the last decade have only implemented or evaluated classic IC-based similarity measures, such as the Resnik [108], Lin [70] and Jiang-Conrath [52] measures, avoiding the replication of IC models and similarity measures introduced by other researchers. Some works have not included all the details of their methods, or the experimental setup to obtain the published results, thus, preventing their reproducibility. Most works have copied results published by others"
"The first known IC model is based on corpus statistics and was introduced by Resnik [108], and subsequently detailed in [109]. The main drawback of the corpus-based IC models is the difficulty in getting a well-balanced and disambiguated corpus for the estimation of the concept probabilities. To bridge this gap, Seco etÂ al. [119] introduce the first intrinsic IC model in the literature, whose core hypothesis is that the IC models can be directly computed from intrinsic taxonomical features."
"The pioneering WNSim library was developed in Perl by Pedersen etÂ al. [99], and subsequently migrated to Java by Tedeki Shima, under the name of WS4J [121]. WS4J includes, like its parent library, the most significant path-based similarity measures, the three aforementioned classic IC-based measures and several corpus-based IC models [95]. However, WNSim and WS4J do not include most ontology-based similarity measures developed during the last decade, nor any intrinsic IC model."
"Finally, we have the WNetSS semantic measures library introduced recently by Aouicha etÂ al. [15], which is based on an off-line pre-processing and caching in a MySQL server of WordNet, as well as all WordNet-based topological features and implemented IC models. As we mentioned previously in sectionÂ 1.1.1, the caching strategy used by WNetSS severely impacts its performance and scalability. In addition, WNetSS exhibits two other significant extensibility drawbacks which prevent its use for researching and prototyping of new methods, as follows: (1) the current distribution of WNetSS does not include its source files, thus, their architecture, representation model for taxonomies and implementation details are missing; and (2) the current WNetSS version does not allow any type of functional extension, such as including a new taxonomy parser, as well as a new semantic similarity library or IC model. Finally, despite one of the main motivations of WNetSS being to provide a software implementation for the most recent methods, looking at tablesÂ 2 and 3, you can see that WNetSS [15] neither implements nor cites many recent similarity measures and IC models reported in the literature."
"A long-known but much neglected practice first advocated by Dijkstra in Dijkstra (1982), separation of concerns strives to separate different aspects of software design and implementation to enable separate reasoning and focused specification for each of them. "
"Components and connectors are present in most component-oriented approaches: a wealth of literature discusses their various possible flavours (see for example (Szyperski, 2002; Lau and Wang, 2007; Mehta et al., 2000)). Containers have a much lesser prominence in the literature, perhaps a token of the insufficient penetration of the concept of separation of concerns in component-based software engineering. "
"The nature of our target systems reduces the variety of necessary connectors to a few basic kinds, which are required to perform function/procedure calls, remote message passing or data access (I/O operations on files in safeguard memory). This also means that we do not require an approach for the creation or composition of complex connectors (Spitznagel and Garlan, 2001)."
" As in the HB model, recognition is signalled by a transient synchronization event, and this synchronization is brought about by coupling feature detectors that have nonstationary, pattern-dependent frequency response profiles. However, the synchronization process itself is not implemented using balanced excitation and inhibition among IF cells as in Hopfield and Brody (2001), but is rather described at the level of phase dynamics. This allows us to be equivocal about the details of the neural circuits that generate the oscillations themselves. We see this as a benefit as there are currently a large number of possible candidates for the underlying processes (see next section)."
" We should also bear in mind, however, that our results were obtained by training the system on clean utterances whereas the results in Lee etÂ al. (2011) were obtained from a system trained on noisy utterances. Our results are more in line with those of Rouat, Loiselle, and Molotchnikoff (2011) who obtained word error rates of 78% when training an MFCCâHMM system on clean utterances and testing it on 10Â dB noisy utterances using noise samples from Aurora-2."
"Third, we have shown that the dynamical pattern recognition process can act as a forward model of neuroimaging data. Previous studies in this area (Corchs & Deco, 2004; Husain, Tagamets, Fromm, Braun, & Horwitz, 2004) have used computational models of auditory processing as forward models for fMRI data. Corchs and Deco (2004), for example, have used a neurodynamical model of feature-based attention in combination with a haemodynamic process as a forward model of fMRI activity. "
"The Liquid State Machine (LSM) (Maass, Natschlager, & Markram, 2002), for example, uses OT features and the temporal embedding idea proposed in the HB model, but then applies standard methods for recognizing the resulting static patterns. This results in good pattern discrimination abilities (Verstraeten etÂ al., 2005), though not as accurate as a recent approach based on OT features (Gutig & Sompolinsky, 2009). Further, LSMs do not generate a gamma burst as an integral part of the recognition process, so would not be so appropriate as a forward model for the sort of neuroimaging data addressed here."
"The notion that regions higher up in the auditory cortical hierarchy process information at longer time scales has recently been made use of in a model of auditory sequence recognition based on stable heteroclinic channels (Kiebel, Kriegstein, Daunizeau, & Friston, 2009). Moreover, the approach developed in that work derives from a Bayesian perspective in which cortical hierarchies embody a generative model which is then inverted during the pattern recognition process. Generative models of speech production are, as yet however, still in the early stages of development. This currently limits the ecological validity of such a generative modelling approach."
"While there is a lot of folk wisdom on how to design good algorithms for these highly-threaded machines, in addition to a significant body of work on performance analysis  [16-20], there are no systematic theoretical models to analyze the performance of programs on these machines. "
" Over the years, computer scientists have designed various models to capture important aspects of the machines that we use. The most fundamental model that is used to analyze sequential algorithms is the Random Access Machine (RAM) modelÂ  [21], which we teach undergraduates in their first algorithms class. This model assumes that all operations, including memory accesses, take unit time. While this model is a good predictor of performance on computationally intensive programs, it does not properly capture the important characteristics of the memory hierarchy of modern machines. "
"For parallel computing, the analogue for the RAM model is the Parallel Random Access Machine (PRAM) modelÂ  [30], and there is a large body of work describing and analyzing algorithms in the PRAM model  [31,32]. The PRAM model also ignores the vagaries of the memory hierarchy and assumes that each memory access by the algorithm takes unit time. For modern machines, however, this assumption seldom holds. Therefore, researchers have designed various models that capture memory hierarchies for various types of machines such as distributed memory machines  [33-35], shared memory machines and multi-cores  [36-40], or the combination of the two  [41,42].All of these models capture particular capabilities and properties of the respective target machines, namely shared memory machines or distributed memory machines."
"Many machine and memory models have been designed for various types of parallel and sequential machines. In an early work, Aggarwal etÂ al.  [25] present the Hierarchical Memory Model (HMM) and use it for a theoretical investigation of the inherent complexity of solving problems in RAM with a memory hierarchy of multiple levels. It differs from the RAM model by defining that access to location x takes logx time, but it does not consider the concept of block transfers, which collects data into blocks to utilize spatial locality of reference in algorithms."
" Alpern etÂ al. propose the Memory Hierarchy (MH) FrameworkÂ  [26] that reflects important practical considerations that are hidden by the RAM and HMM models: data are moved in fixed size blocks simultaneously at different levels in the hierarchy, and the memory capacity as well as bus bandwidth are limited at each level. But there are too many parameters in this model that can obscure algorithm analysis. "
"In the parallel case, although widely used, the PRAMÂ  [30] model is unrealistic because it assumes all processors work synchronously and that interprocessor communication is free. "
"Culler etÂ al.Â  [33] offer a new parallel machine model called LogP based on BSP, characterizing a parallel machine by four parameters: number of processors, communication bandwidth, delay, and overhead. It reflects the convergence towards systems formed by a collection of computers connected by a communication network via message passing. Vitter etÂ al.Â  [35] present a two-level memory model and give a realistic treatment of parallel block transfers in parallel machines. But this model assumes that processors are interconnected via sharing of internal memory.More recently, several models have been proposed emphasizing the use of private-cache chip multiprocessors (CMPs). Arge etÂ al.Â  [36] present the Parallel External Memory (PEM) model with P processors and a two-level memory hierarchy, consisting of the main memory as external memory shared by all processors and caches as internal memory exclusive to each of the P processors. Blelloch et al.Â  [37] present a multi-core-cache model capturing the fact that multi-core machines have both per-processor private caches and a large shared cache on-chip. Bender etÂ al.Â  [44] present a concurrent cache-oblivious model. Blelloch etÂ al.Â  [38] also propose a parallel cache-oblivious (PCO) model to account for costs of a wide range of cache hierarchies. Chowdhury etÂ al.Â  [39] present a hierarchical multi-level caching model (HM), consisting of a collection of cores sharing an arbitrarily large main memory through a hierarchy of caches of finite but increasing sizes that are successively shared by larger groups of cores. They inÂ  [42] consider three types of caching systems for CMPs: D-CMP with a private cache for each core, S-CMP with a single cache shared by all cores, and multi-core with private L1 caches and a shared L2 cache. All the models above do not accurately describe highly-threaded, many-core systems, due to their distinctive architectures, i.e.Â the explicit use of many threads for the purpose of hiding memory latency."
" Liu etÂ al.Â  [19] describe a general performance model that predicts the performance of a biosequence database scanning application fairly precisely. Their model incorporates the relationship between problem size and performance, but only targets their biosequence application."
" Govindaraju etÂ al.Â  [45] propose a cache model for efficiently implementing three memory intensive scientific applications with nested loops. It is helpful for applications with 2D-block representations while choosing an appropriate block size by estimating cache misses, but is not completely general."
"Ryoo etÂ al.Â  [46] summarize five categories of optimization mechanisms, and use two metrics to prune the GPU performance optimization space by 98% via computing the utilization and efficiency of GPU applications. They do not, however, consider memory latency and multiple conflicting performance indicators."
" Kothapalli etÂ al. are the first to define a general GPU analytical performance model inÂ  [47]. They propose a simple yet efficient solution combining several well-known parallel computation models: PRAM, BSP, QRQW, but they do not model global memory coalescing."
" Meantime, Baghsorkhi etÂ al.Â  [16] measure performance factors in isolation and later combine them to model the overall performance via workflow graphs so that the interactive effects between different performance factors are modeled correctly. The model can determine data access patterns, branch divergence, and control flow patterns only for a restricted class of kernels on traditional GPU architectures."
" Zhang and OwensÂ  [15] present a quantitative performance model that characterizes an applicationâs performance as being primarily bounded by one of three potential limits: instruction pipeline, shared memory accesses, and global memory accesses. More recently, Sim etÂ al.Â  [48] develop a performance analysis framework that consists of an analytical model and profiling tools. The framework does a good job in performance diagnostics on case studies of real codes. Kim etÂ al.Â  [49] also design a tool to estimate GPU memory performance by collecting performance-critical parameters. Parakh etÂ al.Â  [50] present a model to estimate both computation time by precisely counting instructions and memory access time by a method to generate address traces. All of these efforts are mainly focused on the practical calibrated performance models. No attempts have been made to develop an asymptotic theoretical model applicable to a wide range of highly-threaded machines."
"In his thesis [12], Korp generalizes Definition 17 (cf. [12, Definition 3.10]) by incorporating an auxiliary relation âª°ÏA that may be viewed as a precursor to our relation â«. The modified definition permits smaller automata, which benefits implementations, but is more complicated than Definition 17. The modification also does not add expressive power."
"In this section we show how tree automata can be used to prove termination via match-bounds [9]. To this end, we first recapitulate the basic concepts and important results about match-bounds. Note that match-bounds require special treatment for non-left-linear TRSs (see Example 30)."
"On the analyzer side we have extended the termination tool Image 1 [11] and the confluence tool CSI [19] to produce state-coherent, state-compatible automata. Since both tools use quasi-deterministic automata in their completion process, we apply the construction of Theorem 20 as a post-processing step, resulting in a state-coherent, state-compatible automaton. CeTA can then be used to certify this output. In contrast to the earlier version of CeTA corresponding to [5], CeTA now supports match-bounds for non-left-linear TRSs as well."
" In fact, for some algorithms we just relied on the automatic refinement provided in [16] which turns set operations into operations on trees. For other algorithms we performed manual data refinement. Although the latter approach is more tedious, it has the advantage for the user that we additionally integrated detailed error messages which are displayed if the certifier rejects a proof."
"The Chen system with the parameter set {a,c,b}={35,28,3} is chaotic [12], but it may not be chaotic for some other parameter. The Lu system with the parameter set {a,c,b}={36,20,3} is also chaotic [61] and, likewise, it may not be chaotic for any other parameter.It is easy to see that a generalized system [36,14,37]"
"But the question of importance is what was known about repulsor (or repellor) in the Lorenz system and the dynamics of time-reversal Lorenz system before the works [12,61] were published? Note that in [2,3] there is no discussion of the following important questions for the consideration of the Lorenz system in the backward time or with non positive parameters: the existence of the extension of solutions, the existence of attractors, and the possibility of consideration of invariant sets in the backward time. Some necessary results can be found in [16], [15, p. 35]. "
" For Î± different from 0, there exists only one partial result in [17], in which existence and uniqueness is proved for the equation, under very strong assumptions on the data. However, the small data and non negativity of dh/dt assumptions in [17] are very far from conditions in the present situation. "
"Many researchers have addressed effort estimation and, therefore, consider productivity factors (PFs) (Wang et al., 2009), but they do not address the possibility of potentially inappropriate variables in the UCP algorithm itself, which is important for software size estimation."
"However, the existing use case-based estimation methods have some well-known issues (Diev, 2006). Use cases are written in natural language; consequently, there is no rigorous approach for comparing use case quality or fragmentation. "
" Rosa et al., (2014)investigated whether a linear model based on both size and application type was better than a model based on size only; however, this study did not investigate the effects of each variable nor evaluate additional types of regression models."
"López-Martín (2015) described linear regression models as less accurate than neural networks, but they provided no description of the regression models studied. Moreover, they did not consider the stepwise principle for model construction nor did they investigate whether all the UCP variables contribute to size estimation."
" A discussion of variable significance can be found in Urbanek et al., (2015a). Silhavy et al., (2015a, 2015b) offered a linear model obtained by the least squares approach, in which two prediction coefficients were used to adjust the UAW and the UUCW. These studies did not focus on evaluating of variables for use in regression models, nor did they compare linear and polynomial regression models."
"Urbanek et al., (2015b) described the number of points in the use case scenario as the most significant factor, but the scope of this paper is analytical programing; therefore, this finding is not applicable to MLR."
"The complexity of a use case is based on the number of scenario steps or, sometimes, on the number of transactions it contains (Ochodek et al., 2011a). However, a transaction typically refers to a set of activities, not a simple step in a structured scenario."
"MLR models depend on assumptions, and when those assumptions are violated, data transformation (Christensen, 2006) may be required. However, such transformations also change the method of prediction because the predicted value of the dependent variable does not represent the project size."
"The approaches to motion estimation are biomechanical based [5,7], silhouette based [2,3,6,11,12] and image based [1,4,8–10]. A biomechanical-based approach involves tissue analysis and bone and joint location, which requires expensive devices and equipment. Silhouette-based estimation is analyzed by the silhouette extract from a human image file. It is always challenging when the estimation involves more than one subject."
"A two-stage Monte Carlo Markov Chain (MCMC) inference algorithm was implemented by Zhang et al. [37] using part-based gait estimation. Because some motions such as walking and running only focus on the lower body segment, the gait motion has been essential, with emphasis placed on the lower body segment. However, for motions that also involve the upper and lower body segments, such as dancing and sword playing, gait motion is not a proper option to yield good estimation."
"On the other hand, silhouette-based motion estimation investigates the image silhouette of human motion. Güdükbay et al. [38] demonstrated that the human silhouette can be labeled using a model-based approach. However, Rosenhahn et al. [2] reported that silhouette information is insufficient for estimating the model correctly, as the extracted silhouette is hard to determine."
"Image-based estimation is liable in motion capture and suitable for direct analysis of the image motion data. However, when an individual is in contact with other objects, it is difficult to differentiate between the subject and the objects."
"Recent human motion classification works [18,20–22,54] applied the classification method to the available or captured motion data, but the authors did not classify estimated matching motions."
"It is well known that polynomial fitting approximations have played a central role in numerical analysis. Polynomial fitting is able to generate smooth functions, which are easy to manipulate and evaluate. In addition, polynomial fitting is also able to provide precise accuracy of convergence in the approximation of numeric data. However, polynomial fitting is only able to generate precise approximation in short intervals, where the large interval will cause the approximation to oscillate widely [56]."
"Foot skating refers to the error that occurs in animation whereby the feet slide or float on the ground [64]. Foot skating often occurs when the recorded motion data are applied to different subjects whose position no longer fits the motion of the limbs, which is not applicable to our study. "
"There are of course many limitations in this approach. Firstly it is entirely possible that other articles have been published in OR/MS journals that have analytics-related content, however do not use this term in their abstract or title. Secondly there is the potential that academics in the OR/MS community would pub-lish analytics-orientated research in journals not directly associ-ated with OR/MS (e.g. Coghlan et al. (2010)). "
"Alternative approaches to detecting the OPMD from microscopic images could be to threshold the cell images [5,6], to apply either the graphical model method [7,8] or the contour-based method [9,10], or to open them morphologically using structuring elements to eliminate background objects while preserving the shape of cells [11-15]. However, the shape-based methods [6-15] require considerably complex preprocessing steps, and also would be useful only when the appropriate shapes of images are available.Due to the simplicity and efficiency of the histogram based techniques, the histogram based approaches are widely used for image analysis. "
" Although good performance was achieved by the method of watershed transform (derived from the morphology) using the image of region of interest for non-uniform images [14,15], we keep our approach simple for three main reasons. First, the complexity in selecting constraint parameters, structuring elements, cost functions, etc., was quite high in [5,15]. Second, the initial regional minima was manually defined as highly related to the over segmentation issue in [14,15], and thus the applicability to different microscopic image data, e.g., other data of PCD, is uncertain."
"At the end, using the synthesized features, the framework of HROIT/GP-EM with MDC [37,38] achieved an average accuracy rate of 90.20% with a Std of 3.50% on the OPMD dataset in the ten-fold cross-validation.Using each of the best feature functions with the same MDC, the average accuracy rates over the ten-fold test sets are reported in Table 4 for the CDF-DRVC/GP-EM framework and are compared to the performance of the HROIT/GP-EM framework [37,38]. From Table 4, we can see that the average rate obtained by CDF-DRVC/GP-EM is higher than that obtained by HROIT/GP-EM [37â38], with an improved average rate of 4.3%; the CDF-DRVC/GP-EM also yields a lower Std of 2.39%, compared with a Std of 3.50% obtained by HROIT/GP-EM in the ten-fold cross-validation."
"Developing valuable software is a goal that has long been on the Agile manifesto (Beck et al., 2001). However, it is not an easy goal to achieve. Before adopting CD, some of our teams had been using an Agile method called Kanban (Anderson, 2010); however, due to delivery problems, we still had situations where a team had completed a feature but could not deliver it to production to ob-tain users’ feedback. Consequently, they built additional function-alities on top of that feature, simply assuming it was useful. Un-fortunately, when they finally delivered the software to the users, they found out that the feature was not what the users needed. "
"Many works have reported challenges and solutions in adopt-ing CD (Leppanen et al., 2015; Claps et al., 2015; Olsson et al., 2012; Karvonen et al., 2015; Rissanen and Münch, 2015; Marschall, 2007; Zhu et al., 2015; Debbiche et al., 2014; Puneet, 2011; Noured-dine and Foutse, 2014; Laukkanen et al., 2015; Rogers, 2004; Sek-itoleko et al., 2014; Krusche and Alperowitz, 2014; Adams et al., 2015; Souza et al., 2015; Feitelson et al., 2013; Rahman et al., 2015; Neely and Stolt, 2013), including a very recent systematic literature review (Laukkanen et al., 2017). However, none of the existing lit-erature on CD has included the strategies I reported in this paper."
" These challenges are not reported in the recent Systematic Liter-ature Review (SLR) (Laukkanen et al., 2017), which systematically reviewed the existing literature to identity problems and solutions in adopting CD."
"My previous work (Chen, 2015a) did not include any of the strategies reported in this paper. In terms of challenges, four of the further challenges for research were not included in my pre-vious paper. For those challenges that were mentioned in (Chen, 2015a), I provided new and additional descriptions in this paper."
"However, from an economic perspective, traditional CDNs require significant investments for scaling up, as it requires deployment and management of geographically distributed data centres [54]."
" Early on, it was identified that P2P swarms possess the so-called self-scaling property [73,83] – available capacity increases with the number of users in the swarm, as each user downloading content also adds new capacity by acting as a server for other users. However, obtaining content through self-organised P2P swarms has proved to be unreliable due to availability issues [52], because in selfish swarming protocols such as BitTorrent, users leave the swarm after obtaining the item they need, making it difficult to put together complete copies of content."
"However, this makes peer-to-peer systems prone to unpredictable changes in content availability - a phenomenon called peer churn - when peers frequently and suddenly leave or join the system due to network failures or their own intent [103]."
"For example they may upload and share corrupted, malicious or illegal contents in the system or block nodes from using the service [89]. Indeed, pollution of the system with corrupted and unauthorized contents has been named as a serious problems in peer-to-peer networks [105]. Privacy is yet another issue in decentralized P2P systems: private information of the users including their IP addresses, geographic locations and viewing preferences might be exposed to unauthorized users."
"The accesses of Spotify users [120] feature a vivid diurnal pattern: The lengths of user sessions are the longest during morning hours and are gradually decreasing by the end of the day. The user sessions are also shorter during weekends than during working days, and on mobile devices than on desktop computers [120]."
" Indeed, in  [50], the authors reported that the Top-10% popular videos account for up to 80% of the total traffic in BBC iPlayer. Similarly, in  [42] is reported that the Top-10 videos constituted a significant fraction of CDN traffic in MSN Video. The authors in  [108] reported that, video popularity in online social networks (OSNs) follows the Zipf-like distribution. However, the content popularity would change quite dynamically, for example, due to daily releases of highly popular news and business-related shows [42]."
"The delivery of high quality multimedia contents to the customers is the challenging issue for many network and content delivery services  [14,15,62,79]."
"The simulation results suggest that this approach can reduce the startup delay by up to 2 sec in comparison to the previous results in  [22,42]."
"In summary, our analysis in this section suggests that, the most promising compromise in mixing advantages of peer-assisted and traditional CDNs to improve QoS has been so far found in bootstrapping video streaming with initial chunks downloaded from CDN nodes and delegating the remaining work to peers [39,65]. "
"Similarly, the peer-assisted content delivery system may benefit from redirecting streaming requests to CDNs in emergency cases when no sufficient upload bandwidth is available among the peers to meet playback deadlines [112]."
"The peers inaccessibility problem; when a peer inside a private network can initiate a connection with the peers of public networks, but a reverse connection is often complicated by administrative policies [29], has been discussed in some of the earliest partially centralized PA-CDN articles  [101,102]. "
"The simulation results in  [102] suggest that, the users which are connected outside a firewall have slower downloading rates since they are unable to establish connections with the nodes in private networks, whereas peers behind firewalls exchange more, therefore, leading to an increase in downloading speed."
"It is estimated that, 90% of users in China are located behind the firewalls  [45,61]. Among those, as the user trace collected [61] from LiveSky is revealed, 37% are not available for peer-to-peer distribution and nearly 30% can provide uploading capacity sufficient to serve only one user at a time."
"This includes, among others a need for economically sound mechanisms to incentivize user participation, which is reportedly low in some of the existing systems [124] and is recognized as a significant obstacle factor in the others [38,50]. "
"Das et al. [8] suggested an algorithm for constructing the (unweighted) straight skeleton of monotone polygons, which they claim runs in O(nlogâ¡n) time, where n denotes the number of vertices of the polygon. However, we have simple examples that show that their Lemmas 5, 6, and 7 do not hold for all valid inputs. In particular, their approach hinges upon the assumption that no event introduces a new reflex vertex during the wavefront propagation process, which is clearly incorrect for general input. (See the node marked in green in Fig. 2, on the right-hand side of the lower chain.) Note that a perturbation of the input in order to avoid such a vertex event, as suggested by Das et al. [8], cannot be applied as the straight skeleton changes discontinuously [2]."
"Our algorithm can compute the positively weighted straight skeleton of a monotone polygon in O(nlogâ¡n) time and O(n) space, which constitutes a significant improvement over the O(n17/11+Ïµ) worst-case time and space complexity of the currently best algorithm for arbitrary simple polygons by Eppstein and Erickson [2]. "
"APFA may be useful when there is interest in understanding the dependence structure between the variables, and when this structure is expected to vary over the time interval of the study. The methodology assumes that the variables are measured at common times (or, in the case of genomic data, at common spatial positions) and so is not appropriate when times between transitions vary. It has proven to be well-suited to highly-structured, high-dimensional data such as DNA chip data, but we believe that it may be of more general interest and utility. Note that here, except for a brief mention in Section  6, we do not include explanatory variables in the models; for a way to do this see Edwards and Ankinakatte (in press, Section 8)."
"A set of discrete longitudinal data with  observations of discrete variables  can be represented as a tree in the following way. Starting with the root node, edges branch out to nodes at the first level or stage. (Previous papers Ron et al., 1998, Browning and Browning, 2007b and Edwards and Ankinakatte, in press use level. But since in statistics this term usually refers to the value of a discrete variable, we here choose to use stage.)"
"In Browning (2006) the same dissimilarity score is used but the algorithm is modified in two ways. In Ron et al. (1998) the order in which nodes within a stage are compared and possibly merged was unspecified. Instead, Browning (2006) describes a greedy approach in which dissimilarities between all pairs of nodes at the stage are computed, and the most similar pair  is merged. The scores are re-computed as necessary, and the process is repeated until the resulting nodes are pairwise dissimilar. The second modification is to allow the threshold to depend on the nodes counts  and , using"
"The results of applying the model selection procedure (using the AIC) to the Duroc SNP data set and the Biofam data set are shown in Fig. 4. In Fig. 4(a) it is seen that the APFA for the Duroc SNP data is highly structured, with relatively few, long blocks (particularly for the first 50 or 60 SNPs). This reflects the relatively low haplotype diversity in this region (Edwards, 2013). In contrast, the APFA for the Biofam data in Fig. 4(b) shows a more diffuse structure. In this context it is of interest to display the relative frequencies of the different life courses and this is done here by setting the width of the edges proportional to the edge counts. The red edges represent children staying with their parents. We observe that a large number of children live with their parents till the age of 20 and then the number gradually decreases. The number of those who left home (blue edges) increase correspondingly. The different parts of the plot show the life courses of those that left home without getting married (blue edges), got married (green edges), got divorced (purple edges) and got married, left home and had children (orange edges)."
"Fig. 5 compares the four model selection procedures in terms of rate of convergence to the true model as the sample size increases for the three data sets. The AIC-based procedure and that in Beagle with tuning parameters  and appear to have very similar convergence patterns. The BIC-based procedure is slightly slower, and Beagle with the settings suggested in Browning and Browning (2007a) ( and ) performs substantially worse than the alternatives, particularly at smaller sample sizes"
"where  and  are two APFA (defined on the same variable set), and  and  are the maximum-likelihood (ML) estimates of the probabilities of  under  and . In other words, it is the expectation of  under . This can be shown to non-negative, and equal to zero only when the distributions are identical. It is commonly used to compare non-nested models (Kent, 1986). It is not a true distance, since it is asymmetric in  and  and does not fulfil the triangle inequality."
"Automatic intelligent labelling of fMRI data is by no means a trivial task. Modelling of a fixed haemodynamic response function in the MRI literature [13,15] can be perceived as one attempt to address this issue, yet to the best of our knowledge no pure data-driven approach based on machine learning techniques exists"
"As it can be seen, a simple linear classifier (ldc) was able to achieve an average accuracy of 74.2%, not only vastly outperforming other tested classifiers, but also outperforming the Random Forest ensemble from [17] at the fraction of computations, yet still leaving room for improvement."
"On the other hand, the third and fourth TRs (between 5 and 10 s) appear to be the easiest to classify on average, which is also more or less consistent with the effect of the haemodynamic lag causing the blood oxygenation level to peak around 5 s after stimulus presentation [13]."
"In the future we plan to upgrade the tooling with optimizations for fast and massive slicing (Binkley et al., 2007) and to merge the clustering phase into the slicing to reduce the runtime significantly.Although the clustering and building the visualization data can take a long time for large projects, it is still useful because the clustering only needs to be done once. "
"Moreover, the analysis produces an is-in-the-slice-of relation and graph with even more edges. We have tried several clustering and visualization tools to cluster the is-in-the-slice-of graph for comparison, but most of the tools (such as Gephi Bastian et al., 2009) failed due to the large dataset. "
"Other tools such as CCVisu (Beyer, 2008) which were able to handle the large data set simply produced a blob as a visualization which was not at all useful. The underlying problem is that the is-in-the-slice-of graph is dense and no traditional clustering can handle such dense graphs."
"Various reviews of the literature on the evaluation of participative methods suggest that most of the justifications provided by researchers are based on personal reflections alone (Entwistle et al., 1999; Connell, 2001; Rowe and Frewer, 2004; Sieber, 2006; White, 2006). Clearly, many researchers are highly experienced, so their reflections should not be dismissed out of hand. Nevertheless, unless they think broadly and from different perspectives about the criteria they use to evaluate their participative interventions, they may miss evidence that does not fit their current thinking about what is important (Romm, 1996; Midgley, 2011)."
"We suggest that the use of systemic PSMs is relatively intensive compared with several of the other participative processes investigated by Beierle and Cayford (2002), so this gives us grounds to be cautiously optimistic. However, we cannot take this study as strong evidence because they did not specifically identify systemic PSMs as a category for comparison with other participative approaches."
"There may be socio-economic and ecological systems providing resources that can be used constructively by participants, or these systems may impose limits on what is achievable without incurring negative side-effects (Clayton and Radcliffe, 1996). Economic issues may point to concerns about social justice, which (if present) could influence people’s perceptions of the effects of systemic PSMs: i.e., the use of a particular method may be seen as supportive of just or unjust social relationships (Jackson, 1991, 2006), so it can be useful to look at the effects of socio-economic systems as part of boundary critique"
"While Rowe and Frewer (2004) say that an appropriate response is to set aside the purposes and preferred criteria of diverse stakeholders in favour of a single criterion of ‘acceptability of the method to all parties’, more nuanced findings will be generated by evaluating the method against multiple criteria of relevance to different stakeholders (Murphy-Berman et al., 2000)."
"Even when an academic researcher makes a significant effort to be responsive to stakeholders, there may still be mistrust stemming from expectations of divergent purposes (Adams and McCullough, 2003), and this may affect the evaluation of methods."
"Outcomes may also be longer term in nature, and these are not always predictable or easy to measure (Duignan and Casswell, 1989). "
"This is the approach taken by Bjärås et al. (1991) and Beierle and Konisky (2000). However, while it is useful to identify ‘common denominators’ and assess methods against these, this does not help in evaluating the unique attributes of methods that might make them complementary rather than competing."
"This is arguably one of the most significant limitations in terms of conducting longer-term research based on multiple case studies: it appears that, after around 20 years of relative stability in the number of systemic PSMs that are widely used in practice, systems/OR practitioners are now producing a new generation of methodologies and methods (Rosenhead and Mingers, 2004; Shaw et al., 2006; Franco et al., 2007), and it is important that the questionnaire does not go out of date."
"The authors of [9] provide a mitigation for this drawback. Their solution is rooted in deflation [21,22,15,14]. Unfortunately, deflation causes an undesirable increase in the size of the polynomial system and can be rather costly, particularly for solutions of high multiplicity."
"Generalizations of all three lemmas appear in Appendix A of [24] as consequences of an algebraic version of Sardâs Theorem. Indeed, Lemma 3.2 is proved as Theorem A.6.1 in [24]. Similarly, Lemma 3.4 is proven in more generality as Corollary A.4.19 of [24]."
"Unlike the model introduced by GonzÃ¡lez-Manteiga etÂ al. (2008b) with a common random effect for all the components of the target variable, the new models have multivariate vectors of random effects with the same dimension as the target variable and allowing for different correlation structures. "
Datta etÂ al. (2002) considered current population survey (CPS) estimates of median income of four-person families for states of the US for nine years (1981â1989) to produce estimates for the year 1989. They chose 1989 since the corresponding estimates were available from the census which allowed them to compare their hierarchical Bayes (HB) estimates with a few multivariate HB estimates. This comparison showed that the more complex multivariate HB estimators did not perform better than their univariate HB estimator. Datta etÂ al. (2002) conclusions were thus restricted to their case of study.
"Datta etÂ al. (2002) also recommended the use of univariate methods because they are more simple to implement. This is true. However, once the methods are implemented and available (for example, in R or SAS code), we do not find great usability differences. Our opinion is that univariate models (simpler models in general) are good enough if we have a good set of auxiliary variables. If this is not the case, then more complex models, that takes into account additional data relationships, might provide estimators with a sensible gain of precision.This paper introduces multivariate FayâHerriot models for estimating small area parameters. "
"Neurophysiological data about perceptual experiences like binocular rivalry illustrate this fact (e.g.,  Logothetis, 1998) and neural models have explained, and indeed predicted, key properties of these data (Grossberg, 1987; Grossberg, Yazdanbakhsh, Cao, & Swaminathan, 2008). In this regard, ART predicts that “all conscious states are resonant states”, but the converse statement that “all resonant states are conscious states” is not true. "
"Grossberg (1980) called the problem whereby the brain learns quickly and stably without catastrophically forgetting its past knowledge the stability–plasticity dilemma. ART was introduced to explain how brains solve the stability–plasticity dilemma. Since its introduction in Grossberg (1976a, 1976b), ART has been incrementally developed into a cognitive and neural theory of how the brain autonomously learns to attend, recognize, and predict objects and events in a changing world, without experiencing catastrophic forgetting."
"Spatial and orientational competition prevent this catastrophe from occurring by closing boundary gaps at line ends using end cuts (Grossberg & Mingolla, 1985). These competitive stages thus illustrate hierarchical resolution of uncertainty: They overcome the spatial uncertainty at line ends that is caused by using simple cell receptive fields."
" The spatial competition stage is sensitive to the length of lines, a sensitivity that helps to create end gaps and end cuts. Its cells are often called hypercomplex cells (Hubel & Wiesel, 1968). However, many boundaries would still remain incomplete if boundary processing stopped with hypercomplex cells. "
"This coarseness can be understood as the embodiment within bipole receptive fields of perceptual experiences with nearly collinear and aligned visual stimuli during cortical development (Grossberg & Swaminathan, 2004; Grossberg & Williamson, 2001). However, if all perceptual groupings remained fuzzy, visual perception would be significantly degraded. "
"All of these properties have been reported in experiments on working memory whose recordings were taken from lateral prefrontal cortex and the frontal eye fields (Lundqvist et al., 2016). The authors interpret their results in terms of working memory, and discuss the gamma oscillations in terms of “encoding/decoding events” and “re-activation of sensory information”, whereas beta oscillations are viewed as a “default state interrupted by encoding and decoding” (p. 152). This “default” interpretation does not explain why beta bursts are “interrupted by encoding and decoding” or why the gamma oscillations embody “encoding/decoding events”. "
"However, such extinction can be eliminated if the two events get grouped as a single object, even if the link between the two stimuli is amodally completed behind an occluder (Mattingley, Davis, & Driver, 1997). "
"For example, Herzog, Sayim, Chicherov, and Manassi (2015, p. 1)assert “that the spatial configuration across the entire visual field determines crowding. Only when one understands how all elements of a visual scene group with each other, can one determine crowding strength."
"fMRI data of Burr and Morrone (2011, p. 504) illustrate this subtlety: “We firstly report recent evidence from imaging studies in humans showing that many brain regions are tuned in spatiotopic [head-centered] coordinates, but only for items that are actively attended”. The current theory provides a clear mechanistic explanation of this otherwise potentially confusing assertion."
"Timbres have more complex spatio-temporal spectral distributions that can also include frequency sweeps, different onset and offset times, and different relative amplitudes in different frequency bands (Grey, 1977; McAdams, 2013). "
" A stream-shroud resonance is proposed to play a similar role in sustaining auditory spatial attention and conscious auditory quality, and problems with sustained auditory attention, say due to a parietal lesion, can cause unilateral auditory neglect (e.g.,  Robertson et al., 1997)."
"The position of auditory sources is computed from interaural time differences (ITD) and interaural level differences (e.g.,  Bronkhorst & Plomp, 1988; Darwin & Hukin, 1999; Rayleigh, 1907). As a result, auditory spatial attention in humans does not appear to have a map structure (Kong et al., 2012), but rather seems to be embodied by an opponent process whose neurons are tuned to (e.g.) ITDs (Magezi & Krumbholz, 2010). "
"Because of this shared design, it becomes easier to understand how language in young children can begin to develop in a way that parallels the motor behaviors of adult teachers during mutual play (Bruner, 1975), or how sign language by hearing adults can coordinate signing with speaking (Neville et al., 2002). Both of these activities need much more model development to be fully understood."
"At the time that this circuit was published (Cohen, Grossberg, & Stork, 1988), it was not yet understood how speaker normalization might occur, so these acoustic features were simply said to be “invariant”;—that is, speaker-normalized—at the model’s Invariant Feature Detectors level."
"For example, hypoactivity of amygdala or orbitofrontal cortex can prevent a cognitive–emotional resonance from occurring, thereby causing failures in Theory of Mind processes (Baron-Cohen, 1989; Perner, Frith, Leslie, & Leekam, 1989) in both autism and schizophrenia (Grossberg, 2000b; Grossberg & Seidman, 2006). Such failures include problems with activating motivationally directed goals and intentions."
"These theoretical results suggest that, contrary to Clark and Squire (1998), episodic memory may not be necessary to consciously experience emotions."
"Dennett’s highly cited book called Consciousness Explained (Dennett, 1991) argued against a Cartesian Theater model, a place in the brain where “it all comes together” and generates subjective judgments. Instead, Dennett advocated a Multiple Drafts model where discriminations are distributed in space and time across the brain, a concept that, without further elaboration, is too vague to have explanatory power."
"Perhaps the theory of Damasio (1999) comes closest to theoretically linking brain to mind by providing what is, in effect, a heuristic derivation of the CogEM model to explain his clinical data about cognitive–emotional interactions (Section  19.4). But this theory provided no mechanistic account, could therefore provide no data simulations, and did not situate this heuristic derivation within a larger theory of how brain resonances and consciousness may be linked."
"Datta et al. [11] showcase a demo for skill-based, cohesion-aware team formation that utilizes common citations for establishing a social network. The authors, however, remain silent on specific details about the actual algorithm to find an optimal team."
General research on the formation of groups in large scale social networks [17] helps to understand the involved dynamic aspects but does not provide the algorithms for identifying optimal team configurations
"A prominent example of a graph-based global importance metric is Google's page rank [34]. An extended version [35] yields total ranks by aggregating search-topic-specific ranks. Balog and De Rijke [36] extract a social profile from collaborations within intranets to find suitable experts. The Aardvark search engine by Horowitz and Kamvar [37] leverages social ties for expert finding in a user's extended network. Inspired by the page rank algorithm, Schall [38] applies interaction intensities and skills to rank humans in mixed service-oriented environments. These algorithms and frameworks provide additional means to determine person-centric metrics but do not address the team composition problem per se. The potential of expert finding applications, however, and subsequently the impact on team formation cannot be underestimated. "
"In highly connected networks, we risk having the recommendations overpower the direct interaction links. Especially social networks that lack a rich-club structure (see [18]) are prone to produce compositions of non-connected experts."
"However, investigations of the rich-club phenomenon in scientific collaboration networks (e.g., [19]) have shown that such tight collaborative groups exist only within particular research domains but not beyond. "
"Such comparison can be made by either a ratio or a difference, where amplification (smoothing) is indicated by a ratio larger (smaller) than one, or a difference greater (less) than zero (Cachon, Randall, & Schmidt, 2007). Due to data availability, some empiricists use alternatives such as production quantity, sales and shipments which are easier to observe than orders and demand (Blinder & Maccini, 1991). "
" Wang (2002) extracted data from 46 product items and pinpoints price variation as a contributing factor to the production smoothing phenomenon. However, incorporation of price and seasonal fluctuation does not always generate results in support of production smoothing (Miron & Zeldes, 1988). "
"Using U.S. industry-level data, Cachon et al. (2007) also found that bullwhip primarily appears in the wholesaler, rather than in the retailer or manufacturer, echelon. Dooley, Yan, Mohan, and Gopalakrishnan (2010) studied the bullwhip effect during the 2007–2009 recession and concluded that retailers responded to market changes rapidly and adaptively, whereas wholesalers responded late and drastically."
"Sterman (1989) understood the order volatility from the perspective of bounded rationality and sub-optimal decisions. By analysing Beer Game results he discovered that most participants tend to overlook the on-order inventory (the supply-line or work-in-process) when making replenishment decisions. This phenomenon repeatedly occurred in subsequent experimental studies. This underweighting does not improve when: the supply line is made visible (Wu & Catok, 2006); demand is known and stationary (Croson & Donohue, 2006); or even when demand is known and constant (Croson et al., 2014)."
" In this regard, both Sterman's (1989) and Lee et al. (1997) explanations are inadequate since the cost assumptions in both approaches inherently induce amplification. Hence a lot of questions remain open regarding the emergence of bullwhip in real supply chains"
"Sodhi and Tang (2011) considered an arborescent supply chain and calls for a need to remove structural complexity in order to reduce bullwhip. Chatfield (2013)challenged the opinion that multi-echelon system can be approximated by cascading two-echelon systems, a.k.a. the decomposition assumption. They found that such an assumption leads to underestimated bullwhip measures."
"Lee et al. (1997) suggested that price stabilization or everyday low price (EDLP) helps to mitigate this problem. This strategy has been implemented in several retail chains, such as ASDA and Walmart. However, the validity of this measure remains questionable."
Muhanmmad Irfan Ali discussed another view on reduction of parameters in soft sets [41]
"Contrary to our prediction is the finding that the quality of negotiation outcomes, in terms of contract balance (fairness) and joint utility (efficiency) is lower when negotiators are provided with the history graph compared to those provided with tables. The results indicate that negotiators provided with the history graph followed a non-compensatory strategy. Usually, non-compensatory strategies are used when decision makers face a vast amount of information and balance a strategy's accuracy against its cognitive effort [3,24]"
"When comparing the effects of different information levels provided by the two graphs, we find that negotiation behavior becomes tougher. If negotiators are provided with the utilities of their opponent, then the visualization of offer-ratings according to the preferences of both negotiators makes it impossible to outwit the counterpart. The high level of control of both negotiation partners may actually act as a barrier to deceive the partner. Therefore, negotiators use more hard and soft tactics to substantiate their own position. At the same time, the negotiation dance graph may act as an ex-post monitoring system. When users make a concession, they can easily see whether their counterparts reciprocate, and the dance graph reduces the risk of being exploited. We observe that negotiators provided with the negotiation dance graph offer more unconditional concessions. The effect of these differences in behavior is visible in the quality of outcomes: in contrast to the history graph, the negotiation dance graph facilitates efficient and fair agreements. Nevertheless, it does not make negotiators more satisfied. On the contrary, their holistic assessment of the negotiation outcome is significantly lower compared to the negotiators who have no access to utility values of their opponent. This can be explained by the tougher negotiation process visible through the increased use of hard tactics and by the fact that negotiators compare their individual outcome with the opponent's outcome. Even a small difference in utilities might lead to the feeling of being a loser instead of a winner (e.g. [17,37,77])."
"Itoh etÂ al. [24] proposed to overlay pie-like glyphs over the nodes in a graph to encode multiple categories. Each set is hence represented using disconnected regions that are linked by having the same colour. This causes difficulties with tasks that involve finding relations between sets such as T1, T3 and T4 "
"Riche and Dwyer introduced two Euler diagram-based techniques, ComEd and DupEd, designed to visualize sets using simple regions and to draw individual data elements as text-annotated nodes [35]. Unlike the methods described in detail above, Richeâs and Dwyerâs two methods lay out the network with regard to the set structure. ComEd represents each set as one or more rectangles connected with concave curves, assigned a unique colour. The use of rectangles and irregular curves could impose cognitive difficulties in perceiving the sets, as they might give the impression of different semantics [45]. Another issue with ComEd is the artifacts caused by overlaps between the curves that connect the rectangles. "
"In [25], each node belonged to only one group and, in addition, the groups were all disjoint. This meant that no set overlaps were present. By contrast, for our study the nodes belonged to multiple groups and there were richer relationships between the groups such as subset and intersection"
"Of particular note, though, is that our tasks differed significantly from those used in previous studies [3,25,28]. All of our questions required the participants to access information about both the sets and the network. The tasks used in [3,28] did not include any network or group-network tasks. Thus, our results suggest that, as the complexity of the task increases, the difference in effectiveness of Bubble Sets, KelpFusion and LineSets becomes insignificant."
"They argue that their results show that perception is a top-down process, in contrast to the Ecological bottom-up process, where the readers recognize the genres through the attributes of the layout which forms the basis of document recognition (or perception for recognition), and although Toms and Campbell, like Lakoff (1987), refer to the bottom-up process and suggest that genres may “act as a single gestalt” Toms & Campbell (1999a, p. 2015) they do not explore other possibilities, such as perception for action and how a genre is perceived when the document is displayed to a reader (in all fairness Watt (2009, chap. 8) also fails to explore the perception for recognition concept)."
"The scanpath mirrors clearly the unfolding of visual attention over time and indicates which features or contents in a visual context are attended (Coco, 2009). The movement represented by these scanpaths are not random, rather they reflect the viewer’s frame of mind, expectations and purpose (Yarbus, 1967). "
"The techniques were detected using the methodology employed in Campbell & Maglio (2001, p. 3) and Buscher, Dengel, van Elst, and Mittag (2008), with some modifications. These two papers reported on the detection of skimming and reading techniques, not skimming and scanning techniques. "
"Ulf Grenander was working on abstract mathematical
and statistical models and methods for many computer vision problems
and presented his findings in books that were not easily understood by
mainstream computer vision researchers. As a result, Prof. Grenander's
work was seen as esoteric. David Cooper was also vigorously pursuing
Bayesian methods for boundary and object recognition [4]."
"Firstly, there has been some considerable work on optimizing resource usage while keeping QoS goals. These papers, however, concentrate on specific subsystems of Large Scale Distributed Systems, such asÂ [11] on the performance of memory systems, or only deal with one or two specific SLA parameters. "
Petrucci etÂ al.Â [12] or Bichler etÂ al.Â [13] investigate one general resource constraint and Khanna etÂ al.Â [14] only focuses on response time and throughput.
"A quite similar approach to our concept is provided by the Sandpiper frameworkÂ [15], which offers black-box and gray-box resource management for VMs. Contrary to our approach, though, it plans reactions just after violations have occurred."
"Also the VCONF model by Rao etÂ al.Â [16] has similar goals as presented in SectionÂ 1, but depends on specific parameters, can only execute one action per iteration and it neglects the energy consumption of executed actions. "
 Hoyer etÂ al.Â [20] also undertake a speculative approach as in our work by overbooking PM resources. 
" [27] viewed the system in four layers (i.e.,Â business, system, network and device) and broke down the SLA into relevant information for each layer, which had the responsibility of allocating required resources. Again, no details on how to achieve this have been given"
"Thirdly, commercial Cloud IaaS platforms such as Amazon EC2Â [29], RackspaceÂ [30] or RightScaleÂ [31] have a very limited choice of preconfigured and static VM resource provisioning types."
"MonitoringCurrent monitoring systems (e.g.,Â gangliaÂ [35]) facilitate monitoring only of low-level systems resources, such as free_disk or packets_sent, but SLA parameters typically are, e.g.,Â storage and outgoing bandwidth."
"As opposed to the CBR approach inÂ [7], the rule-based approach is able to fire more than one action at the same iteration, which inherently increases the flexibility of the system."
" In this framework, a very interesting method has been developed by Evgeniou and Pontil in [10]. They present a preprocessing algorithm that computes clusters of points in each class, based on Euclidean distance, and substitute each cluster with the mass center of the points in the cluster. The algorithm tends to produce large (small) clusters of data points which are far (near) from the boundary between the two classes. These strategies did not focus on both large and imbalanced data learning."
" In [8] the authors proposed a novel classification approach for large data sets using Minimum Enclosing Ball (MEB) clustering: after partitioning the training data via MEB method, the centers of the clusters were used for the first time SVM classification; the algorithm used only the clusters whose centers are support vectors or those clusters which have different classes to perform the second time SVM classification. In this way many data points were recursively removed. However, the above mentioned methods are not helpful for classification of large data sets with imbalanced classes."
"There have been many works in literature that apply different techniques to the SVM framework in order to overcome problems due to imbalance. Most of them assign different error costs to different classes in order to shift the decision boundary and to guarantee that it is better defined [1,12]. Another major category of kernel-based learning research efforts focuses more concretely on the mechanics of the SVM itself; this group of methods is often called kernel modification methods [12]. However these methods could be useless for large data set, because they use all the data for training the classifier. The undersampling techniques are useful for large training set. "
" In [30] the Granular Support Vector Machines - Repetitive Undersampling algorithm (GSVM-RU) was proposed to integrate SVM learning with undersampling methods. This method uses the SVM itself as a mechanism for undersampling in order to sequentially develop multiple subsets with different informative samples, which are later combined to develop a final SVM for classification. Also this method is not tailored for very huge data sets, because the SVM problem should be hard to solve due to the training set size. "
"Although a similar technique was successfully used by Qamar and Sanghi [19], the approach does not appear to have been widely adopted as an alternative to projection techniques. A possible limitation of the method is that it can rapidly become expensive as the number of parameters of interest increases. "
As well as the previously discussed problems with doing this it is also known that a particular set of snapshots may result in POD modes which are only appropriate in a limited parameter space. For example Lieu et al. [15] applied Galerkin projection to a complete F-16 aircraft model which depended only on the angle of attack and free-stream Mach number Mâ
" To avoid this problem and others introduced by projection methods the strategy adopted in this work is to treat POD as a basis for interpolation.POD coefficient interpolation type techniques, as introduced by Ly and Tran [11], have not been widely adopted as an alternative to projection type techniques. "
"It is known that the accuracy of the RBF approximation depends upon the value that is adopted for the parameter c [22]. Rippa [23] introduced an algorithm to find the optimum value for a particular data set but, for all the cases considered here, the additional expense incurred in performing an analysis of this type did not prove to be justified. Instead, the value of c is simply calculated to be the mean distance between the data points."
"In contrast to Sivaraman et al. (2016); Mittal et al. (2016), our research findings show that a chain of NFs requires a global sched-uler to make chain-level decisions, rather than an internal sched-uler that executes local switch policies. To address this gap, we de-signed and implemented the Service Chain Coordinator (SCC)."
