"When HTC introduced the Windows-based smartphone, it recruited 1000 T-Mobile or AT&T customers to write product reviews and Facebook and Twitter posts, reaching more than 234,000 consumers and significantly increasing the brand awareness [12]. When Dunkin’ Donuts launched Latte Lite, it used 3000 consumers to spread the word about the new beverage, reaching 111,272 consumers over twelve weeks and increasing sales by 26% in test markets [13]. Both examples illustrate that electronic or online word of mouth (eWOM) has become an important factor in consumer buying decisions [37]. "
"Consumers trust eWOM more than advertisements, as they regard their peers as more reliable than companies [65]. "
"Although eWOM is implemented by consumers, companies can initiate eWOM campaigns for marketing communications [35]. "
"To launch an effective eWOM campaign, companies need to identify a small number of disseminators known as opinion leaders who exert personal influence upon other people [68]. "
"Identification of opinion leaders relies on the “two-step flow of communications” theory: as senders, opinion leaders cultivate their knowledge from a variety of sources including mass media in the first step, and then spread their opinions (messages) to the general public (receivers) via WOM in the second step [47]. Thus, sender, message, and receiver are key components in the WOM process [6,20], and provide three important bases for searching for opinion leaders."
"However, a survey may capture self-confidence rather than opinion leadership for two reasons [61,68]."
"Opinion leader WOM has long been used to promote products or to criticize competitors’ offerings [44,47]; its positive impact on new product introduction was first reported by Arndt [3]. "
"One of the most important capabilities of the Internet is interactive communication at a larger scale: “for the first time in human history, individuals can make their personal thoughts, reactions, and opinions easily accessible to the global community of Internet users”, and the interactive communication provides an online feedback mechanism to serve multiple functions, including brand building and customer acquisition, product development and quality control, and supply chain quality assurance [26]."
Electronic commerce performs better than the traditional market in acquiring customers [77]. 
"To be effective in viral marketing campaigns, companies must identify opinion leaders properly and then let them communicate information to their followers [43]."
Opinion leaders are consumers who provide information to others that influences their consumption decisions [22] by obtaining key information through research and shaping their own opinions earlier than the general public.
"Opinion leaders in women’s fashion, for example, acquire fashion knowledge from fashion magazines first and then spread it to followers via WOM [70]."
Rogers and Cartano [68] noted that sender-based surveys are “dependent upon the accuracy with which respondents [senders] can assess and report their self-images on opinion leadership”.
"The network structure method has been widely used by marketers and network analysis researchers [41,43]. Network analysis determines opinion leaders by identifying those who connect with many people (i.e., hubs) and those who connect two clusters of densely connected people (i.e., bridges) in a social network [41]. "
"Since the advent of the Internet in the 1990s, WOM is no longer restricted to personal influence networks because the Internet allows one to reach strangers at a larger scale [26]."
Godes & Mayzlin [35] adopted the King and Summers scale to measure how many followers an opinion leader reaches.
"Buzz is generated around a product when a large number of followers receive eWOM [11,31]. Previous studies suggest that opinion leaders are progressive attention-seekers [70] and fulfill their self-enhancement motivation via buzz creation [33]. "
"Trust is an important issue in electronic commerce and eWOM studies (e.g., [26,80]), as it is one of the main reasons for followers to seek advice from opinion leaders."
"An indirect approach to measure trustworthiness is analyzing the structural, lexical, and semantic aspects of eWOM, which are found to be associated with trustworthiness [10]. "
"The amount of eWOM influences consumers in two ways: eWOM increases exposure to a product and therefore increases consumer awareness of its existence [54]; and a large amount of eWOM suggests a product’s popularity [17,79]. Previous studies reveal that the amount of eWOM created by the general public drives sales [21,27,29,54]."
"Consumers communicate their satisfaction using online user ratings [18,71]. Positive ratings created by the general public can improve consumer attitude, while negative ratings created by the general public can worsen consumer attitude [54]. Customer satisfaction among the general public has been found to have a positive impact on future sales [4,78]."
"They are also motivated to contribute their knowledge to other consumers [19,39]."
" According to Childers [22], opinion leadership is product category specific. The more a consumer purchases and consumes within the same product category, the more likely the consumer is to acquire complex category knowledge. "
"Our dataset contains a sample of 350,122 book, music, video and DVD titles, which, as experience goods, have qualities difficult to ascertain before consumption, making user reviews helpful for consumers [60,63]. "
The overlap between different types of opinion leaders is consistent with extant literature [43].
"Previous research suggests that an opinion leader needs to have both knowledge and influence [47,57]. Since opinion leaders have different breadths of product category knowledge, it is important to examine which kinds of opinion leaders can be more effective in driving sales [34]. Knowledge and influence, the two components of opinion leadership, are not independent. "
"Katz and Lazarsfeld [47] argue that personal influence does not flow from highly interested individuals to less interested individuals, but rather between those with shared interests. Therefore, if general knowledge implies general interest, an opinion leader with broad product category knowledge will attract more followers [58]."
"Fashion industry companies use celebrities such as Madonna as opinion leaders [76], while the pharmaceutical industry uses physicians on editorial boards/scientific committees and with prestigious academic appointments [32,38,49]. When consumers are not celebrities or experts, practitioners use sender-based and receiver-based methods."
"For example, the eWOM company BzzAgent measures engagement as the number of likes, comments, and retweets a consumer’s post receives [9]. The opinion leaders identified through this process are similar to the buzz-generating opinion leaders in our paper."
"We recommend that companies implement online user review systems using Amazon’s patented design [8], and identify the top 1% of communicative, buzz-generating, and trustworthy eWOM opinion leaders among users by measuring eWOM volume, feedback received, and helpful votes received. "
This article presents a new opinion leader identification method rooted in the interpersonal communication theory that inspired sender- and receiver-based methods [47]. 
"By demonstrating how to identify opinion leaders from a large number of consumers, this article shows that companies can collect useful business intelligence from increasingly large amounts of data available to them—a central theme emerging in big data analytics [16]."
"Wireless Mesh Sensor Networks (WMSNs) comprise a technological field that has arisen as the natural evolution from the conventional Wireless Sensor Networks (WSNs), composed of only a few nodes, to large-scale networks in which sources and destinations are interconnected through different paths of intermediate nodes forming a mesh layoutÂ  [1]."
"To this end, new mesh capabilities such as multi-hop mesh routing, scalability, robustness, reliability, self-organization or energy efficiency must be satisfiedÂ  [2]. "
" To achieve this purpose, ASES integrates, in a single solution, an efficient duty-cycle strategy for saving energy along with most of the above mesh capabilities. This fact makes IEEE 802.15.5 standard and its ASES mechanism advantageous with respect to other WMSN approachesÂ  [2], such as Zigbee ProÂ  [3], the International Engineering Task Force (IETF) with its solution IPv6 over Low power Wireless Personal Area Networks (6LoWPAN)Â  [4], WirelessHARTÂ  [5] or ISA SP100.11aÂ  [6]."
" Second, physical-space constraints (e.g.,Â obstacles such as walls, and pillars)Â  [38,39] impose strict restrictions in the application management, such as the impossibility of using relay/sink mobility to improve the network performance. Similarly, Assumption (ii) copes with the premises of data monitoring in the aforementioned domain-driven applications, where physical sensing information is collected and reported periodically. "
"Despite these known problems related to large-scale agile, there is an industry trend towards adopting agile methodologies in-the-large ( VersionOne,Inc, 2016; Paasivaara et al., 2013, 2014; Dings yr and Moe, 2014)."
"According to the latest survey ( VersionOne, Inc, 2016), 62% of the almost 4000 respondents had more than a hundred people in their software organization and 43% of all the respondents worked in development organizations where more than half of the teams were agile"
"However, this indicates that there seems to exist a large number of companies that have taken or are taking agile into use in large-scale settings ( VersionOne, Inc,2016)."
"In two recent workshops on large-scale agile development organized in XP2013 and XP2014 conferences, adoption of agile methods was one of the highlighted themes needing more research ( Dings yr and Moe,2013,2014)."
"Traditional methods focus on up-front planning and strict management of change, but agile methods were designed to accept and efficiently manage change ( Highsmith and Cockburn, 2001; Cockburn and Highsmith, 2001)."
"Agile methods have been both criticized and advocated, and research has shown that accommodating change may be a factor in both success and failure ( Boehm, 2002)"
"It has been shown that agile methods have improved satisfaction of both customers and developers, but on the other hand there is evidence that agile methods may not be a good fit for large undertakings ( Dyb  and Dings yr, 2009)"
"For example, one paper would highlight the viewpoint of the developers ( Fry and Greene, 2007), and another would consider the transformation from the user experience designers point of view ( Federoff and Courage, 2009)."
"Surveys on challenges and success factors for agile projects in general have been conducted, eg( Chow and Cao, 2008)"
" The traditional methods for estimating PDFs, e.g. binning methods and kernel density techniques, require specification of a bandwidth parameter that heavily influences the shape of the resulting PDF (Silverman, 1986; Wilks, 2006; Srihera and Stute, 2011). "
"While methods exist for estimating an optimal bandwidth, these methods usually require some assumption about the shape of the underlying PDF (Wand and Jones, 1995; Srihera and Stute, 2011; Bernacchia and Pigolotti, 2011). Given that our application requires an unbiased determination of the normality of the velocity increments, estimation methods utilizing such assumptions would not be suitable for our analysis. "
"Kernel density estimation is a widely used method for estimating the probability distribution function (PDF) of a given dataset (e.g., Silverman, 1986, Wilks, 2006), in which the PDF is approximated as a normalized sum of kernel functions K(Ï) centered on each data point Ïj"
"The convolution part of this algorithm requires íª(Nâq) calculations and the FFT portion requires íª(MâlogM) (Cooley and Tukey, 1965). Simple algebraic manipulation can show that if q<M and logMâªN, then Nâq+MâlogM<NâM, and so the nuFFT is theoretically faster than the direct DFT calculation"
"We also note that we implemented the selective frequency filter, In, in a slightly different manner than Bernacchia and Pigolotti (2011). They show that the self-consistent density estimate converges on the true density provided the filter In is set to 1 for some subset of the frequencies for which C is above the estimate stability threshold given by |Cn|2â¥4â(Nâ1)âNâ2 and set to 0 for all other frequencies. Whereas they choose the subset based on a frequency cut-off tâ such that C is above the stability threshold for half of the frequencies within [âtâ,tâ], we choose a cut-off frequency based on the occurrence of three consecutive C values below the stability threshold. In our implementation In=0 for all n>nâ, where nâ is the index of the lowest frequency for which Cnâ+1,Cnâ+2, and Cnâ+3 are below the stability threshold. We choose this criterion because it is fast to implement and we find that it avoids an occasional, spurious leakage of high-frequency components that manifests as high-frequency waves superimposed on the density estimate."
" We use a version of CAM4 that includes the Model for Prediction Across Scales atmospheric (MPAS-A) dynamical core, which predicts the evolution of the atmosphere by evaluating conservation laws (e.g., conservation of mass, momentum, etc.) on a centroidal Voronoi tessellation of the sphere (Rauscher etÂ al., 2013; Skamarock etÂ al., 2012)."
"As every software artifact, also software models ( B zivin,2005) are subject to continuous evolution"
"Knowing the operations applied between two successive versions of a model is not only crucial for helping developers to efficiently understand the model's evolution ( Koegel et al., 2010), but it is also a major prerequisite for model management tasks, such as model co-evolution ( Herrmannsdoerfer et al., 2009; Mens,2008) and model versioning ( Brosch et al., 2010; Koegel et al., 2010)"
"The second category comprises composite operations ( Suny et al.,2001) consisting of a set of cohesive atomic operations, which are applied within one transaction to achieve one common goal"
The most prominent class of such composite operations are refactorings introduced by Opdyke (1992)
"As reported in Herrmannsdoerfer et al.(2009), Mens (2008), Brosch et al. (2010) and Koegel et al.(2010), the detection of applied refactorings is a crucial prerequisite for automating model management tasks"
"However, composite operations are not limited to refactorings; they may be used to implement any kind of in-place model transformation for a specific purpose, such as model completion ( Sen et al.,2010), refinement ( Ruhroth and Wehrheim, 2012), and evolution ( Meyers and Vangheluwe, 2011)."
"One way to acquire the set of applied composite operations is to use operation recording ( Herrmannsdoerfer and K gel, 2010; Lippe and Oosterom, 1992); that is, the execution of operations is tracked within the modeling environment while they are performed"
"A set of manually applied atomic operations, having together the intent of a composite operation, which is indeed frequently happening in practice ( Murphy-Hill et al.,2009), cannot be identified, because no explicit command has been issued in the modeling environment"
"In the absence of an operation log, the applied operations have to be detected a posteriori using state-based model comparison approaches using either generic model comparison algorithms ( Brun and Pierantonio, 2008; Kelter et al., 2005; Lin et al., 2007; Schmidt and Gloetzner, 2008) or language-specific comparison algorithms ( Kolovos, 2009; Xing and Stroulia, 2005)"
"In an endeavor to establish a commonly accepted conceptual framework for the rapidly growing number of domain-specific modeling environments, the Object Management Group (OMG) released the specification for Model Driven Architecture (MDA) ( Object Management Group, 2005), standardizing the definition and usage of (meta-)metamodels resulting in a metamodeling stack as depicted in Fig1"
Hartung et al.(2010) present an approach for generating so called semantically enriched evolution mappings between two versions of an ontology
"For limiting the search space of combinable composite operations, we may use the critical pair analysis comparable to how it has been done in Mens (2006)."
"Modern trends in manufacturing are defined by mass customization, small lot sizes, high variability of product types, and a changing product portfolio during the lifecycle of an automated production system (aPS) (Lüder et al., 2005; Rzevski, 2003). These trends imply more complex aPS (Mcfarlane and Bussmann, 2000), which support changes in the physical layout of the aPS including extensive technical updates."
"Since the proportion of system functionality that is realized by software is increasing (Thramboulidis, 2010), concepts for supporting automation engineers in handling this complexity are strongly required. "
"Software as well as software engineering in this domain need to fulfill specific requirements, e.g. regarding real-time and reliability (Vogel-Heuser et al., 2014a, 2014b, 2014c). "
"aPS are comprised of mechanical parts, electrical and electronic parts (automation hardware) and software, all closely interwoven, and thus represent a special class of mechatronic systems (Bonfè and Fantuzzi, 2003; Rzevski, 2003) and consist of mechatronic sub-systems like sensors and actuators."
 Lehman (1980) defined laws of software evolution and – among others – identified that systems are subject to dynamics causing continuing changes of software resulting into increasing complexity.
" Lehman (1980) defined laws of software evolution and – among others – identified that systems are subject to dynamics causing continuing changes of software resulting into increasing complexity. Evolution might be triggered in each phase of an aPS's life. Hence, due to their evolution and the long-living character of aPs, their life is characterized as a cycle (Birkhofer et al., 2010). "
" Software maintainability is “the ease with which a software system can be modified to correct faults, improve performance or other attributes or adapt to change environments” (IEC, 1990)."
"Accordingly, maintenance can either involve repair or modification actions, which in turn can be adjustments to the environment (referred to as adaptive maintenance) or augmentation of a system's function (Avizienis et al., 2004). "
"In the context of maintainability, obsolescence management, i.e., managing, mitigating and resolving the impact of (sub-)component obsolescence, is one important issue regarding long-living systems (ISO/IEC 25010, 2011)."
 An approach to explicitly modeling changes of aPS' physical structures and for analyzing their effects on the system's functions is proposed in Göring and Fay (2013) alongwith the physical causes of these changes.
"According to Wiendahl et al. (2007), changeability “is defined as characteristics to accomplish early and foresighted adjustments of the factory's structure and processes”. In contrast, reconfigurability is defined as the “ability of a manufacturing or assembly system to switch with minimal effort and delay to a particular family of workpieces or subassemblies through the addition or removal of functional elements” (Wiendahl et al., 2007)."
"Drivers for evolution are manifold (Westkämper, 2003), and basically result in changes of requirements on the aPS (Legat et al., 2013)."
"aPS are typically designed-to-order, i.e. they are unique systems, which are designed and implemented once a customer has awarded a contract to an aPS supplier (Birkhofer et al., 2010)"
"The specification and implementation is carried out in the form of a project (VDI/VDE 3695 Part 2, 2010)."
"The development of these (partial) solutions follows a typical product development workflow (cp. upper part of Fig. 1), as described e.g. in VDI/VDE 2206 (2004), and is decoupled from the projects on the timeline. "
"The categorization is based on and extends the one introduced in Ladiges et al. (2013), Vogel-Heuser et al. (2014c) and (2014d)."
" Anticipated software changes in accordance to Buckley et al. (2005) are software changes, which “can be foreseen during the initial development of the systems and, as such, can be accommodated in the design decision taken”. "
"Unanticipated software changes according to Buckley et al. (2005) are not foreseen during the development phase, but they are frequently undertaken at short notice during commissioning and operation in order to, e.g., clear defects in other disciplines, such as an unexpected behavior of the mechanics. "
"The PPU performs a manufacturing (discrete) process and handles, stamps and sorts different kinds of workpieces (Fig. 3) (Vogel-Heuser et al., 2014d)."
"Regarding Buckley et al.'s (2005) criteria temporal change, also history of change is introduced with the characteristic sequential and parallel synchronous and parallel asynchronous changes."
" Accordingly, the development of requirements on a production plant is an integrated part of the production design (Blanchard, 2004; Buede, 2000). In the development process model described in (VDI/VDE 2206, 2004) (cp. Section 2) for the design of mechatronic systems, such as production plants, the phases of identification and documentation of requirements are explicitly foreseen at the beginning of the process. "
"Similarly, other common development process models like the spiral model (Sage and Rouse, 2009) (for software-intensive systems) or the waterfall model include requirements elicitation and specification as a (repetitive) action during engineering. "
" Quality requirements, also called non-functional requirements or extra-functional requirements, are usually more general and include, among others, performance requirements expressed in Key Performance Indicators (KPIs) (see e.g. ISO 22400-2, 2012), flexibility requirements, reliability and availability requirements, safety and security requirements, and maintainability requirements (ISO/IEC 25010, 2011)."
"Unfortunately, the adaptation of the formal specification is often omitted, especially when changes need to be implemented within a short time window and during operation (Vogel-Heuser et al., 2014a)."
"The main requirements to be considered are reliability, performance efficiency, compatibility, maintainability and portability (Frank et al., 2011)."
"General sources of requirements are usually stakeholders, documents, and systems in operation (Pohl and Rupp, 2011). "
" Haubeck et al. (2013), for example, derive four stages in the requirement hierarchy of a single system. "
"Similar hierarchies can be found in the other literatures, like Blanchard (2004) and IEEE-Standard 1220 (1999). However, the hierarchy presented by Haubeck also refers to the process of formalizing requirements."
"Formalization may be a first refinement of the informal requirements (VDI/VDE 3694, 2008). However, informal requirements sometimes need to be defined on several or each hierarchical level (IEEE-Standard 1233, 1998)."
"Formalization usually results in a set of models, and the variety of used models is huge, and the choice of the right model type depends on the domain, industry branch, as well as the specific plant to be designed (VDI/VDE 3681, 2005). The right model choice also depends on the degree of granularity and the current refinement stage."
"These properties can be operationalized in a way that they are measurable by the available online measurements (Ladiges et al., 2013). "
"Feldmann et al. (2014b) presented a light weight notational approach (cp. 3.3) to model requirements for testing and evaluated it with experts from industry showing that the modeling approach provides the means to systematically, comprehensively and efficiently specify mechatronic sub-systems as sensors and actuators. "
"To increase the quality of requirements Rösch et al. (2015) and Teufl and Hackenberg (2015) introduced description of behavior based on MSCs included in the MIRA (Broy and Stølen, 2001) approach and UML SD."
"An important part of the system design is the definition of components and interfaces, which in software engineering is called the architectural configuration (Barais et al., 2008; Medvidovic and Taylor, 2000). During evolution, the engineers working on the different parts of the architecture will evolve their respective system parts."
"The complete system design covers different aspects (Goldschmidt et al., 2012). "
" In a multiple-case embedded system case study in 5 large software companies (Martini et al., 2014), different factors for architectural technical debt have been identified, like pressure to deliver, prioritization of features over products, non-complete refactoring, or technology evolution."
"CONSENS (Anacker et al., 2011) – CONceptual design Specification technique for ENgineering of complex Systems – is a method and specification language that targets the overall, discipline independent system design. "
"The MechatronicUML (Heinzemann et al., 2013) is a modeling language and process for the engineering of mechatronic systems. It is based on a rigorous specification of structure and behavior based on a refinement of the Unified Modeling Language (UML) (Object Management Group, 2011). "
"SysML4Mechatronics (Kernschmidt and Vogel-Heuser, 2013) is a language for interdisciplinary modeling, which addresses mechanical, electrical/electronic and software aspects in product automation and for aPS explicitly."
 Shah et al. (2010) present a multi-discipline modeling framework based on SysML. 
"A formal modeling framework for verifying aPS' engineering models has been proposed in (Hackenberg et al., 2014). The formal models contain the necessary aspects for verifying the system's correctness after evolution changes."
"ne major language for the definition of consistency constraints is the Object Constraint Language OCL (Object Management Group, 2014), which allows for the specification of constraints based on first-order predicate logic. "
 A rule based approach to identify structural inconsistencies during evolution of aPS is presented in Strube et al. (2011).
"Dynamic semantics is concerned with behavior of the model. Here, approaches can be used, for example, to compare whether two finite automata are consistent by identifying whether one simulates the other or both simulate each other (bisimulation) (Clarke et al., 1999). "
"More complex relations (called refinement notions) between automata exist, which can be used to define and check various degrees of consistency (Baier and Katoen, 2008; Heinzemann and Henkler, 2011; Jensen et al., 2000; Weise and Lenzkes, 1997)."
" In the area of model transformations, Bidirectional Model Transformations like JTL (Cicchetti et al., 2010) and Triple Graph Grammars (TGG) (Hermann et al., 2014; Hildebrandt et al., 2013; Schürr, 1995) have been developed to transform between models. "
"Transformation languages that support incremental change propagation (Giese and Wagner, 2009) are particularly interesting as they enable to only propagate single changes from the source, i.e., they change only those parts of the target model which are affected by the change in the source. "
" Similarly, Song et al. (2013) propose a compositional approach for the probability reachability analysis of Discrete Time Markov Chains that decomposes the system into strongly connected components or even parts of them, analyze them using Gauss–Jordan Elimination (Althoen and McLaughlin, 1987), and afterwards use value iteration to compute the result for the complete model based on the individually analyzed parts."
"One specific type of architectural technical debt (Martini et al., 2014) is non-compliance between architectural guidelines and the system architecture. "
"Architectural guidelines, patterns and styles have been presented in Buschmann et al. (1996). "
" specific architectural pattern for embedded systems is the Operator Controller Module (OCM) (Burmester et al., 2008), which defines concrete layers and interfaces between them for different parts of the embedded software – feedback controllers, hard real-time communication and reconfiguration of the feedback controllers, and a soft real-time layer."
Herold et al. (2013) present an approach to automatically check the conformance of the architecture against guidelines and architectural styles. 
A complementary approach has been proposed in Herold and Mair (2014)which uses a meta-heuristic to efficiently search for violations of architectural compliance rules and to propose a sequence of repair actions to remove the identified violations automatically.
"Since the proportion of system functionality that is realized by software is increasing (Thramboulidis, 2010), concepts for supporting automation engineers in handling this complexity are required."
"“Reusable artefacts are mostly fine-grained and have to be modified on different positions, so that errors are probable and important reuse potential is wasted” (Maga et al., 2011). Reuse is mostly achieved through copy, paste and modification actions (Katzke et al., 2004). Feldmann et al. (2012) identified as reasons for this situation: the multitude of disciplines involved (such as software engineering, automation engineering, mechanical engineering, electrical engineering, safety engineering, perhaps also chemical engineering), and the interdependencies of software modules with mechanical and electrical modules (Jazdi et al., 2011). "
"But also in traditional software engineering, the main technique to derive adapted or improved implementation versions of software functionality is clone and own (Ray and Kim, 2012). The main idea of clone and own is to copy existing code and modify the copy until the desired functionality is realized."
"Code clones have been known for more than two decades as serious flaw that may impede evolution. For instance, Juergens et al. (2009) have shown that code clones are more prone to introduce errors. Moreover, Harder and Tiarks (2012) have shown, that error-correcting tasks tend to be incorrect in the presence of clones. "
"Beyond code clones, a broader notion of code smells has been introduced (Fowler, 1999) and extended by the notion of anti-patterns (Brown et al., 1998). "
" Particularly, Abbes et al. (2011) have shown that the presence of two anti-patterns impedes the performance of developers."
"While other studies show that perception of code smells may differ between developers (Yamashita et al., 2012), it is generally admitted that code smells hinder evolution because they impede extending and maintaining the underlying system."
"Design patterns such as those proposed by Gamma et al. (1994), are means in classical software engineering to support code modularity and evolution. "
" Amaptzoglou et al. (2011) reveal that using design patterns increases reusability, and thus, supports the evolution of software systems. However, other studies also indicate that design pattern may also impede evolution and maintenance and have to be applied properly (Khomh et al., 2008, 2009)."
Fuchs et al. (2014) conducted a detailed analysis of IEC 61131-3 code from machine manufacturing industry and introduced an analysis and visualization approach
 Feldmann et al. (2012) and Fuchs et al. (2012) analyzed and refactored the software structure on IEC code level in an industrial case study in a world market leading plant manufacturing company.
"For IEC 61499-based applications (IEC, 2011), common solutions and guidelines were proposed for hierarchical automation solutions (Zoitl and Prähofer, 2013), failure management (Serna et al., 2010) and portable automation projects (Dubinin and Vyatkin, 2012)."
"Even software design patterns for IEC 61499 programs, e.g., Distributed Application, Proxy and Model-View-Controller, were defined (Christensen, 2000) and evaluated (Strömman et al., 2005). However, although IEC 61499 runtimes on state of the art controllers exist (Vyatkin, 2011), “IEC 61499 has a long way in order to be seriously considered by the industry” (Thramboulidis, 2013)."
" As such, refactoring has been established and numerous studies have demonstrated its usefulness (Fanta and Raijlich, 1999; Kegel and Steinmann, 2008). However, certain challenges such as proper tool support and side-effect freeness of refactorings are still open research questions."
"The interdisciplinary dependencies as well as the complexity of aPS lead to the risk of unpredictable side effects of evolution in the resulting system (Jaeger et al., 2011). A detection of these side effects becomes necessary and should be carried out automatically to avoid much effort, as explained in Braun et al. (2012)"
"Management of aPS evolution has to deal with diverging evolution cycles of the involved disciplines (Li et al., 2012). "
"Because of the high complexity of the automation software and the plant itself, it is usually not obvious how the evolution in one part of the system affects other parts or the whole process (Jaeger et al., 2011). "
"Instead of analyzing the state space of a model within a certain time frame, formal techniques aim at analyzing models exhaustively with respect to all reachable states (Bérard et al., 2001; Clarke et al., 1999). For example, Lahtinen et al. (2012) state that in the nuclear engineering domain, automatic verification is beneficial compared to simulation because it can – besides exploring all reachable states – be applied earlier in the design phase."
Hametner et al. (2010) and Hussain and Frey (2006) identify useful diagrams for modeling and deriving test cases from UML for the field of automation software development and especially for IEC 61499 implementations.
" Interaction diagrams are recommended for the extraction of test sequences. In Hussain and Frey (2006), the extraction of test sequences from state charts using round-trip path coverage is shown."
Hametner et al. (2011) show a first implementation of the recommended test case generation process using state chart diagrams especially for IEC 61499 applications. Hametner et al. (2010) also mention the timing diagram of the UML as a useful diagram for test case generation but no implementation is shown in their work. 
"Rösch et al. (2014) realize this test case generation, but focus especially on testing machine's reaction to faults by using fault injection."
" In Kumar et al. (2011), UML test case generation approaches from state charts are combined with the aim of making them executable by mapping them to the Testing and Control Notation (TTCN-3). "
"In order to integrate the requirements and test specification, a template for test cases has been developed by Feldmann et al. (2014b). "
Ulewicz et al. (2014) present a first approach for selecting and reusing existing test cases based on changes within the control program for efficient testing of changes.
"Nevertheless, in Fuchs et al. (2014) and Prähofer et al. (2012) the benefits of static code analysis for IEC 61131-3 software quality improvement are highlighted and an approach for improving compliance to programming conventions and guidelines is proposed,"
"The compositional approach supports multiple refinement notions to guarantee different types of system behavior (Heinzemann and Henkler, 2011). "
"Sünder et al. (2013) propose an approach on verifying PLC programs by means of model checking, taking predefined modifications of the software into account. "
Gourcuff et al. (2008) presented an approach for verifying cyclically executed PLC software. 
Witsch and Vogel-Heuser (2011) proposed an approach to implement PLC software based on UML state charts (so called PLC state charts) and described its behavioral semantics and application for model checking.
 Mertke and Frey (2001) proposed an approach on formal verification based on Petri nets by using the SPIN model checker. 
Machado et al. (2006) investigated the impact of applying a model of the physical plant additionally to the formal specification of controller behavior. 
"An integrated approach to verify conformance of aPS designs is presented in Vyatkin et al. (2009). It considers the overall behavior of mechatronic components in a plant model, i.e. the behavior resulting from the combination of different disciplines. "
"aPS are highly diverse. They may differ in the processes which they are designed to execute, but also in the mechanical and electrical/electronic parts and software parts used to complete their tasks. The inherent variability in such systems results in a huge number of possible variants which may be described by a product family, e.g., a software product line (SPL) (Clements and Northrop, 2001; Pohl et al., 2005). "
"Feldmann et al. (2012) analyzed the approaches and challenges for modularity, variant and version management in aPS."
"For variability management from an electrical engineering viewpoint, tools like EPLAN Engineering Center7 as well as the Siemens Platform COMOS8 exist, and for mechanical engineering, Design Structure Matrices are most popular (Kortler et al., 2011). "
Feldmann et al. (2012) showed that module structures in different disciplines differ. 
"Variability modeling in the problem space defines the commonality and variability of a product family in order to specify the valid configuration space i.e., specifying the set of valid software variants (Czarnecki and Eisenecker, 2000). "
"In the context of feature-oriented domain analysis, Kang et al. (1990) introduce feature models for capturing the commonality and variability of variant-rich systems by means of features and their interrelations."
Pohl et al. (2005) proposed the orthogonal variability model also for capturing the commonality and variability of variant-rich software systems in a graphical way by means of variation points.
"Another variability modeling approach is decision modeling introduced by the Synthesis project (Synthesis, 1993)."
"Model transformations are, for instance, applied in the common variability language (Haugen et al., 2008) where the variability of a base model is described by rules how modeling elements of the base model have to be substituted in order to obtain a particular product model. "
"Delta modeling has so far been applied to represent variability of software architectures (Haber et al., 2011), Java programs (Schaefer, 2010) and a multi-perspective modeling approach for manufacturing systems (Kowal et al., 2014)."
 Elsner et al. (2010) consider evolving software product lines and focus on variability in time.
"Schubanz et al. (2013)developed their own specific modeling approach, which is integrated in a prototypical tool chain and focuses on a high level of abstraction, i.e., evolution of development goals and requirements. "
The approach by Dhungana et al. (2010) uses model fragments for capturing the solution space variability following the principle that smaller models are better to understand.
"While model-driven engineering shows an increase in effectiveness and quality and is used in the embedded software industry (Liebel et al., 2014) by exploiting domain knowledge, it also introduces general challenges and challenges specific to aPS."
"Typically, the models that are developed during system design are in later phases converted to source code by code generation. This process is called Forward Engineering (Sendall and Küster, 2004). "
As Vogel-Heuser et al. (2012) showed usability of MDE approaches is strongly related to students' basic skills and appropriate fade out training approach.
" Furthermore, in Vepsalainen et al. (2010), it was identified that the modeling of user-defined control logic is required in addition to the application of predefined control blocks. For acceptance of novel concepts, those have to be easily applicable and reproducible for other researchers (Goldberg, 2012). "
" Obermeier et al. (2014) introduce a domain specific UML for aPS (modAT4rMS based on plcML) supporting the quality of structural modeling for the creation of new models, the reuse as well as the evolution (building new variants)."
 Duschl et al. (2014) attempt to reveal the reasons behind the errors especially in module creation in the above mentioned study conducting interviews after the experiments.
"As a basis to describe different aspects of aPS at different hierarchical levels, adaptations of UML and SysML are proposed in Bassi et al. (2011), Bonfè et al. (2005) and Secchi et al. (2007). Concepts for supplying object-oriented models with a formal basis, e.g., in order to apply methods for verifying modeled system requirements (Sünder et al., 2013), have been proposed by Secchi et al. (2007)."
"The integration of MDE, i.e. plcML being a UML dialect, into a PLC programming environment is realized by Witsch and Vogel-Heuser (2011) and available for state chart and class diagrams in CODESYS V39."
"Code generation from plcML to Siemens S710 platform has been introduced in Tikhonov et al. (2014). For hybrid models combining closed loop control and interlocking, Bayrak et al. (2012) and Schneider et al. (2014)"
"Trace links can also be created using information-retrieval techniques (Cleland-Huang et al., 2007). The authors give an overview on two techniques for automatic generation of trace link candidates from requirements and other system artifacts based on probabilistic network models (Antoniol et al., 2002) and vector space models (Cleland-Huang et al., 2005). "
"Berkovich et al. (2011) investigate tracing on interdisciplinary Product-Service Systems (PSS), e.g. mechatronic products including services. "
" Wolfenstetter et al. (2015) analyzed and evaluated different traceability techniques from a requirements engineering perspective regarding ten criteria, e.g., variability and configuration management, version management, simultaneous development of different views, with the result provides sufficient support."
"In a Banach space setting, the source inequality (30) has already been used in [36,50] to derive convergence rates for Tikhonov regularization with convex functionals and in [34] for multiparameter regularization. Eq. (29) is an alternate for the missing triangle inequality in the non-metric case."
"In the case that the operator F is linear and R is convex, (2) and (3) are basically equivalent, if is chosen according to Morozov discrepancy principle (see [39], Chap.3])."
"While the theory of Tikhonov regularization has received much attention in the literature (see for instance [1,14,21,22,33,37,45,49,52,56,58]), the same cannot be said about the residual method"
"In [21,50], where convergence and stability of Tikhonov regularization have been investigated, the stability results are of the following form: for every sequence (yk)k N y and every sequence of minimizers xk argmin{ F(x)-yk 2+ R(x)} there exists a subsequence of (xk)k N that converges to a minimizer of F(x)-y 2+ R(x)"
"In this paper we prove similar results for the residual method but with a different notation using a type of convergence of sets (see, for example, [41, Section 29])"
"If X satisfies the first axiom of countability, then x Lim sup k k, if and only if there exists a subsequence ( kj)j N of ( k)k N and a sequence of elements xj kj such that x j x (see [41, Section 29.IV])"
"From Proposition 4.3 we now obtain that (xk)k N weakly converges to x and xk pp x pp_ Thus, in fact, the sequence (xk)k N strongly converges to x (see [44, Corrollory 5.2.19])"
This r-coercivity has already been applied in [2] for the minimization of Tikhonov functionals in Banach spaces
"The spaces X = L p( , ) for p (1, 2] and some -finite measure space ( , ) are examples of 2-convex Banach spaces (see [42, p.81, Remarks following Theorem 1.f.1.])"
"In a finite dimensional setting with p = 1, the minimization problem (40) has received a lot of attention during the last years under the name of compressed sensing (see [7,8,10,16 18,20,26,57])"
"however, for many LDS’s, the parameters are unknown and must be estimated in a process often called system identification[17]."
Reduced-rank LDS models can partially address this problem by reducing the number of latent states. [7]
"An alternative is to use subspace identification methods such as N4SID and PCA-ID, which give asymptotically unbiased closed-form solutions [8,27]."
"The Amari error [2], which is another permutation-invariant measure of similarity, is also provided."
"Mohammad et al. have discussed the impact of autocorrelation on functional connectivity, which also provides some direction for extension [3]."
"Diversifiersare people whose passion is to explore details. They are in love with the heterogeneity of nature […]They are happy if they leave the universe a little more complicated than they found it.”[1, chap. 3, p. 44]"
"Dyson’s treatment of this is relatively short, at just one 18-page chapter [1]. It is therefore important to moderate our contemporary biases, if we are to understand what he intended."
"The field was spawned from the dreams of Artificial Intelligence, though the reality has encompassed a far broader scope of study than originally envisioned at the Dartmouth Conference [2]."
"“Therefore, my success as a man of science, whatever this may have amounted to, has been determined, as far as I can judge, by complex and diversified mental qualities and conditions. Of these the most important have been […]industry in observing and collecting facts, and a fair share of invention as well as of common-sense.”[3, p. 144]"
"The first academic city in the world was Athens, and the first industrial city was Manchester, so I like to use the names of Athens and Manchester as symbols of the two styles of scientific thinking.”[1, p. 37].He clarifies later, “The science of Athens emphasises ideas and theories; it tries to find unifying concepts which tie the universe together. The science of Manchester emphasises facts and things; it tries to explore and extend our knowledge of nature’s diversity.”[1, p. 40]. To clarify, he is not stating that all academics are unifiers, nor that all of industries are diversifiers."
"“Science belongs to both worlds, but the style of academic science is different from the style of industrial science. The science of the academic world tends to be dominated by unifiers, while the science of the industrial world tends to be dominated by diversifiers.”[1, p. 36]."
"The Manchester exemplar is particularly illuminating in this respect, given a deeper look at its historical context. Manchester, situated in the North of England, was the birthplace of the Industrial Revolution, and the growth of its intellectual capital is well documented by Thackray [4]."
"“Science did flourish in Manchester during the crucial formative years of the industrial revolution, but […]did not arise in response to the needs of industrial production. The driving force of the Manchester scientific renaissance were not technological and utilitarian; they were cultural and aesthetic.”[1, p. 38]"
"In later a communication he states that only “roughly speaking, unifiers are following the tradition of Descartes, diversifiers are following the tradition of Bacon”[7]."
A controversial idea of the past century in philosophy has been the distinction between analytic and synthetic statements [8]. 
"Kant [8] referred to analytic statements as clarifying or explicating our knowledge, or in other words, making explicit what was once implicit. "
 A modern perspective on this divide is given by Jogalekar [9].
"So here, Dyson explicitly thinks of himself in a unifying role. In another communication [11] he recounts his discussions on Quantum ElectroDynamics with Richard P. Feynman, who was apparently obsessed with finding a unifying theory of the large (gravity) and small (nuclear forces). In contrast, Dyson was comfortable with more than one set of equations, each useful at different scales. Referencing Gödel’s theorem says:"
“a beautiful or elegant theory is more likely to be right than a theory that is inelegant.”[12]
"The only instance even vaguely like this (that I know of) is the Boosting family of algorithms—the existence of which was predicted by studies in computational complexity theory [14], and discovered later by Schapire [15]. "
"Though this article is focused on Pattern Recognition, it is fair to note that similarly vague work appears in related communities [18]. "
Kuhn [20] presents a treatise on the nature and reasons behind revolutions in scientific understanding.
"Whilst we may never have a truly “unified theory of inference”, there are a number of technical elements of our field which could benefit from a little unification; in classic papers, Breiman [22] and Langley [17] present ideas along this line. "
Breiman [22] discusses two cultures of statistical modelling: data modelling versus algorithmic modelling.
"Langley [17] wrote a striking editorial for an issue of Machine Learning Journal, on the topic of unifying machine learning as it stood in the late 1980s."
"Essentialism is the view that entities in the world have inherent, essential and immutable properties, by which they can be described. Pelillo and Scantamburlo [25] discuss how dissimilarity measures in PR sit in direct opposition to this philosophical view, in that an entity is best described by its relation (similarities) to other entities."
 It could be argued that such devices are at prototype stage already with neuromorphic computing [26].
The methods for automated tumor detection evolved from the approaches that are used for normal tissue detection on medical images. These include a fuzzy set based algorithm [3] that allows the detection of healthy brain substructures with a precision of 95% measured with the Dice similarity coefficient (DSC) [4].
"Another method designed for the same task, but based on self-organizing maps technique (SOM) [5], demonstrates an accuracy of 80% measured with the Tanimoto index (TI). Another popular method of healthy brain segmentation based on active contours [6] results in an F-measure of 90%."
"Publicly available methods for automated tumor detection on MRI images may be divided into two groups with respect to the dimensionality of input data. The first group consists of methods operating on one sequence only. Examples of such methods include: template moderate classification proposed by Kaus etÂ al. [7], a method based on fluid vector flow by Lee etÂ al. [8] or a generative model based method proposed by Prastawa etÂ al. [9]. This group of methods gives accuracy at 50â70% DSC. The second group consists of algorithms that use information stored in multiple modalities/sequences. The most popular and successful examples of such approaches are: a technique based on support vector machines (SVMs) proposed by Verma etÂ al. [10], a random forest classifier by Tusiom etÂ al. [11] and an algorithm based on the fuzzy sets theorem proposed by Fletcher etÂ al. [12]. The methods based on multimodal segmentation in a set of different MRI sequences result in average of 80% DSC obtained on independent validation sets. While performing a literature review, one can notice several other techniques, some related to the famous BRATS challenge held annually by MICAII [13]. The accuracy of methods published in the newest BRATS report varies from 57-82% DSC."
"The diffusion of water molecules depends mostly on the temperature and tissue structure, and is independent of patient gender and age [14â19]."
"DWI allows for the quantitative measurement of water molecules activity, in clinical practice known as the apparent diffusion coefficient value (ADC) [18,20]"
"The DWI image translation into an ADC map was performed with the MITK framework [28], with the original DWIâs additionally filtered with the non-local means filter [29,30]."
" To efficiently filter the skull signal, the T2 FLAIR sequence together with the FSL brain extraction tool (FSL-BET) [31] were used."
"To neglect the problem of cerebrospinal fluid (CSF) distortion, OTSU based CSF filtration routines [32] were applied giving a binary CSF mask."
"In the second step of MiMSeg algorithm, k-means clustering [36] was applied to find the groups of GMM components similar in mean value, variance and weight"
The clustering procedure was repeated for number of clusters ranging from 1 to 10 with intial conditions set according to [37]. The optimal number of clusters was determined by Dunnâs cluster consistency criterion [38]. 
"a neural network based technique named Self-Organizing-Map (SOM) [39] methods proposed by Murakami etÂ al. [40], and by Kang etÂ al. [41]."
The unconstrained version of classical GMM technique was used in a comparison study [42].
"According to the WHO, the family of astrocytomas tumours, a subgroup of gliomas originated from abnormal astrocytic cells, is divided into the following four groups with respect to the increasing malignancy [44]"
"However, the absence of enhancement does not necessarily imply a histopathologic diagnosis of low-grade tumor: one-third of nonenhancing diffuse gliomas in adults are high-grade tumors [45]."
"In addition, ADC changes due to the presence of cystic, necrotic, and/or hemorrhagic areas and the influence of artifacts caused by inhomogeneous structures such as the skull base bone and sinus air must be considered [48]."
"To avoid the influence of susceptibility artifacts or ADC changes, regions with infratentorial components or gross hemorrhage should be excluded from measurement or during data analysis by applying robust and efficient pre-processing techniques [32]"
"Android is the dominant operating system for mobile devices; it currently has the largest installed base ( IDC,2013) mainly because (a) it supports a huge variety of different devices such as watches, tablets, TV sets, etc., and (b) it provides end-users with a large variety of applications (a.k.a)"
"We have performed a complete isolated evaluation of the policy evaluation delay and scalability issues in a previous publication by some of the co-authors ( Neisse et al.,2015)."
"According to Callaham (2014) users have an average of 95 apps installed in their Android phones, and according to Au et al.(2012) there are around 750 API methods associated with the available permissions in Android version 4.3.1"
"This average number of apps and possible API methods would be equivalent to around 71 thousand policies, which according to the PDP evaluation results published in Neisse et al. (2015) would correspond to a response time of around 100 ms"
"Kirin, proposed by Enck et al.(2009), is a security service running on the phone which analyses the requested permissions of an app and detects potential security flaws"
"Batyuk et al.(2011) introduced Androlyzer, a server based solution that focuses mainly on informing users about apps potential security and privacy risks"
"Complementary, AppFence proposed by Hornyack et al.(2011) ., also implemented as a modified OS on the basis of TaintDroid, shadows and ex-filtrates users' private data according to their preferences."
"SEDalvik, introduced by Bousquet et al. (2013), proposes a MAC mechanism to regulate information flows between apps objects building on the advantages of Dalvik internal debugger"
"AppGuard, introduced by Backes et al. (2013), is an app instrumentation framework that runs directly in users' device and allows user-centric security policies customisations"
"We also plan to launch a community-based release of our tool where users can contribute with abstract policies and refinement models of privacy sensitive activities, for example, integrating the results of other Android security approaches (eg Rasthofer et al.,2014) that cannot be easily reused at the moment."
" The objective of PAP is more realistic than seeking an approach that performs the best for all problem instances, since such an approach may not even exists because of the no-free-lunch theorem [26]."
" In case that the number of candidate EAs is huge and direct enumeration becomes prohibitive, Eq. (4) can still be integrated with some existing search method, say forward selection [12], to yield at least a sub-optimal set of constituent algorithms within an acceptable time period."
"Despite the promising preliminary results, both empirical and theoretical analysis revealed that the performance of PAP is sensitive to its constituent EAs [17]"
"Although the accuracy of such an estimate is by no means guaranteed, it has been commonly employed in many other scenarios, eg, fine-tuning the parameters of an EA [4], where the performance of an EA needs to be estimated."
"In case that the number of candidate EAs is huge and direct enumeration becomes prohibitive, Eq (4) can still be integrated with some existing search method, say forward selection [12], to yield at least a sub-optimal set of constituent algorithms within an acceptable time period."
"In the context of ensemble learning, the term different is referred to as diversity and a lot of studies have been carried out to investigate how diversity can be quantitatively defined and be utilized to construct good ensembles [22]"
"Neural networks (NN) are mathematical representations modelled on the functionality of the human brain (Bishop, 1995). "
"For example, in Yobas, Crook, and Ross (2000), the authors found that linear discriminant analysis (LDA) outperformed neural networks in the prediction of loan default, whereas in Desai, Crook, and Overstreet (1996), neural networks were reported to actually perform significantly better than LDA"
"The logistic regression model then takes the form: (1)logit( ) log 1- = + Tx,where is the intercept parameter and T contains the variable coefficients ( Hosmer & Stanley, 2000)."
"The least square support vector machine (LS-SVM) proposed by Suykens, Van Gestel, De Brabanter, De Moor, and Vandewalle (2002) is a further adaptation of Vapnik original SVM formulation which leads to solving linear KKT (Karush Kuhn Tucker) systems (rather than a more complex quadratic programing problem)"
"The Nemenyi post hoc test states that the performances of two or more classifiers are significantly different if their average ranks differ by at least the critical difference (CD), given by (12)CD=q , ,KK(K+1)12D.In this formula, the value q , , K is based on the studentised range statistic ( Nemenyi, 1963)"
"Note that, even though the differences between the classifiers are small, it is important to note that in a credit scoring context, an increase in the discrimination ability of even a fraction of a percent may translate into significant future savings ( Henley & Hand, 1997)."
"The analysis of texture images has played a fundamental role in computer vision and pattern recognition during the last decades and the importance of this area of study is illustrated by the number of applications appearing in diverse areas such as Engineering [12], Medicine [1] and Physics [6]."
"Examples are the theory of textons [17], local patterns [14], local affine regions [11], invariants of scattering transforms [15], Fast Features Invariant to Rotation and Scale of Texture [16] and others [3]"
"In simple terms, it is a generalisation to the three-dimensional Euclidean space of the local connected fractal dimension of binary images previously reported in [10,18]."
The parameter rmax was set to 5 in all tests based on the observation in [5] that larger radii severely increase the computational cost and number of descriptors without a significant gain in the classification performance
"In order to improve the diversity of the search in the PSO, various kinds of models have been investigated [4,19]."
"Moreover, in [17], Tatsumi etÂ al. presented a sufficient condition for chaoticity of the system used in CPSO-VQO and showed that the parameter values in the system can be selected by utilizing its bifurcation diagram."
" Moreover,another popular method based on the same idea,which is called the self-organizing hierarchical PSO with time-varying acceleration coefficients(HPSO-TVAC)[13],was proposed."
"Besides,there is another method of generating a chaotic sequence, the gradient model with sinusoidal perturbations (GP) [16]."
"Recently, a combination of the PSO and GP methods, CPSO-VQO,was proposed [17]."
"The chaoticity of system (C1) is guaranteed by the property that a system derived by using the steepest descent method with a perturbation can be chaotic [16], and it is summarized."
It is well-known that there are an infinite number of orbits which are repelled by the snap-back repeller but which are attracted to its neighborhood [12].
"Then,we have the following theorem with respect to chaos in the sense of Li–Yorke[9]"
"In this subsection, we show the results of numerical experiments in which the standard PSO(PSO), the four existing improved PSOs,PSO-IWA,CEPSOA[1],CPSO-VQO[17]andHPSO-TVAC[13],and the five proposed PSOs,PSO-TPC,PSO-SPC,PSO-SDPC,IPSO-SPCandIPSO-SDPC,were applied to the following 50,200,and 400-dimensional benchmark problems."
"For the standard PSO,we used (w,c1,c2)=(0.729,1.494,1.494),as suggested in[5]. "
"These values were also used for the standard updating system (SP)used in CPSO-VQO, PSO-TPC, PSO-SPC, and PSO-SDPC. For HPSO-TVAC, we set (cu,cl)= (2.5,0.5) based on the paper [13]."
"For CPSO-VQO, we used (wd,α,β,m,rmin,dF)=(0.9,0.35,0.045,30,4.0,40.0),as shown in [17],where the width of the rectangular feasible region of each problem in Table1 is scaled into the same constant dF."
"In the SADE, the parameters F and CR were randomly selected from (0.1,1.0) and (0,1), respectively, and we used DE/rand/1/bin strategy,which was based in the paper [2]."
"The users are malware analysts (domain experts) whose main tasks are to select different rules, categorize them by their task and store them in the database as well as manual adaption and/or tuning of found rules ( Wagner et al.,2014)."
"In a preliminary study, we found that malware analysts preferred visualization concepts containing multiple views and arc-diagrams or word trees ( Wagner et al.,2014)."
"By externalizing and storing of the experts' implicit knowledge ( Chen et al., 2009), it can be made system-internally available as computerized knowledge to support further analysis or other analysts."
"Other than that, Pretorius and Van Wijk(2009) point out that it is important for visualization designers to ask themselves: What does the user want to see? and What do the data want to be? as well as how these two points mutually enhance one another"
Goodall et al. (2004) conducted contextual interviews to gain a better understanding of the intrusion detection workflow and proposed a three-phased model in which tasks could be decoupled by necessary know-how to provide more flexibility for organizations in training new analysts
Conti (2007) dedicated a part of his book to visualization for malware detection but focused on the network level
"On the one hand, there are systems for Individual Malware Analysis which support the analysis of a single malware sample to gain new insights (eg, Donahue et al., 2013; W chner et al.,2014)"
"Additionally, Gove et al. (2014) introduced SEEM , which allows the behavioral comparison of a large set of malware samples in relation to the imported DLLs and callback domains."
"Some problem-oriented projects (eg, Mistelbauer et al.,2012) and general frameworks (eg, Tominski,2011) integrate explicit knowledge in visualization but not in a form similar to malware behavior patterns."
"Therefore, we set up a design study project to find a visualization solution that followed a user-centered design process ( Sharp et al.,2007)"
"Furthermore, the graphical summary is based on the Gestalt principle of similarity ( Johnson,2014) in order to support the analyst in quickly recognizing related call combinations"
"It lasted one hour on average and encompassed five analysis tasks, the system usability scale questionnaire (SUS) ( Brooke,1996), and a semi-structured interview"
"The SUS is a standardized, technology-independent questionnaire to evaluate the usability of a system ( Brooke,1996)"
"These comments were also apparent concerning SUS question ten: I needed to learn a lot of things before I could get going with this system ( Brooke,1996), with a score of 50%"
"Categorization of KAMAS: If we categorize KAMAS along the Malware Visualization Taxonomy ( Wagner et al.,2015), we can see that KAMAS can be categorized on the one hand as a Malware Forensics tool which regards to the analysis of malicious software execution traces"
"On a general level, the workflow for knowledge generation and extraction is mostly similar and always includes the system user as an integral part of the loop ( Endert et al.,2014)"
"It can be defined as the minimal total cost of a sequence of elementary edit operations to transform one sequence into the other; for a se- quence x of length m and a sequence y of length n , it can be com- puted in time O(mn ) [8] ."
"In [21] , the authors show that computing the edit distance can be used to classify handwritten digits, where the contours of the digits are represented with an 8-direction chain-code [11] ; a sequence over an eight-letter alphabet, representing the eight cardinal directions that the contour faces when following the outline of an image in a clockwise motion."
Example applications where image retrieval is required include digital libraries and multimedia editing [28] .
Few exact algorithms exist which are able to compute the cyclic edit distance between x and y . Maes designed an elegant divide- and-conquer algorithm which runs in time O(mn log m ) [18] .
Several heuristic approaches exist for approximating the cyclic edit distance. One of the first ones is the Bunke and Buhler ( BBA ) algorithm [6] .
"The ex- tended Bunke and Buhler method ( EBBA ) computes an estimation of the upper bound for the exact cyclic edit distance, also in time O(mn ) [22] ."
"Informally, the q -gram similarity, de- fined as a distance in [29] , is the number of q -grams shared by the two sequences."
The rotation of x that minimises a generalisation of the q -gram distance between x and y is computed using the algorithm in [13]
"We begin with a few definitions, following Crochemore et al. [8] . We think of a string x of length m as an array x [0 . .m −1] , where every x [ i ], 0 ≤i < m , is a letter drawn from some fixed alphabet  of size |  | = O(1) . We refer to any string x ∈  q as a q-gram ."
"In many applications, we only want to count the number of edit operations, considering the cost of each to be 1 [17] . This distance is known as Levenshtein distance , a special case of edit distance where unit costs apply."
"The cyclic edit distance , denoted by δCE ( x, y ), is defined as δCE (x, y ) = min i ( min j δE (x i , y j )) = min i δE (x i , y ) [18] ."
"We give some further definitions following Ukkonen [29] . The q-gram profile of a string x is the vector G q ( x ), where q > 0 and G q ( x )[ v ] denotes the total number of occurrences of q -gram v ∈  q in x ."
Jokinen and Ukkonen [15] showed the following bound which is directly applicable to the Levenshtein distance. Lemma 1 [15] . Let x and y be strings with Levenshtein distance k. Then at least | x | + 1 −(k + 1) q of the | x | −q + 1 q-grams of x occur in y.
"For a given integer parameter β≥1, Grossi et al. [13] defined a generalisation of the q -gram distance by partitioning x and y in βblocks as evenly as possible, and computing the q -gram distance between each pair of blocks, one from x and one from y . The ratio- nale is to enforce locality in the resulting overall distance."
"The algorithm is based on constructing the suffix array [19] forstringxxyandas- signing a rank to the prefix with length q of each suffix with length at least q , based on its order in the suffix array."
"Myers bit-vector algorithm is used to compute the edit distance when using unit costs for insertion, deletion, and substitution [24] ."
Myers bit-vector algorithm was implemented using the SeqAn library [9] .
"DNA datasets were simulated using INDELible [10] , which produces sequences in a (Multi)FASTA file."
"Handwritten digits from the MNIST database [16] were also used and sorted into ten sets. Each image was placed in one of ten datasets, depending on the value of the drawn digit."
"Normalising the chain-code allows the image to be treated as a circular sequence of minimum magnitude. This produces a sequence independent of the rotation of the image. This was calculated by identifying the number of direction changes between two adjacent elements of the chain-code in an anticlockwise direction (see [12] , for details)."
"Assuming a stationary camera, which is a reasonable constraint for most applications, using scene specific information can help to reduce the number of false alarms (e.g., Hoiem et al., 2006)"
"To further improve the classification power and to further reduce the number of required training samples an adaptive classifier using an on-line learning algorithm can be applied (Nair and Clark, 2004; Javed et al., 2005; Wu and Nevatia, 2007a)."
"The complexity object detection can be even further reduced by using classifier grids. The main idea of classifier grids (Grabner et al., 2007; Roth et al., 2009) is to exploit the prior knowledge, that the camera is fixed."
"To overcome this problem, at time t fixed updates (Grabner et al., 2007) can be applied for updating a classifier Ci,t−1. Given a set of representative positive (hand) labeled"
The second problem was addressed in Stalder et al. (2009) and in Sternig et al. (2010b). Stalder et al. (2009) introduced context-based classifier grids to extract additional positive information from a specific scene
Multiple instance learning (MIL) was first introduced by Dietterich et al. (1997).
"Most of these approaches are based on popular supervised learning algorithms such as SVM (Andrews et al., 2003) or boosting (Viola et al., 2005), that are adapted in order to incorporate the MIL constraints."
"Building on (Roth et al., 2009) the model describing the object class (the positive model) is pre-calculated by off-line boosting for feature selection and only negative updates are performed."
"However, in our case we apply the approximated median background model (McFarlane and Schofield, 1995)."
"To demonstrate the benefits of the proposed approach, we run five experiments considering two tasks, namely pedestrian and car detection. We first give an illustrative comparison between the original grid approach (e.g., Roth et al., 2009) and the proposed method."
"For this experiment we used a sequence from the publicly available PETS 2006 dataset consisting of 308 frames (720 × 576 pixels), which contains 1714 pedestrians. We compare our approach to other state-of-the-art person detectors, namely the deformable part model (Felzenszwalb et al., 2008) (FS) and the Histograms of Oriented Gradients approach (Dalal and Triggs, 2005) (DT)."
"In addition, we compared our method to the classifier grid (CG) approach (Roth et al., 2009)"
"Finally, we want to demonstrate that the long-term stability, which was already shown for the original classifier grid approach in Roth et al. (2009), also holds for the IMIL extension"
"In particular, as in Roth et al. (2009) we kept the positive representation fixed and generated a bag of negative samples from an estimated background model."
"In the European Union, the population share of persons older than 60 was 17 percent in 1980 and increased to 22 percent in 2004/5 (it is expected to reach 32 percent in 2030). Life expectancy of men (women) has risen from 68 (76) years to 74 (80) years during the same time period (European Commission, 2007)."
"In addition, many elderly people prefer to grow old in the privacy of their homes rather than in a nursing home. On the other hand, willingness for informal care by relatives is decreasing. This is partly due to the fact that women and men are both working (Tarricone & Tsouros, 2008)."
"Therefore, organizations providing home care services are inclined to optimize their activities in order to meet the constantly increasing demand for home care (Koeleman, Bhulai, & van Meersbergen, 2012)"
"The latter aspect concerns, e.g. penalties for deviations from preferred visit times or from the set of preferred nurses. Trautsamwieser et al. (2011) consider seven different terms in the objective function and Hiermann et al. (2015) consider as many as 13 (see Table 1, column “# OF terms”)."
"Besides the daily routing and scheduling problem, authors have also addressed the long term problem. Nickel, Schröder, and Steeg (2012) look at weekly schedules and link them to the operational planning problem."
"Weekly home care scheduling problems are also addressed in, e.g., Borsani, Matta, Beschi, and Sommaruga (2006), Gamst and Jensen (2012), Cappanera and Scutellà (2013), Maya Duque, Castro, Sörensen, and Goos (2015) and Trautsamwieser and Hirsch (2014), while Nowak, Hewitt, and Nataraj (2013) investigate planning horizons of two to three months, anticipating future requests."
"Successful implementations of home health care scheduling tools are described, e.g., in Eveborn et al. (2006, 2009) or Begur et al. (1997)"
"An overview of home care routing and scheduling and related problems can be found in Castillo-Salazar, Landa-Silva, and Qu (2015). More information on home care worker scheduling is provided in the survey by Gutiérrez, Gutiérrez, and Vidal (2013) and on personnel scheduling in general by Van den Bergh, Beliën, De Bruecker, Demeulemeester, and De Boeck (2013) and De Bruecker, Van den Bergh, Beliën, and Demeulemeester (2015)."
"We then design a metaheuristic solution framework that is based on multi-directional local search (Tricoire, 2012) to solve instances of realistic size (see Section 4)."
"Using the notation of Vidal, Crainic, Gendreau, and Prins (2015), the scheduling problem may be described as in (43)"
"For a detailed description of these concepts and their underlying principles, the reader is referred to Ehrgott and Gandibleux (2002, 2004) and Ehrgott (2005)."
"The algorithm is based on the multi-directional local search framework (Tricoire, 2012) and uses large neighborhood search (LNS) as a subheuristic."
"Multi-directional local search (MDLS) is a recently proposed meta-heuristic framework for multi-objective optimization problems (Tricoire, 2012)"
"Large neighborhood search (LNS) is a metaheuristic which was first introduced by Shaw (1998). It uses the concept of ruin and recreate to define an implicit, large neighborhood of a current solution as the set of solutions that may be attained by destroying a large part of the solution and subsequently rebuilding the resulting partial solution."
"In recent years, many routing problems have been successfully solved using LNS-based methods. For details and an overview of recent developments on LNS, the reader is referred to Pisinger and Ropke (2010)."
"The structure of our MDLS algorithm is very similar to the structure described by Tricoire (2012), although in our case a single-objective local search procedure may result in more than one new solution as is discussed in Section 4.2."
"Several standard removal and insertion operators from the LNS literature (Pisinger & Ropke, 2007; Ropke & Pisinger, 2006; Shaw, 1998) are adapted to the specific problem context of the BIHCRSP"
The implementation of the operator is the same as in Shaw (1998). Initially a job is removed randomly.
"The minimal overtime cost is used instead of the actual overtime cost as the latter would require resolving the scheduling problem when removing a job while the former may be calculated in constant time (Vidal et al., 2015)."
"As in Shaw (1998) and Ropke and Pisinger (2006), a parameter P ≥ 1 is used in all worst removal operators to introduce some randomness in the selection of jobs, thereby avoiding the same jobs to be removed over and over again."
"For a detailed discussion of these operators, the reader is referred to Ropke and Pisinger (2006) and Pisinger and Ropke (2007)."
"To diversify the search, a noise term may be added to the objective functions of the insertion heuristics (Pisinger & Ropke, 2007; Ropke & Pisinger, 2006)."
"However, the parameter values that have been applied are based on both real-life data of two Viennese companies and real-life-based benchmark data for a related problem (Hiermann et al., 2015)."
Four types of instances may be distinguished based on the travel cost and travel time matrices used. The first three types are based on the travel time matrices for car and public transportation provided by Hiermann et al. (2015) and are generated using OpenStreetMap.
"Several quality indicators have been proposed in the literature to evaluate approximations of the Pareto frontier generated by heuristic solution procedures (Knowles, Thiele, & Zitzler, 2006; Zitzler, Thiele, Laumanns, Fonseca, & Grunert da Fonseca, 2003)."
"The hypervolume indicator (IH(A)), introduced by Zitzler and Thiele (1999), measures the portion of the objective space that is weakly dominated by an approximation set A."
"Technical debt (TD) is a metaphor used to describe a situation in software development, where a shortcut or workaround is used in a technical decision (Kruchten et al., 2012b)."
"TD has also similarities to three aspects of financial debt: repayment, interest, and in some cases high cost (Allman, 2012)."
"In software development, a shortcut or workaround can give the company a benefit in the short term with quicker release to the customer and an advantage in time-to-market over the competition (Kruchten et al., 2012a; Yli-Huumo et al., 2015a). "
"However, if these shortcuts and workarounds are not repaid, TD can accumulate and hurt the overall quality of the software and the productivity of the development team in the long term (Zazworka et al., 2011b)."
"The current literature has identified and developed some tools and practices to conduct TDM. However, according to a recent mapping study, the problem is the lack of empirical evidence about TDM in a real-life software development environment (Li et al., 2015a)."
"For data collection and analysis, we used the eight TDM activities identified by Li et al. (2015a) in semi-structured interviews to gather empirical data about TDM in the selected software development teams."
"We used the exploratory case study method (Robson, 2002) to answer the following main research question:"
"Technical debt management can be separated into the following activities: identification, measurement, prioritization, prevention, monitoring, repayment, representation/documentation, and communication (Li et al., 2015a). "
"There are a number of possible methods, models, practices or tools for every TDM activity (Li et al., 2015a). They have been developed and suggested in the literature, but they lack empirical evidence of their usability and functionality (ibid.). "
"Some software development teams may use more time on TDM, while some development teams may not pay much attention to it (Power, 2013). Therefore, it is important to understand if it is possible to distinguish between different maturities of TDM, similarly as in the capability maturity model (CMM)(Paulk et al., 1993). The results of this study can be used to develop a similar maturity model for TDM, which researchers and practitioners could use to conduct more research, or to improve companies’ internal and external practices."
"The TD metaphor was first associated with compromises on the code level of software (Cunningham, 1992). In addition, terms like code smells (Fowler et al., 1999) have described situations where poor technical choices in software development have caused problems in code quality and architectural soundness. However, the TD metaphor has been rapidly expanded after the initial concept on the code level, and it has been associated with other stages of the software development lifecycle as well (Tom et al., 2013; Alves et al., 2014)."
"The TD metaphor was first associated with compromises on the code level of software (Cunningham, 1992). In addition, terms like code smells (Fowler et al., 1999) have described situations where poor technical choices in software development have caused problems in code quality and architectural soundness."
" However, the TD metaphor has been rapidly expanded after the initial concept on the code level, and it has been associated with other stages of the software development lifecycle as well (Tom et al., 2013; Alves et al., 2014)."
"The current literature identifies such terms as requirements (Brown et al., 2010), design (Zazworka et al., 2011b; Zazworka et al., 2011a), architectural (Nord et al., 2012), test (Brown et al., 2010), process (Lim et al., 2012), documentation (Kruchten et al., 2012a), and people debt (Kruchten et al., 2012b) to demonstrate the same effect of shortcuts or workarounds happening in the other stages of the software development lifecycle."
"Shortcuts and workarounds in software development usually happen for intentional reasons, such as for business deadlines and development complexity (Yli-Huumo et al., 2015a)."
"Time-to-market and customer feedback are important factors for companies’ success, and it is essential to deliver solutions on time (Lim et al., 2012). This is the reason why business stakeholders are often more focused on deadlines and customers than the actual quality of the software, which is more in the developers’ interest area (Barney et al., 2008; Boehm, 2006). Therefore, strict deadlines may sometimes force the development team to create solutions with second-tier quality to meet the requirements within the deadlines set by the business stakeholders (Yli-Huumo et al., 2014)."
"TD can also occur unintentionally (McConnell, 2007). The reason for unintentional TD can be lack of competence, a need to upgrade existing technologies, or a customer or market -induced need for change"
"When taking TD, companies are able to speed up the release cycles to the customer, which can increase customer satisfaction and provide advantage in the market. Another benefit for companies is customer feedback (Yli-Huumo et al., 2015b). Companies are able to adjust the product and its business model based on faster customer feedback"
"According to some scholars, TD should be associated only with intentional decisions happening in the code base, and messy code should not be counted as TD, while some think that old technologies in legacy software should also be counted as TD (Norton, 2009; Fowler, 2009)"
"Even though concepts such as social debt (Tamburri et al., 2013) and people debt (Alves et al., 2014) describe similar phenomena of having shortcuts and non-optimal solutions in software development and organization, we believe that they should be categorized as sources for TD rather than as actual TD"
 Li et al. (2015b) have also developed similar TD list management for architectural technical debt (ATD).
" Implementing coding standards to the development process can prevent TD, when the developers have a cohesive way to produce a similar style code, which makes it readable and modifiable (Green and Ledgard, 2011)."
"Code reviews can be used to check other developers' solutions before the release to catch possible TD issues in the design (Mantyla and Lassenius, 2009). Also simple practices in agile methodologies, such as the Definition of Done practice can reduce TD in the early stages of development (Davis, 2013)."
" A similar approach has also been used by other researchers to identify and document TD issues in order to make TD easier to manage (Zazworka et al., 2013)."
"Klinger et al. (2011) interviewed four experienced software architects to understand how decision-making regarding TD was conducted in an enterprise environment. The results showed that the decisions related to TD issues were often informal and ad hoc, which led to a lack of tracking and quantifying the decisions and issues."
"In addition, an interpretive case study aims at understanding phenomena through the participants’ interpretation of their context (Runeson and Höst, 2008)."
"We decided to use semi-structured interviews (Charmaz, 2014) for data collection, which makes this research a flexible study (Runeson and Höst, 2008). Semi-structured interviews include a mixture of open-ended and specific questions, designed to elicit not only the information foreseen, but also unexpected types of information (Seaman, 1999)."
The analysis of causes and effects of TD is available as a separate publication by Yli-Huumo et al. (2014). The second reason for focusing more on TDM was the publication of the TDM mapping study by Li et al. (2015a).
"In exploratory case studies, the technique for the analysis of qualitative data is hypothesis generation (Seaman, 1999). "
"The data coding and analysis were completed in various steps, guided by the work of Robson (2002). "
Similar practices identified in a study by Codabux and Williams (2013) were reengineering and repackaging.
" A set of other practices for TD prevention have been identified in other studies (Codabux et al., 2014; Krishna and Basu, 2012). These practices include approaches such as education and training, pair programming, test-driven development, refactoring, continuous integration, conformance to process and standards, tools, and customer feedback (Codabux et al., 2014). "
"Developers may also value documentation differently, and they document only issues that they personally think are important (Lethbridge et al., 2003). "
"The literature has suggested approaches for TD prioritization (Eisenberg, 2012; Seaman et al., 2012; Theodoropoulos et al., 2011; Zazworka et al., 2011a). "
" Prioritization can also be based on customer needs, but this can leave the most important TD from the technical perspective out of sight (Codabux and Williams, 2013). These prioritization issues exist also in requirements prioritization (Lehtola and Kauppinen, 2006)."
"When a development team considers for example the criteria in SonarQube (2015) as TD, in can guide TD management and other TD activities. However, TD can also be considered to consist of issues of a larger scale, such as architectural or structural issues and technology gaps (Kruchten et al., 2012a). "
"The goal of TDM is to provide practices and tools to manage and reduce TD during software development (Li et al., 2015a). "
"External validity is concerned with ‘to what extent it is possible to generalize the findings, and to what extent the findings are of interest to other people outside the investigated case’ (Runeson and Höst, 2008, p. 154). "
"On the basis of our findings we believe that TDM in software development has similarities to the characteristics of the capability maturity model (CMM) (Paulk et al., 1993). "
"A similar maturity model to CMM is also adaptable in TDM, where development teams have different TDM maturities in activities and practices. This kind of maturity as a concept has been applied to other processes and domains as well (De Bruin et al., 2005)."
" Chen [7] used fuzzy numbers to determine the fuzzy reliability of the two systems, whereas Singer [12] used LR-type fuzzy numbers to consider the fuzzy reliability problem."
"Therefore, the triangular fuzzy numbers used by Chen [7] are R A=R B=(0.00888,0.02,0.03112;1)R C=R D=(0.75552,0.80,0.84448;1)R E=(0.94434,1.00,1.05566;1)R F=R G=(0.04722,0.05,0.05278;1)R H=(0.00944,0.01,0.10560;1) Using Definition 8, we defuzzified R X(1) and obtained the estimated reliability of the system in the fuzzy sense as dR X(1),0 =0.137755"
[C] Comparison of this study with that of Cheng and Mon [6].Cheng and Mon [6] considered the following:
"Existing models predict different vortex number densities and they are only valid for the inertial subrange of the energy spectrum of turbulent flows. No model is available which is valid for a wider range of flow situations [7,8]. "
" In addition the vortex number distributions should be used to derive expressions relating to the fractional rate of surface renewal and mass transfer coefficients across gasâliquid and solidâliquid interfaces [9]. For these reasons, a systematic evaluation of available models on vortex number density is required as a foundation for further investigations."
"Several Eulerian methods for flow structure visualization are proposed in the literature. They are generally formulated in terms of the invariants of the velocity gradient tensor. These criteria includes the iso-surfaces of vorticity, stream lines, Helicity, Q-criterion, complex eigenvalues of the velocity gradient tensor, Î»2, swirl strength and pressure minimum [21]. "
"The applications are of a wide range and significance, including gesture recognition and human computer interaction ( Bilalet al., 2012; Radlak and Smolka, 2012; Nalepa et al., 2014), objectionable content filtering ( Lee et al.,2007), image retrieval ( Kruppa et al., 2002), image coding using regions of interest ( Chen et al., 2003), and many more."
An approach introduced by Hsu et al. (2002) takes advantage of common skin color properties in nonlinearly transformed YCbCr color space using an elliptical skin color model
"Skin color can be modeled using a number of techniques, including the Gaussian mixture model ( Greenspan et al., 2001) and the Bayesian classifier ( Jones and Rehg, 2002)"
An approach for adapting the segmentation threshold in the probability map based on the assumption that a skin region is coherent and should have homogenous textural features was introduced by Phung et al. (2003)
"Analysis of facial regions for effective adaptation of the skin model to local conditions was investigated by Fritsch et al.(2002); Stern and Efros, 2002; Kawulok, 2008; Kawulok et al., 2013 and Yogarajah et al., 2012."
"Simple textural features were used to boost the performance of a number of skin detection techniques and classifiers, including the ANN ( Taqa and Jalab,2010), non-parametric density estimation of skin and non-skin classes ( Zafarifar et al., 2010 ), Gaussian mixture models ( Ng and Pun, 2011), and many more ( Forsyth and Fleck,1999, Conci et al., 2008, Fotouhi et al., 2009, Abin et al.,2009)"
"A number of skin segmentation techniques emerged based on this observation: Kruppa et al. (2002) assumed that the skin blobs are of an elliptical shape, a threshold hysteresis was applied by Argyros and Lourakis (2004) and recently by Baltzakis et al. (2012)"
"In the case of the DSA ( del Solar and Verschae, 2004), the ROC curves were obtained by applying different values of the diffusion threshold ( ), explained earlier in Section 2"
"In order to provide a thorough comparison, we also investigated how the wavelet-based method behaves when the textural features are extracted from the skin probability maps rather than from the luminance as proposed in the original work of Jiang et al. (2007)."
The analysis of skin probability map domain for skin segmentation using a controlled diffusion was proposed by del Solar and Verschae (2004)
"Although the color-based skin models can be efficiently adapted to a given image, it was proved by Zhu et al.    (2004)  that it is hardly possible to separate skin from non-skin pixels using such approaches"
Conditional random fields were used by Chenaoua and Bouridane (2006) to exploit spatial properties of skin regions
An approach based on the cellular automata for determining skin regions was proposed by Abin et al. (2009).
"Although empirical experiments have shown that it is practically usable on a wide range of programs ( Fraser and Arcuri, 2012), Genetic Algorithms are a global search technique, which tend to induce macro-changes on the test suite"
"For example, to generate tests for specific branches to achieve branch coverage of a program a common fitness function ( McMinn, 2004) integrates the approach level (number of unsatisfied control dependencies) and the branch distance (estimation of how close a branching condition is to being evaluated as desired)"
"However, there remain several different parameters ( El-Mihoub et al., 2006): How often to apply the individual learning? On which individuals should it be applied? How long should it should be performed? In EvoSuite, the choice of how often to apply local learning depends on two parameters: Rate: The application of individual learning is only considered every r generations"
"As the problem is too complex to perform a theoretical runtime analysis (e.g., such as that presented by Arcuri (2009)), we therefore aim to empirically answer the following research questions"
"This is different from the result of our initial experiment ( Fraser et al.,2013), where the best configuration used seeding, small population size (five individuals), low rate of local search (every 75 generations), and a small budget of five fitness evaluations for local search"
"A particular aspect of this real-life, unbiased sample of classes is that the problems it represents are quite different to those considered as difficult search problems ( Fraser and Arcuri,2012): for example, a large share of the classes have environmental dependencies that make high coverage with EvoSuite impossible"
"Methods by which such OSINT data may be used to increase effectiveness in this manner include (but are not limited to): selection of vulnerable personalities, inclusion of ploys personally attractive to the target, and impersonation of a person in authority ( Huber et al.,2009)."
"The nearest approximation we are aware of is Scheelen et al.(2012), who investigated a single company by connecting with followers on LinkedIn, where the social media structure is based around employment."
"Irani et al. (2009) suggest that a record-matching approach to the problem can be fruitful, with identifiers like last name, birth year and country unlikely to change across records"
"Working with a wider range of features, Malhotra et al. (2012) design an ensemble classifier, with subclassifiers relying on individual features such as profile pictures and usernames."
"Goga et al.(2013) exploit innocuous social media profile information such as time-stamps, geographical location and writing styles to match user profiles, demonstrating that even where usernames and other traditional identifiers are disguised, users can still be identified based on their usage of the media"
"The general identity resolution approach of pairwise comparison is supported even more broadly by similar approaches in other domains of security analytics ( Zhang et al., 2016)."
"Each interviewee held internationally recognised certification in the domain area, reaching a minimum level of CREST CRT , allowing level of expertise to be compared and verified to international standards ( Knowleset al.,2016)"
"Furthermore, Kotson and Schulz (2015) and Dewan et al (2014) propose that organisations could use natural language processing techniques to maintain awareness of their online footprint, to be compared with received phishing emails, allowing identification of collection of OSINT by an attacker, and potential early warning of an Advanced Persistent Threat (APT)"
neither general MV-algebras [19] nor even the special case of Łukasiewicz MV-algebra are considered in this paper.
"According to the most commonly accepted definition, Big Data is characterized by the 4 Vs  [3]: volume, velocity, variety, and veracity"
"From the business perspective, the goal of these technologies is to process fast input streams, obtain real-time insights, and enable prompt reaction to them  [4]."
The resurgence of interest in CEP and SP systems has been accompanied by the use of cloud environments as their runtime platform. Clouds are leveraged to provide the low latency and scalability needed by modern applications  [5–7].
"First, cloud environments are subject to variations that make it difficult to reproduce the environment and conditions of an experiment  [9]"
"Simulators have been used in many different fields to overcome the difficulty of executing repeatable and reproducible experiments. Early research into distributed systems  [10] and grid computing  [11] used simulators, as well as the more recent field of cloud computing  [12–14]."
CEPSim extends CloudSim   [12] using a query model based on Directed Acyclic Graphs (DAGs) and introduces a simulation algorithm based on a novel abstraction called event sets.
"This article significantly extends the authors’ previous work  [15] by improving the discussion about CEPSim’s goals and assumptions, by introducing the event set concept, by presenting detailed descriptions of all simulation algorithms and a thorough evaluation of CEPSim."
"The basis of Complex Event Processing (CEP) was established by the work of Luckham on Rapide   [10], a distributed system simulator.Later on, the concepts were generalized and applied to the enterprise context in another study by Luckham  [16]. At about the same time, the database community developed the first classical Stream Processing (SP) systems such as Aurora   [17] and STREAM   [18]. CEP and SP technologies share related goals, as both are concerned with processing continuous data flows coming from distributed sources to obtain timely responses to queries  [19]."
"In CEP systems, users create queries (or rules) that specify how to process input event streams and derive “complex events”. These queries have usually been defined by means of proprietary languages such as Aurora Stream Query Algebra (SQuAl)  [17] and CQL  [21]."
"Despite standardization efforts  [22], a variety of languages are still in use today. Cugola and Margara  [19] classify existing languages into three groups"
"Pattern-based: languages are used to define patterns of events using logical operators, causality relationships, and time constraints. The Rapide   [10] language is an example of this category."
"The recent emergence of cloud computing has been strongly shaping the Big Data landscape. Many authors have recognized the symbiotic relationship between these areas  [23–25], as cloud computing environments can be used to store and process Big Data and also to enable new models for data services. For instance, Chang and Wills  [26] used a cloud platform to store big biomedical data, whereas Grolinger et al.  [27] proposed a platform for knowledge generation and access using cloud technologies."
"Current CEP research has been strongly influenced by cloud computing too. For instance, TimeStream   [5], StreamCloud   [6], and StreamHub   [7] are CEP systems that use cloud infrastructures as their runtime environments."
The prevalence and success of MapReduce has motivated many researchers to work on systems that leverage its advantages while at the same time try to overcome its limitations when used for low-latency processing. StreamMapReduce   [29] and M3   [30] are examples of MapReduce-inspired systems intended for stream processing
"Simulators are a popular tool that has been used in Grid Computing research  [11,33] for many years."
"Because of its limitations, CloudSim has originated many extensions in the literature  [9,34,35]."
"Guérout et al.  [34], on the other hand, focused on implementing the DVFS model on CloudSim. Finally, Grozev and Buyya  [35] presented a model for three-tier Web applications and incorporated it into CloudSim."
"Finally, the iCanCloud simulator  [14] is similar to CloudSim, but it can also parallelize simulations and has a GUI to interact with the simulator. Its application model, however, is based on low-level primitives and needs to be significantly customized to represent CEP applications"
"For instance, most CEP systems based on imperative languages also use DAGs to represent user queries. This is the case with Aurora   [17], StreamCloud   [6], Storm   [31], S4   [32], FUGU   [37], and many others."
"Systems using declarative languages, on the other hand, create execution plans from queries that can often be mapped into DAGs  [5,18]. Even for pattern-based query languages, previous studies  [38] have shown that is possible to transform them into DAGs."
"A stateless operator, or simply an operator, can process incoming events in isolation with no dependency on any state computed from previous events. For example, an Aurora filter is an operator that routes events to alternative outputs based on attribute values  [17]."
"A scheduling strategy can fundamentally determine the performance of a CEP system by optimizing for different aspects of the system, such as overall QoS  [17] or memory consumption  [40]. Because of this significance, CEPSim also allows different scheduling strategies to be plugged in and used during a simulation."
"The queries used in the experiments in this section have been extracted from Powersmiths’ WOW system  [41], a sustainability management platform that draws on live measurements of buildings to support energy management. Powersmiths’ WOW uses Apache Storm  [31] to process in near real-time sensor readings coming from buildings managed by the platform"
"This validation approach is similar to the ones adopted by other simulators, such as NetworkCloudSim  [9], iCanCloud  [14], and Grozev and Buyya  [35]."
"This experiment analysed CEPSim’s behaviour when simulating multiple queries running concurrently. To do so, first a Storm cluster was created at the Amazon EC2 service  [45]"
"DSPLs produce software capable of adapting to changes, by means of binding the variation points at runtimeÂ  [1]. This means that we have to model the elements that could be adapted dynamically as dynamic variation points and generate, at runtime, the different variants of the DSPL."
"Variability is modelled at different abstraction levels, mostly using feature models (FM)  [3] at the requirements level and UML profiles or Architecture Description Languages (ADLs)  [4-6] at the architectural level."
"Both services are designed to be integrated in a middleware for adaptive applications developmentÂ  [21], although in this paper we focus on presenting the details of how the DRS accomplishes the runtime reconfiguration of mobile applications. "
" In contrast, in DSPLs the variability model describes the potential range of variations that can be produced at runtime for a single product, i.e.Â  the dynamic variation points, which must refer to the system architectural components. Therefore, in DSPLs the system architecture supports all possible adaptations defined by the set of dynamic variation pointsÂ  [1].Then, as part of a DSPL definition the engineer must define:"
"In this context, most DSPL approaches share some common properties with the Autonomic Computing paradigmÂ  [2] (AC) such as the monitoring of the environment and the generation of successive configurations."
"An overview of our approach and a case study are also presented.3.1ChallengesFrom the literature it is possible to identify the differences between existing DSPL approaches. The main ones areÂ  [24]: (1)Â how they model the dynamic variation points; (2)Â whether the successive configurations generated by the approach are optimal regarding some criteria or not; (3)Â if the reconfiguration plan is generated at design or runtime; (4)Â the decision making process used to trigger a reconfiguration, and (5)Â if they can be used to develop applications for resource constraint devices, or not."
" So, at run-time we generate the successive software architecture configurations by binding the architectural variation points specified in a variability languageÂ  [25]. Regarding the variability language used, we propose to use CVL as it eases the generation of the architectural variation points compared with, for instance, feature modelsÂ  [25]. In CVL the VSpecs tree is linked with the base model (i.e.Â the software architecture model) so, the architectural variation points are obtained almost directly, as we will show later."
" Note, that an exact algorithm cannot be used because the problem to be solved has been proven to be NP-hard (non-deterministic polynomial-time hard)Â  [26]."
"Generating the reconfiguration plan at runtime. Most DSPL approaches generate, at design time, the configurations that will be deployed at runtime  [13,27-29]. However, the potential number of configurations normally grows exponentially with the number of dynamic variation points. In order to cope with this serious problem, some approaches consider only a subset of the valid configurations at runtime (e.g.Â the most probable ones), which are pre-loaded in the system."
"Concretely, as shown inÂ  [23], exact techniques can only be applied to small cases at the cost of a very high execution time. Nevertheless, artificial intelligence algorithms can find nearly-optimal solutions in an efficient and scalable way. "
"Synthesis, also known as Church's solvability problem [5], is the problem of defining a circuit that continually reacts on an infinite input stream by producing one output letter after receiving one input letter"
"We assume that the reader is familiar with temporal logics [11,6] and the distributed synthesis question [12,10,9,7]. For architectures [12,9,10,7], it is enough to know that they are directed graphs, whose vertices are processes that operate synchronously on a joint system clock. "
e follow [7] by restricting the variables such that each variable can only occur on the outgoing edges of one process (but may be read by many processes) and by allowing for processes that have a fixed finite implementation
"The extension to decidable architectures is a straightforward adaptation from the extension described by Finkbeiner and Schewe [7]. Their argument is that, in the case where there is an information fork, a situation where there are two processes p0 and p1 who can receive information from the environment â directly or forwarded through a communication chain of arbitrary length â such that this pathway cannot be intercepted by the other process, then synthesis is undecidable."
"In this section we establish a close relation between our notion of strong regularity and the one usually studied in max min algebra [2 4,12,14]"
"As K is closed, by the usual Minkowski theorem [25, Corollary 18.5.1] it can be represented as the set of positive linear combinations of its extremal rays (recall that w K is called extremal if u+v=w and u,v K imply that u and v are proportional with w), which generate the whole Lin(C y)"
"For a monograph in conventional convexity see, eg, Rockafellar [25]."
"The utility function introduced there relies on the notion of possibilistic mixture, where the possibilistic mixture (which under some natural conditions is also a possibilistic measure [10]) of the possibilistic measures 1, 2 with possibilities , ,max ( , )=1, is defined as max (min ( , 1),min ( , 2)), that is, as a point on the max min segment [ 1, 2]"
"For example, in [8] Dubois and Prade developed an axiomatic approach to quantitative utility theory"
"The ability to integrate multisensory information is a fundamental feature of the brain that provides a robust perceptual experience for an efficient interaction with the environment ( Ernst & Bulthoff, 2004; Stein & Meredith, 1993; Stein, Stanford & Rowland,2009)"
"Similarly, computational models for multimodal integration are a paramount ingredient of autonomous robots to forming robust and meaningful representations of perceived events (see Ursino, Cuppini, & Magosso (2014 for a recent review)"
"Multimodal representations have been shown to improve robustness in the context of action recognition and action-driven perception, human-robot interaction, and sensory-driven motor behavior ( Bauer, Magg, & Wermter, 2015; Kachouie, Sedighadeli, Khosla, & Chu, 2014; Noda, Arie, Suga & Ogata 2014)"
"Since real-world events unfold at multiple spatial and temporal scales, artificial neurocognitive architectures should account for the efficient processing and integration of spatiotemporal stimuli with different levels of complexity ( Fonlupt, 2003; Hasson, Yang, Vallines, Heeger, & Rubin, 2008; Lerner, Honey, Silbert, & Hasson, 2011; Taylor, Hobbs, Burroni, & Siegelmann, 2015)"
"The human cortex comprises a hierarchy of spatiotemporal receptive fields for features with increasing complexity of representation ( Hasson et al., 2008; Lerner et al., 2011; Taylor et al., 2015), ie higher-level areas process information accumulated over larger spatiotemporal receptive windows"
"During their development, children have a wide range of perceptual, social, and linguistic cues at their disposal that they can use to attach a novel label to a novel referent ( Hirsch-Pasek, Golinkoff, & Hollich, 2000, chapter 6)"
"Recent experiments have shown that human infants are able to learn action word mappings using cross-situational statistics, thus in the presence of sometimes unavailable ground-truth action words ( Smith & Yu, 2008)"
"This hypothesis is supported by neurophysiological studies evidencing strong links between the cortical areas governing visual and language processing, and suggesting high levels of functional interaction of these areas for the formation of multimodal representations of audiovisual stimuli ( Belin, Zatorre, Lafaille, Ahad, & Pike, 2000; Foxe et al., 2000; Belin, Zatorre, & Ahad, 2002; Pulverm ller, 2005; Raij, Uutela, & Hari, 2000)."
"From a neurobiological perspective, neurons selective to actions in terms of time-varying patterns of body pose and motion features have been found in a wide number of brain structures, such as the superior temporal sulcus (STS), the parietal, the premotor and the motor cortex ( Giese & Rizzolatti, 2015)"
"In particular, it has been argued that the STS in the mammalian brain may be the basis of an action-encoding network with neurons driven by the perception of dynamic human bodies ( Vangeneugden, Pollick, & Vogels, 2009), and that for this purpose it receives converging inputs from earlier visual areas from both the ventral and dorsal pathways ( Beauchamp, 2005; Garcia & Grossman, 2008; Thirkettle, Benton, & Scott-Samuel, 2009)"
"Furthermore, neuroimaging studies have shown that the posterior STS shows a greater response for audiovisual stimuli than to unimodal visual or auditory stimuli ( Beauchamp,Lee, Argall, & Martin, 2004; Calvert, 2001; Senkowski, Saint-Amour, Hfle, & Foxe, 2011; Wright, Pelphrey, Allison, Mckeown, & Mccarthy, 2003)"
"Thus, the STS area is thought to be an associative learning device for linking different unimodal representations and accounting for the mapping of naturally occurring, highly correlated features such as body pose and motion, the characteristic sound of an action ( Barraclough, Xiao, Baker, Oram, & Perrett, 2005; Beauchamp et al., 2004) and linguistic stimuli ( Belin et al., 2002; Stevenson & James, 2009; Wright et al. 2003)"
"These findings together suggest that multimodal representations of actions in the brain play an important role for a robust perception of complex action patterns, with the STS representing a multisensory area in the brain network for social cognition ( Adolphs, 2003; Allison, Puce, & McCarthy, 2000; Beauchamp, 2005; Beauchamp, Yasar, Frye, & Ro, 2008)."
"Each network layer comprises a self-organizing neural network that employs neurobiologically-motivated Hebbian-like plasticity and habituation for stable incremental learning ( Marsland, Shapiro, & Nehmzow, 2002)"
"Network layers 1 and 2 comprise a two-stream hierarchy for the processing and subsequent integration of body pose and motion features, resembling the ventral and the dorsal pathway respectively for the processing of complex motion patterns ( Giese & Poggio, 2003)"
"The integration of pose and motion cues is carried out in network layer 3 (or G STS) to provide movement dynamics in the joint feature space ( Parisi et al.,2015)"
"Hierarchical learning from contiguous Growing When Required (GWR) networks ( Marsland et al., 2002) shapes a functional hierarchy that processes spatiotemporal visual patterns with an increasing level of complexity by using neural activation trajectories from lower-level layers for training higher-level layers"
"The modeling of neurobiologically observed principles underlying audiovisual integration in the STS for speech and non-speech stimuli, such as superadditivity ( Calvert, Campbell, & Brammer, 2000), spatial and temporal congruence ( Bushara, Grafman, & Hallett, 2001; Macaluso,George, Dolan, Spence, & Driver, 2004), and inverse effectiveness ( Stevenson & James, 2009), was out of the scope of this paper and will be subject of future research."
"For instance, several neurophysiological studies have evidenced strong interaction between the visual and motor representations, more specifically including the STS, parietal cortex, and premotor cortex (see Giese & Rizzolatti (2015) for a recent survey), with higher activation of neurons in the motor system for biomechanically-plausible, perceived motion sequences ( Miller & Saygin, 2013)"
"Motivated by the process of input-driven self-organization exhibited by topographic maps in the cortex ( Miikkulainen, Bednar, Choe, & Sirosh, 2005; Nelson, 2000; Willshaw & von der Malsburg, 1976 ), we proposed a learning model encompassing a hierarchy of Growing When Required (GWR) networks ( Marsland et al., 2002)"
"This kind of hierarchical aggregation is a fundamental organizational principle of cortical networks for dealing with perceptual and cognitive processes that unfold over time ( Fonlupt, 2003)."
"It has been shown that lexical features can be learned using recursive self-organizing architectures ( Strickert & Hammer, 2005), obtaining action word representations from a phonemic representation of recognized audio"
"Such a processing scheme would be in line with neurophysiological evidence supporting the hierarchical processing of aural features in the auditory cortex with increasing temporal receptive windows ( Lerner et al., 2011 )"
"Mental imagery concerns cognitive processes for the creationandmanipulationofmentalimages,andfordecisionmakingtasksonvisualobjectmatching(Kosslyn,1996;Lamm,Windischberger,Moser,&Bauer,2007).Inatypicalmentalrotationexperimentofcognitivepsychology,aparticipanthastomentallyrotateanobjectperceivedinapicturetodecideifitisthesameasatargetobjectordifferentfromit(i.e.aflippedversionofit),andthenindicatetheanswerbypressingoneoftwobuttons(Shepard&Metzler,1971;Wexler,Kosslyn,&Berthoz,1998)."
"Mental rotation has been widely investigated not only incognitive psychology, but also in cognitive neuroscience andcomputational modelling (Kosslyn, 1996; Zacks, 2008). Initially,it was proposed that the brain mechanisms underlying mentalrotation mainly involve visual and spatial perception systems(Corballis & McLaren, 1982; Shepard & Metzler, 1971). "
"According to this view, off-linecognition, such as mental rotation and imaging, is body based:‘‘even when decoupled from the environment, the activity of themind is grounded in mechanisms that evolved for interactionwith the environment—that is, mechanisms of sensory process-ing and motor control’’ (Wilson, 2002). "
"Brain-imaging evidence on the brain areasmost involved in mental rotation supports the idea that mentalrotation indeed depends on a strong integration of sensorimotor processes and covert mental simulation of motor movements (see Zacks, 2008, for a review). In particular, based on refined experimental paradigms, mental rotation has been proposed to Involve the following processes (Lammetal.,2007):"
"Indeed, endowing robots with mental rotation capabilities could increase their planning abilities in relation to the manipulation of objects, in particular, as planning in robots has mainly focused on navigation (Baldassarre, 2001, 2003; Dissanayake, Newman, Clark, Durrant-Whyte, & Csorba, 2001; Meyer & Filliat, 2003) and reaching tasks (Khatib,1986 ; Masehian & Sedighizadeh, 2007) rather than on object manipulation. "
"Based on this evidence, these ar-eas are thought to play a key role in implementing the propri-oceptive and visual information integration and transformationsupportingthecoreprocessesofthedynamicmentalrotationpro-cesses(Zacks,2008)"
"Givenitshigh-levelwithinthemotorhierarchy,thisareamightorchestratemental rotation at a high-level, as suggested by its role in motorimaging(Grafton,Arbib,Fadiga,&Rizzolatti,1996)"
"Neural maps are suitable to model cortical areas as they capture their important2D topological organisation and also facilitate the analysis andvisualisation of the processes occurring within them (Caligiore,Parisi, & Baldassarre, 2014). Population codes (Pouget, Dayan, &Zemel, 2003) are based on the idea that information (on stimuli and actions) is encoded in the brain on the basis of the activation of populations of units organised in neural maps having a broad response field"
"To implement the decision making process involved in themental rotation task, the model uses amutual inhibition model (Bogacz et al., 2006; Usher & McClelland, 2001)"
"This model (together with otheranalogous models, e.g. Bogacz et al., 2006) is very important, as it allows there production of there action times of ten recorded in psychological experiments (Caligioreetal., 2010,2008; Erlhagen & Schöner, 2002). It is one of the most accredited models of decision making processes taking place in the human brain (Bogacz, 2007)."
"For example, the interpolation methods used in some commercial mocap systems such as EVaRT and Vicon are only suitable to deal with the short-time (<0.5s) data missing problem [1]. "
"Aristidou et al. [1] used Kalman Filters to estimate missing markers in real-time without the support of any human model. However, the filtered human motion exhibits visible short latency. Moreover, Kalman Filters may fail when markers are missing or the motion data is corrupted by noise for an extended time period [12]."
"As shown in the last sub-image of Fig11(c), one head marker predicted by Dynammo [30] drifts a little away due to the reason that this marker is missing for a long period of time at the end of the motion data"
"To demystify this inconsistency, we find that SVT [30] convergences too early in handling a large-scale imperfect human motion matrix in these two cases where the total frame numbers are 4840 and 4905"
"As shown in [1–4], optimal preview control for linear systems has been well studied. The problem of preview control has been extended via further research."
"In [5–9], H∞ and H2 criteria were introduced into preview control, and robust optimal preview control for linear and nonlinear systems has been discussed."
The authors in [10–12] investigated optimal preview control for systems with multiple sampling rates by using the discrete-time lifting technique.
"In [13–15], optimal preview control for linear time-variant systems was considered."
"In recent years, based on descriptor system theory, researchers have considered some descriptor causal systems and designed an optimal controller with preview compensator [16–18]."
"The authors in [31] designed a piecewise linear time-invariant switching control law, which leads to a guarantee of Lyapunov stability with an exponential rate of convergence for the state."
"Based on the method, a kind of switching controller for discrete time system was proposed [32]."
"In [33], a localization-based method was introduced, which is manifested as the rapid convergence of the switching controller. Ref. [34] promoted the results and discussed the problem of optimal localization. Other several methods of adaptive control based on multiple models were examined in [35]."
" By defining monitoring function and switching principle similarly to [31], the problem is formally converted into a normal optimal control problem that includes preview information. The preview control controller for systems with large uncertainties can be finally obtained in this way, and adaptive preview control theory using multiple models can be established."
"Nuclear power is often regarded to be amongst the safest forms of electricity generation, taking into account the complete world-wide electricity production chains, with some arguing that this result holds even after the possibility of large nuclear accidents is included in the analysis (Kearns, Thomas, Taylor, & Boyle, 2012). "
"A number of methodologies and software packages, often referred to as Decision Support Systems, have been developed to aid this (Bartzis et al., 2000; French, 1996; Geldermann et al., 2009; Hämäläinen, Lindstedt, & Sinkko, 2000; Hoe & Müller, 2003; Landman, Päsler-Sauer, & Raskob, 2014; OECD/NEA, 2000; Papamichail & French, 2005; Wex, Schryen, Feuerriegel, & Neumann, 2014). "
"The literature on dynamic decision making and economic optimisation in the response and recovery phases is, however, considerably less mature (Altay & Green III, 2006)."
"Immediate response to a nuclear accident involves procedures for evacuation, sheltering, iodine tablets distribution, whereas recovery measures include long-term relocation and remediation, as well as potential repopulation of the affected areas (Gering, Gerich, Wirth, & Kirchner, 2013; Higgins et al., 2008; Munro, 2013). "
"The key difference between the response and recovery stages is, therefore, the timescale on which the relevant measures are implemented: while emergency response can take place on the timescale of minutes, hours and days in the immediate aftermath of a nuclear disaster, recovery measures often span over weeks, months and years (DECC, 2013)."
"Short-term response, therefore, is expected to have typical features of emergency planning, when precautionary actions may not necessarily be justified in economic terms (Dana, 2002; Klinke & Renn, 2001)."
"Long-term post-accident response and recovery planning, on the other hand, is characterised by significantly lower levels of radiological uncertainty, and requires multiple economic as well as non-economic factors to be taken into account (French, 1996; Geldermann et al., 2009; Hämäläinen et al., 2000)."
"It is possible that governments are going to prioritise the economic factors by seeking to minimise the total cost of preventative and recovery measures (Munro, 2011; 2012)."
"Of the vast literature on Chernobyl, the two key studies on the long-term measures are (Lochard & Schneider, 1992) (contamination and population distributions, resettlement and health costs) and (Jacob et al., 2009) (remediation costs in rural areas); see also (IAEA, 2006; Karaoglou, 1996). These studies are based on the actual data from the affected populations and territories, and recommend a variety of cost-efficient strategies."
"The existing studies on Fukushima have focused on providing contamination maps and summarizing early-stage radiological impacts on the environment (IRSN, 2011; 2012), analysing health effects for the affected populations (González et al., 2013; Harada et al., 2014; WHO, 2013), assessing economics of decontamination (Munro, 2013) and people’s intention to return to their evacuated family homes (Munro & Managi, 2014). A comprehensive up to date review of the multiple consequences of the Fukushima disaster (Ahn et al., 2015) advocates that “scientific and academic communities should start efforts for establishing the scientific bases, both natural and social, for better societal resilience.”"
"The detailed input data for these packages can be obtained from economic and population databases such as GIS for many locations throughout the world (De Silva & Eglese, 2000), including those near the existing nuclear installations."
"The DSS that is most relevant to the present study is COCO-2 (Higgins et al., 2008)."
"It is possible that emergency evacuations and long-term relocations inside these relatively unaffected areas might have caused psychological and economic harm comparable to (or even exceeding) the potential radiological harm averted by these actions (Ahn et al., 2015), as arguably was also the case in for a number of evacuations in the aftermath of Chernobyl (IAEA, 2006; Karaoglou, 1996; Walinder, 1995)."
"A dartboard-like structure (Gering et al., 2013) might be a good starting point, although a more detailed mosaic-like pattern tailored around the urban areas within the circular zones could provide a greater level of control "
"Ultimately, the feasibility of having different treatments within specified boundaries will depend on how densely populated the entire prototype exclusion zone is, which is part of a wider issue of siting for nuclear power installations (Grimston et al., 2014)."
"To address the need for the spatial and temporal flexibilities, we develop a decision-making model for a single economic location (say, a town, a village or an area of agricultural land) based on Bellman’s Optimal Control Theory (Bellman, 1956), which is at the basis of the Operations Research (OR) methodology. "
"In reality, any long-term decision making takes into account a number of non-economic factors such as acceptance of a decision by various groups including the public, decision-makers, stakeholders and experts (Geldermann et al., 2009) alongside the standard radiological and economic evaluation methods."
"Even though some short-term temporal and spatial flexibilities in sheltering times, iodine prophylaxis and evacuation are also possible (Dillon, 2014; Gering et al., 2013), the present study only focuses on the largely deterministic mid- and long-term problem setting."
"Much of the mid-term and long-term radiation exposure, both in urban and in rural areas, comes from ground shine, defined as “external dose direct from radioactive materials deposited on the ground” WHO (2013), which is a result of the initial radioactive deposition (Harada et al., 2014; Lochard & Schneider, 1992)."
"We restrict our analysis to ground shine and ingestion of contaminated food produce originated in rural areas, the two types of exposure most relevant on the longer timescales (Jacob et al., 2009)."
"The deposition period could last several hours, days or weeks (Ahn et al., 2015; Katata et al., 2015), and the radioactive material will usually be carried in a plume of smoke or ash depending on the type of accident that has occurred. "
"Chernobyl, the biggest nuclear disaster in history, provided extensive information on the economics of a severe nuclear accident (Jacob et al., 2009; Lochard & Schneider, 1992). "
"The economics of nuclear decontamination and assessment of policy options for the management of land around Fukushima is described in Munro (2013); see also (Munro, 2011; 2012) and the relevant WNN reports."
"According to (Walinder, 1995), “it is impossible to predict, by means of a mathematical expression, the specific outcome of a low radiation dose”."
"The concept of VSLY was developed in public policy making to put monetary value on the reduction of risk of death for an average ‘statistical’ individual (Higgins et al., 2008)."
"In addition, a number of studies since Chernobyl have shown that the health effects due to stress may be commensurate with the health effects associated directly with the radiological exposure (IAEA, 2006; Karaoglou, 1996)."
"It is assumed that the collaborating organisations follow OWLS ontology for services, as OWLS is the most widely used standard specification for adding semantics to web services [10]. OWLS provides a standard set of ontologies to the collaborating organ-isations for describing and composing web services. "
"The existing execution mechanisms from litera-ture enact automatically generated workflows for single organisa-tions only, however they can handle adhoc processes that are outside the boundaries of the organisation in the workflows [1]."
"A Business process can be defined as a set of one or more linked procedures or activities which collectively realise a business objective or policy goal, normally within the context of an organisational structure defining functional roles and relationships [32]"
"1 is the automation of a business process, in whole or part, during which documents, information or tasks are passed from one participant to another for action, according to a set of procedural rules [32]"
A workflow has two main stages [32]: Build-time stage this refers to the stage where workflow descriptions of the business process are defined or changed
"For any two organisations to proceed in business, they need to have compatible workflows and compatible means that there should be an agreed sequence of activities exchanging collaborative messages and information [6]"
The set of all interface activities in a workflow is called interface process [6]
"When two of more organisations do business together, the need for workflow collaboration across multiple organisations arises [5]"
"Considerable amount of effort is needed to ensure that workflows are compatible [7,23] and proceed into the execution stage."
"Recent research on workflow collaboration focuses on reconciling existing incompatible workflows [13,3]"
"In an alternative top-down approach, organisations meet, discuss and design collaborative processes and then implement them [5]"
"Another paradigm in the literature is automatic workflow generation, which is based on AI planning where a workflow is considered as a plan [2,10]"
"If every activity in a workflow is treated as a web service, a workflow represents a plan of web services to achieve the desired goal state from a given initial state [22]"
"Furthermore, it is a highly efficient planning system and has a Web Ontology Language for Services (OWLS) type mechanism for representing atomic tasks and decomposing composite tasks into atomic tasks [27]"
"The similar mechanism of OWLS and SHOP2 to hierarchically decompose complex tasks into sub tasks makes it straightforward to map OWLS definitions directly into SHOP2 domain [27,33] and create workflows based on the translated domain."
"In the context of semantic web services, PDDL is neither too restrictive nor too expressive and is considered as a viable compromise between expressivity and efficiency [20]"
"It is a workflow modelling language [16] and lacks the semantic precision required for automatic business process generation and execution [17], and so we cannot use it as a notation for the proposed framework"
"An aggregation significantly can reduce computational costs for obtaining inference results in DPGMs, but requires a careful consideration of indirect influences as Motzek and Mller(2015a) has shown"
"Then, in fact, the presented DMIA model represents a so-called activator dynamic Bayesian network (ADBN) Motzek and Mller (2015a,2015b) and one obtains well-defined semantics"
"One has to differentiate between induced observations and true observations; a differentiation related to Pearl's ( Pearl,2002) introduction on the do-calculus and is best explained at an example: Considering an infectious-disease monitoring system, one has to differentiate between a person being healthy, because he has been healed and between a person being tested to be healthy"
"(Boutilier et al.1996) of Xt on all its possible dependencies Z , given +set, Xt is decoupled from all other potential sources of (non)impact, and the observation xt is completely accredited to SEt"
"1 1Stephen Elop, the former Executive Vice President of Microsoft's Devices and Services and (at the time of the comment) the CEO of Nokia Corporation, speech at D9, June 1, 2011 Hence, the sheer number of applications in the marketplace has become increasingly important in marketing new mobile devices (see eg, Chen, 2010;Reuters, 2012; Lee, 2015; Smith, 2015)"
"The logic behind establishing the ecosystems is grounded on the theory of network externalities ( Katz and Shapiro,1985)"
"Due to network externalities, a large number of application developers within the ecosystem is expected to lead to a large number of applications that, in turn, will attract customers and drive device sales, leading to a virtuous circle ( Holzer and Ondrus, 2011)."
"In this study, the concept of mobile application ecosystem refers to an interconnected system comprising an ecosystem orchestrator, mobile application developers, and mobile device owners, all of whom are connected through a marketplace platform ( Hyrynsalmi, Sepp nen and Suominen,2014)"
"Hence, a mobile application ecosystem is a derivate of the more general concept of a software ecosystem ( Jansen, Finkelstein, and Brinkkemper, 2009; Bosch, 2009; Manikas and Hanssen, 2013 )."
"The increased complexity calls for a better understanding of the boundaries and structures of the ecosystems (eg, Jansen et al. 2009; Gueguen and Isckia, 2011; Hanssen, 2012)"
"Prior research has investigated the success factors of the iPhone ( Laugesen and Yuan, 2010; West and Mace,2010), the distribution and capture of value in the mobile phone supply chains ( Dedrick, Kraemer, andLinden, 2011), developers perspectives on the mobile application markets ( Lee, Lee, Shim, and_Choi, 2010; Holzer and Ondrus, 2011; Schultz, Zarnekow, Wulf, and Nguyen, 2011 ), the dynamics of the application marketplaces ( J rvi and Kortelainen, 2011; Hyrynsalmi, Suominen, and Sepp nen, 2013; Jansen and Bloemendal, 2013), standard wars and platform battles ( Heinrich, 2014; Gallagher, 2012; van de Kaa and de Vries, 2015; van de Kaa, van den Ende, de Vries and van Heck,. 2011 ) and cooperation within ecosystems ( Gueguen and Isckia, 2011)"
"Network effects can accrue from direct externalities, whereby utility increases as the number of users consuming increases; and indirect externalities, whereby the demand for a product depends on the existence of another product ( Katz and Shapiro, 1985)"
"Sellers engage in multi-homing to gain access to larger potential markets ( Rochet and Tirole, 2006), to offer their products to the same customers across different platforms, and to reduce dependency on a single market and orchestrator ( Idu, van de Zande, and Jansen, 2011)"
"Prior research has focused on software vendors multi-homing in console games marketplaces ( Landsman and Stremersch,2011), Software as a Service (SaaS) marketplaces ( Burkard, Draisbach, Widjaja, and Buxmann, 2011; Burkard, Widjaja, and Buxmann, 2012), and also within Apple's ecosystem ( Idu et al.,2011)"
"In their study on the gaming console market, Landsman and Stremersch (2011) found that the multi-homing of games has a negative effect on sales at the marketplace level, although the negative effect decreases when a platform matures or gains market share"
"Idu et al. (2011) investigated the iPhone, iPad, and Mac software marketplaces, and found that, out of the top 1,800 applications, 17.2% were multi-homed in two marketplaces and 2.1% in all three marketplaces."
"Furthermore, as pointed out by Hyrynsalmi et al. (2012) , only a small share of all applications published in the marketplace are actually downloaded, and even fewer are actually used by customers"
"For instance, in the fields of Natural Language Processing (NLP) and IR, ontology-based semantic similarity measures have been used in Word Sense Disambiguation (WSD) methods [92] , text similarity measures [86], spelling error detection [20], sentence similarity models [44,66,91], paraphrase detection [36], unified sense disambiguation methods for different types of structured sources of knowledge [73], document clustering [31], ontology alignment [30], document [74] and query anonymization [11], clustering of nominal information [9,10], chemical entity identification [40], interoperability among agent-based systems [34], and ontology-based Information Retrieval (IR) models [55,62] to solve the lack of an intrinsic semantic distance in vector ontology-based IR models [23] "
"For this reason [57,1.1], ontology-based semantic similarity measures exclusively based on is-a relationships are currently the best and most reliable strategy to estimate the degree of similarity between words and concepts [58], whilst the corpus-based similarity measures are the best strategy for estimating their degree of relatedness [8]."
"Thus, the proposal for concept similarity models to estimate the degree of similarity between word and concept pairs has been a very active line of research in the fields of cognitive sciences [106,124], artificial intelligence and Information Retrieval (IR) [107]"
"In the field of bioengineering, ontology-based similarity measures have been proposed for synonym recognition [24] and biomedical text mining [14,98,112]"
"Mendling et al [80] study the current practices in the activity labeling of business processes, whilst Dijkman et al [32] propose a similarity metric between business process models based on an ad-hoc semantic similarity metric between words in the node labels and attributes, as well as the structural similarity encoded by the concept map topology"
"HESML V1R2 [60] is distributed as a Java class library ( HESML-V1R2.jar) plus a test driver application ( HESMLclient.jar), which have been developed using NetBeans 8.0.2 for Windows, although it has been also compiled and evaluated on Linux-based platforms using the corresponding NetBeans versions"
"Likewise, Leopold et al [68] propose an automatic refactoring method of activity labels in business process modeling based on the automatic recognition of labeling styles, and Leopold et al [67] propose the inference of suitable names for business process models automatically"
"In addition to the aforementioned IC models [46], Seddiqui and Aono [120] and Pirr and Euzenat [104] propose two further intrinsic IC models not implemented by HESML which are based on the integration of all types of taxonomical relationships, and thus especially designed for semantic relatedness measures"
"In addition, we plan to provide ongoing support for further ontologies such as Wikidata [126] and the Gene Ontology (GO) [5] among others, as well as further similarity and relatedness measures"
"PosetHERep is based on our adaptation of the well-known half-edge representation in the field of computational geometry [19], also known as a double-connected edge list [17,2.2], in order to efficiently represent and interrogate large taxonomies."
"The functionality and software architecture of HESML allow the efficient and practical evaluation of large word similarity benchmarks such as SimLex [50] and ontology-based similarity measures based on the length of the shortest path, whose implementation in other software libraries requires a high computational cost that prevents their evaluation in large experimental surveys [58] and datasets"
"All the experiments compute the Pearson and Spearman correlation metrics for a set of ontology-based similarity measures on each word similarity benchmark shown in table 22, as detailed in [56]"
"These scripts take the raw output files generated by the experiments in table 11 and produce the final assembled tables as shown in [56 58], as well asfigures 2 and 3 showing the interval significance analysis in [56]"
The ReproZip program was used for recording and packaging the running of the HESMLclient program with all the reproducible experiments shown in table 11 in the HESMLv1r1 reproducible exps.rpz file available at [64]
"Because of the lack of space, WNSimRep v1 is detailed in a complementary paper, which together with the dataset files, is publicly available at [63]"
"All the corpus-based IC models are derived from the family of *add1.dat WordNet-based frequency files included in the [95] dataset, which is a dataset of corpus-based files created for a series of papers on similarity measures in WordNet, such as [93] and [96]"
"The goals of the experiments described in this section are as follows: (1) the experimental evaluation of the PosetHERep representation model and HESML, as well as their comparison with the state-of-the-art semantic measures libraries called SML [48] and WNetSS [15]; (2) a study of the impact of the size of the taxonomy on the performance and scalability of the state-of-the-art semantic measures libraries; and finally, (3) the confirmation or refutation of our main hypothesis and research questions; Q1 and Q2 introduced in section 1.1."
"On the other hand, in order to evaluate and compare the performance of WNetSS with HESML and SML, we compare the running-time of the three libraries in the evaluation of the Jiang-Conrath similarity measure [52] with the Seco et al IC model [119] in the SimLex665 dataset [50]."
"We have introduced a new and linearly scalable representation model for large taxonomies, called PosetHERep, and the HESML V1R2 [60] semantic measures library based on the former"
"ISO/IEC/(IEEE) (2007) defines an architecture as composed of: (a) the fundamental organization of a system embodied in its components; (b) their relationships to each other, and to the environment; and (c) the principles guiding its design and evolution. "
"On that basis, Panunzio and Vardanega (2013) regards the concept of software reference architecture as proceeding from:"
"In his 1972 ACM Turing lecture (Dijkstra, 1972), E.W. Dijkstra advocated a constructive approach to program correctness where program construction should follow – instead of precede – the construction of a solid proof of correctness."
"In accord with Sifakis (2005), we maintain that composability is achieved when the designated properties of individual components, captured in terms of needs and obligations, are preserved on component composition, deployment on target and execution."
"The WCET is a local property of the program (that is, the service attached to the interface in question): composability in the time dimension (Puschner et al., 2009) is achieved if the interfering effect caused by the presence of other components in the system does not prevent a safe and tight WCET bound to be determined for every single interface service"
"They are used in approaches (like the one presented herein) which present an endogenous treatment of non-functional properties (i.e., outside of the component) (Crnkovic et al., 2011). "
Chaudron and Crnkovic (2008) defines a software component as “a software building block that conforms to a component model”.
The requirement of “independent deployment of components” entailed by the definition by Szyperski (2002) is currently not a core requirement for us (and neither is in many other component-oriented approaches). 
"The semantics of that declarative language emanates from the chosen computational model: the Ravenscar Computational Model (RCM) (Burns et al., 2014) in our case. "
"In terms of the rich classification proposed in Crnkovic et al. (2011), our component model (i) addresses the modeling and implementation phases of the development life cycle, (ii) is independent from the programming language, (iii) provides constructs for interface specification, (iv) allows expressing a limited set of interaction patterns, (v) supports specification, composition and analysis of non-functional properties, and (vi) is special-purpose as intended to high-integrity embedded real-time systems."
Gößler and Sifakis (2002) focuses on integration of components with heterogeneous interactions and execution paradigms. 
"The framework aims at correct-by-construction design by achieving component composability and compositionality. That work later evolved in the conception of the BIP framework (Behaviour, Interaction, Priority) (Basu et al., 2006)."
"SaveCCM (Hansson et al., 2004) targets heavy vehicular systems. That component model supports both time-triggered and event-triggered activation events and its components are hierarchical."
"The ROBOCOP component model (Muskens et al., 2005), targets the consumer electronic domain. "
"Giotto (Henzinger et al., 2001) is a progenitor in a family of time-triggered languages and tools, which specialize for control processing where deterministic time of execution is inbred to the domain culture (Henzinger et al., 2003). "
"The Ptolemy project (Lee, 2001) studies modeling, simulation, and design of concurrent, real-time, embedded systems realized as an assembly of concurrent components."
"The ISO standard 42010 (ISO/IEC/(IEEE), 2007) stipulates that “architectural description of the system is organized into one or more constituents called views”, and a view is a partial representation of a system from a particular viewpoint, which is the expression of some stakeholders’ concerns."
"The value for the WCET can later be refined with bounds for a given target platform obtained by timing analysis (Wilhelm et al., 2008)."
"we defined the whole set of allowable containers and connectors in a library of code archetypes (Panunzio and Vardanega, 2012), which vastly simplifies automatic code generation."
"The possibility that synchronization increases with frequency is commensurate with in-vitro cell recordings ( Rocha,Doiron,Shea-Brown, Josic,&Reyes,2007) and computer simulation of both integrate and fire and Hodgkin Huxley type models ( Chawla,Lumer,&Friston,1999)."
"The first and second order derivatives of the MFCCs (with respect to time) are sometimes used as additional features ( Gutig&Sompolinsky,2009) but did not improve recognition performance on our database."
"Whilst the work in this paper provides a useful starting point, it does not make use of the network view of brain function; Price, Thierry, and Griffiths (2005), for example, propose that the human brain has not developed macro-anatomical structures dedicated to speech processing, but rather that speech-specific processing emerges at the level of functional connectivity among distributed regions"
"As the magnitude spectrum of speech is known to be stationary over a period of approximately twin=20 100ms ( Rabiner & Juang,1993) we broke up each speech time series into frames of length twin=50ms"
"The importance of a hierarchy of temporal scales is emphasized in recent work by Ghitza (2011) who provides evidence that current models of speech perception, which are driven by acoustic features alone, provide an incomplete description of speech recognition phenomena"
"The notion that regions higher up in the auditory cortical hierarchy process information at longer time scales has recently been made use of in a model of auditory sequence recognition based on stable heteroclinic channels ( Kiebel, Kriegstein, Daunizeau, & Friston, 2009)"
"The Liquid State Machine (LSM) ( Maass, Natschlager & Markram, 2002), for example, uses OT features and the temporal embedding idea proposed in the HB model, but then applies standard methods for recognizing the resulting static patterns"
"This results in good pattern discrimination abilities ( Verstraetenet al.,2005), though not as accurate as a recent approach based on OT features ( Gutig & Sompolinsky,2009)"
"As the way in which one cell or circuit couples with another can be summarized using phase interaction functions ( Penny et al.,2009) we envisage that it should be possible to identify families of neurons or neural circuits that have the appropriate synchronization properties."
"We thought it might be possible that the OT system performed better at low signal levels because it had fewer parameters than the MFCC system, and so might generalize better ( Bishop,1995)"
"We also compare augmented and minimal models using the model evidence, as computed using the Posterior Harmonic Mean (PHM) ( Gelman et al.,1995)"
"The resulting Bayes factors were 0.98 and 0.99 indicating that neither of the augmented models are significantly better than the minimal model (this would require a Bayes factor of at least three ( Gelman et al.,1995))."
"Human speech is characterized by a four-fold variation in the speed at which words are spoken ( Miller, Grosjean & Lomanto,1984), and any speech recognition system whether artificial or natural, will have to deal with this range of time-warp"
"Each nonword matched one of the words (action verbs) in duration, intensity and power spectrum, but was rendered unintelligible by removing components of the modulation power spectrum using the Modulation Transfer Function (MTF) algorithm described in Elliott and Theunissen (2009)"
"The corresponding spectrograms G(ywi,f,t) and G(yni,f,t) were then computed using a windowed multitaper method with window size N=256 samples (0.128 s), a window offset of 32 samples (0.016 s), and time-bandwidth parameter set to NW=3 ( Mitra & Pesaran, 1999 )"
"More physiologically realistic filters can be implemented by linear spacing the filter bands on a mel-frequency scale ( Ghitza, 1986), and this was implemented for the pattern recognition results described in Section 3.1."
"Such frequency tuned onset and offset detectors have been observed in the inferior colliculus of the auditory midbrain ( Casseday et al.,2002)"
"Optical imaging reveals larger time windows of temporal integration as one moves from primary to secondary auditory areas ( Harrison, Harel, Panesar,Mori,& Mount, 2000)"
" To motivate this enterprise and to understand the importance of high thread counts on highly-threaded, many-core machines, let us consider a simple application that performs Bloom filter set membership tests on an input stream of biosequence data on GPUs  [3]."
" Using a different approach, Hong etÂ al.Â  [17] propose another analytical model to capture the cost of memory operations by counting the number of parallel memory requests in terms of memory-warp parallelism (MWP) and computation-warp parallelism (CWP)."
Accessing the off-chip global memory usually takes 20 to 40 times more clock cycles than accessing the on-chip shared memory/L1 cacheÂ  [51]. 
" Based on the description from Alverson etÂ al.Â  [52] about the nature of the computations this processor was designed to run, it is a purpose-built appliance for real-time graph analytics featuring graph-optimized hardware that provides up to 512 terabytes of global shared memory, massively-multi-threaded graph processors (named Threadstorm) supporting 128 threads/processor, and highly scalable I/O. "
Dijkstra algorithm is a greedy algorithm for calculating single source shortest paths. The pseudo-code for Dijkstra algorithm is given in Algorithm [55]. 
"Such over-approximations have been used, among other things, in the analysis of cryptographic protocols [8], for termination analysis [9,13] and for establishing non-confluence of term rewrite systems [19]"
"This research was born of involvement in the development of three tools for term rewriting, CeTA [18], a certifier for termination and confluence proofs generated by provers, CSI [19], an automated confluence prover, and Image 1 [11], an automated termination prover"
"The distinguishing feature of a certifier is that it is highly trustworthy; in the case of CeTA, this is achieved by proving its code correct in the proof assistant Isabelle [17]"
"Both CSI and Image 1 use quasi-deterministic automata [13] to produce overapproximations of reachable terms, and CeTA could not certify the resulting proofs"
"Note that in both cases, we did not define new recursive functions, but just proved equalities which are treated as function definitions by Isabelle's code generator [10]."
"Consider the famous Lorenz system [60] (1)x = (y-x),y = x-y-xz,z =- z+xy,where , , are positive parameters."
"Consider the Chen system [12] (2)x =a(y-x),y =(c-a)x+cy-xz,z =-bz+xy and the Lu system [61] (3)x =a(y-x),y =cy-xz,z =-bz+xy,where a,b,c are real parameters"
"Leonov suggested to consider the following substitutions [35] (4)x hx,y hy,z hz,t h-1twith h=a"
"Remark that the transformation (4) with h=a does not change the direction of time for the positive chaotic parameters considered in the works [12,61]."
"Note that here in contrast to the previous transformation: 1) the transformation (4) with h=-c change the direction of time for c>0 considered in the works [12,61], 2) for c=0 the Chen and the Lu systems do not become linear and their dynamics may be of interest"
"Various rigorous approaches to the justification of its existence are based, for example, on the investigation of instability (hyperbolicity) of trajectories with the help of computing Lyapunov exponents, or the computation of attractor dimensions (see, eg, [27,19] and many others)"
"On the other hand, we would like to recall the classical 16th Hilbert problem (second part, [23]) on the number and mutual disposition of limit cycles in two-dimensional polynomial systems, where one of the tasks is to find the simplest system, from a certain class, with the maximum possible number of limit cycles"
"Many chaotic polynomial systems have been discovered (eg, such particular cases of three-dimensional quadratic systems as the Lorenz, the Rossler, the Sprott, the Chen, the Lu and other systems) and studied over the years, but it is not known whether the algebraically simplest chaotic flow has been identified [73,72]"
"While Lyapunov dimension of the Lorenz attractor can be obtained analytically [47], for the Chen and Lu attractors it is still an open problem"
"The Chen system is a special case of the Lorenz system and The Lu system is a particular case of the Lorenz system of [2,3] may lead to the conclusion that the study of the Chen and the Lu systems is useless, the Chen and the Lu systems stimulate the development of new methods for the analysis of chaotic systems."
The estimate from above of the Lyapunov dimension by Lyapunov functions [36] and its comparison with the local Lyapunov dimension in zero stationary point permit one to obtain the exact formula of dimension for a generalized Lorenz system (9) with a certain parameter d
" In the mechanical and mathematical literature concerning the models for piezoviscous hydrodynamic thin film lubrication problems, different classical devices have been considered, such as journal-bearings, rolling-bearings or rolling-ball-bearings (see [1], for example). "
"In these more complex settings, this way of including piezoviscous regimes has given rise to several mathematical analysis results that state the existence and the uniqueness of solution, as well as to the design of suitable numerical methods to approximate the corresponding solutions, for which there are no analytical expressions (see, for example, [4-8])"
"For the spatial discretization we consider a classical piecewise linear Lagrange finite element space, so that the discretized problem consists of finding p hn+1 Kh, such that (38) - /2 /2G(t,p hn)(p hn+1) ( h-p hn+1) dt -6 - /2 /2h ( h-p hn+1)dt, h Kh,where Vh denotes the finite element space (see [22], for example): Vh={ h C0( )/ h|E P1, E h},"
"In order to apply the duality method in [20], we introduce the indicatrix function of the convex Kh denoted by IKh, so that the variational inequality of first kind (38) can be transformed in the equivalent variational inequality of second kind that consists of finding p n+1 V0h, such that - /2 /2G(t,p n)(p n+1) ( -p n+1) dt+IKh( )-IKh(p n+1) -6 - /2 /2h ( -p n+1)dt, V0h.Moreover, we use the notation (Anp n+1, )= - /2 /2G(t,p n)(p n+1) ( -p n+1) dt,(fn, )=-6 - /2 /2h ( -p n+1)dt.Then, using the tools of subdifferential calculus, we obtain that (39) n+1=-(Anp n+1-fn) IK(p n+1),where IKh denotes the subdifferential of the indicatrix function"
"In order to apply the results in [20], for a given parameter >0, we introduce the new variable (42) n+1 IKh(p n+1)- p n+1.In terms of this new variable n+1, the equivalent formulation to (40) (41) can be written in the form:"
"The idea is to make a comparative analysis of the different models, formulations and numerical schemes presented in the previous sections, mainly taking as a reference the data set of the numerical examples in [12]"
It seems that the results obtained with the numerical schemes here implemented for the Rajagopal and Szeri model are very similar to the solution presented in [12]
"Predicting the effort required to create software has been based on numerous software size models such as the Constructive Cost Model (Anandhi and Chezian, 2014; Clark, 1996; Manalif et al., 2014) and all its alternatives (Attarzadeh and Ow, 2011; Kazemifard et al., 2011; Tadayon, 2004; Yang et al., 2006) as well as on function points (Borandag et al., 2016) and analogy based models (Idri et al., 2015). The main goal of all these approaches is to minimize prediction error. Prediction is needed during the initial phase of software project developments."
"As reported in Silhavy et al., (2015a,b) UCP has some important drawbacks. Several approaches help identify the drawbacks of the UCP method and offer solutions, many of which are based on an analogy approach."
"Analogy based size estimation is commonly used for prediction in all the methods mentioned above (Idri et al., 2015; Shepperd and MacDonell, 2012). "
"The UCP method is based on use case models, which are commonly used as functional descriptions of proposed systems or software. The method involves assigning weights to groups of actors and use cases. Karner's original UCP method (Karner, 1993) identifies three groups: simple, average and complex."
"A number of use case scenario steps are typically involved in the initial estimation process. There have also been several modifications of the original UCP principles including use case size points (Braz and Vergilio, 2006), extended UCP (Wang et al., 2009), modified UCP (Diev, 2006), adapted UCP (Mohagheghi et al., 2005), and transaction or path analysis (Robiolo et al., 2009)."
"This approach is based on analysing a scenario, not step by step, but using steps merged logically into so-called transactions in which each transaction should include more than one step. Robiolo et al., (2009) improved transactions by calculating paths by which the complexity of each transaction is based on the number of binary or multiple conditions used in the scenarios"
"Their approach is based on Robiolo and Orosco (2008), where number of transactions is equal to the number of stimuli. A stimulus is a system entry point, which generates response (transaction) of an actor action in a use case. "
"Ochodek et al., (2011a) discusses a reliability of transaction identification process and Jurkiewicz et al. (2015) discusses event identification in use cases, which should be useful for path identification"
"Diev (2006) noted that when the actors and use cases are precisely defined, unadjusted UCP (the sum of the UAW and the UUCW) can be multiplied by the technical factors. The product of the technical complexity factors (see Table 3) and unadjusted UCP is considered as the coefficient of the base system complexity in Diev (2006)."
"According to Nageswaran (2001), added effort must be taken to consider support activities such as configuration management or testing."
"Yet another modification to the UCP is called adapted UCP (Mohagheghi et al., 2005). In this method, the UCP method is adapted to provide incremental development estimations for large-scale projects. Initially, all actors are classified as average (based on the UCP native classifications) and all use cases are classified as complex. "
"Ochodek et al., (2011b) also proposed omitting UAW and the decomposition of use cases into smaller ones, which are then classified into the typical three use case categories."
"Analogy based estimation methods are discussed in Azzeh et al., (2015b), which evaluated 40 variants of the single adjustment method using four performance measures and eight test datasets."
Amasaki and Lokan (2015) addressed the problem of selecting projects using a linear regression model by testing the window principle. The window principle involves first selecting a subset of the data.
"Instead, the study by Urbanek et al., 2015b is based on artificial intelligence and is an application of the approach proposed by Senkerik et al., (2014) but with theoretical aspects of Oplatkova et al., (2013). Urbanek et al., 2015b used a symbolic regression tool, analytic programming, together with differential evolution."
"Several works have attempted to apply various prediction models to UCP. Nassif et al., (2013) presented a linear regression model with a logarithmic transformation that they created to estimate software effort from use case diagrams."
" In Nassif et al., (2011), a multiple linear regression model was developed to predict the values of the productivity factor. To adjust the values of the productivity factor, they first employed a fuzzy logic approach (Nassif et al., 2011). Then, they created an artificial neural network (multi-layer perceptron) model (Azzeh and Nassif, 2016; Nassif et al., 2015; Nassif et al., 2012, 2013)."
"Actors play roles in the UAW variables (Azzeh et al., 2015a; Silhavy et al., 2015a). A simple actor typically represents an application programming interface and a complex actor represents a human using a graphical user interface."
"The least squares method is the most common method used to fit a regression line. The case when a linear regression has only one independent variable is called simple linear regression (Bardsiri et al., 2014; Jorgensen, 2004; Montgomery et al., 2012; Shepperd and MacDonell, 2012), whereas multiple linear regression (Bardsiri et al., 2014; Jorgensen, 2004; Montgomery et al., 2012; Shepperd and MacDonell, 2012) involves more than one independent variable."
"Another type of linear regression is polynomial regression (Bardsiri et al., 2014; Jorgensen, 2004; Shepperd and MacDonell, 2012) in which the relationship between the dependent variable and the independent variables is modelled as an mth degree polynomial"
"In the case of multiple independent variables it is appropriate to use stepwise regression (Bardsiri et al., 2014; Jorgensen, 2004; Shepperd and MacDonell, 2012). The aim of the stepwise regression technique is to maximize the estimation power using the minimum number of independent variables. "
"The experiment described above was evaluated using two datasets. Dataset 1 was obtained from Silhavy et al., (2015a), in which the dataset was based on Ochodek et al., (2011b) and Subriadi and Ningrum (2014). The UAW, UUCW, TCF, and ECF are known from the UCP method."
"We can conclude that the UAW also has an effect on size estimation, which is different than the findings published previously in Ochodek et al., (2011b). Here, the UAW values were valuable because the number of actors helps determine the number of interfaces required in the project, which, in turn, impacts software product construction."
"Human motion estimation is one of the most important areas of computer vision study [1–11]. It refers to the automated prediction and estimation of human motion posture based on rigid body motion, joint angles and body segment location."
"Previous studies have shown that the performance of motion estimation is highly dependent on the quality of motion data as well as the algorithm that is developed for modeling and estimation of the model [1,4,8,9,11]. The quality of motion relies on the method of captured data, either by marker-based or marker-less motion capture. "
"The approaches to motion estimation are biomechanical based [5,7], silhouette based [2,3,6,11,12] and image based [1,4,8–10]. A biomechanical-based approach involves tissue analysis and bone and joint location, which requires expensive devices and equipment."
"The approaches to motion estimation are biomechanical based [5,7], silhouette based [2,3,6,11,12] and image based [1,4,8–10]. A biomechanical-based approach involves tissue analysis and bone and joint location, which requires expensive devices and equipment. Silhouette-based estimation is analyzed by the silhouette extract from a human image file. It is always challenging when the estimation involves more than one subject. Image-based estimation involves the extraction of still figures from video motion data."
"The methods used for estimation include cross-entropy regularization [1], Artificial Neural Network (ANN) [8], hierarchical fitting [9] and human pose recovery [10]."
" The model is applied to short temporal daily activities include walking, running, and jumping obtained from publicly available video [25–27]and experimental captures of Yoga motion activity."
"Typically, motion postures in the whole time duration are framed in image snapshots similar to those presented in Wang and Baciu [1], Tong et al. [4], Zhang et al. [8], Shen et al. [9] and Hofmann and Gavrila [10]. The images are transformed into numeric data in a 2D coordinate system of the main body joints"
"Instead of using the point-cluster technique and the Kalman filter approach as in the case of Wolf and Senesh [7], we use a polynomial fitting approach. Polynomial fitting has been used in the estimation of air quality [28] and for operator prediction in image processing [29]. "
"In addition, classification performance is compared between the actual and estimated model data aided by the Waikato Environment for Knowledge Analysis (WEKA) software [30]. The classification processes are tested on 34 built-in algorithms for the seven categories of classifiers: Bayes, Function, Lazy, Meta, Misc, Rules and Trees. "
Human motion is often represented by the original motion frame or by representing the original motion frames with a parametric or probabilistic model [31]
" By creating parameterized motions, human action can be altered based on momentary moods that describe emotions such as happiness and sadness [32]. At the same time, the motion style can also be modified by adjusting the stylistic parameter as in the work by Brand and Hertzmann [33], in which a statistical model was introduced to generate new motion sequences based on the number of stylistic degrees of freedom."
"On the other hand, motion estimation is the process of estimating the configuration of the underlying kinematic or skeletal articulation structure of a person [34]. It often begins by video motion capture or analysis of the available motion dataset. In the past, different models have been proposed to simplify captured motion matching, including sticks to indicate the human skeleton as well as ellipsoids and cylinders to represent solid human models [35]. "
"Gait motion analysis often studies walking movements, focusing on the lower body segments: the thigh, lower leg and foot. For instance, Veeraraghavan et al. [36] created a shape-based recognition system for gait motion while Zhang et al. [37] proposed a visual gait generative model (VGGM) and a kinematic gait generative model (KGGM) to represent part or whole gait modelling."
Shen et al. [12] used a Gaussian process to study the low-dimensional manifold of visual input data to reconstruct the corrupted silhouette for motion estimation. 
 Luo et al. [11] proved that multi-view video is efficient in solving high-dimensional space problems and estimating a 3D surface in a temporal sequence. 
"Biomechanical-based approaches involve the analysis of soft tissue, bone and joint locations in the human body. For this purpose, Xiao et al. [5] performed an optical motion data capture via a marker-based approach. Their method uses biomechanical information based on 28 infrared markers placed on the human body. Estimation was possible using human skeleton mapping."
" Wolf and Senesh [7], on the other hand, proposed a numerical model with no consideration for mechanical properties. They focused on the soft tissue deformation and bone position of the human body using a statistical solid dynamic method. "
"Of all of the approaches, the image-based approach is the most common method for motion estimation; an example is the monocular image sequence, a model-based approach whereby the object shape is employed in motion estimation, and an independent method is used for a priori shaped models [1]. "
"An example of object shape employment in model estimation was presented by Tong et al. [4], who estimated the joint and global location parameters of a human pose based on a monocular image using the deterministic nonlinear constraint optimization method. "
" In addition, an optical flow method to estimate the motion of gestures was proposed by Zhang et al. [8]. The optical flow method was applied on the flat surface of images to segment and recognize the gestures. ANN has also been used to estimate gesture patterns. "
Another simple and straightforward approach by Daubney et al. [41] was the sparse set of features for pose estimation in low-level motions. Low-level motion suffers from perturbations such as noise and occlusion. The strength of their method is that it can be used in low-level motions without the need for pose initialization. 
"Whereas the studies on low-level motion focus on a single type of motion activity, Bruderlin and Williams [51] proposed a time-warping method as a non-linear method to combine different movements and control the speed of motion."
"Nanni et al. [17] and Saripalle et al. [18] proposed methods using support vector machine for three orthogonal planes of motion data and posturography data, respectively. "
"To achieve a perfect fit on every segment model, the BB segment has a 2nd order polynomial fitting, while the UB and LB have a 6th order of polynomial fitting in our motion model. Another approach that is similar to the polynomial fitting adopted here is the piecewise polynomial that is commonly used in the temperature data of heat transfer [58]."
"The missing data contributes approximately 12.8% of the overall study data. Therefore, the data elimination cum regression imputation approach as reported in Chan et al. [62] is applied to eliminate and impute the missing data."
"Despite assertions some thirty-five years ago that “thefuture of operational research is past” (Ackoff, 1979), the tech-niques and methodologies are still taught in universities across theglobe and regularly used in business decision-making, in both thepublic and private sectors. "
"Many organisations have changed the name oftheir departments to include analytics; such as IBM’sBusiness Analyt-ics and Mathematical Sciences(Sutor, 2013) and Proctor and Gamble’sGlobal Analytics(Ericson, 2006) teams"
"This conclusion confirms that periodization is not only the product of theory, but it is also a producer of theory (Green, 1995). "
"Whilst examples of data mining and machine learning algorithms applied within distributed systems are numerous (e.g. Zaki & Ho, 2000), no academic literature on the application of OR/MS methods within these new architectures was found."
"The bioinformatics analysis reveals that many human diseases, such as cancer, cardiovascular disease, amyloidoses, neurodegenerative diseases, and diabetes, are correlated with proteins diagnosed to be disordered [1,2]."
"The nuclear Poly(A)-Binding Protein 1 (PABPN1) induces the formation of muscle Intranuclear Inclusions (INIs) that are the pathological hallmarks of OPMD. There is currently no cure for OPMD [2,4]."
"In image analysis, the HSV (Hue, Saturation, Value) transformation is useful for developing image processing algorithms based on descriptions of colors that are natural to humans [16]."
There are existing nonlinear functions of features known to be effective (which can be interpreted as mathematical expression models) [17-21]. 
" The Expectation Maximization (EM) is a widely used approach to learning in the presence of unobserved variables, such as in the applications of fitting high-dimensional Gaussian mixture models [24] and reducing the difference in feature distributions [25]."
"Mathematical morphology provides an approach to extracting image components, such as size, shape, boundary and connectivity, and to eliminating irrelevancies for detecting various blood cells [11,12] and cancer cells [13]."
" Yet, the graphical model method, with a prior knowledge of object shapes, is able to provide a probabilistic model to represent the relationship among the image pixels, region labels and underlying object contour [7]."
" Since each iteration of the algorithm consists of an Expectation step (E-step) followed by a Maximization step (M-step), we call it the EM algorithm [26,27]."
"The advantage of the cross-validation is that each training/test subset is independent of the others [33,34]"
"According to a recent survey of 600 software developers, managers, and executives in the United States and the United Kingdom, only 3% of the respondents said they had no plans to adopt CD ( Perforce Software Inc 2015)."
"However, implementing CD can be quite challenging ( Chen, 2015, Leppanen et al., 2015, Claps et al., 2015 "
"Although CD as a goal (a target state) is no longer a new idea and has been well documented ( Humble and Farley, 2010), the adoption journey for CD is not yet a smooth path"
"Many times, these releases would be followed by P1 (priority 1) incidents ( Rob, 2007), meaning that release activity was always full of uncertainty, failures, and stress"
"According to this distinction, Continuous Delivery is compatible with a wide range of scenarios, but Continuous Deployment is suitable only under special conditions ( O'Dell and Skelton, 2016)"
"For example, in our organization, we needed to create a change ticket, place the change request on the agenda of the next Change Advisory Board (CAB) meeting ( Rob, 2007), present the change at the meeting, receive CAB approval, confirm the deployment window, and so forth."
"However, little, if any, work has been reported that specifically seeks to understand and address this important type of software ( Rodr guez et al.,2016)."
"Recent years have witnessed tremendous growth in video traffic on the Internet as a result of higher broadband data rates, proliferation in smart handheld devices [24,95]and affordable unlimited data plans offered by Internet Service Providers [51]."
An estimated one-third of all online activities on the Internet is spent watching video according to the recent report [100]. 
" Netflix alone is reportedly streaming over 1 billion h of video each month which is equivalent to almost 7,200,000 Terabytes of video traffic [37], and this figure is rising constantly. "
"The skyrocketing demand for serving video traffic have questioned the effectiveness of the traditional solution of employing special purpose Content Delivery Networks (CDNs), to serve such content. Invented at the turn of the century [96], CDNs now constitute the backbone for serving content [25,80]. "
"Although replacing selfish self-organising swarming with centrally managed P2P content exchange can improve completion rates considerably [81], for CDN-grade reliability, peers would still need to stick around to allow other users in the swarm to complete, or to maintain a distributed copy of the entire content item across the set of active users in the swarm."
"In short, PA-CDNs work as follows: Whenever possible, i.e., whenever there is sufficient capacity to deliver content in the swarm, peers distribute chunks of content amongst each other (typically using centrally managed swarming techniques [81,93,124])."
"Unlike the previous study [66], we analyze a significantly wider range of obstacle factors, including not only the traditionally discussed technical challenges such as reliability and QoS, but also various other factors, including heterogeneity and scale, and inhomogeneous distribution of resources among users."
"Our work, with its focus on deployable peer-assisted content delivery, is complementary to several survey articles which have focused on traditional P2P-based content delivery [7], or on specific P2P issues such as P2P overlay construction [68], or chunk scheduling [62]. "
"The closest work is that of Lu et al. [66], which surveys the design space of PA-CDN architectures, and highlights a fundamental choice between a tightly coupled or loosely coupled model of co-operation between the server-assisted and peer-assisted components of the PA-CDN"
The request-routing system in CDNs typically includes two basic modules for routing user requests to the most suitable edge server; i) request-routing algorithm and ii) request-routing mechanism [80]. 
"A special purpose DNS server is programmed to redirect users’ requests to the IP address of an appropriate edge server by considering some important parameters such as load on edge server and its distance from the client, network proximity, and user perceived latency [19]."
"Content Delivery Network is a complex content distribution system and several issues and decisions are involved in managing and administrating the entire CDN infrastructure, including, where to place edge servers [18], what content to replicate [32,48,97], and on which cluster of servers to copy each piece of content [8]. "
To provide a cheap pay-as-you-go service to a broad variety of customers some CDNs have adopted cloud technologies which became known in the literature as cloud CDNs [4]. 
"Last but not least, multiple telecom operators (AT&T, BT, Orange, Telefonica, KPN and Verizon among others) to gain a better control over the data services served to their users have deployed their private telco-CDNs [16,30]."
"Overall, Cisco has estimated that content delivery network traffic will carry nearly two-thirds of all Internet video traffic by 2020 [24]. Yet, several recent studies have reported that CDNs are being stressed by the demands placed during peak hours [60,109]. In a search"
"when users agree to share their resources in return of access to the system - the content swarms are said to be self-scalable [28]- as an increase in demand for a content item yields an equal increase in the number of the content suppliers [73,83]. "
"Some peer-to-peer applications may rely on dedicated nodes to control, coordinate and manage content swarm. This structure is referred to as partially centralized P2P system  [88]"
"Similarly, PPLive - the largest P2P live streaming service - rely on dedicated Trackernodes to store the information about streaming channels, available video chunks and peers [40]. Some versions of Bit-Torrent - a popular file sharing P2P system - also rely on Tracker nodes to distribute bulk software updates or multimedia files [88]."
"For instance, the authors of [7] present a critical analysis on different design features and infrastructural properties of P2P systems and their influence on non-functional aspects such as scalability, resource management, security, fairness and self-organization"
"A comprehensive survey of various techniques proposed for structured and un-structured P2P networks has been presented in [68]. Similarly, the authors of  [62] provided an overview of different approaches to address chunk scheduling techniques and peering mechanisms."
"Not surprisingly, the majority of research efforts in peer-assisted content delivery literature have focused on developing strategies for improving quality of service in terms of reducing startup delay and playback delay [39,41,53,58,65,98,112]."
"Indeed, it has been reported in  [31] that the presence of middle boxes is a challenging issue. A peer inside the private network can initiate a connection with Peers of public network, but a reverse connection is often complicated by administrative policies [29]."
" In [11] the authors have analyzed the user trace collected from the Conviva media platform and reported a very low completion ratio among users, when they abandon sessions after watching first few chunks."
"Similarly, a measurement study of PowerInfo [119] – a video-on-demand system deployed by the world’s largest mobile phone operator (China Mobile) - has reported a 70% abandon rate among users as measured by the fraction of sessions which were abandoned after first 20 mins."
"The results suggested that mobile users abandon sessions with a higher rate, i.e., only around 30% of mobile sessions last for longer than a half of a content’s duration in comparison to around 50% for the fixed-line sessions [51]. "
"This phenomenon can be explained by the typical diurnal patterns in user accesses, when most of the users come online in the evening peak hours [11], but also by the content availability policies specific for some video on-demand websites"
" for example, in catch-up TV systems, such as BBC iPlayer [51], the content items are typically released for a limited amount of time, e.g., 7 to 30 days, and feature a burst of accesses in the first few hours after the release [50]. "
"The length of the popular video content matters, too. It has been reported in [42] that, for small size MSN videos, users generally opt to view the entire or most of the video clip, and only 20% of users watch 60% of video content with the length greater than 30 min. Moreover, a large fraction of users (i.e., 60%), watch videos without interactions (e.g., stop, forward, rewind etc.), whereas this fraction increases to 80% for videos shorter than 30 min. "
"A user session might be interrupted due to a network failure, overloaded CPU or a software crash [55]. "
"The authors in  [38] devised a mathematical model to evaluate the impact of peer-churn on a PA-CDN, when the users of set-top-boxes are not willing to share their resources. "
"Similarly, to deal with peer churn the authors in  [72] proposed Home Box-assisted approach which relies on exploiting set-top-boxes as proxies between users and CDNs."
"To improve the quality of service without putting too much of a burden on the peers, the authors of  [42] also suggest two different peer selection policies: Water Levelling (WL) and Greedy Policy (GP). "
"A biased selection of peers without considering underlying physical topology might lead to severe performance degradation in terms of access delays and bandwidth wastage if, for instance, peers located within the same building use two different ISPs and so, although physically placed close to each other, are very distant in network terms [5]. "
"The authors of  [111] proposed to limit the P2P traffic within sub-networks, or behind common gateways. The authors exploited the modified version of Kademlia distributed hash tables (DHT) to conduct searches for the closest peers."
"To achieve a lower startup delay, the authors in [112] proposed a three phase streaming hybrid CDN-P2P architecture that allows peers to download initial chunks of a content item from the geographically closed CDN nodes and remaining chunks from a P2P swarm. "
"The authors of  [39,65] proposed a strategy for improving startup delay via an effective buffer management on a peer’s side."
"Particularly, Ha et al. in [39]suggested that, for minimizing startup latency the buffer’s part at the start of the playback must be filled in with a high priority. "
"Similarly, Lu et al. in  [65] suggested organizing the playback buffer into three different regions, where a startup region and a common region are equivalent to the ones proposed in  [39]. "
"The authors of  [58,98] exploit the social ties between users and the locality of interests to assist peer-assisted sharing of content in Facebook"
An SP exploits a distributed hash tables algorithm (DHT) called a content addressable network (CAN) [84]. The algorithm is based on the binning technique proposed in  [85] and operates as follows.
"To address this concern the authors of  [115]proposed a peers authorization mechanism and a network coding scheme in which each packet is encoded and decoded at the node level using efficient linear codes, thus, allowing for copyrights protection."
The authors of  [49] proposed a control schema over copyrights at a Tracker server in which only legally authorized content items are distributed to the peers.
"A limited contribution policy presented in  [112] obliges every user to contribute some fraction of its upload bandwidth resources to a limited number of sessions, for a limited period of time or both. "
In  [34] the authors present an economic model for PA-CDNs in which user participation in peer-assistance is incentivized via free high quality video offers. 
The authors of  [76] have proposed a peer-assisted model with economic incentives for all participating parties including both peers and ISPs.
Cho and Yi  [23] presented a cooperative game theory approach to validate a profit sharing mechanism with multiple content providers and peers. 
Historically peer-to-peer systems have been ISP-oblivious and could generate significant amounts of the cross-ISP traffic - a fact which reportedly polarized ISPs attitude towards peer-to-peer systems  [88].
Xunlei is the 10th largest Internet company in China and Kankan is its peer-assisted on-demand streaming service with 31.4 million unique daily users as of the end of 2012 [122]. 
"In 2007 ChinaCache deployed about 500 cache servers in 8 districts of China, out of which 50 are the core service nodes responsible for live streaming and over 400 edge caches deployed in a close proximity to users [117]."
"NetSession [124] is a global peer-assisted content distribution network, originally developed by Red Swoosh, a P2P content delivery company founded by Travis Kalanick and Michael Todd in 2001 and acquired by Akamai Technology in 2007 [86]. "
"Spotify is a popular on-demand music streaming service which, according to [35]exploits peer-assistance to serve its 10 million-large user base around the world. "
"NetSession and LiveSky [61,63] use standard DNS request routing techniques and redirect users based on their location and the current load on the edge servers: if the nearest edge server is overloaded, the request is redirected to the next nearest and less loaded one."
"Yet, it is reported in [124] that only around 30% of NetSession users agree to participate in peer-assistance."
"However,  [59] has raised an important concern with respect to ISP-friendly peer-assisted design, suggesting that localizing ISP traffic may negatively impact the quality of service. Therefore, a detailed investigation in this regard is required in future works.

"
"Now MNOs have started to deploy their own CDN infrastructure for better control and management on the resources and bulk of video traffic [1,2,118]. "
"(iii) For input that is not in general position even more complex interactions such as vertex-events or multi-split-events are possible [2,3]. The straight skeleton is the union of the traces of wavefront vertices over the entire time of the wavefront propagation, see Fig. 1."
"Several algorithms are known for constructing unweighted straight skeletons, such as those by Aichholzer et al. [1], Eppstein and Erickson [2], Cheng and Vigneron [6], Huber and Held [3], or Vigneron and Yan [7]."
"These may be (Diggle et al., 2013; Molenberghs and Verbeke, 2005) grouped into three broad classes: (i) marginal models, that seek to relate the marginal distribution of the response variable at each time point to explanatory variables; (ii) subject-specific or random effect models, which account for heterogeneity between individuals by adopting regression-type models with random subject effects; and (iii) conditional or transition models, that focus on the conditional distribution of the response at each time-point given prior responses and possibly explanatory variables."
"The  prior responses and other covariates are treated on an equal footing as explanatory variables in a convenient parametric model, for example, a generalized linear model (Diggle et al., 2013, Chapter 10"
"Other ways to construct parsimonious higher-order Markov chain models have been proposed (Raftery and Tavare, 1994)."
"There is no requirement that the determinants of  be immediately prior to  in the ordering. An example concerning side-effect profiles in a clinical trial of neuroleptica is given in Edwards (2000, Section 7.1.3)"
"Some recent work has extended Bayesian network methodology to support context-specific modelling (Boutilier et al., 1996; Myers and Troyanskaya, 2007)"
"In this paper we study a class of models due to Ron et al. (1998) called acyclic probabilistic finite automata. Note that we use the same acronym, APFA, for both singular and plural forms (automaton and automata)"
"So an APFA can be regarded as a time-heterogeneous context-specific graphical model for discrete longitudinal data. See Edwards and Ankinakatte (in press, Section 10) for a more precise comparison of APFA with Bayesian and Markov networks"
"The structure of the paper is as follows. Section  2 introduces APFA from a statistical perspective. Section  3 describes the model selection algorithm of Ron et al. (1998) which (in a modified form) forms the core of the haplotype clustering algorithm implemented in the Beagle program (Browning and Browning, 2007a,b) that is widely used for phasing and imputation of DNA chip data."
"This section gives a brief introduction to APFA from a statistical perspective: see Edwards and Ankinakatte (in press) for a more detailed exposition. We first describe sample trees, that are closely related to APFA."
"An example is shown in Fig. 1. There are 36 observations with  and 34 with . From each node at stage one, edges branch out to stage two, based on the distinct values of  given . The process continues up to stage , and results in the construct called a sample tree. Sample trees are also known as prefix tree acceptors in machine learning (Carrasco and Oncina, 1994), and event trees in Bayesian decision theory (Smith and Anderson, 2008)"
"Automata are essentially finite state machines that output (or input) strings of symbols. Probabilistic finite automata (PFA) are automata in which strings are generated in a probabilistic manner (Vidal et al., 2005a,b), and APFA are the subclass of PFA that generate symbol strings of constant length, and so can be regarded as probability models for ordered sequences of discrete random variables."
"Thus an APFA expresses a set of context-specific conditional independence constraints on the distribution of , and in this respect it resembles the dependence graph of a traditional graphical model (Lauritzen, 1996; Edwards, 2000)."
The state merging operation and corresponding LRTs are studied in detail in Edwards and Ankinakatte (in press). It is shown that the tests are closely related to standard LRTs of independence () in two-way contingency tables
"The algorithm proposed by Ron et al. (1998) to select an APFA given a data sample proceeds as follows. The sample APFA is constructed and then simplified in a series of state merging operations. As mentioned above, the idea is to merge two nodes  and  at stage  whenever the distributions of the future , given that the process has passed through  or , are similar. To assess this (Ron et al., 1998) proposed a dissimilarity score between nodes  and , written , and a fixed threshold, . When  is small the conditional distributions of , given that the process has passed through  and  respectively, are similar. More precisely,  and  are called similar if : otherwise they are called dissimilar. Dissimilar nodes are not merged."
"The dissimilarity score proposed by Ron et al. (1998) was
"
"For various values of , we take  independent samples from a given APFA , using the data generating process described in Section  2. The simulated data sets of varying sizes are then used to build the APFA, , using the model selection methods under comparison. Then we compute the dissimilarity of the selected model  to the true model , using two measures: the Kullback–Leibler divergence (KLD), and the Kullback–Leibler increment (KLI) (see Appendix A). This is replicated ten times, and the average KL-divergence  and KL-increment  are reported.

This process is performed three times: once for each of the three data sets described in Section  5. The “true” model is constructed by applying the minimal AIC selection procedure described above to the data set. But note that since computation of the KL-divergence is computationally demanding (see Appendix A), only the first 10 variables were used to construct  for the Biofam and Duroc data sets for the KL-divergence computations. For the KL-increment comparisons, all the variables were used."
"4.2. Goodness-of-fit using 10-fold cross validation
Suppose we are given an APFA  with known edge probabilities , and a commensurate data set  of the form  for . As a measure of how well the model  fits the data set, , Thollard et al. (2000) and others suggest using a quantity called the per symbol log likelihood (psLL) for this purpose. It is defined as
(9)
where  is the number of observations in  whose root-to-sink path in  passes through , and  are the known edge probabilities. Note that since each observation in passes through  edges in , psLL is the average value of  obtained when  generates . Thus psLL is a measure of how well  fits ."
"The mildew data stem from a cross between two isolates of the barley powdery mildew fungus (Christiansen and Giese, 1990). For each of  offspring,  binary markers, each corresponding to a single locus, were recorded. One objective of the analysis is to determine the order of the loci along the chromosome. The data were obtained from the experimenters, are analysed in Edwards (1992) and are available from the Comprehensive R Archive Network (CRAN), being supplied along with the package gRapfa.

The Duroc SNP data come from a study in which 4239 pigs of the Duroc breed were genotyped using the Illumina Porcine SNP60 BeadChip. In all approximately 60 000 single nucleotide polymorphisms (SNPs) were recorded for each pig. The data and its preprocessing are further described in Edwards (2013). The data analysed here consist of  observations of  SNPs (the first 100 SNPs on chromosome 1). From a statistical point of view, SNPs are trichotomous variables (two homozygotes and a heterozygote)"
"To illustrate application of the methodology to data outside of genetics, we consider the analysis of a social science data set. The Biofam data set is derived from data obtained in a retrospective biographical survey carried out by the Swiss Household Panel in 2002. The data are freely available to the scientific community, and can be downloaded from CRAN as part of the package TraMineR (Gabadinho et al., 2011). They contain sequences of family life states recorded once a year from age 15 to 30 for  individuals born between 1909 and 1972, including only individuals who were at least 30 years old at the time of the survey. Family life state is classified into 8 categories: (i) living with parents, (ii) left home, (iii) married, (iv) left home and married, (v) have children, (vi) left home and have children, (vii) left home, married and have children, and (viii) divorced. In addition, a large number of covariates were recorded. Here for the sake of simplicity we only include sex and religion, the latter coded as ‘catholic’, ‘protestant’ or ‘other’. We combine these into one factor with six levels, i.e. the six combinations of sex and religion, and we organize the data so that this variable is placed prior to the family life state variables"
"Note also that all six covariate nodes at stage one in the sample tree are merged into one node at stage one in Fig. 4(b), implying that sex and religion do not affect the future life courses. However, as we discuss below, the tests for merging at the initial stages may suffer from low power: it would be interesting to examine this with other methods (Edwards and Ankinakatte, in press, Section 8), but we do not attempt this here."
"The computations were run under Redhat Fedora 10 Linux on a Intel i7 quad-core 2.93 GHz machine with 48 GB RAM. Beagle version 3.3.2 was used to perform the model selection algorithm of Browning and Browning (2007a,b). The remaining computations were performed in R. An R package (named gRapfa) implementing the methods described in this paper has been prepared by the authors and is available on CRAN."
"A further advantage of the likelihood-based approach is that is easily extended: for example, selection algorithms may consider steps in which more than two nodes are merged. As described in Edwards and Ankinakatte (in press, Section 10), first order Markov models correspond to APFA in which, for each stage, all incoming edges with the same symbol are merged. Thus a variant of the algorithm could be devised, based on as far as possible ‘same colour’ merging, that favourizes conceptually simpler models: this deserves further study."
"In the last years there has been a great deal of research on applying machine learning techniques and tools to processing of the functional Magnetic Resonance Imaging (fMRI) outputs [15,22,27]"
"The research has been mostly organized around the following three key areas [19]: (1) application of classification methods to fMRI data (e.g. [1,8–11,23–26]), including combinations of classifiers also known as ensemble models (e.g. [17,28,29]), (2) dimensionality reduction techniques (e.g. [2,3,10,21,23,30]) and (3) spatio-temporal filtering (e.g. [18]). "
"The alignment issue is however often ignored and all snapshots taken while a certain stimulus is active are routinely averaged [27] (in some cases trimming 1–2 initial snapshots), or labelled with this particular stimulus [17]."
" Following [17], we have first selected a subset of voxels by cross-training6 10 Support Vector Machines (SVMs) with linear kernels and then extracting top 200 contribution weights of voxels in terms of their absolute value, from each model."
 In our experiments a Support Vector Classifier (svc) from the PRTools Pattern Recognition Toolbox version 4.2.1 for MATLAB [12] has been used.
" Thus, any unordered pair (x, y) with xâBSlice(y)â§yâBSlice(x) creates an edge (x, y) in an undirected graph in which a complete subgraph is equivalent to a backward-slice MDS and a backward-slice cluster is equivalent to a maximal clique. Therefore, the clustering problem is the NP-Hard maximal cliques problem (Bomze et al., 1999) making Definition 2.2 prohibitively expensive to implement."
"Recall that the definition of a coherent dependence cluster is based on an underlying depends-on relation, which is approximated using program slicing. Pointer analysis plays a key role in the precision of slicing and the interplay between pointer analysis and downstream dependence analysis precision is complex (Shapiro and Horwitz, 1997)."
"In testing, dependence analysis has been shown to be effective at reducing the computational effort required to automate the test-data generation process (Ali et al., 2010). In software maintenance, dependence analysis is used to protect a software maintainer against the potentially unforeseen side effects of a maintenance change. "
"They are used by management researchers and practitioners (as well as other social scientists) in the context of interventions to stimulate deliberative dialogue and the development of change proposals (Beierle and Cayford, 2002; Rowe and Frewer, 2004)."
"A substantial number of these have been developed by operational researchers over the past 50 years, although the term ‘problem structuring’ itself was only introduced into the operational research (OR) lexicon a couple of decades ago (Rosenhead, 1989, 2006; Rosenhead and Mingers, 2001, 2004)."
"A distinguishing feature of PSMs, compared with many other participative methods developed by social scientists, is the use of models as ‘transitional objects’ to structure stakeholder engagement (Eden and Sims, 1979; Eden and Ackermann, 2006) and provide a focus for dialogue (Franco, 2006). "
" These evaluations are often based on explicit criteria reflecting the researcher’s experience, a given theory, a literature review and/or stakeholder expectations generated through a consultative exercise (Beierle and Konisky, 2000; Rowe and Frewer, 2004). "
"In some cases, formal evaluation instruments have been developed and applied (e.g., Duram and Brown, 1999; Rowe et al., 2004; Berry et al., 2006; Rouwette, 2011)."
"What is clear from the literature, however, is that only a very small minority of studies (e.g., Valacich and Schwenk, 1995a; Halvorsen, 2001; Rouwette et al., 2011) seek to compare between methods or across case studies undertaken by different researchers."
"A particularly significant study was undertaken by Beierle and Cayford (2002), who quantitatively compared broad classes of methods using a standard set of variables applied to 239 case studies of public participation. "
"Rowe and Frewer (2004), reflecting on social science approaches to evaluating participative methods, classify them into three types."
"White (2006) argues that very similar distinctions have been made in the OR and group decision support literatures, and preferences for universality (to a greater or lesser extent) or specificity reflect the positivist and interpretivist paradigms respectively."
"Our epistemological argument is that knowledge (or understanding) is always linked to the purposes and values of those producing or using it, and is dependent on the boundary judgements that they make (Churchman, 1970; Ulrich, 1994; Alrøe, 2000; Midgley, 2000)."
"Some features of the context-purposes-methods-outcomes relationship may be apparent early on in an intervention, while others may only emerge as the inquiry unfolds. Hence the utility of an emergent approach for the evaluation of methods, which remains open to new understandings as inquiry deepens (e.g., Kelly and Van Vlaenderen, 1995; Jenkins and Bennett, 1999; Gopal and Prasad, 2000; Allsop and Taket, 2003)."
Relevant aspects of context identified by Jackson and Keys (1984) are the complexity of the issue being addressed using a systemic method and the relationships between the participants. 
"In contrast, Margerum (2002) identifies potential contextual inhibitors of effective participation:"
" Ong (2000) discusses the facilitative effects of strong social capital, and Alberts (2007) documents the negative effects of participant inexperience and ignorance of technical issues."
Branch and Bradbury (2006) claim that a key aspect of context is managerial attitude
" McCartt and Rohrbaugh (1995) argue that a key aspect of managerial attitude is openness to change, and participative methods are often ineffective without it."
"Kelly and Van Vlaenderen (1995) and Brocklesby (2009) concentrate on stakeholder interactions, looking at how patterns of mistrust and miscommunication can become established and affect the use of participative methods."
"Champion and Wilson (2010) provide a particularly useful set of contextual variables to be considered, based on a literature review and feedback from practitioners: organisational structure; influence of the external environment; length of history of the problem in focus; politics and personalities; perceived implementation difficulty; and the level of experience of stakeholders."
"Underpinning different boundary judgements may be quite different perspectives on the nature of the context (Churchman, 1970). Therefore, exploring diverse perspectives (e.g., as advocated by Checkland (1981)) may lead to the identification of alternative possible ways of bounding a contextual analysis (Ulrich, 1994)."
"Purposes are closely linked with values and motivations (McAllister, 1999), and they are important to an evaluation because particular methods are likely to appear more or less useful depending on the purposes being pursued. "
"Different methods are generally good for different things (Flood and Jackson, 1991), and it is the perceived ‘fit’ between purpose and method that is important to evaluate: a disjunction may be responsible for an attribution of failure."
"It is important to consider possible hidden agendas as well as explicitly articulated purposes. These may significantly affect the trajectory of an intervention (for instance through sabotage), and thereby the evaluation of the method used (Ho, 1997)"
"The process of application of a method is important as well, not just the method as formally constructed (Keys, 1994). For instance, the same basic method may be enacted in quite different ways depending on the preferences and skills of the researcher/facilitator and the demands of the situation at hand."
"Compare, for example, two significantly different accounts of soft systems methodology (SSM): Checkland and Scholes (1990) discuss how the methods from SSM should be utilised in a flexible and iterative manner, while Li and Zheng (1995) insert some of the same methods into a ‘general systems methodology’. "
" Mingers (1997) describes these as the “intellectual resources” that the researcher brings into an intervention, and it is important to be able to distinguish whether problems encountered in the use of a method derive from the limitations of the method itself or from the inadequate resources of the researcher. "
"Our questionnaire was first developed in the context of a research programme aiming to generate and evaluate new systemic problem structuring methods for use in promoting sustainable resource use (Winstanley et al., 2005; Hepi et al., 2008)."
" Because of the latter, the questions had to be reasonably generic. Other authors suggest a number of different ways of producing generic evaluation criteria, and these have been summarised by Beierle and Konisky (2000) and Rowe and Frewer (2004)."
"facilitating consultation with land owners and community interest groups as part of a feasibility study for the construction of a new water storage dam (Winstanley et al., 2005)."
"working with an Australian NGO and its stakeholders in exploring policy options to address the public injecting of illicit drugs (Midgley et al., 2005);"
"facilitating workshops with the police and other stakeholders in the New Zealand criminal justice system to look at ethical issues associated with anticipated future developments of forensic DNA technologies (Baker et al., 2006);"
"reviewing the process used by the New Zealand Ministry of Research, Science and Technology to develop ‘roadmaps’ for long-term investments in environment, energy, biotechnology and nanotechnology research (Baker and Midgley, 2007)"
"developing a new collaborative evaluation approach in partnership with regional council staff responsible for facilitating community engagement in sustainability initiatives (Hepi et al., 2008)."
"At its most flexible, a pluralist practice may involve the integration of several previously distinct methods into a new whole, perhaps also incorporating the design of novel elements (Midgley, 2000)."
"The field of numerical algebraic geometry [3,24] includes a wide array of algorithms for finding and manipulating the solution sets V(f) of polynomial systems, including both isolated solutions (points) and positive-dimensional solution sets (curves, surfaces, etc.)."
"All isolated solutions have associated to them a positive integer, the multiplicity of the solution, which is greater than 1 for singular solutions [3]."
"Section 5, we describe the connection of this perturbation approach to the deflation approach of [9], the method of regenerative cascade [10], and a very early technique in the field known as the cheaterâs homotopy [17] in which the authors made use of a perturbation of fË(z) for somewhat different reasons. It is important to note that our perturbation is virtually the same as the cheaterâs homotopy, in the case where there are no parameters."
"It is observed in [17,24] that a perturbation can cause positive-dimensional irreducible components to âbreakâ into a (possibly very large) number of isolated solutions."
" The major software packages for carrying out such computations include Bertini [2], PHCpack [25], and HOM4PS-2.0 [13]."
"One very special type of homotopy is the parameter homotopy [17,19]"
This and other optimizations of regeneration are described in [9]
"Much of the theory underlying the ideas of this article was known by Morgan and Sommese in the 1980s [18] and has since been repeated in various forms, for example in [24,8]"
"Let rk(f) denote the rank of the polynomial system f(z), i.e., the dimension of the closure of the image of f(z), f(CN)â¾âCN. The rank of f(z) is an upper bound on the codimension of the irreducible components of V(f) [24]"
It should be noted that this theorem is in fact a corollary to the main result in [17].
 Regeneration can compute all of these nonsingular solutions [9]
"First, we may trivially compute the multiplicity, Î¼(zi), of each isolated solution zi of f(z)=0, as defined in [24]"
"This is based on the fact, proved as Theorem A.14.1(3) in [24], that each isolated solution zi will be the endpoint of Î¼(zi) paths beginning at points in V(f-pË)"
In this section we consider several examples where perturbed regeneration provides some benefit. All runs made use of Bertini 1.4 [2]. All reported timings except those of the last example come from runs on a 3.2GHz core of a Dell Precision Workstation with 12GB of memory.
"Next, we consider the system cpdm5, from the repository of systems [25] but originally considered in [6]"
"In the article [20], Morrison and Swinarski study a polynomial system with 13 equations, having 51 isolated solutions."
"A more specialized sort of homotopy, the 2-homogeneous homotopy [24], performs even better in this case."
"Computing the numerical irreducible decomposition [3,24], the solution set consists of 10 irreducible components of various dimensions"
"As a final example, we consider the nine-point four-bar design problem, exactly as formulated in Chapter 5 of [3]"
The regenerative cascade of [10] provides an equation-by-equation approach to computing the numerical irreducible decomposition of the solution set of a polynomial system.
"Parameter homotopies are the right tool for this job, as described briefly in Section 2.1. This idea has been implemented in Bertini [2] and Paramotopy [4]. Some background may be found in [19,17]"
"The trick to such methods is choosing an intermediate system f(v,pË) which satisfies some necessary properties, including that the solutions are smooth. The cheaters homotopy in [17] addresses this issue by including the same perturbation parameter as in Lemma 3.2."
"The numerical irreducible decomposition is the data type used in numerical algebraic geometry to store positive-dimensional solution sets. The technical definition is not necessary here but may be found in [3,24]. "
" If desired, monodromy and the trace test [3,23] could be used to find dZ points on each component Z"
"However, the introduced class of multivariate FayâHerriot models does not contain the Rao and Yu (1994) model or the GonzÃ¡lez-Manteiga etÂ al. (2008b) model as special cases."
"As Chalmers (1995) has noted: “The really hard problem of consciousness is the problem of  experience. When we think and perceive, there is a whir of information-processing, but there is also a subjective aspect."
"As Nagel (1974) has put it, there is  something it is like  to be a conscious organism. This subjective aspect is experience."
"Indeed, early mathematical results about the brain’s functional units of short-term memory (STM) and long-term memory (LTM) proved that the functional units of both STM and LTM are distributed patterns across networks of feature-selective cells (Grossberg, 1968a, 1968b, 1973). "
"ART predicts that all brain representations that solve the stability–plasticity dilemma use variations of CLEARS mechanisms (Grossberg, 1978a, 1980, 2007, 2013a)."
"Since ART was introduced in Grossberg (1976a, 1976b), it has undergone continual development to explain and predict increasingly large behavioral and neurobiological databases, ranging from normal and abnormal aspects of human and animal perception and cognition, to the spiking and oscillatory dynamics of hierarchically-organized laminar thalamocortical and corticocortical networks in multiple modalities. "
"The first paradigm is called Complementary Computing (Grossberg, 2000a). "
"Likewise, because excitatory matching is needed to generate resonances that support conscious internal representations, spatial and motor processes (“procedural memories”; Cohen & Squire, 1980; Mishkin, 1982; Scoville & Milner, 1957; Squire & Cohen, 1984) that use inhibitory matching cannot generate conscious internal representations."
"Perhaps the most basic fact about 3D vision and figure-ground perception is that its functional units are 3D boundaries and surfaces, where these words need to be properly understood. These processes were first modeled in Grossberg (1984a) and have provided a foundation for subsequent explanations and predictions about many data, including how looking at 2D pictures can generate conscious 3D percepts of occluding and occluded objects "
"Neon color spreading was reported in Varin (1971), who studied a “chromatic spreading” effect that was induced when viewing an image similar to the one "
"In summary, end gaps and end cuts are formed as a result of two successive stages of spatial and orientational competition between contrast-sensitive and orientationally tuned boundary cells (Grossberg, 1984a; Grossberg & Mingolla, 1985)."
"These orientationally-tuned simple cells  (Hubel & Wiesel, 1968) can respond to an oriented distribution of contrasts in response to scenic lines, edges, textures, and shading, not just edges alone (Fig. 4(a)). If the brain did use specialized detectors like edge detectors, then it would require many different types of specialized detectors, followed by complicated subsequent processing to try to fuse together all their information. Such an endeavor would fail if only because, in many natural scenes, lines, edges, textures, and shading are all overlaid."
"They cannot discriminate between dark-light and light-dark contrasts, or red–green and green–red contrasts, or blue–yellow and yellow–blue contrasts, because they pool together inputs from simple cells that are sensitive to all of these differences (Thorell, De Valois, & Albrecht, 1984) to form the best possible boundaries"
"These boundary completion cells are often called bipole cells (Cohen & Grossberg, 1984; Grossberg, 1984a; Grossberg & Mingolla, 1985) because they complete boundaries inwardly in an oriented manner between pairs (bipoles!) of boundary inducers. "
"More recently, Brincat and Miller (2015) have reported neurophysiological data that support the distinction between category learning within the attentional system that includes prefrontal cortex, and the orienting system that includes the hippocampus. "
"Among others,Banquet and Grossberg (1987) provide ERP markers during memory search; Brincat and Miller (2015) provide oscillatory neurophysiological markers of the interplay between prefrontal cortex and hippocampus during learning and mismatch; Otto and Eichenbaum (1992) provide neurophysiological data during hippocampal mismatch processing; and Spitzer, Desimone, and Moran (1988) provide neurophysiological data from cortical area V4 during the learning of easy vs. difficult discriminations, a process that is regulated within ART by a vigilance parameter"
"Spitzer et al. (1988) report “in the difficult condition, the animals adopted a stricter internal criterion for discriminating matching from non-matching stimuli…"
"More difficult discriminations, at least under proper circumstances, should lead to higher vigilance, more mismatch events, and thus more of the hippocampal novelty responses found by Brincat and Miller (2015) and Otto and Eichenbaum (1992). "
"Grossberg and Versace (2008), Palma, Grossberg, and Versace (2012), and Palma, Versace, Grossbergand (2012) have furthermore proposed how mismatch-activated acetylcholine release may regulate vigilance in laminar neocortical circuits that are described by spiking neurons during category learning."
"A related set of experiments concerns measuring more carefully what happens during both attentive recognition vs. mismatch reset in response to sequences of familiar vs. unfamiliar cues. Here, the following surprising discovery in Grossberg and Versace (2008) may provide a useful marker"
"In addition, the Lundqvist et al. (2016) article describes modeling ideas in which there is no temporal order represented in working memory, although temporal order information is essential for proper functioning of a working memory."
 The term attentional shroud for such a form-fitting distribution of spatial attention was introduced by Tyler and Kontsevich (1995).
"Kelly and Grossberg (2000) explain stratification percepts, including simulations of their conscious 3D surface percept properties. "
"Grossberg and Yazdanbakhsh (2005)provide model explanations and simulations of these transparency percepts, among others, including simulations of the consciously seen surface percepts, much as the model has simulated 3D surface percepts in response to many other stimuli, including stereogram images "
"Crick and Koch (1995) also proposed that visual awareness may be related to planning of voluntary movements, but without any analysis of how 3D vision occurs. "
"The classical article of Driver and Mattingley (1998) reviews visual neglect properties in individuals who have experienced IPL lesions, particularly in the right hemisphere. The text below takes as explanatory targets properties emphasized in that article"
"A neglect patient who appeared to be blind in the left visual field when fixating straight ahead, or to her left, could detect events in her left visual field when she fixated to the right (Kooistra & Heilman, 1989)."
"The implicit knowledge of parietal patients includes object attributes of neglected stimuli such as their color, shape, identity, and meaning (Mattingley, Bradshaw, & Bradshaw, 1995; McGlinchey-Berroth, Milberg, Verfaellie, Alexander, & Kilduff, 1993)."
"Lesions of the right IPL that cause visual neglect also impair the ability to maintain visual attention over sustained temporal intervals (Rueckert & Grafman, 1998), whether for visual or auditory attention (Robertson et al., 1997). This impairment can be explained by the fact that a surface-shroud resonance maintains spatial attention on an object surface"
"ARTSCAN’s explanation of visual crowding (Foley et al., 2012), and with it an explanation of the Koch and Tsuchiya (2007) discussion of how “subjects can attend to a location for many seconds and yet fail to see one or more attributes of an object at that location” "
"Foley et al. (2012) have supported this qualitative explanation of visual crowding by using the distributed ARTSCAN, or dARTSCAN, model to simulate how objects that have their own shrouds when viewed by the fovea may be enveloped by a single shroud when they are moved to the retinal periphery. "
"That is the main point of the article by Grossberg, Mingolla, and Ross (1994) whose title “A neural theory of attentive visual search: Interactions of boundary, surface, spatial, and object representations” emphasized the role of these four kinds of processes."
"Further experimental and modeling studies of crowding and visual search, and their interactions, from this perspective are much to be desired and, as illustrated by the modeling simulations of Fazl et al. (2009) and Foley et al. (2012), need to include surface-shroud resonances as one part of a unifying theory."
" In one striking classical example, alternating displays of an original and a modified scene are separated in time by brief blank fields (Rensink et al., 1997)."
"This property is called contrast normalization (Grossberg, 1973, 1980; Heeger, 1992). Due to contrast normalization, when attention focuses upon one position, activity decreases at other positions."
Foley et al. (2012) also model why the remainder of a scene does not go totally dark when such a surface-shroud resonance of focused spatial attention forms.
Mitroff and Scholl (2005) showed in this case “that object representations can be formed and updated without awareness” by making changes in displays when they were out of awareness 
Chiu and Yantis (2009) used rapid event-related MRI in humans to provide strong evidence for the ARTSCAN prediction of how a surface-shroud resonance in the Where/How stream protects an emerging view-invariant category from being prematurely reset in the What stream
"In particular, Cao et al. (2011) developed the positional ARTSCAN, or pARTSCAN, extension of the ARTSCAN model to explain how these additional object category invariances can be learned. They have used this extended model to simulate neurophysiological data of Li and DiCarlo (2008; see also Li & DiCarlo, 2010) which show that features from different objects can be merged through learning within a single invariant IT category when monkeys are presented with an object that is swapped with another object during an eye movement to foveate the original object."
"These target positions hereby control shifts of spatial attention across an attended object, and have properties that Cavanagh, Hunt, Afraz, and Rolfs (2010) have called attention pointers"
"This is proposed to occur in cortical area V3A (Fig. 21). As noted by Caplovitz and Tse (2007, p. 1179): “neurons within V3A…process continuously moving contour curvature as a trackable feature…not to solve the ‘ventral problem’ of determining object shape but in order to solve the ‘dorsal problem’ of what is going where”."
"ART proposes that this happens because both resonances interact with shared visual cortical areas, such as V4, and can thus synchronize with each other, often with gamma oscillations (Fries, 2009; Grossberg & Versace, 2008)."
" An outflow representation of the current hand/arm position, called the present position vector, or P, is subtracted from the target position to compute a difference vector, or D (Georgopoulos, Kalaska, Caminiti, & Massey, 1982; Georgopoulos, Schwartz, & Kettner, 1986) that codes the direction and distance that the hand/arm needs to move to reach the target. "
"The Vector Integration to Endpoint, or VITE, model (Fig. 27, left panel; Bullock & Grossberg, 1988) modeled these processes to clarify how the Three S’s of reaching are carried out: the flexible choice of motor Synergies, and their Synchronous performance at variable Speeds."
"A refinement that sheds the most light on auditory–visual homologs of reaching and speaking circuits is called the DIRECT model (Bullock, Grossberg, & Guenther, 1993), which also learns through a circular reaction."
"During the development of the DIRECT model by Bullock et al. (1993), an evolutionary rationale was noted for why both the hand/arm and speech articulator systems may use similar, indeed homologous, neural circuits; namely, eating preceded speech during human evolution (MacNeilage, 1998), and skillful eating requires movements that coordinate hand/arm and mouth/throat articulators, including motor-equivalent solutions for reaching and chewing. "
"The auditory continuity illusion (Bregman, 1990) illustrates ART properties during auditory streaming."
"Grossberg (1978a, 1978b) introduced a neural model of working memory upon which the more recent models listed above consistently built."
"A more recent name for this class of models is competitive queuing (Houghton, 1990). When an Item-and-Order working memory can store repeated items in a sequence, it is called an Item–Order–Rank working memory (Bradski, Carpenter, & Grossberg, 1994; Grossberg & Pearson, 2008; Silver et al., 2011)."
"Phonemic restoration (Warren & Sherman, 1974) illustrates the operation of ART mechanisms during speech perception in much the same way as the auditory continuity illusion represents them during auditory streaming"
"Jones, Farrand, Stuart, and Morris (1995) reported similar performance characteristics to those of verbal working memory for a spatial serial recall task in which visual locations were remembered. "
"Agam, Bullock, and Sekuler (2005) reported psychophysical evidence of Item-and-Order working memory properties in humans as they performed sequential copying movements, and Averbeck et al. (2002); Averbeck, Crowe, Chafee, and Georgopoulos (2003a, 2003b) reported neurophysiological evidence for such a working memory in monkeys during performance of sequential copying movements."
"The motor-to-auditory selection process mechanistically explicates the “motor theory of speech perception” (Galantucci, Fowley, & Turvey, 2006; Liberman & Mattingly, 1985)."
"The Neural Normalization Network, or NormNet, model (Ames & Grossberg, 2008) proposes that speaker normalization specializes the same kinds of neural mechanisms that are used to form auditory streams."
"This proposal clarifies how speaker normalization can transform auditory signals, right after they are separated into separate streams, for purposes of speech and language classification and meaning extraction, yet how the frequency content of the streams can be preserved for purposes of speaker identification in a separate processing stream, as illustrated in the ARTSPEECH architecture (Fig. 33, right panel) of Ames and Grossberg (2008). "
Boardman et al. (1999) developed the PHONET model to quantify how T and S working memories can use asymmetric T-to-S gain control to create rate-invariant representations of individual speech syllables or words. 
"A classical example of this phenomenon was reported by Repp (1980). Repp constructed VC-CV syllables from the syllables [ib], [ga], and [ba] to form [ib]-[ga] and [ib]-[ba]."
The comparison between resonant fusion and resonant reset that plays an important role in explaining the Repp (1980) data on category boundary shifts also helps to explain data about the way in which masking stimuli can influence error rates and reaction times during lexical decision tasks.
"Grossberg and Stone (1986b) explained the paradoxical pattern of experimental results in terms of how the ART Matching Rule works in different priming situations, including the inability of top-down expectations to act before the mask interferes with the persistence of word and non-word target representations in working memory"
"The TELOS model (Brown et al., 2004) predicted how agreement between prefrontal and parietal representations of a target position causes a parietal–prefrontal resonance that selects this target position, and opens the correct basal ganglia gate, while also enabling basal ganglia-mediated release, in a different part of the brain, of a contextually-appropriate movement command to that position."
"Subsequent neurophysiological data of Buschman and Miller (2007)supported this prediction by describing such a parietal–prefrontal resonance during movement control, and Pasupathy and Miller (2004) additionally described different time courses of activation in the prefrontal cortex and basal ganglia that are consistent with how basal ganglia-mediated gating of prefrontal cortex occurs in TELOS."
"The lisTELOS model (Silver et al., 2011) extended the TELOS model to explain and predict how sequences, or lists, of eye movements can be carried out, while continuing to simulate everything that TELOS could. "
"On the other hand, it does propose how parallel neural mechanisms for rate-invariant and speaker-normalized representations of speech, and for pitch-dependent and rhythm-dependent speech intonation (Ladefoged & Disner, 2012), may interact to achieve “online sequencing of syllables into fast, smooth, and rhythmically organized larger utterances” (Ackerman, 2008, p. 265), including how these several kinds of information are learned, stored, and combined during fluent performance and conscious awareness thereof. "
"Learning also goes on throughout life of a parallel circular reaction that links learned spectral-pitch-and-timbre categories for the recognition of heard sounds (Table 2), which are not speaker-normalized, to the motor synergies that control the pitches generated by the vocal folds (Sundberg, 1977)."
"Neural models of cognitive–emotional resonances began with the articles of Grossberg (1971, 1972a, 1972b) and Grossberg (1975) at a time when there was a major split between studies of cognition, as exemplified by the work of Chomsky (1957) in linguistics, and of emotion, as exemplified by the work of Skinner (1938) on instrumental conditioning."
"For simplicity, consider only the simplest kind of reinforcement learning, called Pavlovian or classical conditioning (Kamin, 1968, 1969; Pavlov, 1927), during which a conditioned stimulus, or CS, that initially may have no emotional significance, is paired with an unconditioned stimulus, or US, that can from the start generate a strong emotional response. "
"Damasio (1999) has derived from clinical data what can be viewed as a heuristic version of the CogEM model, and has used it to describe what can be interpreted as cognitive–emotional resonances that support “the feeling of what happens”. "
"Damasio (1999, p. 171), notes: “Attention is driven to focus on an object and the result is saliency of the images of that object in mind”, leading to what Damasio calls core consciousness. Damasio (1999) also went on to write “I do not know how the fusing, blending, and smoothing are achieved…” (p. 180). "
"These include the “dual competition” model of Pessoa (2009, p. 160): “The proposed framework is referred to as the ‘dual competition’ model to reflect the suggestion that affective significance influences competition at both the perceptual and executive levels—and because the impact is caused by both emotion and motivation”."
"The ARTSCENE model (Grossberg & Huang, 2009) and ARTSCENE Search model (Huang & Grossberg, 2010) illustrate how humans accomplish these goals."
"Tamietto and de Gelder (2010) have reviewed several different kinds of experimental evidence that led them to a similar viewpoint, but without mechanisms of adaptive resonance to derive mechanistic conclusions. "
"Clark and Squire (1998, p. 79) postulated that normal humans acquire trace conditioning because they have intact declarative or episodic memory and, therefore, can demonstrate conscious knowledge of a temporal relationship between CS and US: “trace conditioning requires the acquisition and retention of conscious knowledge [and] would require the hippocampus and related structures to work conjointly with the neocortex”."
"In addition to explaining data about normal delay and trace conditioning, the nSTART model explains and simulates many subtle data about how learning and memory consolidation are influenced by different brain lesions (Franklin & Grossberg, 2016)."
" Crick and Koch (1990)described two forms of consciousness “a very fast form, linked to iconic memory…; and a slower one [wherein] an attentional mechanism transiently binds together all those neurons whose activity relates to the relevant features of a single visual object”. "
"For example, the neural global workspace of Dehaene (2014), which builds upon the global workspace of Baars (2005), claims that “consciousness is global information broadcasting within the cortex [to achieve] massive sharing of pertinent information throughout the brain” (p. 13)."
"Continuing in the spirit of Edelman and Tononi (2000), Tononi (2004) defined a scalar function Φ, “the quantity of consciousness available to a system… as the value of a complex of elements."
"Unlike traditional team-based work, however, members of the crowd are distributed and in many cases without those obligations as found in companies (long-term contracts or roles) [1]. The crowd presents a pool of experts, who are connected amongst themselves forming a social network."
"Most approaches model the problem as finding the best match of experts to required skills taking into account multiple dimensions from technical skills, cognitive properties, and personal motivation [2–4]. Such research focuses only on properties of individual experts that are independent of the resulting team configuration."
"Sozio and Gionis describe the community formation problem [6]. Given a set of fixed members, the approach expands the team up to a maximum upper size boundary such that the communication cost within the community remains small."
"Anagnostopoulos et al. [7]address fair task distribution within a team. They apply skill matching to determine a team's ability to fulfill the overall set of tasks. While their approach takes into account team members' load and skill dependencies, the underlying social network structure has no impact on the team's fitness. "
Yang et al. [8] apply integer programming to determine the best set of group members available at a certain point in time. Their temporal scheduling technique considers the social distance between group members to avoid lacking too many direct links. 
Craig et al. [9] propose an algorithm for reasonably optimal distribution of students into groups according to group and student attributes.
"Xie et al. [10] aggregate a set of recommender results to optimally compose a package of items given some relation between the individual items and an overall package property (e.g., holiday package)."
"To the best of our knowledge, Theodoros et al. [13] discuss the only team composition approach that specifically focuses on the expert network for determining the most suitable team. Our approach differs in three significant aspects. "
"First, we model a trade-off between skill coverage and team connectivity whereas [13] treats every expert above a certain skill threshold as equally suitable and ignores every expert below that threshold."
Also Singh [14] shows that a densely connected team is vital for successful open source developer cooperation. Most importantly we apply recommendations instead of direct interaction links when the underlying network becomes too sparsely connected.
"Analysis of various network topologies [15,16] has demonstrated the impact of the network structure on efficient team composition."
"Investigations into the structure of various real-world networks provides vital understanding of the underlying network characteristics relevant to the team composition problem [18,19]."
"Complementary approaches regarding extraction of expert networks and their skill profile include mining of email data sets [22,23] or open source software repositories [24]. Additional sources include (scientific) publications and financial data [25], social network page visits [26], telecommunication data [27], and online forum posts [28]."
"Related research efforts based on non-functional aspects (i.e., non-skill related aspects) can also be found in the domain of service composition [29]. Here, services with the required capabilities need to be combined to provide a desirable, overall functionality. "
"The model of recommendation-based link establishment is closely related to link prediction in social networks. Such models are used to introduce connections between single members of a community by evaluating various properties. For instance, work by [40,41] discusses link prediction based on similarity, focusing on structural graph properties such as number of neighbors and number of in/out links."
"In social trust networks [42] recommendations reflect transitive relations among members. In that case, unconnected nodes in a trust network are connected through an intermediate node that mediates second hand knowledge among its neighbors. In the future, direct trust between humans will play an ever more important role as privacy remains a largely unsolved challenge [43]. Hence we believe that establishing explicit trust in social networks (e.g., [44,45]), respectively becoming aware of distrust, will become a significant factor in team formation."
"In today's highly dynamic large-scale networks, people are no longer able to keep track of the dynamics, such as registration of new actors in expert networks and emerging skills and expertise of collaboration partners. Since interactions and collaborations on the Web are observable, systems can analyze tasks performed in the past in order to determine network structures and member profiles automatically [46,42]."
Simulated Annealing [47] (SA) and Genetic Algorithms [48] (GA) are two common heuristics suitable for the underlying problem type.
"Previous work suggests dynamic adaptation for crossover and mutation probabilities [49], whereas [50] applies clustering techniques to determine suitable values. The correlation of population size and cross over is investigated in [51]. These three exemplary works, however, address very different problem domains. In the case of simulated annealing, work on optimizing parameters is similarly problem specific: [52] addressing a graph partitioning problem, [53] focusing on the longest common subsequence problem, and [54] dealing with distributing workload across multiple processors."
 Parameterless multi-objective algorithms such as NSGA-II [55] provide multiple pareto-optimal solutions to the team formation problem without setting α to any particular value. 
Finding a single shortest path is in O(|ECand| + |Cand|log|Cand|) [56]. 
"The bullwhip effect also has a close link with the philosophy of lean production (Ohno, 1988). Mura—the waste of unevenness—is the failure to smooth demand and is recognised as the root cause of both Muda (the seven lean wastes) and Muri (the waste of overburden). Indeed Ohno (1988) discusses the benefits of bullwhip avoidance"
"Geary, Disney, and Towill (2006) classified five routes to increase our knowledge of bullwhip effect and 10 principles to reduce it."
" Miragliotta (2006) reviewed bullwhip research in three categories; empirical assessment, causes, and remedies, and then proposed a new taxonomy to model this problem. "
" Giard and Sali (2013) categorised 53 bullwhip papers within 13 coordinates, including modelling approaches, demand models, measures, and causes. "
"Other reviews are more conceptually oriented, attempting to offer a new perspective on bullwhip (Towill, Zhou, & Disney, 2007)."
"Some reviews are not solely confined to the bullwhip effect, but also cover other supply chain modelling issues (Beamon, 1998; Min & Zhou, 2002; Sarimveis, Patrinos, Tarantilis, & Kiranoudis, 2008). "
"Interestingly, a similar phenomenon between P&G and its wholesalers has been documented during 1910s (Schisgall, 1981). "
Forrester (1961) first formalised the variance amplification effect using the ‘industrial dynamics’ approach. He later established a simulation experiment mimicking the decision making behaviour in supply chains—the famous ‘Beer Game’
" Sterman (1989) published 20 years of data from the game attributing the amplification to the tendency that players overlook the inventory-on-order (the orders placed but not yet received), a cause of amplification known as ‘irrational behaviour’."
"The production smoothing hypothesis (Holt, Modigliani, Muth, & Simon, 1960) assumes that production fluctuations increase the operational cost to the manufacturer by inducing excess machine setup, idle time and workforce hiring/firing. "
"There is also a trade-off between inventory cost and production cost, due to the stabilizing effect of inventory (Baganha & Cohen, 1998; Disney, Towill, & van de Velde, 2004). Chen and Samroengraja (2004) showed that when the cost function is concave, the replenishment policy that minimises order fluctuations is not necessarily the one that minimises total cost."
"Under non-stationary demand it is necessary to perform difference operations on the time series. That is, to measure bullwhip by the variance of order changes instead of the variance of orders itself (West, 1986). "
"Alternatively one may compare the difference between order variances and demand variances which has been proved to be finite (Gaalman & Disney, 2012). If the inventory system is to be modelled linearly, then the variance ratio is convenient because it coincides with an engineering concept called the noise bandwidth, a concept with an established theoretical basis (Åström, 1970)."
"In 1960 Holt et al. (1960) proposed the production smoothing model assuming that rational decisions regarding production quantities would lower costs by levelling production, with inventory being used as a buffer. Efforts have been made to optimise this model under various assumptions (Gaalman, 1978; Schneeweiss, 1974; Zangwill, 1966). "
" Quite contrarily, many empirical studies have found amplification between retail sales and production orders, as well as positive correlation between demand and inventory (Blanchard, 1983; Blinder, 1986; Blinder & Maccini, 1991; West, 1986). These can be viewed as early examples of the bullwhip effect in the production echelon, an effect that was then termed ‘excess volatility’"
 Ghali (2003) showed that production smoothing can be found only in a small number of industries where seasonality is stable and inventory holding cost is low.
" In 75 industries, Cachon et al. (2007) observed that 61 exhibited bullwhip when seasonality was removed, but only 39 when not. Similar findings have been reported by Bray and Mendelson (2012), on the basis of firm-level, rather than industry-level, data. "
"Baganha and Cohen (1998) observed that bullwhip effect appears in the wholesaler's echelon, and argued that the wholesaler's inventory acts as a stabiliser in the chain."
" Mollick (2004) described evidence of production smoothing in the Japanese automotive industry, where the production smoothing is more common due to the prevalence of Heijunka (levelling) and Just-In-Time manufacturing strategies. "
" Cantor and Katok (2012) introduced a cost for production and order changes, and found that production is smoothed when demand is seasonal, and that the smoothing behaviour is more eminent when the production change cost is high."
"The simplest demand model is an independently and identically distributed (i.i.d.) Gaussian white noise process (Deziel & Eilon, 1967). This model has some mathematical advantages, but may be an over-simplification as it overlooks temporal correlation in the demand signal."
"More complex ARIMA models for demand have also been studied: AR(2), AR(p) (Luong & Pien, 2007); ARMA(1,1) (Alwan, Liu, & Yao, 2003); ARMA(2,2) (Gaalman & Disney, 2009); and ARMA(p,q) (Gaalman, 2006)."
"A wide range of forecasting methods have been investigated in the bullwhip literature. Chen et al. (2000a) and Duc et al. (2008a) studied the moving average (MA) forecasting method, while Chen, Ryan, and Simchi-Levi (2000b) and Dejonckheere, Disney, Lambrecht, and Towill (2003) investigated the simple exponential smoothing (SES) method. These are both user-friendly forecasting techniques that have been widely adopted in industry."
"The impact of more sophisticated forecasting methods such as Holt's, Brown's and Damped Trend forecasting was discussed by Wright and Yuan (2008) and Li, Disney, and Gaalman (2014). These forecasting techniques are designed for seasonal and trended demand."
"Another interesting topic is the relationship between forecast accuracy and total cost. Zhang (2004a) suggested that MMSE forecasting minimises inventory-related cost. This was supported by Hussain et al. (2012) in a simulation study. However, according to some empirical (Flores, Olson, & Pearce, 1993) and analytical research (Hosoda & Disney, 2009), the most accurate forecasting does not always result in an optimal supply chain when local bullwhip or global inventory costs are taken into account (Disney, Lambrecht, Towill, & Van de Velde, 2008; Gaalman, 2006; Gaalman & Disney, 2006; Gaalman & Disney, 2009)."
"Forrester (1961) highlighted that the delays in information and material flow, a.k.a. the lead-times, is a driving factor of demand amplification."
" Lee et al. (1997) and Chen et al. (2000a) argued that bullwhip increases in lead-time, as did Steckel, Gupta, and Banerji (2004) and Agrawal, Sengupta, and Shanker (2009)."
"Modelling lead-time as a random variable mimics the volatility of real-life logistics. Chatfield, Kim, and Harrison (2004), Kim, Chatfield, Harrison, and Hayya (2006) and Duc, Luong, and Kim (2008b) showed that order variability increases with lead-time variability, a result that is also supported by the behavioural experiment conducted by Ancarani, Di Mauro, and D'Urso (2013)."
"State-dependent lead-times have been examined by So and Zheng (2003) and Boute, Disney, Lambrecht, and Van Houdt (2007), and both studies found that bullwhip is underestimated if the endogeneity of lead-time is neglected."
"The automatic pipeline, inventory and order-based production control system (APIOBPCS) proposed by John, Naim, and Towill (1994) is mathematically equivalent to Sterman's (1989)‘anchoring and adjustment heuristic’."
Deziel and Eilon (1967) proposed the first linear proportional production control policy where the same feedback parameter is assigned to both the inventory and pipeline levels.
"General guidance on tuning the feedback parameters is given by Balakrishnan, Geunes, and Pangburn (2004), Papanagnou and Halikias (2008). Graves, Kletter, and Hetzel (1998) and Boute and Van Miegham (2015) describe other proportional ordering policies."
"The problem of product/location aggregation arises when a supplier faces multiple retailers, or distribution centres in different locations, or by manufacturing different products on the same line. This problem has been investigated under (s,S) (Caplin, 1985; Kelle & Milne, 1999), (Q,T) (Cachon, 1999; Lee et al., 1997) and base stock (Sucky, 2009) policies."
"Lee and Whang (2000) summarised the common schemes for sharing information on inventory levels, sales data, sales forecast, order status and production/delivery schedules"
"From experimental and analytical evidence, some authors have found that information sharing alone cannot eliminate the bullwhip effect (Chen et al., 2000a; Croson & Donohue, 2006; Ouyang, 2007; Sodhi & Tang, 2011). "
"It has been discovered that if the order quantity is constrained to non-negativity (as opposed to the ‘costless return’ assumption, Lee et al., 1997), then highly complex and sophisticated dynamical behaviours can be found in supply chains (Mosekilde & Larsen, 1988). Moreover, this dynamical complexity is also amplified along the chain, in an effect known as chaos amplification (Hwarng & Xie, 2008). "
"The structure of real supply chains is further complicated by sourcing, distribution and transhipment activities. Ouyang and Li (2010) proposed a general supply network model that allows for transhipment, information sharing, and collaboration; they identified conditions for bullwhip."
 Akkermans and Vos (2003) measured workload in a major US telecom company. They detected the amplification of workload and identified a potential cause of the amplification: negative feedback between workload and service quality. 
 Özelkan and Çakanyildirim (2009) studied financial flows in a game theoretical two-echelon supply chain model. 
"Among these are Zhang and Burke (2011), who showed that introducing price fluctuations can either exacerbate or mitigate the bullwhip effect, based on the auto- and mutual-correlation between price and demand. "
"Recently Sodhi, Sodhi, and Tang (2014) incorporated a discretely distributed stochastic price into the economic order quantity model. They showed that the bullwhip effect persists and is positively related to the variance of price."
 Molodtsov [1] proposed an uncertainty-soft set theory that is completely new approach for modeling vagueness.
"Soft set theory is getting popularity among the researchers in these domains. Soft set has been extensively and successfully applied in decision making [2–19], data analysis [20–22], forecasting [23], simulation [24], evaluation of sound quality [25], rule mining [26], and so on."
"Combining soft sets with others mathematical theories, such as fuzzy sets [15,27–31], rough set [28,31–33], vague sets [34], interval-valued fuzzy sets [10,12,35], interval-valued intuitionistic fuzzy soft set [36], intuitionistic fuzzy soft set [35,37–39], and so on, has come forth rapidly to meet various demands in practical situations."
Parameter reduction in soft set is discussed in published papers. Maji et al. considered the initial level reduction soft set with the help of rough set approach [2]
"However, Chen et al. pointed out that the errors of soft set reduction and presented a new notion of parameterization reduction in soft set [40]"
"Kong et al. analyzed two cases, suboptimal choice and added parameter set, and introduced the definition of normal parameter reduction and its algorithm in soft set [42"
Ma et al. simplified the normal parameter reduction algorithm [43
Gong et al. proposed two parameters reduction algorithms based on bijective fuzzy soft set system [14].
"Kong et al. [42] introduced the definition of normal parameter reduction in soft set and its algorithm. In the following, the definitions of normal parameter reduction and indiscernibility relation are given"
"Managers spend up to one fifth of their working time with conflict resolution and negotiation [15,63]. They increasingly negotiate via electronic media such as e-mail, e-meeting and e-negotiation systems [73]. Electronic negotiations are not mere translations of traditional negotiations onto electronic media, but rather they provide additional value by supporting the decision making and/or communication process [62,74]. Electronic negotiation support (eNS) is realized through information and communication technology and can range from a simple message exchange to a complex support system. A negotiation support system (NSS) comprises one or more of the following functionalities: facilitation of communication, decision/negotiation analysis support, process organization and structuring, and access to information, negotiation knowledge, experts, mediators or facilitators [26]. In this context, the representation of information (textual, graphical, and auditory) is important for human–computer interactions. Due to technical advances in the last decades, users can often rapidly and effectively choose from various formats of computer generated reports. We know from empirical evidence that the way information is presented strongly influences human perceptions, preferences and decision making (e.g. [5,76]). Thus, the presentation of information is of essential importance for decision makers [70,77]"
"Although information representation is relevant, it has received little attention in negotiation research. Typically, information in e-negotiation systems is presented in text or tabular format. Except for the suggested utilization of the “negotiation dance graph” [56], to date only a “history graph” has been proposed and implemented [27,63]. A history graph exhibits the history of offers and counteroffers over time of both negotiators based on preferences of the supported user only. In contrast, the negotiation dance graph represents all offers and counteroffers in the utility of both negotiators, while time is only implicitly considered, and it provides users with information about the actual preferences of their counterparts"
"The present study aims to analyze how information presentation in these alternative formats (table, history graph and dance graph) influences the negotiators' behavior and negotiation outcomes. The paper reports on a 2006 controlled laboratory experiment. Students from three universities in Europe and the Middle East negotiated a contract in a scenario with multiple issues in the tourism industry. Using the NSS Negoisst [62,63], subjects were divided into three treatment groups using the three different representation aids on the negotiation process: a table, a negotiation history graph or a negotiation dance graph"
"The paradigm of cognitive fit suggests that effective and efficient problem solving is obtained when all tools or aids used in the problem solving process correspond to the requirements of the task [78–80]. Problem solving is seen as an outcome of the relationship between problem presentation and the problem solving task. Cognitive processes act on the information presentation and the problem-solving task to provide a mental representation of the situation. The latter is the way the problem is represented in human working memory. When the types of information in the problem presentation match those in the task, the problem solver formulates a mental representation that is based on the same type of information. In contrast, a mismatch between the problem presentation and the task leads to a mental representation based only on the problem representation. The decision maker must then mentally transform the task into a suitable form, exerting additional cognitive efforts in order to solve a particular type of problem. Similarly, if a mental representation is formulated according to the task alone, the decision maker has to transform the data of the problem presentation into an appropriate form for the task solution. In both cases, additional cognitive capacities are required for auxiliary mental steps, which typically lead to poor results for the decision maker. The cognitive fit theory encourages the use of problem representations consistent with task requirements in order to improve the decision making process for those using decision aids"
"While text-based systems constitute a minimum requirement, all other representation forms are more sophisticated. One idea to support decision makers is to quantify all available data and to implement it into numerical systems, which have already been shown to provide better support than simple textual messages. Numerical systems require well-structured inputs in a predefined format [19], show impacts of variables on results [7] and provide assessment scores [36]. However, numerical systems do not support decision makers in handling dynamic processes [7]. In negotiations, the history of exchanged offers, the concessions of the negotiation parties over time, their possible change of preferences and similar dynamic processes contain essential information for negotiators [62,81]. A more stylized information representation is essential."
"As graphs can be displayed in various formats, they often differ considerably in terms of their abstraction or arbitrariness. No unique terminology has been used for characterization of graphs. They are described as being “imaginastic,” which means that they convey continuous information, while tables are seen as “verbal” in nature, i.e. they convey discrete information [78,79]. Graphs have visuospatial properties meaning they stress information on data relationship rather than on linguistic intelligence [4,5]. Graphs facilitate the acquisition of information by focusing on single units of information and their characteristics. They also allow for the grouping of information [35] and the establishment of associations among the values of each information package (or variable) across time periods without addressing the elements separately or analytically (e.g. [4,78,79]). Graphical display formats have a sequential structure reflecting an overview of the presented information. Many perceptual inferences, including perceiving and drawing inferences, are automatically supported at low cognitive costs [8,34]. Graphs facilitate the comprehension of large amounts of quantitative information [44,67]. Empirical research has reported that subjects provided with graphical formats are more effective in trend, pattern and time sequence data detection, (e.g. [12,68,77]) and in task execution in terms of processing time (e.g. [31,32,44])."
"Concerning the level of complexity, tables outperform graphs regarding time and decision accuracy in simple decision making settings [45,58]. At a low level of complexity, graphs are perceived to be more difficult to read than tabular displays [12]. An increase in task complexity is better mediated by spatial rather than linear information displays [68]. Studies suggest that graphical decision aids are more efficient and lead to better performance when subjects face a higher cognitive load [45,58,70]. Graphs have been found to be more appropriate for the presentation of large amounts of information [12], because users have to invest less effort in order to “get the message” shown in graphical displays [5,6,40]. Users sometimes prefer graphs to tables due to their appealing format; they enjoy exercises and experience a higher level of satisfaction [40,43,77]. Still, subjects do not always prefer the most appropriate presentation format for the relevant task [20,32]"
"The most common and straightforward way to provide users of NSS with information about multi-issue offers is to present the utility values [27]. This involves analyses of the current offer and all prior offers made in the negotiation. Offers are evaluated and compared to the negotiator's aspirations, reservation level or to the BATNA (Best Alternative To Negotiated Agreement) over several periods of time, while all social interactions are processed simultaneously [1,66]."
"One way to visualize the negotiation process graphically is the history graph (see Fig. 2), which has already been implemented in NSS [63,64,82]. In the history graph, the factor “time” is represented on the horizontal axis and negotiators' “utility” is on the vertical axis. All offers are labeled on the ordinate according to the score associated with an offer. Even though offers of both parties are displayed, the calculation of the utility values is based only on the preferences of the focal user. Therefore, the history graph shows the distance between the offers submitted and received based on the focal users' value function. The history graph is designed to enable users to assess how far they are from reaching an agreement. For example, company A and company B negotiate over a contract including several issues and refer to the history graph. When company A formulates an offer, the utility rating of the offer and consequently its graphical presentation is based on the preferences of company A. When company B analyzes the offer received from company A, company B is provided with a rating and a graphical display of the offer according to the ratings of company B. This implies that each transmitted offer is rated according to the focal user only, while the preferences of the counterpart are not taken into account in the rating of offers or in the graphical displays"
"To answer the research questions, an electronic negotiation support system is required that supports business negotiations, rich communication support and various forms of decision support. Negoisst is a web-based NSS offering sophisticated support and formal document management [62,63]. Therefore, the experiments were conducted using Negoisst (see Fig. 4 for a screenshot of the system). Users negotiate via an electronic message exchange. The content of the messages is written in natural language (shown to the left of Fig. 4). In order to avoid misunderstandings and to prevent re-negotiations due to contractual ambiguities, Negoisst offers semantic and pragmatic enrichment. Semantic enrichment links free text to the negotiation agenda (shown to the right of Fig. 4). Pragmatic enrichment supports explicit intentions, because message types are indicated by the author (see Fig. 4). Negoisst also provides decision support. Negotiators specify their preferences on attributes to be negotiated and the system then computes a utility function. Each offer is rated, and both negotiators can see in a glance how well they have already achieved their goals. If a negotiator writes a message offering a certain package, then the system will calculate the utility immediately. The negotiator can check the utility value before sending the message. Negoisst automatically deduces a contract version from each message sent, as well as a message thread representing the reasons for the decisions taken. Users are able to check the contract versions as well as all exchanged messages at any time during the negotiations."
"Whether an agreement is reached or not is an indicator of the effectiveness of negotiations but not of the quality of negotiation outcomes. In the negotiation theory, three further indicators are often used to measure the quality of negotiation outcomes: joint outcome (as an indicator for efficiency), contract balance (as an indicator for fairness), and negotiator satisfaction with agreement (as a holistic assessment) [16,33,57]. Empirical evidence proves that negotiators pursuing an integrative negotiation strategy produce higher joint outcomes (e.g. [10,83,85]). Furthermore, there exists a trade-off between time/effort and decision quality or accuracy [22,24]. The development of value-creating offers, e.g. through logrolling, requires significantly more cognitive effort. This can be more easily achieved when negotiators are supported with the history graph. Therefore, we assume:"
"We expect that this additional information will change negotiation behavior in several ways. By providing utility information about both negotiators, dyads should be better able to assess whether their negotiation partner behaves fairly. Negotiators provided with this type of graph can easily see if real concessions are being made. Decision makers aware of this fact should consequently ask their opponent for fair treatment and stress the importance of fairness more often [47]. Therefore, we expect:
H 4(a)"
"Although we do not expect differences in the number of agreements between the two groups, we expect the quality of agreements to differ significantly. The visualization of changes in utilities due to modifications in single issues in the negotiation dance graph helps negotiators to identify Pareto movements and efficient alternatives [56]. "
"We assume that the visibility of differences in utilities during the negotiation process makes it more difficult to demand “the bigger share of the cake” [60]. There is an expectation of more balanced agreements when negotiators have information about utilities of both negotiation partners, and we hypothesize:
H 6(b)"
"Consequently, we expect negotiators who reach higher joint outcomes and more balanced agreements will be more content (e.g. [17,37,77]), and we hypothesize:
H 6(c)

Negotiators provided with the negotiation dance graph will be more satisfied with the agreement compared to negotiators provided with the history graph."
"Several factors that could affect negotiation process/outcomes were not investigated in this paper. First, several studies show that the level of conflict in simulation cases influences results significantly [11,53]. Conflict could be induced by varying the discussion issues and creating more integrative/conflicting bargaining settings. Users' performance could be observed by changing only external factors (in this case the bargaining situation in which negotiations are embedded). Variance in the number of issues involved in a case could also affect the end result. Another avenue of future research is the effect of additional information provided to users. The present study shows that the amount of information provided to negotiators leads to either more cooperative or more competitive behavior. Future studies should investigate the impact of different types of information implemented in different information displays. Considering the process of information gathering, future investigations also need to examine the effect of dynamic decision aids at different stages of decision making. A particular focus should be placed on the stages in which information is acquired and in which the information is evaluated. The issue of time duration of the experiment must be taken into account [51]. The effects of additional support provided by graphical aids are often seen as a trade-off between the benefits of minimizing errors and the cognitive effort or time needed in a particular task environment [22]. In the present study, there was an imposed time deadline for all users, thus the variable time was kept constant and all impacts could be considered only with regard to proxies for the quality of decisions. Raiffa [56] argues that a negotiation resembles a dance of negotiation partners. We have demonstrated with this study that there is no straight answer to the question “Shall we dance?” Rather the results suggest that the answer depends on the partners' aims (efficiency vs. fairness) quantitative vs. qualitative outcomes (utility vs. satisfaction), to dance or to skip the dance."
"Thus, every Venn diagram is an Euler diagram, but not vice versa. Euler diagrams support the interpretation of grouping information since elements in a common set are located in the same region [31].There have been a number of empirical studies conducted to ascertain the effect of different Euler diagram layout choices on user comprehension which provide a starting point for effectively laying out Euler diagrams with graphs."
"This was carried out by analyzing the layout and structure of socially constructed texts of “organizational communication” (Yates & Orlikowski, 1992) amongst people in a particular workplace or in a “community of practice” (CoP) as described by Wenger (1999), where, genre, in a textual sense, is sometimes defined as a group of texts or documents that share a communicative purpose, as determined by the discourse community which produces and/or reads them (Swales, 1990). "
"Collins, Mulholland, and Watt (2001)explained that what the community sees as important will be reflected in the implicit structures found in the objects they create and share and as Watt (2009, chap. 8) has observed: “convergence on a set of standardized document structures is both natural and helpfu"
"Layout in organizational communities causes people to focus perceptually on key parts of the text (Schmid & Baccino, 2002) and our empirical research has previously demonstrated that people use layout and other related cues to focus on key parts of the text (Clark, 2008; Clark, Ruthven, & Holt, 2008; Clark, Ruthven, & Holt, 2010; Clark, Ruthven, Holt, & Song, 2012). "
"The reader is able to perceive the meaning through interaction with the cues which exist on the outside and inside of the “frame” (Frow, 2006, chap. 5) – a term that Frow uses synonymously with genre."
"To examine genre and ways of perceiving, we used specific eye movement behavior metrics or ‘ocular metrics’ (Rayner, 1998) which have been in fairly common use in contemporary eye tracking experiments i.e. Scanpath Duration and Scanpath Length, c.f. Goldberg and Kotval (1999, p. 638). "
Aristotle (1954) considered that whatever was perceivable by the individual was reality. 
"Outside objects imposed upon the senses, and due to the power of reason, the mind was able to extricate the form, which determined the nature of the perceived object (Breure, 2001). "
" We contend that the specific contexts of researchers guide the way they delineate genre: as Kwaśnik & Crowston (2005)argue, the researcher chooses the definition applicable to the current context of the study."
"Readers viewing text(s) are always involved or relate to the complete arrays of textual meaning. This is quite closely related to Semiotic ‘intertextuality’ a term that is said to have been coined by the post-structuralist semiotician, Kristeva (1980). "
"In other words, an author or artist refers to an earlier work and subsequently converts a previous creation with it then being referred to in the new text. As Chandler puts it: “The concept of intertextuality reminds us that each text exists in relation to others. In fact, texts owe more to other texts than to their own makers” (Chandler, 2011)."
"Hirsch Jr. (1967, p. 76), explains that genre is an interpretative process called into being by the fact that “all understanding of verbal meaning is necessarily genre-bound.”"
"Hirsch’s explanation could be appropriately linked to the work pertaining to “perceptual hypotheses” by Gregory (1980) or, indeed, as we like to refer to it, ‘perpetual’ perceptual hypotheses, where we are continuously trying to ascertain what an object or text is. "
" Lorch (1989) identifies textual signals, such as headings, previews, summaries, titles, numeric signals and so on. All texts are accompanied by these types of cues or signals which ‘present’ the texts to the reader or ensure the presence of the texts in the world."
"For the purpose of this study, genre was defined by its purpose (sometimes known as substance) but mainly by form (see Fig. 2 for categories of form) as described in Dewdney, VanEss-Dykema, & MacMillan (2001) and Yates & Orlikowski (2002, p. 15). "
"The constructivists assert that the final goal in the perception process is recognition which would require intense cognitive processing, for example, Gregory (1980) and his theory of perceptual hypothesis. "
"Many consider skimming and scanning to be techniques related to searching as opposed to strategies for reading, for example, Just & Carpenter (1987). In fact, they are both correct but reading and searching are two different contexts. "
"The mental spotlight is quite a helpful analogy to describe a task, such as searching for a keyword, etc. Masson (1983) describes skimming “for most of us, rapid reading involves some form of skimming in which we try to focus on information relevant to our goal and skip over irrelevant information”. "
 Holmqvist et al. (2011)suggests that a “sequence of long saccades is likely to reflect skimming over the text”. 
"As Rayner (2009, pp. 1484) points out, equivalents between “visual search and scene perception are greater than with reading, in that “visual saliency” plays a greater role in directing fixations”."
"In each case, different ocular behavior would be expected Rayner (2009, pp. 1484). Many methodologies and algorithms have been devised for the detection of reading, firstly for a baseline, then secondly, comparing those results to other data to detect skimming, scanning or both, for example Campbell & Maglio (2001), Buscher, Dengel, van Elst, & Mittag (2008), and Buscher, Dengel, & van Elst (2008). "
" In Watt (2009, p. 171, chap. 8) he opines, genres behave as “affordances” and in essence can be filtered and categorized by form."
"Gibson’s affordances are intended to describe how meaning and perception are inter-related: he argues in Gibson (1986, p. 127) that instead of perceiving objects (for example, texts) and then adding meaning later, there are visual combinations of invariant and distinctive characteristics of objects which provide cues on how to act and behave in relation to these objects (in this case textual e-mails)."
"Alternatively, Toms & Campbell (1999b), in their study, leaned towards the Constructivist (perception for recognition) process, since they aimed to contrast the content (function) and form in order to discover whether readers can perceive and process form on its own or need semantic content to identify it. "
"Although the research carried out by Toms & Campbell (1999a, 1999b), Toms, Campbell, & Blades (1999), Toms (2001), and then Watt (2009, chap. 8), seems to indicate a leaning towards one process or another (Watt Ecological and Toms Constructivist) the latter does explore Ecological in her thesis (Toms, 1997), it may emerge that they are both correct (or indeed wrong), but for different information searching tasks and in different contexts."
"We conducted an analysis of the eyetracking data studying such basic metrics based on fixations, saccades and number of genres identified correctly along with length of time to identify, c.f. Clark et al. (2010)."
 Goldberg & Kotval (1999)conducted computer interface evaluations with twelve participants testing the interfaces whilst analyzing their scanpath behavior. 
"Lorigo et al. (2006), in an extension of the work in Pan et al. (2004), used scanpath fixations pattern-finding to compare the differences in gender and task type during a web search. They found differences in scanpaths according to gender, and the task comparison results although mixed, did not reveal any effects related to task type on scanpaths. "
"Joachims, Granka, Pan, Hembrooke, & Gay (2005) used scanpath measurements to examine the reliability of implicit feedback generated from click through data in Web searches"
 Brandt & Stark (1997) showed their participants visual imagery of irregularly-chequered diagrams.
The experimental setup of the evaluation was based on commonly used standards c.f. Joachims et al. (2007) and Kelly (2009). 
"The experimental procedures, such as questionnaire design, were based on methods and protocol used by previous interactive experiments (Dupont et al., 2010; Harper & Kelly, 2006; Huang et al., 2006; Kelly et al., 2007; Kelly, Harper, & Landau, 2008; White, Ruthven, & Jose, 2002; White et al., 2006). "
The experimental eyetracking data was input into the SPSS software along with the data used in Clark et al. (2010) and then statistically evaluated.
"The use of skimming and scanning techniques was detected by referring to the 20 possible permutations found in Campbell & Maglio (2001, p. 3)and Buscher, Dengel, van Elst, and Mittag (2008) scoring was based on the short, medium or long movements, which were given a particular score whenever they occurred on the X or Y axes gaze point."
" Just like Watt (2009, chap. 8) – in his timed response design – we balanced for length and still found a very strong effect (an interaction – between layout representations) which indicated that genre speed was a factor independent of length, as in Clark, Ruthven, & Holt (2009b) and Clark et al. (2010)."
"The scanpath duration measure is used to see how much time participants spend on processing information and “complexity” Goldberg & Kotval (1999, p. 638); a longer scanpath duration indicates participants are spending more time processing information and hence classifying information is far more ‘intensive’. "
"We intend to continue our research by looking at other genres on other web communities of practice, notably Wikipedia, to expand on previous work in Clark et al. (2009a) and Clark et al. (2012) and using, in addition, web data collected from two university intranets. "
"When I was looking for a Ph.D. dissertation topic, I accidentally came
across a paper by W. Larimore on Statistical Inference on Random Fields
in the Proceedings of the IEEE [6]."
"A deterministic alternative known as the iterated
condition mode was presented in the paper by Besag [3] on statistical
analysis of dirty pictures that appeared in the Journal of Royal
Statistical Society."
"The vision of Cloud Computing is to provide computing power as a utility, like gas, electricity or water [1]"
"This work can be integrated into the Foundations of Self-governing ICT Infrastructure (FoSII) projectÂ [2], but is on its own completely self-sufficient."
" Besides the already implemented LoM2HiS frameworkÂ [3] that takes care of monitoring the state of the Cloud infrastructure and its applications, the knowledge management (KM) system presented in this article can be viewed as another building block of the FoSII infrastructure."
"[4] proposes an approach to manage Cloud infrastructures by means of Autonomic Computing, which in a control loop monitors (M) Cloud parameters, analyzes (A) them, plans (P) actions and executes (E) them; the full cycle is known as MAPEÂ [5]"
According toÂ [6] a MAPE-K loop stores knowledge (K) required for decision-making in a knowledge base (KB) that is accessed by the individual phases. 
"On the other hand, we gathered real world data from monitoring scientific workflow applications in the field of bioinformaticsÂ [9]"
"These workflows need a huge, yet unpredictable and varying amount of resources, and are thusâdue to the needed flexibility and scalabilityâa perfect match for a Cloud computing applicationÂ [10]."
" [17,18] focus on VM migration andÂ [19] on turning on and off physical machines, whereas our paper focuses on VM re-configuration."
"Stillwell etÂ al.Â [22] in a similar setting define the resource allocation problem for static workloads, present the optimal solution for small instances and evaluate heuristics by simulations."
"Nathani etÂ al.Â [23], e.g.,Â also deal with VM placement on PMs using scheduling techniques. "
"[24] react to changing workload demands by starting new VM instances; taking into account VM startup time, they use prediction models to have VMs available already before the peak occurs."
"Other works such asÂ [25] have already considered the last escalation level (see SectionÂ 4), i.e.,Â outsourcing of applications to other Clouds. "
Paschke and BichlerÂ [26] look into a rule based approach in combination with the logical formalism ContractLog.
"Bahati and BauerÂ [28] also use policies, i.e.,Â rules, to achieve autonomic management. "
"Fourthly, compared to other SLA management projects like SLA@SOIÂ [32], the FoSII project in general is more specific on Cloud Computing aspects like deployment, monitoring of resources and their translation into high level SLAs instead of just working on high-level SLAs in general service-oriented architectures."
The idea of bandwidth sharing is a common idea in network systems as described inÂ [36]. 
" The problem stemming from escalation level 3 alone can be formulated into a binary integer problem (BIP), which is known to be NP-completeÂ [37]."
"The proof is out of scope for this paper, but a similar approach can be seen inÂ [12]. "
" Finally, the last escalation level 5 tries to outsource the application to another Cloud provider as explained, e.g.,Â in the Reservoir projectÂ [38]. "
Case Based ReasoningCase Based Reasoning is the process of solving problems based on past experienceÂ [39].
Following the principle of semantic similarity fromÂ [40] for the summation part this leads to the following equation (3)
The rules have been implemented using the Java rule engine DroolsÂ [41]. 
" Then, an up- or down-trend is randomly drawn, as well as a duration of this trend between a pre-defined number of iterations (for our evaluation this interval of iterations equals [2,6])"
"Applying and evaluating a bioinformatics workflow to the rule-based approachAs detailed inÂ [43,44], bioinformatics workflows have gained a great need for large-scale data analysis."
"Thus, Cloud computing infrastructures offer a promising way to host these sorts of applicationsÂ [10]."
The monitoring data presented in this Section was gathered with the help of the Cloud monitoring framework Lom2HisÂ [3].
"Using Lom2His we measured utilized resources of TopHatÂ [45], a typical bioinformatics workflow application analyzing RNA-Seq dataÂ [46], for a duration of about three hoursÂ [9]."
"The aligner presented here, TopHatÂ [45], consists of many sub-tasks, some of them have to be executed sequentially, whereas others can run in parallel (Fig.Â 10)."
The first sub-task aligns input reads to the given genome using the Bowtie programÂ [47].
"Normally, when setting up such a testbed as described in [9], an initial guess of possible resource consumption is done based on early monitoring data. "
"For example, in the studies on the automated segmentation from magnetic resonance images [19 21], the number of training examples is very huge (up to millions), the classes are strongly imbalanced, and generating accurate statistical solution is not trivial"
"In addition, data imbalance in huge data sets is also reported in other applicative domains, such as marketing data [22], oil spill detection or land cover changes from remote sensing images [16,27], text classification [18]and scene classification [35]"
Many classification algorithms present great limitations on large data sets and show a performance degradation due to class imbalance [14]
"Moreover, SVM classification performance can be hindered by class imbalance [1,30]"
"In fact, it is prone to generate classifier that has a strong estimation bias toward the majority class: since the number of majority class patterns exceeds that of the minority class, the class boundary becomes vulnerable to be distorted [15]"
"Nevertheless, these limitations are common to many other classification schemes such as Multi-Layer Perceptron (MLP) [7] and Logistic Regression (LR) [23]."
"Several methods to select examples in a classification problem are presented in literature, using two different approaches: the example-selection method can be embedded within the learning algorithm or the examples can be filtered before passing them to the classification scheme [2,26]"
"It is worth noting that the first type of selection methods generally work by preserving the original ratio between classes [6,11]: if there is a great skew in the data, it continues to be"
"A first approach consists of modifying SVM algorithm in order to make faster the training on large data sets; for example, Sequential Minimal Optimization (SMO) breaks the large QP problem into a series of smallest possible QP problems [25], allowing SMO to handle large training sets [25]"
"This can be considered as a good performance in terms of simple accuracy, but this is of no use since the classifier does not catch any important information on the patterns of the minority class [12]"
"Intuitively, precision is a measure of exactness (ie, of the examples predicted as positive, how many are actually labelled correctly), whereas recall is a measure of completeness (ie, how many examples of the positive class were labelled correctly) [12]"
The precision of a test is very useful to clinicians since it answers the question: How likely is it that this patient has the disease given that the test result is positive? [17]
"These metrics are simple and useful summary measures of overlapping between actual and predicted labels, which are interestingly applied to studies of reproducibility and accuracy in medical image segmentation [36]"
 Fewer values requiring interpolation also means it becomes feasible to inspect the smoothness of these values with respect to parameter variation and to apply adaptive interpolation schemes [37].
"Today, modern services require combinations of NFs, known as service chains, to satisfy their QoS requirements ( Quinn and Nadeau, 2015)"
"For instance, Amazon offers services that allow tenants to build their own virtual infrastructure by combining functions such as filtering, routing, slicing, and load balancing ( Amazon,2016)"
"In such an environment, even state of the art frameworks such as ClickOS ( Martins et al., 2014) and NetVM ( Hwang et al., 2014) cannot achieve high-performance, as there is a substantial throughput degradation when interconnecting multiple NFs"
"Recent efforts, such as E2 ( Palkar et al.,2015) and OpenNetVM ( Zhang et al.,2016), overcome this problem by eliminating hypervisor and paravirtualization overheads via lightweight NFs (eg, placed in containers) interconnected with fast, custom software switches."
"Although techniques such as single root I/O virtualization (SR-IOV) can bypass the hypervisor and pass packets from the NICs to the virtual machines (VMs) ( Amazon,2016), cloud applications still use costly system calls to interact with the NICs"
"In the first column of Table 1, we state the comparisons we made throughout this paper among ( i) standalone NFs that use different network drivers in user or kernel-space and ( ii) chained user-space NFs, interconnected either with a kernel-based Open vSwitch (OVSK) ( Open vSwitch, 2016) software switch or back-to-back (B2B)"
"Earlier efforts have successfully applied similar techniques ( Rizzo, 2012; Kim et al.,2012;DPDK,2016) to amortize the system calls overhead."
"As a future work, we aim to further improve the I/O performance of SCC by integrating the asynchronous, zero-copy I/O proposed by Drepper ( Drepper,2006) into FastClick"
