Zhou and Nakagawa 1994 have shown in the experiments of word prediction from the previous word sequence that the HMM is more powerful than the bigram model and is nearly equivalent to the trigram model though the number of parameters of the HMM is less than that in the N ram model
In fact we experienced the underflow problem in preliminary experiments with the EDR corpus 
Since more than half of the symbols in the observations may be noise models estimated in this way are not reliable
This algorithm cannot take advantage of the scaling procedure because it requires the synchronous calculation of all possible sequences in the morpheme network 
However he did not apply this algorithm to the estimation of HMM parameters  
Transformationbased tagging as introduced by Brill 1993 also requires a hand tagged text for training 
In a previous paper Schfitze 1993 we trained a neural network to disambiguate partofspeech using context however no information about the word that is to be categorized was used
Compared to learning rulebased approaches such as the one by Brill 1992 a knn approach provides a uniform approach for all disambiguation tasks more flexibility in the engineering of case representations and a more elegant approach to handling of unknown words see eg Cardie 1994 
A remedy is to aggressively limit the feature space eg to syntactic labels or a small fraction of the bilingual features available as in Chiang et al 2008 Chiang et al 2009 but that reduces the benefit of lexical features 
This method avoids the overfitting problem at the expense of losing the benefit of discriminative training of rich features directly for MT However the feature space problem still exists in these published models 
 In addition the filtering method causes some practical issues First such methods are not suitable for real MT tasks especially for applications with streamed input since the model has to be retrained with each new input sentence or document and training is slow  
However their length features only provided insignificant improvement of 01 BLEU point A crucial difference of our approach is how the length preference is modeled 
We refer to He and Gildea 2006 who tested active learning and cotraining methods but found little or no gain from semisupervised learning and to Swier and Stevenson 2004 who achieved good results using semisupervised methods but tested their methods on a small number of VerbNet roles which have not been used by other SRL Systems
Discriminative models have been found to outperform generative models for many different tasks including SRL Lim et al 2004 For this reason we also employ discriminative models here 
Compared to previous approaches Raghavan and Allan 2007 our method can be used for both classification and structured tasks and the feature query selection methods we propose perform better 
Raghavan and Allan 2007 also propose several methods for learning with labeled features but in a previous comparison GE gave better results Druck et al 2008
Note that Liang et al 2009 only use this method in synthetic experiments and instead use a method similar to total uncertainty for experiments in partofspeech tagging
They find that querying certain features outperforms querying uncertain features but this is likely because their query selection method is similar to the expectation uncertainty method described above and consequently nondiscriminative features may be queried often It is also not clear how this graphbased training method would generalize to structured output spaces 
The 746 final accuracy on apartments is higher than any result obtained by Haghighi and Klein 2006 the highest is 741 higher than the supervised HMM results reported by Grenager et al 2005 744 and matches the results of Mann and Mc Callum 2008 with GE with more accurate sampled label distributions and 10 labeled examples
The conclusions are broadly in agreement with those of Merialdo 1994 but give greater detail about the contributions of different parts of the Model
Many freely available natural language processing tools require their input to be divided into sentences but make no mention of how to accomplish this eg Brill 1994 Collins 1996 
Others perform the division implicitly without discussing performance eg Cutting et al 1992 
There is also a less detailed description of Pahner and Hearst's system SATZ in Pahuer and Hearst 1994 
Liberman and Church suggest in Liberlnan and Church 1992 that a system could be quickly built to divide newswire text into sentences with a nearly negligible error rate but do not actually build such a system 
However their parser was not incremental it used global features such as the number of turn changes 
In this area the work most closely related to ours is that of Barrett and Weld Barrett and Weld 1994 who build an incremental bottomup parser to parse plans Their parser however was not probabilistic or targeted at dialog processing 
Also the chunkbased model is representationally inadequate for centerembedded nestings of subtasks which do occur in our domain although less frequently than the more prevalent “tailrecursive” structures
We presented firstorder expectation semirings and insideoutside computation in more detail than Eisner 2002 and developed extensions to higherorder expectation semirings
We speculate that this contrasts with the disappointing findings of Kehler et al 2004 since SRL provides a more fine grained level of information when compared to predicate argument statistics 
Most empirical work in translation analyzes mod els and algorithms using BLEU Papineni et al 2002 and related metrics Though such metrics are useful as sanity checks in iterative system development they are less useful as analytial tools
In an interesting analysis of phrasebased and hierarchical translation Zollmann et al 2008 forced a phrasebased system to produce the translations generated by a hierarchical system Unfortunately their analysis is incomplete they do not perform the analysis in both directions
 However this remains first of all a timeconsuming task Moreover it is not easy for humans to selec tthe best translation among a set of alternatives let alone assign them probabilities Last but not least the beneficial effect on translation is not guaranteed
[Mani et al 1999] employed rules such as the referencing of pronouns with the most recently mentioned noun phrase However this might be inappropriate in MDS where the use of multiple documents increases the number of possible entities with which an anaphor could be referenced 
In our approach the discourse structure is not fixed but predicted for each particular abstract 
 In cutandpaste summarization Jing and mcKeown 2000 sentence combination operations were implemented manually following the study of a set of professionally written abstracts however the particular “pasting” operation presented here was not implemented 
Previous studies on texttotext abstracting Banko et al 2000 Knight and Marcu 2000 have studied problems such as sentence compression and sentence combination but not the “pasting” procedure presented here 
Recently a number of machine learning approaches have been proposed Zettlemoyer and Collins 2005 Mooney 2007 However they are supervised and providing the target logical form for each sentence is costly and difficult to do consistently and with high quality 
 For ex ample when applying their approach to a different domain with somewhat less rigid syntax Zettlemoyer and Collins 2007 need to introduce new combinators and new forms of candidate lexical Entries 
Even an efficient sampler like MCSAT Poon and Domingos 2006 as used in Poon & Domingos 2008 would have a hard time generating accurate estimates within a reasonable amount of time 
In our work we partially address this issue by enumerating some transformations frequently found in our corpusthat are computationally implementable
Sentence reduction is concerned only with the removal of sentence components so it cannot explain transformations observed in our corpus and in summarization in general such as the reexpression of domain conceptsand verbs
Jing and McKeown 2000 have proposed a rulebasedalgorithm for sentence combination but no results have been reported 
Berger et al 1996 proposed an iterative procedure of adding news features to feature set driven by data 
Although this provides an unlimited freedom for rearranging constituents it also complicates the task of learning the parsing steps which might explain why their evaluation results show marginal improvements at best 
However theseapproaches operate in a limited domain eg terrorist events where information extraction systems can be used to interpret the source text
However they did not report any evaluation of their word extraction method
In other words they count an error only when the system segmentation is not acceptable to human judgement while we count an error whenever the system segmentation does not exactly match the corpus segmentation even if it is inconsistent
In English taggers Weischedel et al 1993 proposed a statistical model to estimate word output probability pwi|tl for an unknown word from spelling information such as inflectional endings derivational endings hyphenation and capitalization Our word model can be thought of a generalization of their statistical model 
 First previous methods for computing lexical chains have either been manual Morris and Hirst 1991 or automated but with exponential efficiency Hirst and StOnge 1997 Barzilay and Elhadad 1997 Because of this computing lexical chains for documents of any reasonable size has been impossible 
Recently many phrase reordering methods have been proposed ranging from simple distance based distortion model Koehn et al 2003 Ochand Ney 2004 flat reordering model Wu 1997Zens et al 2004 lexicalized reordering model Tillmann 2004 Kumar and Byrne 2005 to hierarchical phrasebased model Chiang 2005 Setiawan et al 2007 and classifierbased reordering model with linear features Zens and Ney 2006 Xiong et al 2006 Zhang et al 2007a Xiong et al 2008 However one of the major limitations of these advances is the structured syntactic knowledge which is important to global reordering Li et al 2007 Elming 2008 hasnot been well exploited
Assuming that the number of feature templates in a given set is n the algorithm of Ding and Chang 2008 requires On 2  times of training/test routines it cannot handle a set that consists of hundreds of templates 
However in this paper we clarify a number of details that are omitted in major previous publications concerning tagging with Markov models As two examples Rabiner 1989 and Charniak et al 1993 give good overviews of the techniques and equations used for Markov models and partofspeech tagging but they are not very explicit in the details that are needed for their application 
For the Penn Treebank Ratnaparkhi 1996 reports an accuracy of 966 using the Maximum Entropy approach our much simpler and therefore faster HMM approach delivers 967 T
For example the Markov model tagger used in the comparison of van Halteren et al 1998 yielded worse results than all other taggers 
However the difficulty of such tasks and the fact that they are apparently unrelated has led to the development of largely adhoc solutions tuned to specific challenges 
While the literature suggests that BaumWelch training can degrade performance on the tagging task Elworthy 1994 Merialdo 1994 we have found in early experiments that agreement between a tagger trained in this way and the tagger from the XTag Project consistently increases with each iteration of BaumWelch eventually reaching a plateau but not Decreasing 
Clark 2000 presents a framework which in principle should accommodate lexical ambiguity using mixtures but includes no evidence that it does so 
However their use of “perfect” clusters renders some of their algorithmic suggestions problematic 
A major problem with such methods is that each hypothesis is aligned to the backbone independently leading to sub optimal behavior
While PRW is the first attempt to formalize well known relevance weighting Sparck Jones 1972 Salton and McGill 1983 by probability theory there are several drawbacks in PRW 
A similar argument applies to all other problems in Robertson and Sparck Jones 1976 that are caused by having insufficient training cases 
However this technique needs a corpus for computing IDF score causing the genredependent problem for generic text summarization task 
Each technique brings its own terminology from the cubes of Chiang 2007 to the lazy lists of Pust and Knight 2009 into the mix Often it is not entirely clear why they Work 
Unlike many other methods that directly utilize noun phrase NP coreference Nenkova 2008 Mani et al 1999 we propose a method that employs insertion and substitution of phrases that modify the same chunk in the lead and other sentences
As is well known the extractive summary that has been extensively studied from the early days of summarization history Luhn 1958 suffers from various drawbacks 
Thus we found this step unnecessary and currently did not look at this issue any further 
The empirical results show that our instantiation of SCL to parse disambiguation gives promising initial results even without the many additional extensions on the feature level as done in Blitzer et al 2006 
Although we do agree with RST that the structure of text is hierarchical in many cases it is our belief that the relevance and function of certain text pieces can be determined without analyzing the full hierarchical structure of the text 
In the news domain sentence location is the single most important feature for sentence selection Brandow Mitze and Rau 1995 in our domain location information although less dominant can still give a useful indication  
 We feel that incorporating a robust analysis of discourse structure into a document summarizer is one step along this way 
A general solution to the variable length and depth ofdependency for HMM has been already proposed Tao 1992 but has notbeen implemented in taggers
As this encoding strategy is not wellsuited to a free word order language like German we have focussed on a less surfaceoriented level of description most closely related to the LFG fstructure and representationsused in dependency grammar
In beam search incomplete parses of an utterance are pruned or discarded when on some criterion they are significantly less plausible than other competing parses This pruning is fully interleaved with the parsing process 
McCord interleaved parsing with pruning in the same way as us but only compared constituents over the same span and with the same major category Our comparisons are more global and therefore can result in more effective pruning 
A more elaborate scheme is given in Samuelsson 1994b where the chunking criteria are learned automatically by an entropyminimization method the results however do not appear to improve on the earlier ones In both cases the coverage loss due to grammar specialization was about 10 to 12 using training corpora with about 5000 examples In practice this is still unacceptably high for most Applications 
LEXAS achieves a mean accuracy of 874 on this data set which is higher than the accuracy of 78 reported in Bruce and Wiebe 1994 
Unfortunately in the data set made available in the public domain there is no indication of which sentences are used as test sentences 
Early work on WSD such as Kelly and Stone 1975 Hirst 1987 used handcoding of knowledge to perform WSD The knowledge acquisition process is laborious
The work of Miller et al 1994 Leacock et al 1993 Yarowsky 1992 used only the unordered set of surrounding words to perform WSD and they used statistical classifiers neural networks or IRbased Techniques 
 However the POS used are abbreviated POS and only in a window of b2 words No local collocation knowledge is used A probabilistic classifier is used in Bruce and Wiebe 1994 
 However his work used decision list to perform classification in which only the single best disambiguating evidence that matched a target context is used In contrast we used exemplarbased learning where the contributions of all features are summed up and taken into account in coming up with a classification We also include verbobject syntactic relation as a feature which is not used in Yarowsky 1994 
The work of Miller et al 1994 is the only prior work we know of which attempted to evaluate WSD on a large data set and using the refined sense distinction of WORDNET However their results show no improvement in fact a slight degradation in performance when using surrounding words to perform WSD as compared to the most frequent heuristic 
The work of McRoy 1992 pointed out that a diverse set of knowledge sources are important to achieve WSD but no quantitative evaluation was given on the relative importance of each knowledge source No previous work has reported any such evaluation Either
The more similar conditions reported in previous work are those experiments performed on the WSJ corpus Brill 1992 reports 34 error rate and Daelemans et al 1996 report 967 accuracy We obtained a 9739 accuracy with tri grams plus automatically acquired constraints and 9745 when hand written constraints were added  
Since we are using a larger corpus than Padó et al 2007 who train on the BNC a fairer comparison might be the one with our alternative models that are all outperformed by DM by a large margin 
Unfortunately  this is not the case for such widely used MT evaluation metrics as BLEU  and NIST   applied the parser of  developed for English  to Czech  and found thatthe performance wassubstantially lower when compared to the results for English 
Hanks and  proposed using pointwise mutual information to identify collocations in lexicography  however  the method may result in unacceptable collocations for lowcount pairs Unlike   Smadja  1993  goes beyond the  twoword  limitation and deals with  collocations of arbitrary length  
2This can explain why previous attempts to use WordNet for generating sentencelevel paraphrases  were unsuccessful We have also illustrated that ASIA outperforms three other English systems   even though many of these use more input than just a semantic class name 
For unknown words  SCL gives a relative reduction in error of 195  over   even with 40000 sentences of source domain training data For comparison purposes  we revisit a fullygenerative Bayesian model for unsupervised coreference resolution recently introduced by   discuss its potential weaknesses and consequently propose three modifications to their model  Section 3  
Unfortunately  there is no straightforward generalization of the method of  to the two edge marginal problem Previous literature on GB parsing  Wehrli  1984  Sharp  1985    1986  Kuhns  1986  Abney  1986has not addressed the issue of implementation of the Binding theory  The present paper intends in part to fill this gap 
In general  these authors have found that existing lexicalized parsing models for English  do not straightforwardly generalize to new languages  this typically manifests itself in a severe reduction in parsing performance compared to the results for English 
This method was shown to outperform the class based model proposed in  and can thus be expected to discover better clusters of words Although a rich literature covers bootstrapping methods applied to natural language problems  several questions remain unanswered when these methods are applied to syntactic or semantic pattern acquisition 
It also differs from previous proposals on lexical acquisition using statistical measures such as  which either deny the prior existence of linguistic knowledge or use linguistic knowledge in ad hoc ways In pursuit of better translation  phrasebased models  havesignificantlyimprovedthe quality over classical wordbased models  
Several studies have shown that largemargin methods can be adapted to the special complexities of the task  However  the capacity of these algorithms to improve over stateoftheart baselines is currently limited by their lack of robust dimensionality reduction 
The 746  final accuracy on apartments is higher than any result obtained by   the highest is 741    higher than the supervised HMM results reported by Grenager et al One prominent constraint of the IBM word alignment models  is functional alignment  that is each target word is mapped onto at most one source word 
Although the first three are particular cases where N  1 andor M  1  the distinction is relevant  because most wordbased translation models  eg IBM models   can typically not accommodate general MN alignments 
Insideout alignments   such as the one in Example 13  can not be induced by any of these theories  in fact  there seems to be no useful synchronous grammar formalisms available that handle insideout alignments  with the possible exceptions of synchronous treeadjoining grammars   Bertsch and Nederhof  and generalized multitext grammars   which are all way more complex than ITG  STSG and  22   BRCG 
Bilexical contextfree grammars have been presented in  as an abstraction of language models that have been adopted in several recent realworld parsers  improving stateoftheart parsing accuracy  For the Penn Treebank   reports an accuracy of 966  using the Maximum Entropy approach  our much simpler and therefore faster HMM approach delivers 967  
Although such approaches have been employed effectively   there appears to remain considerable room for improvement In terms of alignment  this wordnumber difference means that multiword connections must be considered  a task which 334 Sue J Ker and Jason S Chang Word Alignment is beyond the reach of methods proposed in recent alignment works based on  Model 1 and 2 
2 Motivation and Prior Work While several authors have looked at the supervised adaptation case  there are less  and especially less successful  studies on semisupervised domain adaptation  
In particular  the model in  failed to generate punctuation  a deficiency of the model 1 Introduction Phrasebased translation models   which go beyond the original IBM translation models  1 by modeling translations of phrases rather than individual words  have been suggested to be the stateoftheart in statistical machine translation by empirical evaluations 
Table 2  Figures about clustering algorithms Algorithm  Sentences   Clusters SHAC 623 CHAC 217 QT 232 EM 416 In fact  table 2 shows that most of the clusters have less than 6 sentences which leads to question the results presented by  who only keep the clusters that contain more than 10 sentences 
These methods go beyond the original IBM machine translation models   by allowing multiword units  phrases  in one language to be translated directly into phrases in another language 1 Introduction Recent works in statistical machine translation  SMT  shows how phrasebased modeling  significantly outperform the historical wordbased modeling  
Our system outperforms competing approaches  including the standard machine translation alignment models  and the stateoftheart Cut and Paste summary alignment technique  For example  we would like to know that if a  JJ  JJ  7We also tried using word clusters  instead of POS but found that POS was more helpful 
This is because their training data  the Penn Treebank   does not fully annotate NP structure Although this Wikipedia gazetteer is much smaller than the English version used by  that has over 2000000 entries  it is the largest gazetteer that can be freely used for Japanese NER 
As the tagger of  can not tag a word lattice  we can not back off to this tagging Pointwise mutual information  PMI  is commonly used for computing the association of two terms   which is defined as  nullnullnull null null  null null nullnullnull nullnullnullnull  nullnull nullnull null null null nullnullnullnullnull However  we argue that PMI is not a suitable measure for our purpose 
Moreover  the parameters of the model must be estimated using averaged perceptron training   which can be unstable This method has the advantage that it is not limited to the model scaling factors as the method described in  They reported that their method is superior to BLEU  in terms of the correlation between human assessment and automatic evaluation 
While the idea of exploiting multiple news reports for paraphrase acquisition is not new  previous efforts  have been restricted to at most two news sources Furthermore  we provide a 638  error reduction compared to IBM Model 4  
With all but two formats IBIIG achieves better FZ  l rates than the best published result in  A maximum entropy approach has been applied to partofspeech tagging before   but the approach s ability to incorporate nonlocal and nonHMMtaggertype evidence has not been fully explored 
If we consider these probabilities as a vector  the similarities of two English words can be obtained by computing the dot product of their corresponding vectors2 The formula is described below  similarity  ei  ej   Nsummationdisplay k  1 p  ei fk  p  ej fk   3  Paraphrasing methods based on monolingual parallel corpora such as  can also be used to compute the similarity ratio of two words  but they dont have as rich training resources as the bilingual methods do 
32 Evaluation Metrics AER  Alignment Error Rate   is the most widely used metric of alignment quality  but requires goldstandard alignments labeled with surepossible annotations to compute  lacking such annotations  we can compute alignment fmeasure instead 
Clustering algorithms have been previously shown to work fairly well for the classification of words into syntactic and semantic classes   but determining the optimum number of classes for a hierarchical cluster tree is an ongoing difficult problem  particularly without prior knowledge of the item classification 
Although  there are various manualautomatic evaluation methods for these systems  eg  BLEU   these methods are basically incapable of dealing with an MTsystem and a wpMTsystem at the same time  as they have different output forms However  reordering models in traditional phrasebased systems are not sufficient to treat such complex cases when we translate long sentences  
Current treebased models that integrate linguistics and statistics  such as GHKM   are not able to generalize well from a single phrase pair Our syntacticrelationbased thesaurus is based on the method proposed by   although Hindle did not apply it to information retrieval 
Both Charniak  and Bikel  were trained using the goldstandard tags  as this produced higher accuracy on the development set than using  s tags The utility of ITG as a reordering constraint for most language pairs  is wellknown both empirically  and analytically   howeverITGsstraight  monotone  andinverted  reverse  rules exhibit strong cohesiveness  which is inadequate to express orientations that require gaps 
This latter point is a critical difference that contrasts to the major weakness of the work of  which uses a topN list of translations to select the maximum BLEU sentence as a target for training  so called local update  
Even the creators of BLEU point out that it may not correlate particularly well with human judgment at the sentence level  1 Introduction In recent years  various phrase translation approaches  have been shown to outperform wordtoword translation models  
Sentencelevel approximations to B exist   but we found it most effective to perform B computations in the context of a setOof previouslytranslated sentences  following Watanabe et al In such a process  original phrasebased decoding  does not take advantage of any linguistic analysis  which  however  is broadly used in rulebased approaches 
However  many of these models are not applicable to parallel treebanks because they assume translation units where either the source text  the target text or both are represented as word sequences without any syntactic structure  2 Statistical Word Alignment Statistical translation models  only allow word to word and multiword to word alignments 
Even the 3 A demo of the parser can be found at httplfgdemocomputingdcuielfgparserhtml creators of BLEU point out that it may not correlate particularly well with human judgment at the sentence level  Our method  extending this line of research with the use of labeled LFG dependencies  partial matching  and nbest parses  allows us to considerably outperform  highest correlations with human judgement  they report 0144 for the correlation with human fluency judgement  0202 for the correlation with human overall judgement   although it has to be kept in mind that such comparison is only tentative  as their correlation is calculated on a different test set 
Unlike wellknown bootstrapping approaches   EM and CE have the possible advantage of maintaining posteriors over hidden labels  or structure  throughout learning  bootstrapping either chooses  for each example  a single label  or remains completely agnostic 
1 Introduction Recent approaches to statistical machine translation  SMT  piggyback on the central concepts of phrasebased SMT  and at the same time attempt to improve some of its shortcomings by incorporating syntactic knowledge in the translation process 
For the results in this paper  we have used Pointwise Mutual Information  PMI  instead of IBM Model 1   since  found it to be as effective on Springer  but faster to compute Numbers in the table correspond to the percentage of experiments in which the condition at the head of the column was true  for example figure in the first row and first column means that for 989 percent of the language pairs the BLEU score for the bidirectional decoder was better than that of the forward decoder  proach   
This provides a compelling advantage over previous dependency language models for MT   whichusea5gramLMonlyduringreranking Experimental results indicate that our model outperforms  coreference model by a large margin on the ACE data sets and compares favorably to a modified version of their model 
This method was preferred against other related methods  like the one introduced in   since it embeds all the available semantic information existing in WordNet  even edges that cross POS  thus offering a richer semantic representation Both the global models  use fairly small training sets  and there is no evidence that their techniques will scale to larger data sets 
Our model improves the baseline provided by    i  accuracy is increased by creating a lexicalised PCFG grammar and enriching conditioning context with parent fstructure features  and  ii  coverage is increased by providing lexical smoothing and fuzzy matching techniques for rule smoothing 
While the model of  significantly outperforms the constrained model of   they both are well below the stateoftheart in constituent parsing By doing so we must emphasize that  as described in the previous section  the BLEU score was not designed to deliver satisfactory results at the sentence level   and this also applies to the closely related NIST score 
 focus on alignment and do not present MT results  while May and Knight  2007  takesthesyntacticrealignmentasaninputtoanEM algorithm where the unaligned target words are insertedintothetemplatesandminimumtemplatesare combinedintobiggertemplates  
But without the global normalization  the maximumlikelihood criterion motivated by the maximum entropy principle  is no longer a feasible option as an optimization criterion   word class   measures polarity using only adjectives  however in our approach we consider the noun  the verb  the adverb and the adjective content words While most parsing methods are currently supervised or semisupervised   they depend on handannotated data which are difficult to come by and which exist only for a few languages 
To analyze our methods on IV and OOV words  we use a detailed evaluation metric than Bakeoff 2006  which includes Foov and Fiv Surprisingly  although JESSCM is a simpler version of the hybrid model in terms of model structure and parameter estimation procedure  JESSCM provides Fscores of 9445 and 8803 for CoNLL00 and 03 data  respectively  which are 015 and 083 points higher than those reported in  for the same configurations 
We presented some theoretical arguments for not limiting extraction to minimal rules  validated them on concrete examples  and presented experiments showing that contextually richer rules provide a 363 BLEU point increase over the minimal rules of  
Our study also shows that the simulatedannealing algorithm  is more effective 1552 than the perceptron algorithm  for feature weight tuning By segmenting words into morphemes  we can improve the performance of natural language systems including machine translation  and information retrieval  
Statistical disambiguation such as  for PPattachment or  for generative parsing greatly improve disambiguation  but as they model by imitation instead of by understanding  complete soundness has to remain elusive 
It can be applied to complicated models such IBM Model4  By 17 0 10 20 30 40 50 60 70 80 90 100 10000 100000 1e06 1e07 Test Set Items with Translations  Training Corpus Size num words unigrams bigrams trigrams 4grams Figure 1 Percent of unique unigrams bigrams trigrams and 4grams from the Europarl Spanish test sentences for which translations were learned in increasingly large training corpora increasing the size of the basic unit of translation phrasebased machine translation does away with many of the problems associated with the original wordbased formulation of statistical machine translation Brown et al  1993
2 21 Word Alignment Adaptation Bidirectional Word Alignment In statistical translation models   only onetoone and moretoone word alignment links can be found The method was intended as a replacement for sentencebased methods  eg     which are very sensitive to noise 
This approach addresses the problematic aspects of both pure knowledgebased generation  where incomplete knowledge is inevitable  and pure statistical bag generation   where the statistical system has no linguistic guidance  In addition  the clustering methods used  such as HMMs and Browns algorithm   seem unable to adequately capture the semantics of MNs since they are based only on the information of adjacent words 
We preferred the loglikelihood ratio to other statistical scores  such as the association ratio  or   2  since it adequately takes into account the frequency of the cooccurring words and is less sensitive to rare events and corpussize  The ubiquitous minimum error rate training  MERT  approach optimizes Viterbi predictions  but does not explicitly boost the aggregated posterior probability of desirable ngrams  
However  work in that direction has so far addressed only parse reranking  
The combination is significantly better than  at a very high level  but more importantly  Shens results  currently representing the replicable stateoftheart in POS tagging  have been significantly surpassed also by the semisupervised Morce  at the 99  confidence level  
a timeconsuming process  Other statistical machine translation systems such as  and  also produce a tree a15 given a sentence a16 Their models are based on mechanisms that generate two languages at the same time  so an English tree a15 is obtained as a subproduct of parsing a16 However  their use of the LM is not mathematically motivated  since their models do not decompose into Pa4a5a2a9a8a3a10a6 and a12a14a4a5a3a7a6 unlike the noisy channel model 
WSD systems have been far more successful in distinguishing coarsegrained senses than finegrained ones   but does that approach neglect necessary meaning differences  Secondly  while most pronoun resolution evaluations simply exclude nonreferential pronouns  recent unsupervised approaches  must deal with all pronouns in unrestricted text  and therefore need robust modules to automatically handle nonreferential instances 
12Poon and Domingos  outperformed  As with similar work   the size of the corpus makes preprocessing such as lemmatization  POS tagging or partial parsing  too costly The size of the development set used to generate 1 and 2  compensates the tendency of the unsmoothed MERT algorithm to overfit  by providing a high ratio between number of variables and number of parameters to be estimated 
 have proposed a rulebased algorithm for sentence combination  but no results have been reported  provides anecdotal evidence that only incorrect alignments are eliminated by ITG constraints 
By segmenting words into morphemes  we can improve the performance of natural language systems including machine translation  and information retrieval  It has been difficult to identify all and only those cases where a token functions as a discourse connective  and in many cases  the syntactic analysis in the Penn TreeBank  provides no help 
For example  10 million words of the American National Corpus  will have manually corrected POS tags  a tenfold increase over the Penn Treebank   currently used for training POS taggers The process of phrase extraction is difficult to optimize in a nondiscriminative setting  many heuristics have been proposed   but it is not obvious which one should be chosen for a given language pair 
While several methods have been proposed to automatically extract compounds   we know of no successful attempt to automatically make classes of compounds Many approaches for POS tagging have been developed in the past  including rulebased tagging   HMM taggers   maximumentropy models   cyclic dependency networks   memorybased learning   etc All of these approaches require either a large amount of annotated training data  for supervised tagging  or a lexicon listing all possible tags for each word  for unsupervised tagging  
Our focus is on the sentence level  unlike  and   we employ a significantly larger set of seed words  and we explore as indicators of orientation words from syntactic classes other than adjectives  nouns  verbs  and adverbs  This additional conditioning has the effect of making the choice of generation rules sensitive to the history of the generation process  and  we argue  provides a simpler  more uniform  general  intuitive and natural probabilistic generation model obviating the need for CFGgrammar transforms in the original proposal of  
1 Introduction The most widely applied training procedure for statistical machine translation IBM model 4  unsupervised training followed by postprocessing with symmetrization heuristics  yields low quality word alignments This is in contrast to purely statistical systems   which are difficult to inspect and modify 
In addition  the semisupervised Morce performs  on single CPU and development data set  77 times faster than the combination and 23 times faster than  In a recent study by   nonlocal information is encoded using an independence model  and the inference is performed by Gibbs sampling  which enables us to use a stateoftheart factored model and carry out training efficiently  but inference still incurs a considerable computational cost  suggests use of an approximation summing over the training data  which does not sum over possible tags   h E f j  2 P    p  ti l hi  f j  hi  ti  i  1 However  we believe this passage is in error  such an estimate is ineffective in the iterative scaling algorithm 
IBM Model1  is a simplistic model which takes no account of the subtler aspects of language translation including the way word order tends to differ across languages A number of studies have investigated sentiment classification at document level  eg    and at sentence level  eg    however  the accuracy is still less than desirable 
Automatic evaluation methods such as BLEU   RED   or the weighted Ngram model proposed here may be more consistent in judging quality as compared to human evaluators  but human judgments remain the only criteria for metaevaluating the automatic methods For comparison purposes  we revisit  fullygenerative Bayesian model for unsupervised coreference resolution  discuss its potential weaknesses and consequently propose three modifications to their model 
The classbased kappa statistic of  can not be applied here  as the classes vary depending on the number of ambiguities per entry in the lexicon While the amount of parallel data required to build such systems is orders of magnitude smaller than corresponding phrase based statistical systems   the variety of linguistic annotation required is greater 
Although this method is comparatively easy to be implemented  it just achieves the same performance as the synchronous binarization method  for syntaxbased SMT systems Among the applications of collocational analysis for lexical acquisition are the derivation of syntactic disambiguation cues Basili et al 1991 1993a Hindle and Rooths 19911993 Sekine 1992 Bogges et al 1992 sense preference Yarowski 1992 acquisition of selectional restrictions Basili et al 1992b 1993b Utsuro et al 1993 lexical preference in generation Smadjia 1991 word clustering Pereira 1993 Hindle 1990 Basili et al 1993c etc In the majority of these papers even though the precedent or subsequent statistical processing reduces the number of accidental associations very large corpora 10000000 words are necessary to obtain reliable data on a large enough number of words
Due to limited variations in the NBest list  the nature of ranking  and more importantly  the nondifferentiable objective functions used for MT  such as BLEU    one often found only local optimal solutions to  with no clue to walk out of the riddles We also compare our performance against  and  and find that we manage higher relative improvement than existing work despite starting from a very competitive baseline CRF Although several methods have already been proposed to incorporate nonlocal features   these present a problem that the types of nonlocal features are somewhat constrained 
The morphological processing in PairClass  is more sophisticated than in  In addition  the performance of the adapted model for Joint ST obviously surpass that of   which achieves an F1 of 9341  for Joint ST  although with more complicated models and features Some are the result of inconsistency in labeling in the training data   which usually reflects a lack of linguistic clarity or determination of the correct part of speech in context 
Therefore  sublanguage techniques such as Sager  and  do not work Mutual information  though potentially of interest as a measure of collocational status  was not tested due to its wellknown property of overemphasising the significance of rare events  
While in traditional wordbased statistical models  the atomic unit that translation operates on is the word  phrasebased methods acknowledge the significant role played in language by multiword expressions  thus incorporating in a statistical framework the insight behind ExampleBased Machine Translation  
Note that the minimum error rate training  uses only the target sentence with the maximum posterior probability whereas  here  the whole probability distribution is taken into account A word order correlation bias  as well as the phrase structure biases in  Models 4 and 5  would be less beneficial with noisier training bitexts or for language pairs with less similar word order 
Although various approaches to SMT system combination have been explored  including enhanced combination model structure   better word alignment between translations  and improved confusion network construction   most previous work simply used the ensemble of SMT systems based on different models and paradigms at hand and did not tackle the issue of how to obtain the ensemble in a principled way 
1 Introduction The field of machine translation has seen many advances in recent years  most notably the shift from wordbased  to phrasebased models which use token ngrams as translation units  Although ITA rates and system performance both significantly improve with coarsegrained senses   the question about what level of granularity is needed remains three models in  are susceptible to the O  n 3  method  cf 
Even with the current incomplete set of semantic templates  the hypertagger brings realizer performance roughly up to stateoftheart levels  as our overall test set BLEU score  slightly exceeds that of   though at a coverage of 96  insteadof98  An alternative method  makes decisions at the end but has a high computational requirement 
Several teams had approaches that relied to varying degrees on an IBM model of statistical machine translation Brown et al  1993 with different improvements brought by different teams consisting of new submodels improvements in the HMM model model combination for optimal alignment etc Several teams used symmetrization metrics as introduced in Och and Ney 2003 union intersection refined most of the times applied on the alignments produced for the two directions sourcetarget and targetsource but also as a way to combine different word alignment systems
In the thriving area of research on automatic analysis and processing of product reviews   little attention has been paid to the important task studied here assessing review helpfulness In such tasks  feature calculation is also very expensive in terms of time required  huge sets of extracted rules must be sorted in two directions for relative frequency calculation of such features as the translation probability p  f e  and reverse translation probability p  e f   
Other statistical systems that address word classification probleans do not emphasize the use of linguistic knowledge and do not deal with a specific word class   or do not exploit as much linguistic knowledge as we do  As one can see in Table 4  the resulting parser ranks among the best lexicalized parsers  beating those of Collins  and Charniak and Johnson  8 Its F1 performance is a 27  reduction in error over  et al 
Our experiments on the Canadian Hansards show that our unsupervised technique is significantly more effective than picking seeds by hand   which in turn is known to rival supervised methods 2 Related Work One of the major problems with the IBM models  and the HMM models  is that they are restricted to the alignment of each sourcelanguage word to at most one targetlanguage word This implies that the complexity of structure divergence between two languages is higher than suggested in literature   report better perplexity results on the Verbmobil Corpus with their HMMbased alignment model in comparison to Model 2 of  
 have implemented a dependency parser with good accuracy  it is almost as good at dependency parsing as Charniak   and very impressive speed  it is about ten times faster than  and four times faster than Charniak   Unlike minimum error rate training   our system is able to exploit large numbers of specific features in the same manner as static reranking systems  
However  to what extent that assumption holds is tested only on a small number of language pairs using hand aligned data  Although evaluated on a different test set  our method also outperforms the correlation with human scores reported in  However  it seems unrealistic to expect a onesizefitsall approach to be achieve uniformly high performance across varied languages  and  in fact  it doesnt Though the system presented in  outperforms the best systems in the 2006 PASCAL challenge for Turkish and Finnish  it still does significantly worse on these languages than English  Fscores of 662 and 665  compared to 794  
For comparison purposes  three additional heuristicallyinduced alignments are generated for each system   1  Intersection of both directions  Aligner  int     2  Union of both directions  Aligner  union    and  3  The previously bestknown heuristic combination approach called growdiagfinal   Aligner  gdf   Brill s results demonstrate that this approach can outperform the Hidden Markov Model approaches that are frequently used for partofspeech tagging   as well as showing promise for other applications 
It has the advantage of naturally capturing local reorderings and is shown to outperform wordbased machine translation  Again the best result was obtained with IOB1  which is an imI  rovement of the best reported F    1 rate for this data set    9203  This is well illustrated by the Collins parser   scrutinized by Bikel  2004   where several transformations are applied in order to improve the analysis of noun phrases  coordination and punctuation 
It is faster and more mnemonic than the one in  1 Introduction Translations tables in Phrasebased Statistical Machine Translation  SMT  are often built on the basis of Maximumlikelihood Estimation  MLE   being one of the major limitations of this approach that the source sentence context in which phrases occur is completely ignored  
In what concerns the evaluation process  although ROUGE  is the most common evaluation metric for the automatic evaluation of summarization  since our approach might introduce in the summary information that it is not present in the original input source  we found that a human evaluation was more adequate to assess the relevance of that additional information 
The program takes the output of char_align   a robust alternative to sentencebased alignment programs  and applies wordlevel constraints using a version of Brown el al s Model 2   modified and extended to deal with robustness issues6 Conclusion Traditional approaches for devising parsing models  smoothing techniques and evaluation metrics are not well suited for MH  as they presuppose 13The lack of head marking  for instance  precludes the use of lexicalized models a la  
The experimental results show that our method outperforms the synchronous binarization method  with over 08 BLEU scores on both NIST 2005 and NIST 2008 ChinesetoEnglish evaluation data sets  produced a corpus of 4000 questions annotated with syntactic trees  and obtained an improvement in parsing accuracy for Bikels reimplementation of the Collins parser  by training a new parser model with a combination of newspaper and question data 
1 Introduction Statistical phrasebased systems  have consistently delivered stateoftheart performance in recent machine translation evaluations  yet these systems remain weak at handling word order changes With these linguistic annotations  we expect the LABTG to address two traditional issues of standard phrasebased SMT  in a more effective manner 
At the same time  we believe our method has advantages over the approach developed initially at IBM  for training translation systems automatically Despite relying on a the same concept  our approach outperforms BE in most comparisons  and it often achieves higher correlations with human judgments than the stringmatching metric ROUGE  
More recent work  has considered methods for speeding up the feature selection methods described in   Ratnaparkhi  1998   and Della Pietra  Della Pietra  and Lafferty  1 Introduction Currently  most of the phrasebased statistical machine translation  PBSMT  models  adopt full matching strategy for phrase translation  which means that a phrase pair  tildewidef  tildewidee  can be used for translating a source phrase f  only if tildewidef  f Due to lack of generalization ability  the full matching strategy has some limitations 
1 Introduction For statistical machine translation  SMT   phrasebased methods  and syntaxbased methods  outperform wordbased methods  To our knowledge no systems directly address Problem 1  instead choosing to ignore the problem by using one or a small handful of reference derivations in an nbest list   or else making local independence assumptions which sidestep the issue  
For example  the statistical word alignment in IBM translation models  can only handle word to word and multiword to word alignments Our graphical representation has two advantages over previous work   unifying sentence relations and incorporating question interactions 
 presented a historybased generation model to overcome some of the inappropriate independence assumptions in the basic generation model of  
Unfortunately  longer sentences  up to 100 tokens  rather than 40   longer phrases  up to 10 tokens  rather than 7   two LMs  rather than just one   higherorder LMs  order 7  rather than 3   multiple higherorder lexicalized reordering models  up to 3   etc all contributed to increased system  s complexity  and  as a result  time limitations prevented us from performing minimumerrorrate training  MERT   for ucb3  ucb4 and ucb5 
Unsupervised methods have been developed for WSD  but despite modest success have not always been well understood statistically  In addition  uniform conditioning on mother grammatical function is more general than the casephenomena specific generation grammar transform of   in that it applies to each and every subpart of a recursive input fstructure driving generation  making available relevant generation history  context  to guide local generation decisions 
Section 5 presents an error analysis for  lexicalized model  which shows that the headhead dependencies used in this model fail to cope well with the flat structures in Negra Even the creators of BLEU point out that it may not correlate particularly well with human judgment at the sentence level   a problem also noted by  and  
Methods like McDonalds  including the wellknown Maximal Marginal Relevance  MMR  algorithm   are subject to another problem  Summarylevel redundancy is not always well modeled by pairwise sentencelevel redundancy 
While other systems  such as   have addressed these tasks to some degree  OPINE is the first to report results Thirdly   deploys the dependency language model to augment the lexical language model probability be1183 tween two head words but never seek a full dependency graph 
METEOR was chosen since  unlike the more commonly used BLEU metric   it provides reasonably reliable scores for individual sentences Our method is a natural extension of those proposed in  and   and overcomes their drawbacks while retaining their advantages 
Another consequence of not generating posthead conjunctions and punctuation as firstclass words is that they 19 In fact  if punctuation occurs before the head  it is not generated at alla deficiency in the parsing model that appears to be a holdover from the deficient punctuation handling in the model of  
One conclusion that we can draw is that at present the additional word features used in  looking at words more than one position away from the current do not appear to be helping the overall performance of the models In comparison  the 2D model in Figure 2  c  used in previous work  can only model the interaction between adjacent questions 
The problem is typically presented in logspace  which simplifies computations  but otherwise does not change the problem due to the monotonicity of the log function  hm  log hprimem  log p  t s   summationdisplay m m hm  t  s   3  Phrasebased models  are limited to the mapping of small contiguous chunks of text Though taggers based on dependency networks   SVM   MaxEnt   CRF   and other methods may reach slightly better results  their traintest cycle is orders of magnitude longer 
Several papers have looked at higherorder representations  but have not examined the equivalence of synpara distributions when formalized as Markov chains  Of the methods we compare against  only the WordNetbased similarity measures    and  provide a method for predicting verb similarities  our learned measure widely outperforms these methods  achieving a 136  Fscore improvement over the LESK similarity measure 
In order to capture the dependency relationship between lexcial heads  breaks down the rules from head outwards  which prevents us from factorizing them in other ways Besides  our model  as being linguistically motivated  is also more expressive than the formally syntaxbased models of Chiang  and  
String alignment with synchronous grammars is quite expensive even for simple synchronous formalisms like ITG  but Duchi et al 1 Introduction Phrasebased systems  flat and hierarchical alike   have achieved a much better translation coverage than wordbased ones   but untranslated words remain a major problem in SMT 
Such a quasisyntactic structure can naturally capture the reordering of phrases that is not directly modeled by a conventional phrasebased approach  
Although generating training examples in advance without a working parser  is much faster than using inference   our training time can probably be decreased further by choosing a parsing strategy with a lower branching factor 
This cost can often be substantial  as with the Penn Treebank  2 Previous work on Sentiment Analysis Some prior studies on sentiment analysis focused on the documentlevel classification of sentiment  where a document is assumed to have only a single sentiment  thus these studies are not applicable to our goal 
Since Czech is a language with relatively high degree of wordorder freedom  and its sentences contain certain syntactic phenomena  such as discontinuous constituents  nonprojective constructions   which can not be straightforwardly handled using the annotation scheme of Penn Treebank   based on phrasestructure trees  we decided to adopt for the PCEDT the dependencybased annotation scheme of the Prague Dependency Treebank PDT  13  give an informal example  but do not elaborate on it 
Like WASP1  the phrase extraction algorithm of PHARAOH is based on the output of a word alignment model such as GIZA     which performs poorly when applied directly to MRLs  Section 32  
This restriction is necessary because the problem of optimizing manytomany alignments 5 Our preliminary experiments with ngrambased overlap measures  such as BLEU  and ROUGE   show that these metrics do not correlate with human judgments on the fusion task  when tested against two reference outputs 
Although the authors of  stated that they would discuss the search problem in a followup arti cle  so far there have no publications devoted to the decoding issue for statistical machine translation This strategy is commonly used in MT evaluation  because of BLEUs wellknown problems with documents of small size  
Our system improves over the latent namedentity tagging in   from 61  to 87  The generalized perceptron proposed by  is closely related to CRFs  but the best CRF training methods seem to have a slight edge over the generalized perceptron 2 Previous Work It is helpful to compare this approach with recent efforts in statistical MT Phrasebased models  are good at learning local translations that are pairs of  consecutive  substrings  but often insufficient in modeling the reorderings of phrases themselves  especially between language pairs with very different wordorder 
When compared to other kernel methods  our approach performs better than those based on the Tree kernel   and is only 02  worse than the best results achieved by a kernel method for parsing  Lexical relationships under the standard IBM models  do not account for manytomany mappings  and phrase extraction relies heavily on the accuracy of the IBM wordtoword alignment 
Despite ME theory and its related training algorithm  do not set restrictions on the range of feature functions1  popular NLP text books  and research papers  seem to limit them to binary features 4 Conclusions Compared with other word alignment algorithms   word_align does not require sentence alignment as input  and was shown to produce useful alignments for small and noisy corpora 
1 Introduction Statistical phrasebased systems  have consistently delivered state of the art performance in recent machine translation evaluations  yet these systems remain weak at handling word order changes 
Our approach not only outperformed a notoriously difficult baseline but also achieved similar performance to the approach of   without requiring their thirdparty data resources While we have shown an increase in performance over a purely syntactic baseline model  the algorithm of    there are a number of avenues to pursue in extending this work 
Presently  there exist methods for learning oppositional terms  and paraphrase learning has been thoroughly studied  but successfully extending these techniques to learn incompatible phrases poses difficulties because of the data distribution Formal complexity analysis has not been carried out  but my algorithm is simpler  at least conceptually  than the variablewordorder parsers of Johnson     and Abramson and Dahl  1989  
These scores are higher than those of several other parsers   but remain behind tim scores of Charniak  2000  who obtains 901  LP and 901  LR for sentences _  40 words In contrast to existing approaches   the context of the whole corpus rather than a single sentence is considered in this iterative  unsupervised procedure  yielding a more reliable alignment 
While minimum error training  has by now become a standard tool for interpolating a small number of aggregate scores  it is not well suited for learning in highdimensional feature spaces 
 and Collins and Duffy  2002  rerank the top N parses from an existing generative parser  but this kind of approach 1Dynamic programming methods  can sometimes be used for both training and decoding  but this requires fairly strong restrictions on the features in the model Ever since its introduction in general  and in computational linguistics   many researchers have pointed out that there are quite some problems in using  eg 
1 Introduction The dominance of traditional phrasebased statistical machine translation  PBSMT  models  has recently been challenged by the development and improvement of a number of new models that explicity take into account the syntax of the sentences being translated 1 Introduction Hierarchical approaches to machine translation have proven increasingly successful in recent years   and often outperform phrasebased systems  on targetlanguage fluency and adequacy 
While we do not have a direct comparison  we note that  performs worse on movie reviews than on his other datasets  the same type of data as the polarity dataset 
At any rate  regularized conditional loglinear models have not previously been applied to the problem of producing a high quality partofspeech tagger  Ratnaparkhi   Toutanova and Manning   and  all present unregularized models 
The most commonly used metric  BLEU  correlates well over large test sets with human judgments   but does not perform as well on sentencelevel evaluation  
We will show that some achieve significantly better results than the standard minimum error rate training of  Allomorphs  eg  deni and deny  are also automatically identified in   but the general problem of recognizing highly irregular forms is examined more extensively in  
Most recently   published their Semisupervised sequential labeling method  whose results on POS tagging seem to be optically better than   but no significance tests were given and the tool is not available for download  ie for repeating the results and significance testing 
By increasing the size of the basic unit of translation  phrasebased machine translation does away with many of the problems associated with the original wordbased formulation of statistical machine translation   in particular  The Brown et al  examine the FS of the weighted loglikelihood ratio  WLLR  on the movie review dataset and achieves an accuracy of 871   which is higher than the result reported by  with the same dataset 
While both  and  propose models which use the parameters of the generative model but train to optimize a discriminative criteria  neither proposes training algorithms which are computationally tractable enough to be used for broad coverage parsing Turneys method did not work well although they reported 80  accuracy in  
 tried a different generative phrase translation model analogous to IBM wordtranslation Model 3   and again found that the standard model outperformed their generative model The automatically generated patterns in PairClass are slightly more general than the patterns of  
Although previous work  has tackled the bootstrapping approach from both the theoretical and practical point of view  many key problems still remain unresolved  such as the selection of initial seed set Unlike   who found optimal performance when was approximately 104  we observed monotonic increases in performance as dropped 
While these approaches have had som e success to date   their usability as parsers in systems for natural language understanding is suspect 
2 Motivation and Prior Work While several authors have looked at the supervised adaptation case  there are less  and especially less successful  studies on semisupervised domain adaptation 
Information on WOM content ie message has generally been unavailable to companies in the past because interpersonal communication such as a chat between friends leaves no record for analysis 20 
The network structure approach also requires knowledge on consumers social networks that are often private information 543 
The selfreport method seems to be most popular due to existing scales such as King and Summers 50 although the key informant method has also been used in a recent study 59 
While individuals can arguably expand their social network to include the strangers Dunbars number 150 suggests a cognitive limitation in the number of social relationships that people can maintain 30 
Consumers do not know all opinion leaders as consumers only know a limited number of peers 30 and companies cannot directly compare different opinion leaders reported in either the selfreport or key informant approaches 
Our paper fills the gap by studying opinion leader and eWOM together as the original interpersonal communication theory intends 47
By using this new approach we identify communicative buzzgenerating and trustworthy opinion leaders and find their eWOM positively associated with product sales contrary to a prior study that has raised doubts about the influence of opinion leaders 74 
Our method is both more accurate than traditional survey methods 3559 in measuring opinion leadership and more comprehensive than network structure methods 4143 
 However unlike the study carried out by Di Marco et al authors in  22 did not report results referred to the computational burden required by the analytical model resolution especially in the case in which the number of nodes is big 
Furthermore the procedure proposed by Samaras et al in  22 is restricted to multitier singlehop hierarchical networks ie clustertree topology while the work of Di Marco et al  21 can be extended to other topologies as the mesh one
 In addition the Request to Send Clear to Send RTS CTS mechanism commonly used in IEEE 80211based networks  27 is also employed right before exchanging data acknowledgment ack messages 
Throughput per linkTo calculate the throughput under HT conditions we take inspiration from the work of Cano et al  23 where the HT problem in a WSN is undertaken 
The idea behind distinguishing GP from other more conventional methods of optimization ie singleobjective sequentialobjectives or other MO techniques  4245 is to introduce flexibility into the objective functions as opposed to the rigid constraints of the conventional techniques 
However we excluded cases where an initially small agile organization grew organically Maranzato et al 2012 and discussions focusing on processes or tools without describing organizational change Lyon and Evans 2008
To complicate matters some papers talked about the team in singular when referring to the organization Hodgkins and Hohmann 2007 making it nontrivial to judge whether the organization met the large scale criteria based on the choice of words of the author
Further examples on exclusion by the large scale facet were cases with large organizations but only a single team adopting agile Fulgham et al 2011 
Also piloting cases that reported only single teams using agile even though considering the whole organization would meet the large scale criteria were excluded eg Scott et al 2008 
Instead of using the most complete paper as suggested in the guidelines for systematic literature reviews Kitchenham 2007 we combined the results presented in each paper and considered the case as a single unit in our analysis
 The studies and surveys that do exist have studied agile in general not specifically as largescale nor agile transformations eg Chow and Cao 2008 studied success factors in agile software projects in general 
This selfconsistent estimate converges on the true distribution at a faster rate than traditional binning or kernel density estimation methods Bernacchia and Pigolotti 2011 
Though this manuscript focuses specifically on the case of using the nonuniform FFT to improve the ECF calculation stage of the Bernacchia and Pigolotti 2011 estimation method this method should be applicable to other ECFbased methods 
To address this issue a more concise view of model differences is required that aggregates the atomic operations into composite operation applications such that the intent of the change is becoming explicit Existing solutions Hartung et al 2010 Kster et al 2008 Xing and Stroulia 2006 only provide languagespecific operation detection algorithms
First there is the approach by Xing and Stroulia 2006 for detecting refactorings in evolving software models which is integrated in UMLDiff
Second the approach by Vermolen et al 2011 copes with the detection of complex evolution steps between different versions of a metamodel 
Buckley et al 2005 introduced four aspects of software changes as the basis for software evolution: temporal properties when object of change where does the change occur system properties what and change support how
 The restructuring of a production unit versus a continuous improvement process of these units is another topic in this field Schuh et al 2013
Reasons named by Bellgran and Säfsten 2010 are mainly high timepressure and low priority despite of the assumption that a well performed requirements engineering helps reducing time for fixing design and implementation errors 
 This often includes that no assessment of the quality of changes is done because of lack of time or cost constraints or missing measurements Bellgran and Säfsten 2010 
While engineering approaches exist in the different disciplines to address these problems eg architectural description languages in software engineering Medvidovic and Taylor 2000 the problems become more difficult to identify and solve if a change in one disciplines parts results in problems in another discipline
A formal semantics for automatic verification of structural compatibility has been proposed Feldmann et al 2014a but verifying functional conformance is not considered yet
With respect to the repair of inconsistencies approaches have been proposed to use OCL constraints and the part of the constraint which has not been satisfied to create an appropriate repair action Nentwich et al 2003
Tribastone and Gilmore 2008 and Becker et al 2009 propose similar approaches addressing performance
Despite efforts toward including objectoriented programming aspects within IEC 611313 IEC 2013 the standard in its current version has not yet been fully established in the industry
This may lead to inconsistencies between implementation and design artifacts as well as to unclear code structure Katzke et al 2004 VogelHeuser et al 2014a
 In Kormann et al 2012 the semantics of sequence diagrams are adapted in order to make direct IEC 611313 code generation possible  
Static code analysis is successfully applied for several programming languages and environments eg Lint for C Johnson 1988 and FindBugs for Java Ayewah et al 2008 
Finally the MechatronicUML provides specifically support for systems which selfadapt their behavior at runtime by modeling adaptations by architectural reconfiguration based on graph transformations Eckardt et al 2013 Tichy et al 2008 
Verification of PLC programs  written mainly in the programming languages Sequential Function Charts Bauer et al 2004 and Instruction List Huuck 2005  was investigated by means of the model checker UPAAL but lack in analyzing industrial PLC programs due to size and structure 
Arcade supporting model checking to of PLC programs  written for the programming languages ST IL as well as vendorspecific programming language and their combinations as often applied in IEC 61131 environments  is presented in Biallas et al 2012 
Greifeneder and Frey 2007 proposed the modeling language called DesLaNAS which can be applied in combination with a probabilistic model checking but an automatic translation of the description model into the form needed for model checking is not available 
 Variant annotations eg using stereotypes in UML models Gomaa 2006 or presence conditions Czarnecki and Antkiewicz 2005 define which parts of the model have to be removed to derive a concrete product variant 
Thüm et al 2009 present an algorithm computing the differences between two feature models after changes to a feature model In general the inputs are the original and the evolved feature models
Tool support is fully available and is already tested with an Eclipse as a large product line example Pleuss et al 2012 But again multidisciplinary feature modeling is not considered
 Wille et al 2013 apply this idea to blockbased diagrams eg as available in Matlab Simulink Holthusen et al 2014apply it in a prototypical manner to the IEC 611313 language FBD
This cycle of generating code from models and propagating changes on the source code back to the models is known as roundtrip engineering Hettel et al 2008 in contrast to the oneway forward engineering of source code from models and the oneway reverse engineering from models from source code
Sim and VogelHeuser 2010 proved the benefit of active learning comparing mechatronic engineering students with students of computer science but still an appropriate education for MDE with a focus on aPS is missing
The modeling approaches used in these and other works Bassi et al 2011 Hackenberg et al 2014 Bonfè et al 2013 enable an integration of software models and physical models into a single consistent syntax
Aside from works addressing IEC 611313 implementations eventdriven implementations conforming to the IEC 61499 standard Bianchi et al 2003 Chhabra and Emami 2011 Hirsch 2010 Hirsch et al 2008 Vyatkin et al 2009 were proposed
Another approach Angyal et al 2008 proposes to use the Abstract Syntax Tree of the generated source code as an intermediate model which represents the source code as a model and uses bidirectional incremental model merges to keep the abstract syntax tree consistent with the changed code
Several recent efforts to invert large sparse matrices using a series of computational tricks show promise though they are still extremely computationally expensive 412
it would lead to additional costs and setup burden as it would require a Spark cluster 29
Historians of science are accustomed to call these two traditions in science Cartesian and Baconian since Descartes was the great unifier and Bacon the great diversifier at the birth of modern science in the seventeenth century1 p 40
 In an adult brain white and grey matter despite of their anisotropic WM and isotropic GM properties are characterized by similar ADC values 2627 and cannot be efficiently distinguished
In this context various solutions have been proposed to identify possible malicious apps and behaviours Aafer et al 2013 Arp et al 2014 Google 2012 Wu et al 2012
In contrast to the refinement rules proposed in Neisse and Doerr 2013 our extension considers also the modification of events in addition to only allowing or denying the execution of activities
Approaches like Damopoulos et al 2014 and Grace et al 2012 that focus specifically on malware detection is out of the scope of this paper 
Kirin proposed by Enck et al 2009 is a security service running on the phone which analyses the requested permissions of an app and detects potential security flaws 
Ongtang et al 2009 propose Saint as an extension of Kirin 
Orthogonally to our proposal Dietz et al 2011 propose QUIRE as a solution to protect android apps manipulation by other malicious apps or services
Bai et al 2010 propose a contextaware usage control that focus on a user basis mechanism for granting and revoking permissions similar to the approach introduced in the latest android OS version 
Feth and Pretschner 2012 employ information flow tracking as well 
Constroid introduced by Schreckling et al 2012 also defines a management framework for employing datacentric security policies of fine granularity 
Zefferer and Teufl 2013 propose a solution for device security assessment based on user defined preferences 
AppGuard introduced by Backes et al 2013 is an app instrumentation framework that runs directly in users device and allows usercentric security policies customisations 
DroidForce proposed by Rasthofer et al 2014 relies on the Soot framework for analysing and instrumenting an app to enforce a security policy 
More similar to our approach Cotterell et al 2015 introduce a solution to enable users to install policies for controlling sensitive data however the policies are not based on access to sensitive resources 
Empirical studies showed that by integrating the advantages of different EAs into one framework PAP not only provides practitioners a unified approach for solving his her problem set but also may lead to better performance than a single EA 17
1 In addition some benchmarking studies have been undertaken to empirically compare the performance of these various techniques eg Baesens et al 2003 but they did not focus specifically on how these techniques compare on heavily imbalanced samples or to what extent any such comparison is affected by the issue of class imbalance 
Our proposed method has some particularities that distinguish it from previous textural analyses in the literature First unlike the method in 11 that focuses on particular affine regions our method considers all pixels to have the same importance 
In all cases the connectivity descriptors outperformed all the other approaches by at least 5  UIUC 2  Outex and 13  KTHTIPS2b in relative percentages In KTHTIPS2b the classification performance was also better than that reported in 16 76  using a similar database and protocol
 It has been reported that these PSO variants have a more diverse search than does the standard PSO 131017
Most models derive a chaotic system by transforming the original constraint problem into an unconstrained one with a diffeomorphic function such as a sigmoid function and then applying the steepest descent method with a large stepsize to the unconstrained problem for which it is well known that the derived system is chaotic if the stepsize is sufficiently large 6
As a countermeasure a large inertia weight wd ofC1 was selected in 17 this can reduce the amount of change of the critical value β that is caused by the variation in r
Since in 17 it was reported that the diversity of the search was more significant during the early stages in the numerical experiments for CPSOVQO in this model the initial value of σt is set to be large and is decreased exponentially
In the problem domain of cyber security Fink et al 2009 developed a set of visualization design principles with a focus on highresolution displays and presented prototypes according to these design principles 
On the other hand there are systems for Malware Classification which support the comparison of many samples to identify the common behavior eg Gove et al 2014 Han et al 2014 Long et al 2014 
 For instance the linearised human NC 001807 and chimpanzee NC 001643 mito chondrial DNA mtDNA sequences do not start in the same region 13 
The first one modifies the branch and bound algorithm of Barrachina and Marzal 2 by avoiding exploring ranges known to be lower than the lower bound in the branch and bound computation 
Typical approaches are selftraining eg Rosenberg et al 2005 Li et al 2007 cotraining eg Blum and Mitchell 1998 Levin et al 2003 semisupervised learning eg Goldberg et al 2008 or the application of oracles1 eg Nair and Clark 2004 Wu and Nevatia 2005
Having a large stack assures that the assumption for the negative bag containing at least one negative sample is mostly valid since the probability that an object stays at one specific location over a longer period of time is very low Sternig et al 2010a
This causes shortterm drifting in existing classifier grid approaches eg Roth et al 2009 which is in particular the problem addressed within this paper 
Creating temporary solutions to the code base increases complexity which makes further development hard and timeconsuming YliHuumo et al 2015a YliHuumo et al 2014
The metaphor technical debt TD has been introduced by Ward Cunningham Cunningham 1992 He describes the metaphor as ‘Shipping first time code is like going into debt 
TD is often seen only as a negative concept in software development Lim et al 2012YliHuumo et al 2014 Software developers think that creating shortcuts and nonscalable solutions will increase the complexity within the code base YliHuumo et al 2014
Managing technical debt MTD workshops have gathered multiple studies related to TD and TDM in the past years Seaman et al 2015 
The reduction and repayment of TD are done by refactoring or rewriting the bad solutions Codabux and Williams 2013 
Even though the current literature has started to tackle and identify the concept and solutions of TDM the problem is that there is a need for more empirical evidence from reallife software development Li et al 2015a
TD was an important discussion topic in most of the development teams This is not a surprise considering the popularity of TD research in the past few years Li et al 2015a 
Communication related to TD issues does not often transfer from the development team to the business stakeholders which leads to TD issues not receiving the required time to get fixed YliHuumo et al 2014
Without proper tracking and documentation of architectural changes and issues it is also extremely challenging to quantify TD Klinger et al 2011 
The challenge is how the tools tackle architectural or structural issues and technology gaps Kruchten et al 2012a
Ramasubbu et al 2015 describe TD prioritization with three dimensions: customer satisfaction needs reliability demanded by the business and probability of technology disruption
A case study does not provide statistical generalizability Yin 2003 ie a case study with a limited number of cases cannot be generalized over a population We however consider generalization as theoretical Lee and Baskerville 2003 ie abstraction from concrete events and actions to theoretical constructs 
 Therefore the consideration of fuzziness is superior to that of Chen 7 Chen did not consider the estimated reliability of the system in the fuzzy sense
The cumulative distribution of Eq 11 is shown as the black line in Fig 12 It can be concluded that the zero parameter model by Angelidou et al 25 predicts a reasonable but not perfect distribution of enstrophy
The proposed method was compared with another approach proposed by Jiang et al 2007 who also exploit the textural features which is followed by the spatial analysis Not only do the results indicate that our method outperforms the alternative algorithms but also the gain attributed to the spatial analysis is larger than in case of processing raw skin probability maps
The algorithm was proved to be very competitive and outperformed our energybased method and the method proposed by del Solar and Verschae 2004  
The proposed algorithm was compared with the Gabor waveletbased texture analysis method proposed by Jiang et al 2007 
 However this normally relies on labour intensive manual analysis Creese et al 2012 which is impractical and poses a high cost to a potential attacker 
Alternatively such techniques utilise automated conversational agents Huber et al 2009 which do not scale and are not very effective due to the challenges of imitating human conversational behaviour 
Ball et al 2012 detail how open source information can be used to construct spearphishing attacks on an organisations employees 
Scheelen et al 2012 attempt to map out a companys structure from online sources including gathering information for social engineering In their method they first connect to the company on LinkedIn and then crawl LinkedIn for a list of employees then search Facebook for those employees matching on name profile picture and location 
Clearly the sheer amount of data created by mobile devices the Internet of Things IoT  1 and a myriad of other sources cannot be handled by traditional data processing approaches  2
More recently the usage of simulators in the Cloud Computing field has also become widespread which motivated the development of a number of simulators such as CloudSim   12 GreenCloud   13 and iCanCloud   14 
Existing approaches  3914 mainly consist of analysing at design time the contextual changes and the generation of the reconfiguration plans to fit the new environmental conditions 
Other existing approaches that generate the configurations at  runtime 1520 also have limitations in mobile environments as usually most of them demand high computing resources
Those DSPL approaches that perform the analysis and derivation of reconfiguration plans at design time are usually based on the definition of a set of event condition action ECA rules  2920
However this is an NPhard problem  26 and therefore it is impossible to use exact techniques to solve this optimization problem for our purpose
Shen et al  29 propose a dynamic reconfiguration approach based on dynamic aspect weaving where the set of valid configurations is also generated at design time and the reconfiguration process is triggered by ECA rules
Brataas et al  35 propose a mechanism for extending MUSIC with support for specifying the requirements and the utility of the components of a software architecture
Cheng et al  17 propose a predictive instead of reactive adaptation approach trying to foresee changes in the availability of resources and lower the disruption to the quality of service provided to the user 
In  18 Gomaa et al propose a DSPL approach which enables the dynamic adaptation of software architectures but it is exclusively focused on serviceoriented architectures 
Finally the average adaptation time of our approach is considerably lower than the reported in  19
On the other hand the proposal of Benavides et al  38 always finds the optimal configuration using Constraint Satisfaction Problems with exponentialtime complexity making it unsuitable for runtime optimization
It inspired the great works of Buchi and Landweber on finite games of infinite duration 2 4 and of Rabin on finite automata over infinite structures 1314
In this article we show that distributed synthesis is undecidable even for the syntactic safety and reachability fragments of LTL 116 of ACTL 6 and of their semantic intersection
 While the process of neural growth of the GWR algorithm does not resemble biologically plausible mechanisms of neurogenesis eg Eriksson et al 1998 Gould 2007 Ming & Song 2011 it is an efficient learning model exhibiting a computationally convenient tradeoff between adaptation to dynamic input and learning convergence 
 Similarly Inui and Ashizawa 2011 proposed a radial basis function neural network to mentally rotate 3D objects These models use neural networks but these networks have not been designed to reproduce brain mechanisms suggested to underlie mental rotation in humans 
Although cognitive robotics models of mental simulation have been recently proposed Di Nuovo De La Cruz & Marocco 2013 these do not directly address mental rotation capabilities but rather mental simulation for motor planning tasks 
In this study we propose a new neurorobotic model of mental rotation that builds upon the prior model proposed in Seepanomwan et al 2013a 2013b and overcomes its limitations discussed above
The architecture we proposed and its functioning mechanisms represent a further step with respect to previous computational models eg Inui & Ashizawa 2011 Sasama et al 2009 as these focused on mental rotation mechanisms without relating them to the other supporting processes such as matching processes and decision making processes Lamm et al 2007
The architecture also represents an innovation with respect to previous neurorobotic models Seepanomwan et al 2013a 2013b that did not distinguish between the brain areas possibly performing visual and proprioceptive processes and also used abstract monitoring and decision making mechanisms
To overcome these drawbacks Herda et al 17 introduced a realtime method using an anatomical human model to predict the position of the markers It is unfortunately very difficult and time consuming to setup such a model
 Compared with 23 our model does not only take the lowrank structure property into account but also the temporal stability property of motion data and noise effect 
Our proposed model can handle both subproblems of mocap data refinement at the same time while in 23 they used two separate models to achieve the same goal 
 In Section 4 we also have observed that our method is not only faster than the work 23 but also outperforms it in the experiments on both synthetic and real data Meanwhile we also notice that two lowrank matrix based methods 1947 have been proposed almost at the same time as ours following the work 23 
However when some markers are missing for a long period of time Dynammo 30 also fails to correctly predict the missing values
Conventional robust preview control which has been studied in 59 is restricted to small ranges of variations
Whilst the socalled semiurban installations for example at Hartlepool and Heysham in the UK could reduce operational and transmission costs they pose a greater risk to the nearby population and therefore require detailed emergency planning
The dose conversion factors for the individual elements could vary significantly depending on their decay energy: the factor for Cs134 is around 9 times greater than that for Cs137 Yoo Jang Lee Noh & Cho 2013
These radiation levels are well below the known thresholds for the deterministic effects and tend to cause stochastic effects on human health Choi Costes & Abergel 2015 including cancers 
Unlike the translation algorithm described in Sirin et al 27 which translates only the preconditions of atomic processes into the preconditions of SHOP2 operators TranslateAtomicProcessQ translates both the preconditions and inputs of atomic processes into the preconditions of SHOP2 operators 
Unlike the proposed approach described above the approach by Sirin et al 27 does not combine operators to form a method This means that their system can generate workflows only if the user manually defines the composite processes and passes them to the system 
As the system proposed by Sirin et al 27 targets the creation of workflows for a single organisation only it has a single SHOP2 domain to begin with 
 Buckshaw et al themselves note that a validation of the proposed model requires large amounts of actual data and ground truth which both are not available
 However Jakobson 2011 uses selfdefined metrics for propagating impacts through Boolean gates which cannot provide context and biasfree understandable results or parametrization Moreover an explicit representation of  intraasset  dependencies is required ie all individual critical and noncritical resources must be identified 
However Musman et al 2011 fails to get across any mathematical approaches or formal definitions for impact assessment
For example Goodall et al 2009 focus on modeling and available data integration using ontologies but do not address an impact assessment
 Xiep et al and Liu et al are significantly limited by the lack of supporting cyclic dependencies and do not consider any mission impact relations Chung et al 2013 consider a probabilistic approach as well to determine the likelihoods of explicit attack paths 
However presented probability theory in Chung et al 2013 is not sound and voids fundamental principles of probabilistic inference in multiply connected graphs 
Other impact propagation approaches eg by Kheir et al 2009 or Jahnke et al 2007 claiming to handle details such as disagreeing information sources and cycles are not probabilistic based and degrade to a handcrafted propagation algorithm with arbitrary scores where parameters are only assessable by deeply trained experts and obtained results can only be used in a holistic way as they provide no directly interpretable meaning  
In addition our findings differ from the extant research eg Yamakami 2010 Holzer and Ondrus 2011 Schultz et al 2011 that grounded on network externalities Katz and Shapiro 1985 somewhat simplistically argues that a large base of developers leads to a large number of applications that in turn leads to an increasing number of endusers and vice versa 
This supports the findings by Hyrynsalmi et al 2013 who did not find differentiation between the consumers nor the application offerings of the ecosystems
For instance WordNet::Similarity 99 and WS4J 121 were designed before the emergence of the intrinsic IC models described in section 21 thus these libraries maintain inmemory tables with the concept frequency counts which are interrogated in order to compute the IC values required in a similarity evaluation step however their data structures does not provide any proper abstraction layer or software architecture to integrate new intrinsic IC models easily
Many works introducing similarity measures or IC models during the last decade have only implemented or evaluated classic ICbased similarity measures such as the Resnik 108 Lin 70 and JiangConrath 52 measures avoiding the replication of IC models and similarity measures introduced by other researchers 
The first known IC model is based on corpus statistics and was introduced by Resnik 108 and subsequently detailed in 109  
Finally despite one of the main motivations of WNetSS being to provide a software implementation for the most recent methods looking at tables 2 and 3 you can see that WNetSS 15 neither implements nor cites many recent similarity measures and IC models reported in the literature
However the synchronization process itself is not implemented using balanced excitation and inhibition among IF cells as in Hopfield and Brody 2001 but is rather described at the level of phase dynamics 
 We should also bear in mind however that our results were obtained by training the system on clean utterances whereas the results in Lee et al 2011 were obtained from a system trained on noisy utterances
This results in good pattern discrimination abilities Verstraeten et al 2005 though not as accurate as a recent approach based on OT features Gutig & Sompolinsky 2009 Further LSMs do not generate a gamma burst as an integral part of the recognition process so would not be so appropriate as a forward model for the sort of neuroimaging data addressed here
The notion that regions higher up in the auditory cortical hierarchy process information at longer time scales has recently been made use of in a model of auditory sequence recognition based on stable heteroclinic channels Kiebel Kriegstein Daunizeau & Friston 2009
While there is a lot of folk wisdom on how to design good algorithms for these highlythreaded machines in addition to a significant body of work on performance analysis  1620 there are no systematic theoretical models to analyze the performance of programs on these machines 
The most fundamental model that is used to analyze sequential algorithms is the Random Access Machine RAM model  21 which we teach undergraduates in their first algorithms class This model assumes that all operations including memory accesses take unit time  
In the parallel case although widely used the PRAM  30 model is unrealistic because it assumes all processors work synchronously and that interprocessor communication is free 
 Liu et al  19 describe a general performance model that predicts the performance of a biosequence database scanning application fairly precisely Their model incorporates the relationship between problem size and performance but only targets their biosequence application
It is helpful for applications with 2Dblock representations while choosing an appropriate block size by estimating cache misses but is not completely general
They do not however consider memory latency and multiple conflicting performance indicators
 They propose a simple yet efficient solution combining several wellknown parallel computation models: PRAM BSP QRQW but they do not model global memory coalescing
These models use neural networks but these networks have not been designed to reproduce brain mechanisms suggested to underlie mental rotation in humans 
It differs from the RAM model by defining that access to location x takes logx time but it does not consider the concept of block transfers which collects data into blocks to utilize spatial locality of reference in algorithms
No attempts have been made to develop an asymptotic theoretical model applicable to a wide range of highlythreaded machines
The modified definition permits smaller automata which benefits implementations but is more complicated than Definition 17 
In contrast to the earlier version of CeTA corresponding to 5 CeTA now supports matchbounds for nonleftlinear TRSs as well
Although the latter approach is more tedious it has the advantage for the user that we additionally integrated detailed error messages which are displayed if the certifier rejects a proof
The Lu system with the parameter set {acb}={36203} is also chaotic 61 and likewise it may not be chaotic for any other parameter
Note that in 23 there is no discussion of the following important questions for the consideration of the Lorenz system in the backward time or with non positive parameters: the existence of the extension of solutions the existence of attractors and the possibility of consideration of invariant sets in the backward time
However the small data and non negativity of dh dt assumptions in 17 are very far from conditions in the present situation 
Many researchers have addressed effort estimation and therefore consider productivity factors PFs Wang et al 2009 but they do not address the possibility of potentially inappropriate variables in the UCP algorithm itself which is important for software size estimation
Use cases are written in natural language consequently there is no rigorous approach for comparing use case quality or fragmentation 
Rosa et al 2014investigated whether a linear model based on both size and application type was better than a model based on size only however this study did not investigate the effects of each variable nor evaluate additional types of regression models
LpezMartín 2015 described linear regression models as less accurate than neural networks but they provided no description of the regression models studied 
These studies did not focus on evaluating of variables for use in regression models nor did they compare linear and polynomial regression models
Urbanek et al 2015b described the number of points in the use case scenario as the most significant factor but the scope of this paper is analytical programing therefore this finding is not applicable to MLR
However a transaction typically refers to a set of activities not a simple step in a structured scenario
However such transformations also change the method of prediction because the predicted value of the dependent variable does not represent the project size
A biomechanicalbased approach involves tissue analysis and bone and joint location which requires expensive devices and equipment 
However for motions that also involve the upper and lower body segments such as dancing and sword playing gait motion is not a proper option to yield good estimation
However Rosenhahn et al 2 reported that silhouette information is insufficient for estimating the model correctly as the extracted silhouette is hard to determine
However when an individual is in contact with other objects it is difficult to differentiate between the subject and the objects
Recent human motion classification works 18202254 applied the classification method to the available or captured motion data but the authors did not classify estimated matching motions
However polynomial fitting is only able to generate precise approximation in short intervals where the large interval will cause the approximation to oscillate widely 56
Foot skating often occurs when the recorded motion data are applied to different subjects whose position no longer fits the motion of the limbs which is not applicable to our study 
Firstly it is entirely possible that other articles have been published in OR MS journals that have analyticsrelated content however do not use this term in their abstract or title 
Secondly there is the potential that academics in the OR MS community would publish analyticsorientated research in journals not directly associated with OR MS eg Coghlan et al 2010 
However the shapebased methods 615 require considerably complex preprocessing steps and also would be useful only when the appropriate shapes of images are available
Before adopting CD some of our teams had been using an Agile method called Kanban Anderson 2010 however due to delivery problems we still had situations where a team had completed a feature but could not deliver it to production to obtain users feedback 
Unfortunately when they finally delivered the software to the users they found out that the feature was not what the users needed 
These challenges are not reported in the recent Systematic Literature Review SLR Laukkanen et al 2017 which systematically reviewed the existing literature to identity problems and solutions in adopting CD
My previous work Chen 2015a did not include any of the strategies reported in this paper
However obtaining content through selforganised P2P swarms has proved to be unreliable due to availability issues 52 because in selfish swarming protocols such as BitTorrent users leave the swarm after obtaining the item they need making it difficult to put together complete copies of content
However this makes peertopeer systems prone to unpredictable changes in content availability  a phenomenon called peer churn  when peers frequently and suddenly leave or join the system due to network failures or their own intent 103
The accesses of Spotify users 120 feature a vivid diurnal pattern: The lengths of user sessions are the longest during morning hours and are gradually decreasing by the end of the day The user sessions are also shorter during weekends than during working days and on mobile devices than on desktop computers 120
The simulation results suggest that this approach can reduce the startup delay by up to 2 sec in comparison to the previous results in  2242
Similarly the peerassisted content delivery system may benefit from redirecting streaming requests to CDNs in emergency cases when no sufficient upload bandwidth is available among the peers to meet playback deadlines 112
The peers inaccessibility problem when a peer inside a private network can initiate a connection with the peers of public networks but a reverse connection is often complicated by administrative policies 29 has been discussed in some of the earliest partially centralized PACDN articles  101102 
The simulation results in  102 suggest that the users which are connected outside a firewall have slower downloading rates since they are unable to establish connections with the nodes in private networks whereas peers behind firewalls exchange more therefore leading to an increase in downloading speed
Among those as the user trace collected 61 from LiveSky is revealed 37  are not available for peertopeer distribution and nearly 30  can provide uploading capacity sufficient to serve only one user at a time
Note that a perturbation of the input in order to avoid such a vertex event as suggested by Das et al 8 cannot be applied as the straight skeleton changes discontinuously 2
Our algorithm can compute the positively weighted straight skeleton of a monotone polygon in Onlog time and On space which constitutes a significant improvement over the On17 11+Ïµ worstcase time and space complexity of the currently best algorithm for arbitrary simple polygons by Eppstein and Erickson 2 
In Ron et al 1998 the order in which nodes within a stage are compared and possibly merged was unspecified 
The BICbased procedure is slightly slower and Beagle with the settings suggested in Browning and Browning 2007a  and  performs substantially worse than the alternatives particularly at smaller sample sizes
It is not a true distance since it is asymmetric in  and  and does not fulfil the triangle inequality
Modelling of a fixed haemodynamic response function in the MRI literature 1315 can be perceived as one attempt to address this issue yet to the best of our knowledge no pure datadriven approach based on machine learning techniques exists
As it can be seen a simple linear classifier ldc was able to achieve an average accuracy of 742  not only vastly outperforming other tested classifiers but also outperforming the Random Forest ensemble from 17 at the fraction of computations yet still leaving room for improvement 
We have tried several clustering and visualization tools to cluster the isinthesliceof graph for comparison but most of the tools such as Gephi Bastian et al 2009 failed due to the large dataset 
The underlying problem is that the isinthesliceof graph is dense and no traditional clustering can handle such dense graphs
Clearly many researchers are highly experienced so their reflections should not be dismissed out of hand Nevertheless unless they think broadly and from different perspectives about the criteria they use to evaluate their participative interventions they may miss evidence that does not fit their current thinking about what is important Romm 1996 Midgley 2011
However we cannot take this study as strong evidence because they did not specifically identify systemic PSMs as a category for comparison with other participative approaches
However while it is useful to identify ‘common denominators and assess methods against these this does not help in evaluating the unique attributes of methods that might make them complementary rather than competing
Unfortunately deflation causes an undesirable increase in the size of the polynomial system and can be rather costly particularly for solutions of high multiplicity 
This comparison showed that the more complex multivariate HB estimators did not perform better than their univariate HB estimator 
However once the methods are implemented and available for example in R or SAS code we do not find great usability differences  
This default interpretation does not explain why beta bursts are interrupted by encoding and decoding or why the gamma oscillations embody encoding decoding events 
However such extinction can be eliminated if the two events get grouped as a single object even if the link between the two stimuli is amodally completed behind an occluder Mattingley Davis & Driver 1997 
As a result auditory spatial attention in humans does not appear to have a map structure Kong et al 2012 but rather seems to be embodied by an opponent process whose neurons are tuned to eg ITDs Magezi & Krumbholz 2010 
At the time that this circuit was published Cohen Grossberg & Stork 1988 it was not yet understood how speaker normalization might occur so these acoustic features were simply said to be invariant—that is speakernormalized—at the models Invariant Feature Detectors level
These theoretical results suggest that contrary to Clark and Squire 1998 episodic memory may not be necessary to consciously experience emotions
Instead Dennett advocated a Multiple Drafts model where discriminations are distributed in space and time across the brain a concept that without further elaboration is too vague to have explanatory power
But this theory provided no mechanistic account could therefore provide no data simulations and did not situate this heuristic derivation within a larger theory of how brain resonances and consciousness may be linked
The authors however remain silent on specific details about the actual algorithm to find an optimal team
General research on the formation of groups in large scale social networks 17 helps to understand the involved dynamic aspects but does not provide the algorithms for identifying optimal team configurations
These algorithms and frameworks provide additional means to determine personcentric metrics but do not address the team composition problem per se 
Especially social networks that lack a richclub structure see 18 are prone to produce compositions of nonconnected experts
However investigations of the richclub phenomenon in scientific collaboration networks eg 19 have shown that such tight collaborative groups exist only within particular research domains but not beyond 
Due to data availability some empiricists use alternatives such as production quantity sales and shipments which are easier to observe than orders and demand Blinder & Maccini 1991 
However incorporation of price and seasonal fluctuation does not always generate results in support of production smoothing Miron & Zeldes 1988 
Using US industrylevel data Cachon et al 2007 also found that bullwhip primarily appears in the wholesaler rather than in the retailer or manufacturer echelon Dooley Yan Mohan and Gopalakrishnan 2010 studied the bullwhip effect during the 20072009 recession and concluded that retailers responded to market changes rapidly and adaptively whereas wholesalers responded late and drastically
This underweighting does not improve when: the supply line is made visible Wu & Catok 2006 demand is known and stationary Croson & Donohue 2006 or even when demand is known and constant Croson et al 2014
In this regard both Stermans 1989 and Lee et al 1997 explanations are inadequate since the cost assumptions in both approaches inherently induce amplification 
Sodhi and Tang 2011 considered an arborescent supply chain and calls for a need to remove structural complexity in order to reduce bullwhip Chatfield 2013challenged the opinion that multiechelon system can be approximated by cascading twoechelon systems aka the decomposition assumption They found that such an assumption leads to underestimated bullwhip measures
Muhanmmad Irfan Ali discussed another view on reduction of parameters in soft sets 41
On the contrary their holistic assessment of the negotiation outcome is significantly lower compared to the negotiators who have no access to utility values of their opponent
 The use of rectangles and irregular curves could impose cognitive difficulties in perceiving the sets as they might give the impression of different semantics 45
 By contrast for our study the nodes belonged to multiple groups and there were richer relationships between the groups such as subset and intersection
The tasks used in 328 did not include any network or groupnetwork tasks T
They argue that their results show that perception is a topdown process in contrast to the Ecological bottomup process where the readers recognize the genres through the attributes of the layout which forms the basis of document recognition or perception for recognition and although Toms and Campbell like Lakoff 1987 refer to the bottomup process and suggest that genres may act as a single gestalt Toms & Campbell 1999a p 2015 they do not explore other possibilities such as perception for action and how a genre is perceived when the document is displayed to a reader in all fairness Watt 2009 chap 8 also fails to explore the perception for recognition concept
The scanpath mirrors clearly the unfolding of visual attention over time and indicates which features or contents in a visual context are attended Coco 2009 The movement represented by these scanpaths are not random rather they reflect the viewers frame of mind expectations and purpose Yarbus 1967 
These two papers reported on the detection of skimming and reading techniques not skimming and scanning techniques 
Ulf Grenander was working on abstract mathematical and statistical models and methods for many computer vision problems and presented his findings in books that were not easily understood by mainstream computer vision researchers
These papers however concentrate on specific subsystems of Large Scale Distributed Systems such as 11 on the performance of memory systems or only deal with one or two specific SLA parameters 
Petrucci et al 12 or Bichler et al 13 investigate one general resource constraint and Khanna et al 14 only focuses on response time and throughput
Contrary to our approach though it plans reactions just after violations have occurred
Also the VCONF model by Rao et al 16 has similar goals as presented in Section 1 but depends on specific parameters can only execute one action per iteration and it neglects the energy consumption of executed actions 
Again no details on how to achieve this have been given
Thirdly commercial Cloud IaaS platforms such as Amazon EC2 29 Rackspace 30 or RightScale 31 have a very limited choice of preconfigured and static VM resource provisioning types
MonitoringCurrent monitoring systems eg ganglia 35 facilitate monitoring only of lowlevel systems resources such as free_disk or packets_sent but SLA parameters typically are eg storage and outgoing bandwidth
As opposed to the CBR approach in 7 the rulebased approach is able to fire more than one action at the same iteration which inherently increases the flexibility of the system
These strategies did not focus on both large and imbalanced data learning
However the above mentioned methods are not helpful for classification of large data sets with imbalanced classes
However these methods could be useless for large data set because they use all the data for training the classifier 
Also this method is not tailored for very huge data sets because the SVM problem should be hard to solve due to the training set size 
Although a similar technique was successfully used by Qamar and Sanghi 19 the approach does not appear to have been widely adopted as an alternative to projection techniques 
POD coefficient interpolation type techniques as introduced by Ly and Tran 11 have not been widely adopted as an alternative to projection type techniques 
Rippa 23 introduced an algorithm to find the optimum value for a particular data set but for all the cases considered here the additional expense incurred in performing an analysis of this type did not prove to be justified
In contrast to Sivaraman et al 2016 Mittal et al 2016 our research findings show that a chain of NFs requires a global scheduler to make chainlevel decisions rather than an internal scheduler that executes local switch policies 
Merialdo 1994 and Elworthy 1994 have insisted based on their experimental results that the maximum likelihood training using an untagged corpus does not necessarily improve tagging accuracy H
This algorithm cannot take advantage of the scaling procedure because it requires the synchronous calculation of all possible sequences in the morpheme network 
However he did not apply this algorithm to the estimation of HMM parameters  
In a previous paper Schfitze 1993 we trained a neural network to disambiguate partofspeech using context however no information about the word that is to be categorized was used 
A remedy is to aggressively limit the feature space eg to syntactic labels or a small fraction of the bilingual features available as in Chiang et al 2008 Chiang et al 2009 but that reduces the benefit of lexical features 
 This method avoids the overfitting problem at the expense of losing the benefit of discriminative training of rich features directly for MT 
First such methods are not suitable for real MT tasks especially for applications with streamed input since the model has to be retrained with each new input sentence or document and training is slow  
A crucial difference of our approach is how the length preference is modeled We approximate the length distribution of nonterminals with a smoothed Gaussian which is more robust and gives rise to much larger improvement consistently 
We refer to He and Gildea 2006 who tested active learning and cotraining methods but found little or no gain from semisupervised learning and to Swier and Stevenson 2004 who achieved good results using semisupervised methods but tested their methods on a small number of VerbNet roles which have not been used by other SRL Systems
Discriminative models have been found to outperform generative models for many different tasks including SRL Lim et al 2004 For this reason we also employ discriminative models here 
Compared to previous approaches Raghavan and Allan 2007 our method can be used for both classification and structured tasks and the feature query selection methods we propose perform better 
Raghavan and Allan 2007 also propose several methods for learning with labeled features but in a previous comparison GE gave better results Druck et al 2008 
Unlike the experiments presented in this paper Liang et al 2009 conduct only simulated active learning experiments and do not consider skipping queries 
It is also not clear how this graphbased training method would generalize to structured output spaces 
The conclusions are broadly in agreement with those of Merialdo 1994 but give greater detail about the contributions of different parts of the Model
Others perform the division implicitly without discussing performance eg Cutting et al 1992 
There is also a less detailed description of Pahner and Hearsts system SATZ in Pahuer and Hearst 1994 
Liberman and Church suggest in Liberlnan and Church 1992 that a system could be quickly built to divide newswire text into sentences with a nearly negligible error rate but do not actually build such a system 
However their parser was not incremental it used global features such as the number of turn changes Also it focused strictly in interpretation of input utterances it could not predict actions by either dialog partner
Their parser however was not probabilistic or targeted at dialog processing 
The chunkbased model has limitations For example dominance relations among subtasks are important for dialog processes such as anaphora resolution Grosz and Sidner 1986 
We speculate that this contrasts with the disappointing findings of Kehler et al 2004 since SRL provides a more fine grained level of information when compared to predicate argument statistics 
Though such metrics are useful as sanity checks in iterative system development they are less useful as analytial tools
Unfortunately their analysis is incomplete they do not perform the analysis in both directions
Moreover it is not easy for humans to selec tthe best translation among a set of alternatives let alone assign them probabilities Last but not least the beneficial effect on translation is not guaranteed
However this might be inappropriate in MDS where the use of multiple documents increases the number of possible entities with which an anaphor could be referenced 
They rely on a more or less fixed discourse structure to accommodate the generation process In our approach the discourse structure is not fixed but predicted for each particular abstract 
In cutandpaste summarization Jing and mcKeown 2000 sentence combination operations were implemented manually following the study of a set of professionally written abstracts however the particular pasting operation presented here was not implemented 
Previous studies on texttotext abstracting Banko et al 2000 Knight and Marcu 2000 have studied problems such as sentence compression and sentence combination but not the pasting procedure presented here 
Recently a number of machine learning approaches have been proposed Zettlemoyer and Collins 2005 Mooney 2007 However they are supervised and providing the target logical form for each sentence is costly and difficult to do consistently and with high quality 
The drawback is that the complexity in syntactic processing is coupled with semantic parsing and makes the latter even harder For ex ample when applying their approach to a different domain with somewhat less rigid syntax Zettlemoyer and Collins 2007 need to introduce new combinators and new forms of candidate lexical Entries 
Even an efficient sampler like MCSAT Poon and Domingos 2006 as used in Poon & Domingos 2008 would have a hard time generating accurate estimates within a reasonable amount of time 
But in fact the issue of editing in text summarization has usually been neglected notable exceptions being the works by Jing andMcKeown 2000 and Mani Gates and Bloedorn 1999 In our work we partially address this issue by enumerating some transformations frequently found in our corpusthat are computationally implementable
 Sentence reduction is concerned only with theremoval of sentence components so it cannot explain transformations observed in ourcorpus and in summarization in general such as the reexpression of domain conceptsand verbs
Jing and McKeown 2000 have proposed a rulebasedalgorithm for sentence combination but no results have been reported
Berger et al 1996 proposed an iterative procedure of adding news features to feature set driven by data We present a simple and effective approach using some statistical heuristics for feature selection 
 Although this provides an unlimited freedom for rearranging constituents it also complicates the task of learning the parsing steps which might explain why their evaluation results show marginal improvements at best 
However they did not report any evaluation of their word extraction method
In other words they count an error only when the system segmentation is not acceptable to human judgement while we count an error whenever the system segmentation does not exactly match the corpus segmentation even if it is inconsistent
One potential benefit of our statistical model and segmentation algorithm is that they are completely independent of the target language and its writing system 
However one of the major limitations of these advances is the structured syntactic knowledge which is important to global reordering Li et al 2007 Elming 2008 has not been well exploited
Assuming that the number of feature templates in a given set is n the algorithm of Ding and Chang 2008 requires On 2  times of training test routines it cannot handle a set that consists of hundreds of templates 
As two examples Rabiner 1989 and Charniak et al 1993 give good overviews of the techniques and equations used for Markov models and partofspeech tagging but they are not very explicit in the details that are needed for their application 
This comparison needs to be reexamined since we use a tenfold crossvalidation and averaging of result while Ratnaparkhi only makes one test run 
For example the Markov model tagger used in the comparison of van Halteren et al 1998 yielded worse results than all other taggers 
However the difficulty of such tasks and the fact that they are apparently unrelated has led to the development of largely adhoc solutions tuned to specific challenges 
While the literature suggests that BaumWelch training can degrade performance on the tagging task Elworthy 1994 Merialdo 1994 we have found in early experiments that agreement between a tagger trained in this way and the tagger from the XTag Project consistently increases with each iteration of BaumWelch eventually reaching a plateau but not Decreasing 
Clark 2000 presents a framework which in principle should accommodate lexical ambiguity using mixtures but includes no evidence that it does so 
A major problem with such methods is that each hypothesis is aligned to the backbone independently leading to sub optimal behavior
While P R W is the first attempt to formalize well known relevance weighting Sparck Jones 1972 Salton and McGill 1983 by probability theory there are several drawbacks in PRW 
A similar argument applies to all other problems in Robertson and Sparck Jones 1976 that are caused by having insufficient training cases  
This massive interest in speed is bringing rapid progress to the field but it comes with a certain amount of baggage
Unlike many other methods that directly utilize noun phrase NP coreference Nenkova 2008 Mani et al 1999 we propose a method that employs insertion and substitution of phrases that modify the same chunk in the lead and other sentences
As is well known the extractive summary that has been extensively studied from the early days of summarization history Luhn 1958 suffers from various drawbacks 
While Blitzer et al 2006 found it necessary to normalize and scale the projection features we did not observe any improvement by normalizing them actually it slightly degraded performance in our case Thus we found this step unnecessary and currently did not look at this issue any further  
As this encoding strategy is not wellsuited to a free word order language like German we have focussed on a less surfaceoriented level of description most closely related to the LFG fstructure and representationsused in dependency grammar
Unfortunately in the data set made available in the public domain there is no indication of which sentences are used as test sentences 
There is now a large body of past work on WSD Early work on WSD such as Kelly and Stone 1975 Hirst 1987 used handcoding of knowledge to perform WSD The knowledge acquisition process is laborious
However their results show no improvement in fact a slight degradation in performance when using surrounding words to perform WSD as compared to the most frequent heuristic 
The work of McRoy 1992 pointed out that a diverse set of knowledge sources are important to achieve WSD but no quantitative evaluation was given on the relative importance of each knowledge source
Since we are using a larger corpus than Pad et al 2007 who train on the BNC a fairer comparison might be the one with our alternative models that are all outperformed by DM by a large margin 
